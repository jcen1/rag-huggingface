{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jianiancen/rag-huggingface?scriptVersionId=183394074\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"fd0f89b0","metadata":{"papermill":{"duration":0.008052,"end_time":"2024-06-14T02:58:43.22385","exception":false,"start_time":"2024-06-14T02:58:43.215798","status":"completed"},"tags":[]},"source":["### This is the overview of logic flow of the code\n","\n","### Pre-production Steps\n","\n","#### Knowledge Base Preparation:\n","- **Chunking**: Split large documents into smaller parts for easier handling.\n","- **Embedding**: Convert these chunks into numerical representations (embeddings) using an embedding model.\n","\n","#### Vector Database:\n","- Store the embeddings in a vector database for fast retrieval.\n","\n","### In-production Steps (Query Handling)\n","\n","#### Retriever:\n","- **User Query Embedding**: When a user asks a question, the system converts (embeds) the question into a numerical format.\n","- **Document Retrieval**: It finds the most similar documents in the vector database by comparing the embeddings.\n","- **Top K Similar Documents**: Selects the top K documents that are most relevant to the query.\n","- **Optional Rerankings**: Selects the top K documents that are most relevant to the query.\n","\n","#### Reader:\n","- **Context Creation**: Combines the user query with the retrieved documents to create a context.\n","- **LLM Prompt**: Builds a prompt using this context and the user query.\n","- **Answer Generation**: Uses a language model (LLM) to generate an answer based on the prompt.\n"]},{"cell_type":"code","execution_count":1,"id":"fd39f1e9","metadata":{"execution":{"iopub.execute_input":"2024-06-14T02:58:43.24054Z","iopub.status.busy":"2024-06-14T02:58:43.239994Z","iopub.status.idle":"2024-06-14T02:59:03.300689Z","shell.execute_reply":"2024-06-14T02:59:03.299566Z"},"papermill":{"duration":20.071162,"end_time":"2024-06-14T02:59:03.302687","exception":false,"start_time":"2024-06-14T02:58:43.231525","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting langchain-community\r\n","  Downloading langchain_community-0.2.4-py3-none-any.whl.metadata (2.4 kB)\r\n","Collecting langchain_huggingface\r\n","  Downloading langchain_huggingface-0.0.3-py3-none-any.whl.metadata (1.2 kB)\r\n","Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (6.0.1)\r\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.0.25)\r\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (3.9.1)\r\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.6)\r\n","Collecting langchain<0.3.0,>=0.2.0 (from langchain-community)\r\n","  Downloading langchain-0.2.4-py3-none-any.whl.metadata (7.0 kB)\r\n","Collecting langchain-core<0.3.0,>=0.2.0 (from langchain-community)\r\n","  Downloading langchain_core-0.2.6-py3-none-any.whl.metadata (5.8 kB)\r\n","Collecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\r\n","  Downloading langsmith-0.1.77-py3-none-any.whl.metadata (13 kB)\r\n","Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (1.26.4)\r\n","Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.32.3)\r\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (8.2.3)\r\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (0.23.2)\r\n","Collecting sentence-transformers>=2.6.0 (from langchain_huggingface)\r\n","  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\r\n","Requirement already satisfied: tokenizers>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (0.19.1)\r\n","Requirement already satisfied: transformers>=4.39.0 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (4.41.2)\r\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\r\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\r\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\r\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\r\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\r\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\r\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\r\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\r\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.13.1)\r\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.3.1)\r\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (21.3)\r\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.4)\r\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.9.0)\r\n","Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.0->langchain-community)\r\n","  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl.metadata (2.2 kB)\r\n","Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.5.3)\r\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\r\n","Collecting packaging>=20.9 (from huggingface-hub>=0.23.0->langchain_huggingface)\r\n","  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\r\n","Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community)\r\n","  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m978.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.6)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (1.26.18)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\r\n","Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.1.2)\r\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.2.2)\r\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.11.4)\r\n","Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (9.5.0)\r\n","Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\r\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain_huggingface) (2023.12.25)\r\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.3)\r\n","Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\r\n","Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.6.0)\r\n","Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.14.6)\r\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.12.1)\r\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.2.1)\r\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.2)\r\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\r\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\r\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.2.0)\r\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.3)\r\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\r\n","Downloading langchain_community-0.2.4-py3-none-any.whl (2.2 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\r\n","Downloading langchain-0.2.4-py3-none-any.whl (974 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.2/974.2 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading langchain_core-0.2.6-py3-none-any.whl (315 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading langsmith-0.1.77-py3-none-any.whl (125 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\r\n","Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hInstalling collected packages: packaging, orjson, langsmith, langchain-core, sentence-transformers, langchain-text-splitters, langchain_huggingface, langchain, langchain-community\r\n","  Attempting uninstall: packaging\r\n","    Found existing installation: packaging 21.3\r\n","    Uninstalling packaging-21.3:\r\n","      Successfully uninstalled packaging-21.3\r\n","  Attempting uninstall: orjson\r\n","    Found existing installation: orjson 3.9.10\r\n","    Uninstalling orjson-3.9.10:\r\n","      Successfully uninstalled orjson-3.9.10\r\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","cudf 24.4.1 requires cubinlinker, which is not installed.\r\n","cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n","cudf 24.4.1 requires ptxcompiler, which is not installed.\r\n","cuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n","dask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n","keras-cv 0.9.0 requires keras-core, which is not installed.\r\n","keras-nlp 0.12.1 requires keras-core, which is not installed.\r\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\r\n","apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\r\n","cudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\r\n","distributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\r\n","google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\r\n","jupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n","jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n","libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n","momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n","osmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\r\n","rapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\r\n","rapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\r\n","spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\r\n","ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0mSuccessfully installed langchain-0.2.4 langchain-community-0.2.4 langchain-core-0.2.6 langchain-text-splitters-0.2.1 langchain_huggingface-0.0.3 langsmith-0.1.77 orjson-3.10.5 packaging-24.1 sentence-transformers-3.0.1\r\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install -U langchain-community langchain_huggingface"]},{"cell_type":"code","execution_count":2,"id":"cf6148da","metadata":{"execution":{"iopub.execute_input":"2024-06-14T02:59:03.327788Z","iopub.status.busy":"2024-06-14T02:59:03.327057Z","iopub.status.idle":"2024-06-14T02:59:18.900593Z","shell.execute_reply":"2024-06-14T02:59:18.899414Z"},"papermill":{"duration":15.588315,"end_time":"2024-06-14T02:59:18.902842","exception":false,"start_time":"2024-06-14T02:59:03.314527","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting faiss-gpu\r\n","  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\r\n","Collecting pacmap\r\n","  Downloading pacmap-0.7.2-py3-none-any.whl.metadata (12 kB)\r\n","Requirement already satisfied: scikit-learn>=0.20 in /opt/conda/lib/python3.10/site-packages (from pacmap) (1.2.2)\r\n","Requirement already satisfied: numba>=0.57 in /opt/conda/lib/python3.10/site-packages (from pacmap) (0.58.1)\r\n","Requirement already satisfied: annoy>=1.11 in /opt/conda/lib/python3.10/site-packages (from pacmap) (1.17.3)\r\n","Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from pacmap) (1.26.4)\r\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.57->pacmap) (0.41.1)\r\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20->pacmap) (1.11.4)\r\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20->pacmap) (1.4.2)\r\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20->pacmap) (3.2.0)\r\n","Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading pacmap-0.7.2-py3-none-any.whl (18 kB)\r\n","Installing collected packages: faiss-gpu, pacmap\r\n","Successfully installed faiss-gpu-1.7.2 pacmap-0.7.2\r\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install faiss-gpu pacmap"]},{"cell_type":"code","execution_count":3,"id":"691a2aae","metadata":{"execution":{"iopub.execute_input":"2024-06-14T02:59:18.932867Z","iopub.status.busy":"2024-06-14T02:59:18.932532Z","iopub.status.idle":"2024-06-14T02:59:31.106515Z","shell.execute_reply":"2024-06-14T02:59:31.105356Z"},"papermill":{"duration":12.19139,"end_time":"2024-06-14T02:59:31.108663","exception":false,"start_time":"2024-06-14T02:59:18.917273","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\r\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\r\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (24.1)\r\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\r\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\r\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\r\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\r\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\r\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\r\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\r\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\r\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\r\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\r\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\r\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\r\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\r\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\r\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install accelerate "]},{"cell_type":"code","execution_count":4,"id":"5a3cc1d1","metadata":{"execution":{"iopub.execute_input":"2024-06-14T02:59:31.140445Z","iopub.status.busy":"2024-06-14T02:59:31.140123Z","iopub.status.idle":"2024-06-14T02:59:48.535623Z","shell.execute_reply":"2024-06-14T02:59:48.534454Z"},"papermill":{"duration":17.414319,"end_time":"2024-06-14T02:59:48.537763","exception":false,"start_time":"2024-06-14T02:59:31.123444","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple/\r\n","Collecting bitsandbytes\r\n","  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\r\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\r\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\r\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\r\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\r\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12.1)\r\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\r\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\r\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.3.1)\r\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\r\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\r\n","Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hInstalling collected packages: bitsandbytes\r\n","Successfully installed bitsandbytes-0.43.1\r\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install -i https://pypi.org/simple/ bitsandbytes"]},{"cell_type":"code","execution_count":5,"id":"f394475f","metadata":{"execution":{"iopub.execute_input":"2024-06-14T02:59:48.577642Z","iopub.status.busy":"2024-06-14T02:59:48.577316Z","iopub.status.idle":"2024-06-14T03:00:26.193186Z","shell.execute_reply":"2024-06-14T03:00:26.191868Z"},"papermill":{"duration":37.638117,"end_time":"2024-06-14T03:00:26.1953","exception":false,"start_time":"2024-06-14T02:59:48.557183","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ragatouille\r\n","  Downloading ragatouille-0.0.8.post2-py3-none-any.whl.metadata (15 kB)\r\n","Collecting colbert-ai==0.2.19 (from ragatouille)\r\n","  Downloading colbert-ai-0.2.19.tar.gz (86 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n","\u001b[?25hCollecting faiss-cpu<2.0.0,>=1.7.4 (from ragatouille)\r\n","  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\r\n","Collecting fast-pytorch-kmeans==0.2.0.1 (from ragatouille)\r\n","  Downloading fast_pytorch_kmeans-0.2.0.1-py3-none-any.whl.metadata (1.1 kB)\r\n","Collecting langchain<0.2.0,>=0.1.0 (from ragatouille)\r\n","  Downloading langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\r\n","Collecting langchain_core<0.2.0,>=0.1.4 (from ragatouille)\r\n","  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\r\n","Collecting llama-index>=0.7 (from ragatouille)\r\n","  Downloading llama_index-0.10.44-py3-none-any.whl.metadata (11 kB)\r\n","Requirement already satisfied: onnx<2.0.0,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from ragatouille) (1.16.1)\r\n","Collecting sentence-transformers<3.0.0,>=2.2.2 (from ragatouille)\r\n","  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\r\n","Requirement already satisfied: srsly==2.4.8 in /opt/conda/lib/python3.10/site-packages (from ragatouille) (2.4.8)\r\n","Requirement already satisfied: torch>=1.13 in /opt/conda/lib/python3.10/site-packages (from ragatouille) (2.1.2)\r\n","Requirement already satisfied: transformers<5.0.0,>=4.36.2 in /opt/conda/lib/python3.10/site-packages (from ragatouille) (4.41.2)\r\n","Collecting voyager<3.0.0,>=2.0.2 (from ragatouille)\r\n","  Downloading voyager-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\r\n","Collecting bitarray (from colbert-ai==0.2.19->ragatouille)\r\n","  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\r\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (2.19.2)\r\n","Requirement already satisfied: flask in /opt/conda/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (3.0.3)\r\n","Collecting git-python (from colbert-ai==0.2.19->ragatouille)\r\n","  Downloading git_python-1.0.3-py2.py3-none-any.whl.metadata (331 bytes)\r\n","Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (1.0.0)\r\n","Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (1.11.1.1)\r\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (1.11.4)\r\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (4.66.4)\r\n","Requirement already satisfied: ujson in /opt/conda/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (5.10.0)\r\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fast-pytorch-kmeans==0.2.0.1->ragatouille) (1.26.4)\r\n","Requirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (from fast-pytorch-kmeans==0.2.0.1->ragatouille) (11.4.1)\r\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /opt/conda/lib/python3.10/site-packages (from srsly==2.4.8->ragatouille) (2.0.10)\r\n","Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (6.0.1)\r\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.0.25)\r\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (3.9.1)\r\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (4.0.3)\r\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.6.6)\r\n","Collecting langchain-community<0.1,>=0.0.38 (from langchain<0.2.0,>=0.1.0->ragatouille)\r\n","  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\r\n","Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.0->ragatouille)\r\n","  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\r\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.1.77)\r\n","Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.5.3)\r\n","Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.32.3)\r\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (8.2.3)\r\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain_core<0.2.0,>=0.1.4->ragatouille) (1.33)\r\n","Collecting packaging<24.0,>=23.2 (from langchain_core<0.2.0,>=0.1.4->ragatouille)\r\n","  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\r\n","Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index>=0.7->ragatouille)\r\n","  Downloading llama_index_agent_openai-0.2.7-py3-none-any.whl.metadata (678 bytes)\r\n","Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index>=0.7->ragatouille)\r\n","  Downloading llama_index_cli-0.1.12-py3-none-any.whl.metadata (1.5 kB)\r\n","Collecting llama-index-core==0.10.44 (from llama-index>=0.7->ragatouille)\r\n","  Downloading llama_index_core-0.10.44-py3-none-any.whl.metadata (2.4 kB)\r\n","Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index>=0.7->ragatouille)\r\n","  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl.metadata (604 bytes)\r\n","Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index>=0.7->ragatouille)\r\n","  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl.metadata (3.8 kB)\r\n","Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index>=0.7->ragatouille)\r\n","  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\r\n","Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index>=0.7->ragatouille)\r\n","  Downloading llama_index_llms_openai-0.1.22-py3-none-any.whl.metadata (559 bytes)\r\n","Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index>=0.7->ragatouille)\r\n","  Downloading llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl.metadata (677 bytes)\r\n","Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index>=0.7->ragatouille)\r\n","  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\r\n","Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index>=0.7->ragatouille)\r\n","  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\r\n","Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index>=0.7->ragatouille)\r\n","  Downloading llama_index_readers_file-0.1.25-py3-none-any.whl.metadata (5.4 kB)\r\n","Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index>=0.7->ragatouille)\r\n","  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl.metadata (3.5 kB)\r\n","Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (1.2.14)\r\n","Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille)\r\n","  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\r\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (2024.3.1)\r\n","Requirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (0.27.0)\r\n","Collecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille)\r\n","  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl.metadata (760 bytes)\r\n","Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (1.5.8)\r\n","Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (3.2.1)\r\n","Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille)\r\n","  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\r\n","Collecting openai>=1.1.0 (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille)\r\n","  Downloading openai-1.34.0-py3-none-any.whl.metadata (21 kB)\r\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (2.2.1)\r\n","Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (9.5.0)\r\n","Collecting tiktoken>=0.3.3 (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille)\r\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n","Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (4.9.0)\r\n","Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (0.9.0)\r\n","Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (1.14.1)\r\n","Requirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from onnx<2.0.0,>=1.15.0->ragatouille) (3.20.3)\r\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.2.2)\r\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (0.23.2)\r\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->ragatouille) (3.13.1)\r\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->ragatouille) (1.12.1)\r\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->ragatouille) (3.1.2)\r\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (2023.12.25)\r\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.19.1)\r\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.4.3)\r\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (23.2.0)\r\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (6.0.4)\r\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.9.3)\r\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.4.1)\r\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.3.1)\r\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->ragatouille) (3.21.2)\r\n","Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain_core<0.2.0,>=0.1.4->ragatouille) (2.4)\r\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.0->ragatouille) (3.10.5)\r\n","Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille)\r\n","  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\r\n","Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (4.2.0)\r\n","Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille)\r\n","  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\r\n","Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index>=0.7->ragatouille)\r\n","  Downloading llama_parse-0.4.4-py3-none-any.whl.metadata (3.5 kB)\r\n","Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (0.6.0)\r\n","Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (2.14.6)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (3.3.2)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (3.6)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (1.26.18)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (2024.2.2)\r\n","Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.0->ragatouille) (3.0.3)\r\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (14.0.2)\r\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.6)\r\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.3.8)\r\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (3.4.1)\r\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.70.16)\r\n","Requirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (3.0.3)\r\n","Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (2.2.0)\r\n","Requirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (8.1.7)\r\n","Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (1.8.2)\r\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13->ragatouille) (2.1.3)\r\n","Requirement already satisfied: gitpython in /opt/conda/lib/python3.10/site-packages (from git-python->colbert-ai==0.2.19->ragatouille) (3.1.41)\r\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.4.2)\r\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (3.2.0)\r\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13->ragatouille) (1.3.0)\r\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (2.5)\r\n","Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (4.2.0)\r\n","Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (1.0.5)\r\n","Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (1.3.0)\r\n","Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (0.14.0)\r\n","Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (1.9.0)\r\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (1.0.0)\r\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython->git-python->colbert-ai==0.2.19->ragatouille) (4.0.11)\r\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (2.9.0.post0)\r\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (2023.3.post1)\r\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (2023.4)\r\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (1.2.0)\r\n","Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19->ragatouille) (5.0.1)\r\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.44->llama-index>=0.7->ragatouille) (1.16.0)\r\n","Downloading ragatouille-0.0.8.post2-py3-none-any.whl (41 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading fast_pytorch_kmeans-0.2.0.1-py3-none-any.whl (8.8 kB)\r\n","Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading llama_index-0.10.44-py3-none-any.whl (6.8 kB)\r\n","Downloading llama_index_core-0.10.44-py3-none-any.whl (15.4 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading voyager-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\r\n","Downloading llama_index_agent_openai-0.2.7-py3-none-any.whl (12 kB)\r\n","Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\r\n","Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\r\n","Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\r\n","Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading llama_index_llms_openai-0.1.22-py3-none-any.whl (11 kB)\r\n","Downloading llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl (5.8 kB)\r\n","Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\r\n","Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\r\n","Downloading llama_index_readers_file-0.1.25-py3-none-any.whl (37 kB)\r\n","Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\r\n","Downloading packaging-23.2-py3-none-any.whl (53 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\r\n","Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\r\n","Downloading llama_parse-0.4.4-py3-none-any.whl (8.0 kB)\r\n","Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading openai-1.34.0-py3-none-any.whl (325 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\r\n","Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hBuilding wheels for collected packages: colbert-ai\r\n","  Building wheel for colbert-ai (setup.py) ... \u001b[?25l-\b \bdone\r\n","\u001b[?25h  Created wheel for colbert-ai: filename=colbert_ai-0.2.19-py3-none-any.whl size=114761 sha256=27b7948f27196a7e18266bd56394504dd7e90a054f16412fc120d7fde96cf964\r\n","  Stored in directory: /root/.cache/pip/wheels/90/b9/63/d4fc276c73c42ef7fc1065a26cf87e5a1cf56ef6498cbfbe5d\r\n","Successfully built colbert-ai\r\n","Installing collected packages: striprtf, dirtyjson, bitarray, voyager, packaging, nltk, faiss-cpu, beautifulsoup4, tiktoken, openai, llamaindex-py-client, git-python, fast-pytorch-kmeans, llama-index-legacy, llama-index-core, langchain_core, sentence-transformers, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, langchain-text-splitters, langchain-community, colbert-ai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, langchain, llama-index-program-openai, llama-index-question-gen-openai, llama-index, ragatouille\r\n","  Attempting uninstall: packaging\r\n","    Found existing installation: packaging 24.1\r\n","    Uninstalling packaging-24.1:\r\n","      Successfully uninstalled packaging-24.1\r\n","  Attempting uninstall: nltk\r\n","    Found existing installation: nltk 3.2.4\r\n","    Uninstalling nltk-3.2.4:\r\n","      Successfully uninstalled nltk-3.2.4\r\n","  Attempting uninstall: beautifulsoup4\r\n","    Found existing installation: beautifulsoup4 4.12.2\r\n","    Uninstalling beautifulsoup4-4.12.2:\r\n","      Successfully uninstalled beautifulsoup4-4.12.2\r\n","  Attempting uninstall: langchain_core\r\n","    Found existing installation: langchain-core 0.2.6\r\n","    Uninstalling langchain-core-0.2.6:\r\n","      Successfully uninstalled langchain-core-0.2.6\r\n","  Attempting uninstall: sentence-transformers\r\n","    Found existing installation: sentence-transformers 3.0.1\r\n","    Uninstalling sentence-transformers-3.0.1:\r\n","      Successfully uninstalled sentence-transformers-3.0.1\r\n","  Attempting uninstall: langchain-text-splitters\r\n","    Found existing installation: langchain-text-splitters 0.2.1\r\n","    Uninstalling langchain-text-splitters-0.2.1:\r\n","      Successfully uninstalled langchain-text-splitters-0.2.1\r\n","  Attempting uninstall: langchain-community\r\n","    Found existing installation: langchain-community 0.2.4\r\n","    Uninstalling langchain-community-0.2.4:\r\n","      Successfully uninstalled langchain-community-0.2.4\r\n","  Attempting uninstall: langchain\r\n","    Found existing installation: langchain 0.2.4\r\n","    Uninstalling langchain-0.2.4:\r\n","      Successfully uninstalled langchain-0.2.4\r\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","cudf 24.4.1 requires cubinlinker, which is not installed.\r\n","cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n","cudf 24.4.1 requires ptxcompiler, which is not installed.\r\n","cuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n","dask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n","keras-cv 0.9.0 requires keras-core, which is not installed.\r\n","keras-nlp 0.12.1 requires keras-core, which is not installed.\r\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n","cudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\r\n","distributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\r\n","google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\r\n","jupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n","jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n","libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n","momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n","osmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\r\n","preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\r\n","rapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\r\n","rapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\r\n","spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\r\n","ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0mSuccessfully installed beautifulsoup4-4.12.3 bitarray-2.9.2 colbert-ai-0.2.19 dirtyjson-1.0.8 faiss-cpu-1.8.0 fast-pytorch-kmeans-0.2.0.1 git-python-1.0.3 langchain-0.1.20 langchain-community-0.0.38 langchain-text-splitters-0.0.2 langchain_core-0.1.52 llama-index-0.10.44 llama-index-agent-openai-0.2.7 llama-index-cli-0.1.12 llama-index-core-0.10.44 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.22 llama-index-multi-modal-llms-openai-0.1.6 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.25 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.4 llamaindex-py-client-0.1.19 nltk-3.8.1 openai-1.34.0 packaging-23.2 ragatouille-0.0.8.post2 sentence-transformers-2.7.0 striprtf-0.0.26 tiktoken-0.7.0 voyager-2.0.6\r\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install ragatouille"]},{"cell_type":"code","execution_count":6,"id":"c9d262ec","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:00:26.26249Z","iopub.status.busy":"2024-06-14T03:00:26.262169Z","iopub.status.idle":"2024-06-14T03:00:26.266263Z","shell.execute_reply":"2024-06-14T03:00:26.26548Z"},"papermill":{"duration":0.039762,"end_time":"2024-06-14T03:00:26.268071","exception":false,"start_time":"2024-06-14T03:00:26.228309","status":"completed"},"tags":[]},"outputs":[],"source":["# !pip install -q torch transformers transformers accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl pacmap"]},{"cell_type":"code","execution_count":7,"id":"07ea25a7","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:00:26.334245Z","iopub.status.busy":"2024-06-14T03:00:26.333978Z","iopub.status.idle":"2024-06-14T03:00:28.117707Z","shell.execute_reply":"2024-06-14T03:00:28.11662Z"},"papermill":{"duration":1.819729,"end_time":"2024-06-14T03:00:28.120456","exception":false,"start_time":"2024-06-14T03:00:26.300727","status":"completed"},"tags":[]},"outputs":[],"source":["from tqdm.notebook import tqdm\n","import pandas as pd\n","from typing import Optional, List, Tuple\n","from datasets import Dataset\n","import matplotlib.pyplot as plt\n","\n","pd.set_option(\"display.max_colwidth\", None)  # This will be helpful when visualizing retriever outputs"]},{"cell_type":"markdown","id":"09f5eddf","metadata":{"papermill":{"duration":0.03256,"end_time":"2024-06-14T03:00:28.191601","exception":false,"start_time":"2024-06-14T03:00:28.159041","status":"completed"},"tags":[]},"source":["## Loading knowledge base"]},{"cell_type":"code","execution_count":8,"id":"53698198","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:00:28.260152Z","iopub.status.busy":"2024-06-14T03:00:28.259254Z","iopub.status.idle":"2024-06-14T03:00:30.51917Z","shell.execute_reply":"2024-06-14T03:00:30.518296Z"},"papermill":{"duration":2.296772,"end_time":"2024-06-14T03:00:30.521286","exception":false,"start_time":"2024-06-14T03:00:28.224514","status":"completed"},"tags":[]},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c0a356638cf46529841506117330a37","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6eadb09b5c4418eaf7123b55bae08ae","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/22.0M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"91417795c8374ff589700b4b5cd1bffc","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/2647 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import datasets\n","\n","ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"]},{"cell_type":"code","execution_count":9,"id":"eaebccca","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:00:30.590695Z","iopub.status.busy":"2024-06-14T03:00:30.590377Z","iopub.status.idle":"2024-06-14T03:00:31.054619Z","shell.execute_reply":"2024-06-14T03:00:31.053704Z"},"papermill":{"duration":0.50185,"end_time":"2024-06-14T03:00:31.057895","exception":false,"start_time":"2024-06-14T03:00:30.556045","status":"completed"},"tags":[]},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27e1aeee80694d86adbca74895c3840e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2647 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from langchain.docstore.document import Document as LangchainDocument\n","\n","RAW_KNOWLEDGE_BASE = [\n","    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)\n","]"]},{"cell_type":"markdown","id":"a7a3c237","metadata":{"papermill":{"duration":0.033086,"end_time":"2024-06-14T03:00:31.125393","exception":false,"start_time":"2024-06-14T03:00:31.092307","status":"completed"},"tags":[]},"source":["## Split documents into chunks and proper chunk length"]},{"cell_type":"code","execution_count":10,"id":"d6686e86","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:00:31.193238Z","iopub.status.busy":"2024-06-14T03:00:31.192934Z","iopub.status.idle":"2024-06-14T03:00:33.097657Z","shell.execute_reply":"2024-06-14T03:00:33.096814Z"},"papermill":{"duration":1.941131,"end_time":"2024-06-14T03:00:33.10003","exception":false,"start_time":"2024-06-14T03:00:31.158899","status":"completed"},"tags":[]},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n","# This list is taken from LangChain's MarkdownTextSplitter class\n","MARKDOWN_SEPARATORS = [\n","    \"\\n#{1,6} \",\n","    \"```\\n\",\n","    \"\\n\\\\*\\\\*\\\\*+\\n\",\n","    \"\\n---+\\n\",\n","    \"\\n___+\\n\",\n","    \"\\n\\n\",\n","    \"\\n\",\n","    \" \",\n","    \"\",\n","]\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n","    chunk_overlap=100,  # The number of characters to overlap between chunks\n","    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n","    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n","    separators=MARKDOWN_SEPARATORS,\n",")\n","\n","docs_processed = []\n","for doc in RAW_KNOWLEDGE_BASE:\n","    docs_processed += text_splitter.split_documents([doc])"]},{"cell_type":"code","execution_count":11,"id":"055903c8","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:00:33.173276Z","iopub.status.busy":"2024-06-14T03:00:33.172995Z","iopub.status.idle":"2024-06-14T03:02:20.941442Z","shell.execute_reply":"2024-06-14T03:02:20.940504Z"},"papermill":{"duration":107.807167,"end_time":"2024-06-14T03:02:20.943531","exception":false,"start_time":"2024-06-14T03:00:33.136364","status":"completed"},"tags":[]},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2617131e81fc4514a7ce3eebc34736e8","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d25d23672f684fdcbbdd5fc5e77eb455","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca6ac7d866794843bc4e4b4d0b812f69","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7604b5911b9d4612a48d4abf586b3d38","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5747d27c4a034c7ab8a2fb5133ce1659","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/17995 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from transformers import AutoTokenizer\n","\n","EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n","\n","\n","def split_documents(\n","    chunk_size: int,\n","    knowledge_base: List[LangchainDocument],\n","    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",") -> List[LangchainDocument]:\n","    \"\"\"\n","    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n","    \"\"\"\n","    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n","        AutoTokenizer.from_pretrained(tokenizer_name),\n","        chunk_size=chunk_size,\n","        chunk_overlap=int(chunk_size / 10),\n","        add_start_index=True,\n","        strip_whitespace=True,\n","        separators=MARKDOWN_SEPARATORS,\n","    )\n","\n","    docs_processed = []\n","    for doc in knowledge_base:\n","        docs_processed += text_splitter.split_documents([doc])\n","\n","    # Remove duplicates\n","    unique_texts = {}\n","    docs_processed_unique = []\n","    for doc in docs_processed:\n","        if doc.page_content not in unique_texts:\n","            unique_texts[doc.page_content] = True\n","            docs_processed_unique.append(doc)\n","\n","    return docs_processed_unique\n","\n","\n","docs_processed = split_documents(\n","    512,  # We choose a chunk size adapted to our model\n","    RAW_KNOWLEDGE_BASE,\n","    tokenizer_name=EMBEDDING_MODEL_NAME,\n",")\n","\n","# Let's visualize the chunk sizes we would have in tokens from a common model\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n","lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n","fig = pd.Series(lengths).hist()\n","plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n","plt.show()"]},{"cell_type":"markdown","id":"4fac7bbe","metadata":{"papermill":{"duration":0.035018,"end_time":"2024-06-14T03:02:21.013969","exception":false,"start_time":"2024-06-14T03:02:20.978951","status":"completed"},"tags":[]},"source":["## Create vector database"]},{"cell_type":"markdown","id":"220a8eee","metadata":{"papermill":{"duration":0.034228,"end_time":"2024-06-14T03:02:21.082653","exception":false,"start_time":"2024-06-14T03:02:21.048425","status":"completed"},"tags":[]},"source":[" "]},{"cell_type":"code","execution_count":12,"id":"64afaea8","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:02:21.153715Z","iopub.status.busy":"2024-06-14T03:02:21.152906Z","iopub.status.idle":"2024-06-14T03:04:10.359239Z","shell.execute_reply":"2024-06-14T03:04:10.358124Z"},"papermill":{"duration":109.244748,"end_time":"2024-06-14T03:04:10.36189","exception":false,"start_time":"2024-06-14T03:02:21.117142","status":"completed"},"tags":[]},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd04d5cfbc3b49dc8390f4f1975eb7b2","version_major":2,"version_minor":0},"text/plain":["modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd167f9c7ceb404d8585adef002ded62","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b013a5a9b24b4463913bdc3af86b60d2","version_major":2,"version_minor":0},"text/plain":["sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b8c46fb5d694da688ef869d32143498","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86b9caf316cf49dcbc9f99a55333af6a","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/66.7M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"831b2aa3ccc24ca88dea2acb7e06fb5f","version_major":2,"version_minor":0},"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from langchain.vectorstores import FAISS\n","# from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain_community.vectorstores.utils import DistanceStrategy\n","\n","embedding_model = HuggingFaceEmbeddings(\n","    model_name=EMBEDDING_MODEL_NAME,\n","    multi_process=True,\n","    model_kwargs={\"device\": \"cuda\"},\n","    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",")\n","\n","KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n","    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",")"]},{"cell_type":"markdown","id":"e4dd9736","metadata":{"papermill":{"duration":0.036238,"end_time":"2024-06-14T03:04:10.435188","exception":false,"start_time":"2024-06-14T03:04:10.39895","status":"completed"},"tags":[]},"source":["## Visulize vector database on one search"]},{"cell_type":"code","execution_count":13,"id":"b73fbd96","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:04:10.510472Z","iopub.status.busy":"2024-06-14T03:04:10.509474Z","iopub.status.idle":"2024-06-14T03:04:18.754011Z","shell.execute_reply":"2024-06-14T03:04:18.753151Z"},"papermill":{"duration":8.285186,"end_time":"2024-06-14T03:04:18.756306","exception":false,"start_time":"2024-06-14T03:04:10.47112","status":"completed"},"tags":[]},"outputs":[],"source":["# Embed a user query in the same space\n","user_query = \"How to create a pipeline object?\"\n","query_vector = embedding_model.embed_query(user_query)"]},{"cell_type":"code","execution_count":14,"id":"ee4aa8f6","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:04:18.83208Z","iopub.status.busy":"2024-06-14T03:04:18.831777Z","iopub.status.idle":"2024-06-14T03:04:51.399893Z","shell.execute_reply":"2024-06-14T03:04:51.398829Z"},"papermill":{"duration":32.608001,"end_time":"2024-06-14T03:04:51.402281","exception":false,"start_time":"2024-06-14T03:04:18.79428","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/pacmap/pacmap.py:828: UserWarning: Warning: random state is set to 1\n","  warnings.warn(f'Warning: random state is set to {_RANDOM_STATE}')\n"]}],"source":["import pacmap\n","import numpy as np\n","import plotly.express as px\n","\n","embedding_projector = pacmap.PaCMAP(n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1)\n","\n","embeddings_2d = [\n","    list(KNOWLEDGE_VECTOR_DATABASE.index.reconstruct_n(idx, 1)[0]) for idx in range(len(docs_processed))\n","] + [query_vector]\n","\n","# Fit the data (the index of transformed data corresponds to the index of the original data)\n","documents_projected = embedding_projector.fit_transform(np.array(embeddings_2d), init=\"pca\")"]},{"cell_type":"code","execution_count":15,"id":"2e06b8db","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:04:51.478099Z","iopub.status.busy":"2024-06-14T03:04:51.477494Z","iopub.status.idle":"2024-06-14T03:04:53.668061Z","shell.execute_reply":"2024-06-14T03:04:53.666978Z"},"papermill":{"duration":2.305031,"end_time":"2024-06-14T03:04:53.744141","exception":false,"start_time":"2024-06-14T03:04:51.43911","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["        <script type=\"text/javascript\">\n","        window.PlotlyConfig = {MathJaxConfig: 'local'};\n","        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n","        if (typeof require !== 'undefined') {\n","        require.undef(\"plotly\");\n","        requirejs.config({\n","            paths: {\n","                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n","            }\n","        });\n","        require(['plotly'], function(Plotly) {\n","            window._Plotly = Plotly;\n","        });\n","        }\n","        </script>\n","        "]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>                            <div id=\"64c5821f-3b02-4014-a2ae-3a4eb1608749\" class=\"plotly-graph-div\" style=\"height:700px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"64c5821f-3b02-4014-a2ae-3a4eb1608749\")) {                    Plotly.newPlot(                        \"64c5821f-3b02-4014-a2ae-3a4eb1608749\",                        [{\"customdata\":[[\"Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](htt...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fraw.githubusercontent.com\\u002fhuggingface\\u002fhf-endpoints-documentation\\u002fmain\\u002fassets\\u002f1_cre...\"],[\"Access and read Logs\\n\\nHugging Face Endpoints provides access to the logs of your Endpoints through t...\"],[\"Hugging Face Inference Endpoints documentation\\n\\n## Setup\\n\\n```bash\\npip install hf-doc-builder==0.4.0 ...\"],[\"Pricing\\n\\n\\u003cdiv class=\\\"flex md:justify-start mb-2 text-gray-400 items-center\\\"\\u003e\\n  \\u003ca href=\\\"https:\\u002f\\u002fui.e...\"],[\"## CPU Instances\\n\\nThe table below shows currently available CPU instances and their hourly pricing. ...\"],[\"## GPU Instances\\n\\nThe table below shows currently available GPU instances and their hourly pricing. ...\"],[\"```\\ninstance hourly rate * ((hours * # min replica) + (scale-up hrs * # additional replicas))\\n```\\n\\n#...\"],[\"Supported Transformers & Diffusers Tasks\\n\\nInference Endpoints offers out-of-the-box support for Mach...\"],[\"```\\n\\n### Text Classification\\n\\n```json\\n{\\n  \\\"inputs\\\": \\\"This sound track was beautiful! It paints the s...\"],[\"```\\n\\n### Text Generation\\n\\n```json\\n{\\n  \\\"inputs\\\": \\\"This sound track was beautiful! It paints the sener...\"],[\"```\\n\\n**Binary**\\n```bash\\ncurl --request POST \\\\\\n  --url https:\\u002f\\u002f{ENDPOINT}\\u002f \\\\\\n  --header 'Content-Type...\"],[\"```\\n\\n**Binary**\\n\\n```bash\\ncurl --request POST \\\\\\n  --url https:\\u002f\\u002f{ENDPOINT}\\u002f \\\\\\n  --header 'Content-Typ...\"],[\"```\\n\\n\\n### Additional parameters\\n\\nYou can add additional parameters, which are supported by the `pipe...\"],[\"Access and view Metrics\\n\\nHugging Face Endpoints provides access to the metrics and analytics of your...\"],[\"# FAQs \\n\\n\\n\\n### Q: In which regions are Inference Endpoints available?\\n\\nA: Inference Endpoints are cu...\"],[\"A: Yes, your Endpoint will always stay available\\u002fup with the number of min replicas defined in the A...\"],[\"### Q: What if I would like to deploy to a different instance type that is not listed?\\n\\nA: Please co...\"],[\"Help & Support \\n\\nWe have a variety of Inference Endpoints blog posts to help you at https:\\u002f\\u002fhuggingf...\"],[\"Pause and Resume your Endpoint\\n\\nYou can `pause` & `resume` endpoints to save cost and configurations...\"],[\"After clicking the button, you will be asked to confirm the action. Click \\\"Pause {ENDPOINT-NAME}\\\" to...\"],[\"API Reference (Swagger)\\n\\n🤗 Inference Endpoints can be used through the [UI](https:\\u002f\\u002fui.endpoints.hug...\"],[\"Use a custom Container Image\\n\\n\\nInference Endpoints not only allows you to [customize your inference ...\"],[\"Autoscaling\\n\\nAutoscaling allows you to dynamically adjust the number of endpoint replicas running yo...\"],[\"## Scaling to 0\\n\\nInference Endpoints also supports autoscaling to 0, which means reducing the number...\"],[\"Create a Private Endpoint with AWS PrivateLink\\n\\nSecurity and secure inference are key principles of ...\"],[\"Once your Inference Endpoint is created successfully, go to the corresponding AWS account and add th...\"],[\"Security & Compliance\\n\\n🤗 Inference Endpoints is built with security and secure inference at its core...\"],[\"Public and Protected Endpoints do not require any additional configuration. For Private Endpoints, y...\"],[\"Send Requests to Endpoints\\n\\nYou can send requests to Inference Endpoints using the UI leveraging the...\"],[\"```\\n\\nThe Endpoints API offers the same API definitions as the [Inference API](https:\\u002f\\u002fhuggingface.co...\"],[\"```\\n\\n### Custom handler\\n\\n`@huggingface\\u002finference` supports tasks from https:\\u002f\\u002fhuggingface.co\\u002ftasks, ...\"],[\"Change Organization or Account\\n\\nInference Endpoints uses your [Hugging Face](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"Update your Endpoint\\n\\nYou can update `running` Endpoints to change some of the configurations. Howev...\"],[\"Advanced Setup (Instance Types, Auto Scaling, Versioning)\\n\\nWe have seen how fast and easy it is to d...\"],[\"_Default: PyTorch if available._\\n\\n**Revision**\\n\\nCreate your Endpoint targeting a specific revision c...\"],[\"Inference Endpoints Version\\n\\nHugging Face Inference Endpoints comes with a default serving container...\"],[\"### GPU\\n\\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\\n- `diffusers`: `0.14.0`\\n- `a...\"],[\"Serialization & Deserialization for Requests\\n\\nHugging Face Inference Endpount comes with a default s...\"],[\"| Content-Type           | Payload                        | \\n| ---------------------- | ------------...\"],[\"| audio\\u002fwebm             | `{\\\"inputs\\\": bytes(body)}`                     |\\n| audio\\u002fwebm;codecs=opus ...\"],[\"Below is a list of supported `accept` headers and the serialized payload is returned.\\n\\n\\n| Accept    ...\"],[\"🤗 Inference Endpoints\\n\\n🤗 Inference Endpoints offers a secure production solution to easily deploy an...\"],[\"## Documentation and Examples\\n\\n* [Security & Compliance](\\u002fdocs\\u002finference-endpoints\\u002fsecurity)\\n* [Supp...\"],[\"Access 🤗 Inference Endpoints\\n\\nTo access the [Inference Endpoints web application](https:\\u002f\\u002fui.endpoin...\"],[\"Add custom Dependencies\\n\\nInference Endpoints’ base image includes all required libraries to run infe...\"],[\"Create custom Inference Handler\\n\\nHugging Face Endpoints supports all of the Transformers and Sentenc...\"],[\"Included examples are for:\\n\\n* [Optimum and ONNX Runtime](https:\\u002f\\u002fhuggingface.co\\u002fphilschmid\\u002fdistilber...\"],[\"The code can also be found in this [Notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1hANJeRa1PK1gZ...\"],[\"```\\n# install git-lfs to interact with the repository\\nsudo apt-get update\\nsudo apt-get install git-l...\"],[\"```\\n!cd distilbert-base-uncased-emotion && touch handler.py\\n```\\n\\nIn there, you define your `Endpoint...\"],[\"```\\n!echo \\\"holidays\\\" \\u003e\\u003e requirements.txt\\n!pip install -r requirements.txt\\n```\\n\\nNext, we have to adju...\"],[\"```\\n\\n### 4. Test EndpointHandler\\n\\nTo test our EndpointHandler, we can simplify import, initialize an...\"],[\"```\\n# add all our new files\\n!git add *\\n# commit our files\\n!git commit -m \\\"add custom handler\\\"\\n# push...\"]],\"hovertemplate\":\"source=hf-endpoints-documentation\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"hf-endpoints-documentation, circle\",\"marker\":{\"color\":\"#EF553B\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"hf-endpoints-documentation, circle\",\"showlegend\":true,\"x\":[5.2852697,4.933163,5.2124834,5.118488,4.775116,-1.0809064,-0.93417835,-1.7885879,-4.7586823,-3.6480985,-7.4892626,6.5836253,5.6325345,4.973902,5.140386,4.6105895,4.718791,5.557421,4.899586,4.5014367,5.1340594,5.0832276,-0.32418463,-1.31355,4.2275085,5.169799,5.1417236,4.9986305,5.314076,5.03341,4.7993894,5.055004,5.282898,4.341291,4.499691,3.6856835,4.4494834,-1.4225495,4.7961216,6.7918425,7.004222,6.433809,4.8918242,4.619038,5.085912,4.309254,4.53234,-5.610704,4.8490295,4.319704,4.2765703,1.4427007,3.4137099,4.4853745],\"xaxis\":\"x\",\"y\":[0.11845589,0.6662711,-0.21628542,-0.31321028,1.0534196,2.6477332,2.7550793,2.8790855,-0.2794432,-3.4629376,-6.2684555,-4.0923967,-3.4427998,-3.6209116,0.27186504,1.1167802,0.93150246,-1.7719928,0.9517673,1.0047352,0.3476219,0.552336,0.26623306,2.795696,1.2832205,0.42241716,0.49890053,0.955448,0.3396425,0.52317625,0.46094236,0.29458156,-0.31885916,0.9212472,1.1491879,0.5429173,1.2548528,1.5112729,0.46929154,-3.832449,-4.035054,-3.297382,1.2054589,1.078951,0.8613151,0.7657943,0.9197837,0.1688808,0.6299552,-0.27730203,0.45299733,-2.1441276,-1.2916255,-0.080351606],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Choosing a metric for your task\\n\\n**So you've trained your model and want to see how well it’s doing ...\"],[\"```\\n\\u003e\\u003e\\u003e precision_metric = evaluate.load(\\\"precision\\\")\\n\\u003e\\u003e\\u003e results = precision_metric.compute(referen...\"],[\"```\\n\\n### Task-specific metrics\\n\\nPopular ML tasks like Machine Translation and Named Entity Recogniti...\"],[\"\\u003cTip warning={true}\\u003e\\n💡\\nGLUE is actually a collection of different subsets on different tasks, so fir...\"],[\"```\\n\\u003e\\u003e\\u003e from evaluate import load\\n\\u003e\\u003e\\u003e squad_metric = load(\\\"squad\\\")\\n\\u003e\\u003e\\u003e predictions = [{'prediction_t...\"],[\"--\\ntitle: poseval\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: a...\"],[\"`references`: a list of lists of reference labels, i.e. the ground truth\\u002ftarget values.\\n\\nIt can also...\"],[\"```\\n\\n## Output values\\n\\nThis metric returns a a classification report as a dictionary with a summary ...\"],[\"`f1`: the average [F1 score](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002ff1), on a scale between 0.0 and 1.0.\\n\\n\\n#...\"],[\"```\\n\\n## Limitations and bias\\n\\nIn contrast to [seqeval](https:\\u002f\\u002fgithub.com\\u002fchakki-works\\u002fseqeval), the...\"],[\"--\\ntitle: MAPE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'mape': array([0.5, 1. ])}\\n```\\n\\n#### Values from Popu...\"],[\"```\\n\\n## Limitations and Bias\\nOne limitation of MAPE is that it cannot be used if the ground truth is...\"],[\"--\\ntitle: ROUGE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app...\"],[\"```\\n\\nOne can also pass a custom tokenizer which is especially useful for non-latin languages.\\n```pyt...\"],[\"```\\n```\\n\\n### Inputs\\n- **predictions** (`list`): list of predictions to score. Each prediction\\n      ...\"],[\"```\\n\\nThe ROUGE values are in the range of 0 to 1.\\n\\n\\n#### Values from Popular Papers\\n\\n\\n### Examples\\nA...\"],[\"```\\n\\n## Limitations and Bias\\nSee [Schluter (2017)](https:\\u002f\\u002faclanthology.org\\u002fE17-2007\\u002f) for an in-dep...\"],[\"--\\ntitle: Word Length\\nemoji: 🤗\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_f...\"],[\"```\\n\\nThis metric outputs a dictionary containing the number of words in the input string (`word leng...\"],[\"Working with Keras and Tensorflow\\n\\n\\n\\nEvaluate can be easily intergrated into your Keras and Tensorfl...\"],[\"x_train = np.expand_dims(x_train, -1)\\nx_test = np.expand_dims(x_test, -1)\\n\\n\\nmodel = keras.Sequential...\"],[\"```\\n\\n## Callbacks\\n\\nSuppose we want to keep track of model metrics while a model is training. We can ...\"],[\"```\\n\\n## Using an Evaluate Metric for... Evaluation!\\n\\nWe can also use the same metric after model tra...\"],[\"--\\ntitle: CharCut\\nemoji: 🔤\\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ap...\"],[\"```\\n\\n## Citation\\n```bibtex\\n@inproceedings{lardilleux-lepage-2017-charcut,\\n    title = \\\"{CHARCUT}: Hu...\"],[\"--\\ntitle: IndicGLUE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"2. **Calculating the metric**: the metric takes two inputs : one list with the predictions of the mo...\"],[\"```\\n    \\n## Output values\\n\\nThe output of the metric depends on the IndicGLUE subset chosen, consisti...\"],[\"```\\n\\nMinimal values for the Wiki-NER subset (which outputs `accuracy` and `f1`):\\n\\n```python\\n\\u003e\\u003e\\u003e indi...\"],[\"```\\n    \\n## Further References \\n- [IndicNLP website](https:\\u002f\\u002findicnlp.ai4bharat.org\\u002fhome\\u002f)...\"],[\"--\\ntitle: Google BLEU\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_fil...\"],[\"The minimum value of precision and recall is then returned as the score.\\n\\n\\n## Intended Uses\\nThis met...\"],[\"```\\n\\n### Inputs\\n- **predictions** (list of str): list of translations to score.\\n- **references** (li...\"],[\"```\\n\\n#### Values from Popular Papers\\n\\n\\n### Examples\\nExample with one reference per sample:\\n```python...\"],[\"```\\n\\nExample with multiple references for the first sample, and with `min_len` adjusted to `2`, inst...\"],[\"```\\n\\nExample with multiple references for the first sample, with `min_len` adjusted to `2`, instead ...\"],[\"```\\n\\n## Limitations and Bias\\n\\nThe GoogleBLEU metric does not come with a predefined tokenization fun...\"],[\"--\\ntitle: \\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\np...\"],[\"```\\n\\nFrugalScore calculates how good are the predictions given some references, based on a set of sc...\"],[\"```\\n\\nPartial values: \\n\\n```python\\n\\u003e\\u003e\\u003e frugalscore = evaluate.load(\\\"frugalscore\\\")\\n\\u003e\\u003e\\u003e results = frugal...\"],[\"```\\n\\n## Limitations and bias\\n\\nFrugalScore is based on [BertScore](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002fber...\"],[\"| FrugalScore                                        | Student     | Teacher        | Method     |\\n|...\"],[\"| [moussaKam\\u002ffrugalscore_medium_roberta_bert-score](https:\\u002f\\u002fhuggingface.co\\u002fmoussaKam\\u002ffrugalscore_med...\"],[\"Depending on the size of the model picked, the loading time will vary: the `tiny` models will load v...\"],[\"```\\n\\n## Further References\\n- [Original FrugalScore code](https:\\u002f\\u002fgithub.com\\u002fmoussaKam\\u002fFrugalScore)\\n-...\"],[\"--\\ntitle: Mean IoU\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ...\"],[\"```\\n\\n### Inputs\\n**Mandatory inputs**\\n- `predictions` (`List[ndarray]`): List of predicted segmentati...\"],[\"The values of all of the scores reported range from from `0.0` (minimum) and `1.0` (maximum).\\n\\nOutpu...\"],[\"```\\n\\n#### Values from Popular Papers\\n\\nThe [leaderboard for the CityScapes dataset](https:\\u002f\\u002fpaperswit...\"],[\"### Examples\\n\\n```python\\n\\u003e\\u003e\\u003e import numpy as np\\n\\u003e\\u003e\\u003e mean_iou = evaluate.load(\\\"mean_iou\\\")\\n\\u003e\\u003e\\u003e # suppos...\"],[\"## Limitations and Bias\\nMean IOU is an average metric, so it will not show you where model predictio...\"],[\"```\\n\\n\\n## Further References\\n- [Wikipedia article - Jaccard Index](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fJacc...\"],[\"--\\ntitle: SuperGLUE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"2. **Calculating the metric**: the metric takes two inputs : one list with the predictions of the mo...\"],[\"```\\n## Output values\\n\\nThe output of the metric depends on the SuperGLUE subset chosen, consisting of...\"],[\"```\\n\\nMinimal values for the MultiRC subset (which outputs `pearson` and `spearmanr`):\\n\\n```python\\nfro...\"],[\"```\\n\\n## Limitations and bias\\nThis metric works only with datasets that have the same format as the [...\"],[\"🤗 Transformers\\n\\nTo run the 🤗 Transformers examples make sure you have installed the following librar...\"],[\"```\\n\\n## Trainer\\n\\nThe metrics in `evaluate` can be easily integrated with the [`~transformers.Trainer...\"],[\"trainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=small_train_dataset,\\n ...\"],[\"```\\n\\n## Seq2SeqTrainer\\n\\nWe can use the [`~transformers.Seq2SeqTrainer`] for sequence-to-sequence tas...\"],[\"model_inputs[\\\"labels\\\"] = labels[\\\"input_ids\\\"]\\n    return model_inputs\\n\\ntokenized_billsum = billsum.ma...\"],[\"trainer = Seq2SeqTrainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=tokenized_bills...\"],[\"```\\n\\nYou can use any `evaluate` metric with the `Trainer` and `Seq2SeqTrainer` as long as they are c...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nYou can adapt the `--build_dir` to set any temporary folder that you prefer. This command will ...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\nFo...\"],[\"```\\n## XXXConfig\\n\\n[[autodoc]] XXXConfig\\n```\\n\\nThis will include every public method of the configurat...\"],[\"```\\n## XXXTokenizer\\n\\n[[autodoc]] XXXTokenizer\\n    - all\\n    - __call__\\n```\\n\\n### Writing source docum...\"],[\"```\\n\\nIf the description is too long to fit in one line, another indentation is necessary before writ...\"],[\"```\\n```\\n# first line of code\\n# second line\\n# etc\\n```\\n````\\n\\nWe follow the [doctest](https:\\u002f\\u002fdocs.pyth...\"],[\"```\\n\\n#### Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that ...\"],[\"--\\ntitle: Spearman Correlation Coefficient Metric \\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradi...\"],[\"## How to Use\\nAt minimum, this metric only requires a `list` of predictions and a `list` of referenc...\"],[\"```\\n\\n### Inputs\\n- **`predictions`** (`list` of `float`): Predicted labels, as returned by a model.\\n-...\"],[\"```\\n\\nThe same example, but that also returns the pvalue:\\n```python\\n\\u003e\\u003e\\u003e spearmanr_metric = evaluate.l...\"],[\"```\\n\\n## Limitations and Bias\\n\\n\\n## Citation\\n```bibtex\\n@book{kokoska2000crc,\\n  title={CRC standard pro...\"],[\"--\\ntitle: TREC Eval\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\n\\n### Inputs\\n- **predictions** *(dict): a single retrieval run.*\\n    - **query** *(int): Query ID...\"],[\"### Output Values\\n- **runid** *(str): Run name.*  \\n- **num_ret** *(int): Number of retrieved documen...\"],[\"```\\n\\nA more realistic use case with an examples from [`trectools`](https:\\u002f\\u002fgithub.com\\u002fjoaopalotti\\u002ftr...\"],[\"```\\n\\n```python\\nresult\\n\\n{'runid': 'InexpC2',\\n 'num_ret': 100000,\\n 'num_rel': 6074,\\n 'num_rel_ret': 31...\"],[\"```\\n\\n## Limitations and Bias\\nThe `trec_eval` metric requires the inputs to be in the TREC run and qr...\"],[\"A quick tour\\n\\n🤗 Evaluate provides access to a wide range of evaluation tools. It covers a range of m...\"],[\"```\\n\\nIf you want to make sure you are loading the right type of evaluation (especially if there are ...\"],[\"```\\n\\n## Module attributes\\n\\nAll evalution modules come with a range of useful attributes that help to...\"],[\"```\\n\\nYou can see that it describes how the metric works in theory. If you use this metric for your w...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nNote that features always describe the type of a single input element. In general we wil...\"],[\"```\\nEvaluation modules return the results in a dictionary. However, in some instances you build up t...\"],[\"```\\n\\n### Distributed evaluation\\n\\nComputing metrics in a distributed environment can be tricky. Metri...\"],[\"```\\n\\nThe `combine` function accepts both the list of names of the metrics as well as an instantiated...\"],[\"```\\n\\nThe content of the JSON file look like the following:\\n\\n```json\\n{\\n    \\\"experiment\\\": \\\"run 42\\\",\\n  ...\"],[\"```\\n\\n## Evaluator\\n\\nThe [`evaluate.evaluator`] provides automated evaluation and only requires a mode...\"],[\"```\\n\\nCalculating the value of the metric alone is often not enough to know if a model performs signi...\"],[\"```\\n\\nThe evaluator expects a `\\\"text\\\"` and `\\\"label\\\"` column for the data input. If your dataset diffe...\"],[\"```\\n\\nWhich lets you visually compare the 4 models and choose the optimal one for you, based on one o...\"],[\"```\\n\\nEvaluation can be run by loading the `EvaluationSuite` and calling the `run()` method with a mo...\"],[\"Using the `evaluator` with custom pipelines\\n\\nThe evaluator is designed to work with `transformer` pi...\"],[\"```\\n\\nFollowing the convention in the `TextClassificationPipeline` of `transformers` our pipeline sho...\"],[\"```\\n\\nThis snippet shows how we can use the `polarity` feature added with `spacytextblob` to get the ...\"],[\"--\\ntitle: Matthews Correlation Coefficient\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_ve...\"],[\"```\\n\\n### Inputs\\n- **`predictions`** (`list` of `int`s): Predicted class labels.\\n- **`references`** (...\"],[\"```\\n\\nThe same example as above, with sample weights that cause a negative correlation:\\n```python\\n\\u003e\\u003e\\u003e...\"],[\"Evaluator\\n\\nThe evaluator classes for automatic evaluation.\\n\\n## Evaluator classes\\n\\nThe main entry poi...\"],[\"Using the `evaluator`\\n\\nThe `Evaluator` classes allow to evaluate a  triplet of model, dataset, and m...\"],[\"## Text classification\\n\\nThe text classification evaluator can be used to evaluate text models on cla...\"],[\"# 2. Pass an instantiated model\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\\"lvwerra\\u002f...\"],[\"```\\n\\u003cTip\\u003e\\n\\nWithout specifying a device, the default for model inference will be the first GPU on the...\"],[\"```\\n\\nNext let's have a look at token classification.\\n\\n## Token Classification\\n\\nWith the token classi...\"],[\"```python\\nimport pandas as pd\\nfrom datasets import load_dataset\\nfrom evaluate import evaluator\\nfrom ...\"],[\"```\\n\\nThe result is a table that looks like this:...\"],[\"|   model                                                            |   overall_f1 |   overall_accu...\"],[\"| philschmid\\u002fdistilroberta-base-ner-conll2003                        |        0.961 |              0...\"],[\"### Visualizing results\\n\\nYou can feed in the `results` list above into the `plot_radar()` function t...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fevaluate\\u002fmedia\\u002f...\"],[\"```python\\nfrom datasets import load_dataset\\nfrom evaluate import evaluator\\n\\ntask_evaluator = evaluat...\"],[\"```\\n\\nResults include confidence intervals as well as error estimates as follows:\\n\\n```python\\n{\\n    'e...\"],[\"```\\n\\nSince we are using `datasets` to store data we make use of a technique called memory mappings. ...\"],[\"--\\ntitle: Exact Match\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_fil...\"],[\"```\\n\\n### Inputs\\n- **`predictions`** (`list` of `str`): List of predicted texts.\\n- **`references`** (...\"],[\"```\\n\\nThis metric's range is 0-1, inclusive. Here, 0.0 means no prediction\\u002freference pairs were match...\"],[\"```\\nNote that in the example above, because the regexes are ignored before the case is normalized, \\\"...\"],[\"```\\n\\nAn example that includes sentences:\\n```python\\n\\u003e\\u003e\\u003e exact_match = evaluate.load(\\\"exact_match\\\")\\n\\u003e\\u003e...\"],[\"--\\ntitle: Wilcoxon\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: green\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file:...\"],[\"--\\ntitle: SQuAD v2\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ...\"],[\"## How to use \\n\\nThe metric takes two files or two lists - one representing model predictions and the...\"],[\"```\\n## Output values\\n\\nThis metric outputs a dictionary with 13 values: \\n* `'exact'`: Exact match (th...\"],[\"The range of `total` depends on the length of predictions\\u002freferences: its minimal value is 0, and ma...\"],[\"```\\n\\nMinimal values for both exact match and F1 (no match):\\n\\n```python\\nfrom evaluate import load\\nsqu...\"],[\"```\\n\\nPartial match (2 out of 3 answers correct) : \\n\\n```python\\nfrom evaluate import load\\nsquad_metric...\"],[\"```\\n\\n## Limitations and bias\\nThis metric works only with the datasets in the same format as the [SQu...\"],[\"--\\ntitle: BLEU\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"## How to Use\\n\\nThis metric takes as input a list of predicted sentences and a list of lists of refer...\"],[\"```\\n\\n### Inputs\\n- **predictions** (`list` of `str`s): Translations to score.\\n- **references** (`list...\"],[\"Output Example:\\n```python\\n{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, ...\"],[\"```\\n\\nBLEU's output is always a number between 0 and 1. This value indicates how similar the candidat...\"],[\"```\\n\\nExample where the second prediction has 2 references:\\n```python\\n\\u003e\\u003e\\u003e predictions = [\\n...     [\\\"h...\"],[\"```\\n\\n## Limitations and Bias\\nThis metric has multiple known limitations:\\n- BLEU compares overlap in ...\"],[\"--\\ntitle: Pearson Correlation Coefficient \\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_ve...\"],[\"```\\n\\n\\n### Inputs\\n- **predictions** (`list` of `int`): Predicted class labels, as returned by a model...\"],[\"```\\n\\nExample 2-The same as Example 1, but that also returns the `p-value`.\\n```python\\n\\u003e\\u003e\\u003e pearsonr_me...\"],[\"```\\n\\n\\n## Limitations and Bias\\n\\nAs stated above, the calculation of the p-value relies on the assumpt...\"],[\"--\\ntitle: Code Eval\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\n\\nN.B.\\nThis metric exists to run untrusted model-generated code. Users are strongly encouraged no...\"],[\"```\\n\\nNo match for k = 1:\\n\\n```python\\nfrom evaluate import load\\ncode_eval = load(\\\"code_eval\\\")\\ntest_cas...\"],[\"```\\n\\n## Limitations and bias\\n\\nAs per the warning included in the metric code itself:\\n\\u003e This program ...\"],[\"More information about the limitations of the code can be found on the [Human Eval Github repository...\"],[\"```\\n    \\n## Further References \\n\\n- [Human Eval Github repository](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002fhuman-ev...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fevaluate\\u002fmedia\\u002fresolve\\u002fmain...\"],[\"🤗 Evaluate is a library that makes evaluating and comparing models and reporting their performance e...\"],[\"# Installation\\n\\n## With pip\\n\\n🤗 Evaluate can be installed from PyPi and has to be installed in a virt...\"],[\"```\\n\\n# Usage\\n\\n🤗 Evaluate's main methods are:\\n\\n- `evaluate.list_evaluation_modules()` to list the ava...\"],[\"Types of Evaluations in 🤗 Evaluate\\n\\nThe goal of the 🤗 Evaluate library is to support different types...\"],[\"Comparisons have yet to be systematically used when comparing and reporting model performance, howev...\"],[\"--\\ntitle: WER\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"This problem is solved by first aligning the recognized word sequence with the reference (spoken) wo...\"],[\"```\\n## Output values\\n\\nThis metric outputs a float representing the word error rate.\\n\\n```\\nprint(wer_s...\"],[\"```\\n\\nNo match between prediction and reference:\\n\\n```python\\nfrom evaluate import load\\nwer = load(\\\"wer...\"],[\"--\\ntitle: chrF\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"See the [sacreBLEU README.md](https:\\u002f\\u002fgithub.com\\u002fmjpost\\u002fsacreBLEU#chrf--chrf) for more information.\\n...\"],[\"```\\n\\nThe chrF(++) score can be any value between `0.0` and `100.0`, inclusive.\\n\\n#### Values from Pop...\"],[\"```\\n\\nThe same chrF++ example as above, but with `lowercase=True` to normalize all case:\\n```python\\n\\u003e\\u003e...\"],[\"--\\ntitle: Regard\\nemoji: 🤗\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ...\"],[\"```\\n\\n### Inputs\\n- **data** (list of `str`): prediction\\u002fcandidate sentences, e.g. sentences describin...\"],[\"```\\n\\nWith the `aggregation='maximum'` option, this measurement will output the maximum regard for ea...\"],[\"```\\n\\nExample 3 (returns the maximum regard score):\\n```python\\n\\u003e\\u003e\\u003e regard = evaluate.load(\\\"regard\\\", \\\"c...\"],[\"```\\n\\nExample 4 (returns the average regard score):\\n```python\\n\\u003e\\u003e\\u003e regard = evaluate.load(\\\"regard\\\", \\\"c...\"],[\"--\\ntitle: Honest\\nemoji: 🤗\\ncolorFrom: blue\\ncolorTo: green\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ap...\"],[\"```\\n\\nArguments:\\n    **predictions** (list of list of `str`): a list of completions to [HONEST prompt...\"],[\"| Model Name       | Top K =1 | Top K =5 |Top K =20 |\\n| ---------------- | -------- | -------- | ---...\"],[\"## Examples\\n\\nExample 1: Calculating HONEST without groups\\n\\n```python\\n\\u003e\\u003e\\u003e honest = evaluate.load('hon...\"],[\"```\\n\\nExample 2: Calculating HONEST with 2 groups (e.g. male\\u002ffemale)\\n```python\\n\\u003e\\u003e\\u003e honest = evaluate....\"],[\"```\\n\\n## Citation\\n\\n```bibtex\\n@inproceedings{nozza-etal-2021-honest,\\n    title = {\\\"{HONEST}: Measuring...\"],[\"--\\ntitle: SQuAD\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app...\"],[\"```\\n{'exact_match': 100.0, 'f1': 100.0}\\n```\\n\\nThe range of `exact_match` is 0-100, where 0.0 means no...\"],[\"```\\n\\nMinimal values for both exact match and F1 (no match):\\n\\n```python\\nfrom evaluate import load\\nsqu...\"],[\"```\\n\\nPartial match (2 out of 3 answers correct) : \\n\\n```python\\nfrom evaluate import load\\nsquad_metric...\"],[\"```\\n\\n## Limitations and bias\\nThis metric works only with datasets that have the same format as [SQuA...\"],[\"--\\ntitle: Recall\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ap...\"],[\"```\\n```python\\n{'recall': array([1., 0., 0.])}\\n```\\n\\nThis metric outputs a dictionary with one entry, ...\"],[\"```\\n\\nExample 4-A multiclass example, using different averages.\\n```python\\n\\u003e\\u003e\\u003e recall_metric = evaluat...\"],[\"--\\ntitle: BERT Score\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file...\"],[\"```python\\nfrom evaluate import load\\nbertscore = load(\\\"bertscore\\\")\\npredictions = [\\\"hello there\\\", \\\"gen...\"],[\"`recall`: The [recall](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002frecall) for each sentence from the `prediction...\"],[\"```\\n\\nPartial match with the `distilbert-base-uncased` model:\\n\\n```python\\nfrom evaluate import load\\nbe...\"],[\"--\\ntitle: Competition MATH\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\nap...\"],[\"```\\n\\nN.B. To be able to use Competition MATH, you need to install the `math_equivalence` dependency ...\"],[\"```\\n\\nPartial match:\\n\\n```python\\n\\u003e\\u003e\\u003e from evaluate import load\\n\\u003e\\u003e\\u003e math = load(\\\"competition_math\\\")\\n\\u003e\\u003e\\u003e...\"],[\"--\\ntitle: MSE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'mse': array([0.41666667, 1. ])}\\n```\\n\\n#### Values fro...\"],[\"--\\ntitle: WikiSplit\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\n\\n### Values from popular papers\\n\\nThis metric was initially used by [Rothe et al.(2020)](https:\\u002f\\u002f...\"],[\"```\\n\\nNo match between prediction and reference:\\n\\n```python\\n\\u003e\\u003e\\u003e wiki_split = evaluate.load(\\\"wiki_spli...\"],[\"--\\ntitle: r_squared\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ...\"],[\"```\\n\\n### How to Use Examples:\\n\\nThe R2 class in the evaluate module can be used to compute the R^2 va...\"],[\"```\\n\\n## Further References\\n\\n- [The Open University: R-Squared](https:\\u002f\\u002fwww.open.edu\\u002fopenlearn\\u002focw\\u002fmo...\"],[\"--\\ntitle: Precision\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\n```python\\n{'precision': array([0.66666667, 0.0, 0.0])}\\n```\\n\\n\\n\\n\\n#### Values from Popular Papers\\n\\n...\"],[\"--\\ntitle: XTREME-S\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ...\"],[\"```\\n\\n2. **Calculating the metric**: the metric takes two inputs : \\n\\n- `predictions`: a list of predi...\"],[\"```\\n\\nIt also has two optional arguments: \\n\\n- `bleu_kwargs`: a `dict` of keywords to be passed when c...\"],[\"- `cer`:  Character error rate (CER) is similar to WER, but operates on character instead of word. T...\"],[\"```\\n\\nFor the `covost2` subset (which outputs `bleu`):\\n\\n```python\\n\\u003e\\u003e\\u003e xtreme_s_metric = evaluate.load...\"],[\"```\\n\\n## Limitations and bias\\nThis metric works only with datasets that have the same format as the [...\"],[\"--\\ntitle: CER\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"```\\n## Output values\\n\\nThis metric outputs a float representing the character error rate.\\n\\n```\\nprint(...\"],[\"```\\n\\nNo match between prediction and reference:\\n\\n```python\\nfrom evaluate import load\\ncer = load(\\\"cer...\"],[\"--\\ntitle: {{ cookiecutter.module_name }}\\ndatasets:\\n- {{ cookiecutter.dataset_name }} \\ntags:\\n- evalua...\"],[\"#### Values from Popular Papers\\n*Give examples, preferrably with links to leaderboards or publicatio...\"],[\"--\\ntitle: McNemar\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: green\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ...\"],[\"`stat`: The McNemar statistic.\\n\\n`p`: The p value.\\n\\n## Examples \\n\\nExample comparison:\\n\\n```python\\nmcne...\"],[\"```\\n\\n## Limitations and bias\\n\\nThe McNemar test is a non-parametric test, so it has relatively few as...\"],[\"--\\ntitle: F1\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py...\"],[\"```\\n```python\\n{'f1': array([0.8, 0.0, 0.0])}\\n```\\n\\nThis metric outputs a dictionary, with either a si...\"],[\"Considerations for model evaluation\\n\\nDeveloping an ML model is rarely a one-shot deal: it often invo...\"],[\"## The impact of class imbalance\\n\\nWhile many academic datasets, such as the [IMDb dataset](https:\\u002f\\u002fh...\"],[\"In cases where there is an imbalance, using [F1 score](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002ff1) can be a b...\"],[\"### Interpretability\\n\\nWhen evaluating models, **interpretability** (i.e. the ability to *interpret* ...\"],[\"Memory footprint refers to the size of the model weights and how much hardware memory they occupy. I...\"],[\"--\\ntitle: Mahalanobis Distance\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19....\"],[\"```\\n\\n#### Values from Popular Papers\\n*N\\u002fA*\\n\\n### Example\\n\\n```python\\n\\u003e\\u003e\\u003e mahalanobis_metric = evaluate...\"],[\"--\\ntitle: MAUVE\\nemoji: 🤗\\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"```\\n\\nIt also has several optional arguments:\\n\\n`num_buckets`: the size of the histogram to quantize P...\"],[\"`seed`: random seed to initialize k-means cluster assignments, randomly assigned by default.\\n    \\n\\n\\n...\"],[\"```\\n\\nPartial match between prediction and reference:\\n\\n```python\\nfrom evaluate import load\\nmauve = lo...\"],[\"```\\n\\n## Limitations and bias\\n\\nThe [original MAUVE paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2102.01454) did not a...\"],[\"For MAUVE to be large, the model distribution must be close to the human text distribution as seen b...\"],[\"```\\n\\n## Further References\\n- [Official MAUVE implementation](https:\\u002f\\u002fgithub.com\\u002fkrishnap25\\u002fmauve)\\n- ...\"],[\"--\\ntitle: BLEURT\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ap...\"],[\"```\\n\\n### Inputs\\n- **predictions** (`list` of `str`s): List of generated sentences to score.\\n- **refe...\"],[\"```\\n\\nBLEURT's output is always a number between 0 and (approximately 1). This value indicates how si...\"],[\"```\\n\\n## Limitations and Bias\\nThe [original BLEURT paper](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2004.04696.pdf) showe...\"],[\"--\\ntitle: Label Distribution\\nemoji: 🤗\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0....\"],[\"```\\n\\nIf skewness is 0, the dataset is perfectly balanced; if it is less than -1 or greater than 1, t...\"],[\"```\\n\\n## Limitations and Bias\\nWhile label distribution can be a useful signal for analyzing datasets ...\"],[\"--\\ntitle: XNLI\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"```\\n\\n## Output values\\n\\nThe output of the XNLI metric is simply the `accuracy`, i.e. the proportion o...\"],[\"```\\n\\n## Limitations and bias\\n\\nWhile accuracy alone does give a certain indication of performance, it...\"],[\"--\\ntitle: Text Duplicates\\nemoji: 🤗\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\na...\"],[\"```\\n\\nWarning: the `list_duplicates=True` function can be memory-intensive for large datasets.\\n\\n### E...\"],[\"Creating an EvaluationSuite\\n\\nIt can be useful to evaluate models on a variety of different tasks to ...\"],[\"The mandatory attributes for a new `SubTask` are `task_type` and `data`.\\n1. [`task_type`] maps to th...\"],[\"```\\n\\nAn `EvaluationSuite` can be loaded by name from the Hugging Face Hub, or locally by providing a...\"],[\"--\\ntitle: MAE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'mae': array([0.5, 1. ])}\\n```\\n\\n#### Values from Popul...\"],[\"```\\n\\n## Limitations and Bias\\nOne limitation of MAE is that the relative size of the error is not alw...\"],[\"--\\ntitle: GLUE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"### Values from popular papers\\nThe [original GLUE paper](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fglue) repor...\"],[\"```\\n\\nMinimal values for the STSB subset (which outputs `pearson` and `spearmanr`):\\n\\n```python\\nfrom e...\"],[\"Scikit-Learn\\n\\nTo run the scikit-learn examples make sure you have installed the following library:\\n\\n...\"],[\"```\\n\\nAlternatively X and y can be obtained directly from the frame attribute:\\n\\n```python\\nX = titanic...\"],[\"```\\n\\nYou can use any suitable `evaluate` metric with the estimators as long as they are compatible w...\"],[\"Logging methods\\n\\n🤗 Evaluate strives to be transparent and explicit about how it works, but this can ...\"],[\"```\\n\\nAll the methods of this logging module are documented below. The main ones are:\\n\\n- [`logging.ge...\"],[\"[[autodoc]] evaluate.logging.enable_progress_bar\\n\\n[[autodoc]] evaluate.logging.disable_progress_bar\\n...\"],[\"Installation\\n\\nBefore you start, you will need to setup your environment and install the appropriate ...\"],[\"--\\ntitle: Word Count\\nemoji: 🤗\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_fi...\"],[\"```\\n\\nExample for a multiple strings\\n```python\\n\\u003e\\u003e\\u003e data = [\\\"hello sun and goodbye moon\\\", \\\"foo bar foo...\"],[\"--\\ntitle: seqeval\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: a...\"],[\"`scheme`: the target tagging scheme, which can be one of [`IOB1`, `IOB2`, `IOE1`, `IOE2`, `IOBES`, `...\"],[\"More recently, seqeval continues being used for reporting performance on tasks such as [named entity...\"],[\"```\\n\\nMinimal values (no match):\\n\\n```python\\n\\u003e\\u003e\\u003e seqeval = evaluate.load('seqeval')\\n\\u003e\\u003e\\u003e predictions = ...\"],[\"```\\n\\nPartial match:\\n\\n```python\\n\\u003e\\u003e\\u003e seqeval = evaluate.load('seqeval')\\n\\u003e\\u003e\\u003e predictions = [['O', 'O', ...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fevaluate\\u002fmedia\\u002fresolve\\u002fmain...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" href=\\\".\\u002ft...\"],[\"--\\ntitle: SARI\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"SARI can be computed as:\\n\\n`sari = ( F1_add + F1_keep + P_del) \\u002f 3`\\n\\nwhere \\n\\n`F1_add` is the n-gram F...\"],[\"```\\n## Output values\\n\\nThis metric outputs a dictionary with the SARI score:\\n\\n```\\nprint(sari_score)\\n{...\"],[\"```\\n\\nPartial match between prediction and reference:\\n\\n```python\\nfrom evaluate import load\\nsari = loa...\"],[\"--\\ntitle: METEOR\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ap...\"],[\"## How to use \\n\\nMETEOR has two mandatory arguments:\\n\\n`predictions`: a `list` of predictions to score...\"],[\"```\\n\\n## Output values\\n\\nThe metric outputs a dictionary containing the METEOR score. Its values range...\"],[\"```\\n\\nMultiple `references` per `prediction`, partial match:\\n\\n```python\\n\\u003e\\u003e\\u003e meteor = evaluate.load('m...\"],[\"```\\n    \\n## Further References \\n- [METEOR -- Wikipedia](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fMETEOR)\\n- [MET...\"],[\"--\\ntitle: CharacTER\\nemoji: 🔤\\ncolorFrom: orange\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file...\"],[\"```\\n\\n### Inputs\\n- **predictions**: a single prediction or a list of predictions to score. Each predi...\"],[\"```\\n\\n## Citation\\n```bibtex\\n@inproceedings{wang-etal-2016-character,\\n    title = \\\"{C}harac{T}er: Tran...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## Submitting a new issue or feature request\\n\\nFollowing these guidelines when submitting an issue or...\"],[\"### Did you find a bug?\\n\\nThank you for reporting an issue. If the bug is related to a community metr...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n   ```bash\\n   $ git checkout -b a-des...\"],[\"```\\n\\n   🤗 Evaluate also uses `flake8` and a few custom scripts to check for coding mistakes. Quality...\"],[\"```\\n\\n6. Once you are satisfied, go to the webpage of your fork on GitHub. Click on 'Pull request' to...\"],[\"### Develop on Windows\\n\\nOn Windows, you need to configure git to transform Windows `CRLF` line endin...\"],[\"```\\n$ git checkout -b your-branch-for-syncing\\n$ git pull --squash --no-commit upstream main\\n$ git co...\"],[\"--\\ntitle: MASE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'mase': array([0.5, 1. ])}\\n```\\n\\n#### Values from Popu...\"],[\"```\\n\\n## Limitations and Bias\\n\\n\\n## Citation(s)\\n\\n```bibtex\\n@article{HYNDMAN2006679,\\n    title = {Anoth...\"],[\"--\\ntitle: sMAPE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app...\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'smape': array([0.5, 1.5 ])}\\n```\\n\\n#### Values from Po...\"],[\"```\\n\\n## Further References\\n- [Symmetric Mean absolute percentage error - Wikipedia](https:\\u002f\\u002fen.wikip...\"],[\"Saving methods\\n\\nMethods for saving evaluations results:\\n\\n## Save\\n\\n[[autodoc]] evaluate.save...\"],[\"--\\ntitle: Exact Match \\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: green\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_f...\"],[\"```\\n\\n```python\\n\\u003e\\u003e\\u003e exact_match = evaluate.load(\\\"exact_match\\\", module_type=\\\"comparison\\\")\\n\\u003e\\u003e\\u003e results ...\"],[\"--\\ntitle: Perplexity\\nemoji: 🤗\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_fi...\"],[\"```\\n\\n### Inputs\\n- **model_id** (str): model used for calculating Perplexity. NOTE: Perplexity can on...\"],[\"```\\n\\nThe range of this metric is [0, inf). A lower score is better.\\n\\n#### Values from Popular Papers...\"],[\"--\\ntitle: Accuracy\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ...\"],[\"```\\n\\nThis metric outputs a dictionary, containing the accuracy score.\\n\\n\\n#### Values from Popular Pap...\"],[\"--\\ntitle: RL Reliability\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_...\"],[\"```\\n\\n\\n### Inputs\\n- **timesteps** *(List[int]): For each run a an list\\u002farray with its timesteps.*\\n- *...\"],[\"### Output Values\\n\\nIn `\\\"online\\\"` mode:\\n- HighFreqEnergyWithinRuns: High Frequency across Time (DT)\\n-...\"],[\"```\\n\\nLoad the sample data:\\n```python\\ndfs = [pd.read_csv(f\\\".\\u002fcsv_data\\u002fsac_humanoid_{i}_train.csv\\\") fo...\"],[\"--\\ntitle: \\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\np...\"],[\"5\\tPart-of-Speech\\n  6\\tParse bit\\tThis is the bracketed structure broken before the first open parenthe...\"],[\"## Metric description\\n\\nCoVal is a coreference evaluation tool for the [CoNLL](https:\\u002f\\u002fhuggingface.co...\"],[\"```python\\nfrom evaluate import load\\ncoval = load('coval')\\nwords = ['bc\\u002fcctv\\u002f00\\u002fcctv_0005   0   0    ...\"],[\"## Examples \\n\\nMaximal values\\n\\n```python\\nfrom evaluate import load\\ncoval = load('coval')\\nwords = ['bc...\"],[\"--\\ntitle: CUAD\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"`references`: a list of question-answer dictionaries with the following key-values:\\n - `id`: the id ...\"],[\"`prec_at_90_recall`: The fraction of true examples among the predicted examples at a recall rate of ...\"],[\"```\\n\\nMinimal values:\\n\\n```python\\nfrom evaluate import load\\ncuad_metric = load(\\\"cuad\\\")\\npredictions = [...\"],[\"```\\n\\nPartial match: \\n\\n```python\\nfrom evaluate import load\\ncuad_metric = load(\\\"cuad\\\")\\npredictions = [...\"],[\"Visualization methods\\n\\nMethods for visualizing evaluations results:\\n\\n## Radar Plot\\n\\n[[autodoc]] eval...\"],[\"--\\ntitle: TER\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"See the README.md file at https:\\u002f\\u002fgithub.com\\u002fmjpost\\u002fsacreBLEU#ter for more information.\\n\\n\\n## How to ...\"],[\"```\\n\\n### Inputs\\nThis metric takes the following as input:\\n- **`predictions`** (`list` of `str`): The...\"],[\"```\\n\\nThe metric can take on any value `0` and above. `0` is a perfect score, meaning the predictions...\"],[\"```\\n\\nExample ignoring punctuation and capitalization, and everything matches:\\n```python\\n\\u003e\\u003e\\u003e predicti...\"],[\"```\\n\\n\\n## Limitations and Bias\\n\\n\\n## Citation\\n```bibtex\\n@inproceedings{snover-etal-2006-study,\\n    tit...\"],[\"--\\ntitle: Toxicity\\nemoji: 🤗\\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ap...\"],[\"```\\n        In this case, the `toxic_label` would be `offensive`.\\n    `aggregation` (optional): dete...\"],[\"```\\n    Example 3 (returns the maximum toxicity score):\\n```python\\n\\u003e\\u003e\\u003e toxicity = evaluate.load(\\\"toxi...\"],[\"Hub methods\\n\\nMethods for using the Hugging Face Hub:\\n\\n## Push to hub \\n\\n[[autodoc]] evaluate.push_to_...\"],[\"--\\ntitle: COMET\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app...\"],[\"```\\n\\nIt has several configurations, named after the COMET model to be used. For versions below 2.0 i...\"],[\"More information about model characteristics can be found on the [COMET website](https:\\u002f\\u002funbabel.git...\"],[\"```\\n\\nPartial match:\\n\\n```python\\nfrom evaluate import load\\ncomet_metric = load('comet') \\nsource = [\\\"De...\"],[\"```\\n\\n## Limitations and bias\\n\\nThe models provided for calculating the COMET metric are built on top ...\"],[\"### Interpreting Scores:\\n\\nWhen using COMET to evaluate machine translation, it's important to unders...\"],[\"```\\n\\n```bibtex\\n@inproceedings{rei-EtAl:2020:WMT,\\n   author    = {Rei, Ricardo  and  Stewart, Craig  ...\"],[\"```\\n\\n## Further References\\n\\n- [COMET website](https:\\u002f\\u002funbabel.github.io\\u002fCOMET\\u002fhtml\\u002findex.html)\\n- [Hu...\"],[\"--\\ntitle: Brier Score\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_fil...\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n## Limitations and Bias\\nThe [brier_score](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002fbrier_score) is appropr...\"],[\"--\\ntitle: Perplexity\\nemoji: 🤗\\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\n\\n### Inputs\\n- **model_id** (str): model used for calculating Perplexity. NOTE: Perplexity can on...\"],[\"```\\n\\nThe range of this metric is [0, inf). A lower score is better.\\n\\n#### Values from Popular Papers...\"],[\"```\\n\\n## Limitations and Bias\\nNote that the output value is based heavily on what text the model was ...\"],[\"Loading methods\\n\\nMethods for listing and loading evaluation modules:\\n\\n## List\\n\\n[[autodoc]] evaluate....\"],[\"Creating and sharing a new evaluation\\n\\n## Setup\\n\\nBefore you can create a new metric make sure you ha...\"],[\"```\\n\\nThis will create a new Space on the 🤗 Hub, clone it locally, and populate it with a template. I...\"],[\"```\\n\\nOr if you need to download the NLTK `\\\"punkt\\\"` resources:\\n\\n```py\\ndef _download_and_prepare(self,...\"],[\"```\\ncd PATH_TO_MODULE\\ngit add .\\ngit commit -m \\\"Add my new, shiny module.\\\"\\ngit push\\n```\\nTada 🎉! Your ...\"],[\"Main classes\\n\\n## EvaluationModuleInfo\\n\\nThe base class `EvaluationModuleInfo` implements a the logic ...\"],[\"--\\ntitle: SacreBLEU\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\nThe score can take any value between `0.0` and `100.0`, inclusive.\\n\\n#### Values from Popular Pap...\"],[\"--\\ntitle: ROC AUC\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: a...\"],[\"This metric has three separate use cases:\\n- **binary**: The case in which there are only two differe...\"],[\"```\\n\\nThe default implementation of this metric is the **binary** implementation. If employing the **...\"],[\"```\\n\\nIn contrast, though, the output takes the following format in the multilabel case when `average...\"],[\"```\\n\\nExample 3, the **multilabel** use case:\\n```python\\n\\u003e\\u003e\\u003e roc_auc_score = evaluate.load(\\\"roc_auc\\\", ...\"],[\"--\\ntitle: NIST_MT\\nemoji: 🤗 \\ncolorFrom: purple\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\n\\n### Inputs\\n- **predictions**: tokenized predictions to score. For sentence-level NIST, a list o...\"]],\"hovertemplate\":\"source=evaluate\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"evaluate, circle\",\"marker\":{\"color\":\"#00cc96\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"evaluate, circle\",\"showlegend\":true,\"x\":[0.3847241,0.2956891,-0.13415313,-4.131564,0.25177455,-0.24952567,0.20995022,0.2021328,0.2662461,-0.18401875,-0.20850122,0.6376831,0.5679475,-0.0076026535,-0.26550925,-0.13214995,-0.072919846,0.265093,-1.0636398,-0.26628265,-0.06318605,0.24567345,-0.46835914,-0.0016505772,0.18571742,-0.27270764,-1.7868563,-0.26868886,0.13890436,0.19777477,-0.53288275,5.2275114,-0.2581244,-0.4202152,-0.2989691,0.28156102,0.054651912,0.13102338,-1.5714465,-0.2611795,0.20774159,0.27386838,-0.23223045,5.546888,12.666055,-5.0666423,5.4050007,-0.20736644,0.5228659,0.27389953,-0.33089933,0.5898261,-0.19661182,5.191356,-0.2789766,0.07554705,0.119516276,0.27841136,-0.8715133,2.5523477,0.13389996,0.058415227,-1.3352896,-0.7958094,-0.6984153,-0.0068527507,5.644299,5.8342743,5.8353877,1.7290395,1.2489201,-0.6634414,-1.702903,5.0709534,-0.063911416,0.26625186,0.40569857,0.33992863,0.25747076,-0.23015486,0.08627772,0.17365603,0.16055061,0.14795855,-0.29681078,0.48057353,0.66962385,0.4290262,0.23918197,0.40283123,0.27783692,0.5057217,0.4575493,1.9276407,0.087895684,0.25268784,0.09004095,0.60348594,0.31569827,-0.0355695,-3.272318,-3.118282,-0.13739708,0.5362298,0.3870712,0.048528418,-0.07520335,-0.12035136,-0.6205691,-0.9172056,-0.54495066,0.037909664,3.670658,-4.338752,5.5880904,0.31129038,0.057428297,0.063023016,-0.077647075,1.9812772,-0.23329778,-0.16172867,0.013557543,-0.16801369,-0.082311794,-0.15898846,-0.34581986,0.057972524,0.13019966,0.19686952,0.2840581,0.21706477,-0.9120328,-0.27865744,-0.07766405,-0.37628308,0.21468729,-0.8539457,0.008512888,-1.1093122,-0.11364409,0.3483495,0.4282024,0.58252555,-0.24743721,0.3344542,0.15372314,0.11858519,-5.5633674,5.114316,5.58885,0.41327903,2.8267398,0.4892008,0.3319786,-0.029432865,-0.38164935,-0.4127433,-0.3208436,-0.36082962,-0.24247997,-0.10950383,-0.04733656,-0.24405044,-0.24521565,-0.017039495,0.2118787,0.18636234,0.11141236,-0.2478742,-0.32215026,-5.337061,-0.027725257,-0.27637988,-1.7343054,-0.26545167,0.14506592,0.23886724,0.298953,-0.96945935,-0.21570075,0.4346936,0.38892576,-4.873279,-0.026643593,-0.123129874,-0.110460214,-0.2238122,0.21471627,0.20133476,-0.20167941,0.4949569,-0.26205853,-0.34285396,-0.379025,-0.13115107,0.31113285,-0.26819918,-0.21121608,0.42547053,-0.3282524,0.1981096,-0.3756909,-0.3677243,0.23069225,-0.92142236,-0.30232647,-0.3618105,-0.4346995,1.1305306,5.1651917,-0.19367945,0.35286242,-0.39016342,-0.20560038,0.34667277,0.76883656,0.12921743,0.1745553,0.45200825,-2.1944764,-0.17789425,0.0565052,-0.29856518,-1.8307785,-0.60919785,0.053473927,-5.266241,-5.139652,-5.9602895,-0.2742804,-0.2762774,-0.14955734,-1.175706,-0.20608811,1.0652131,0.27110654,-0.26604387,0.22683632,-1.0260677,-0.23600233,-0.0021787782,0.5922703,0.4981281,0.24148649,-0.20841552,0.7712141,0.4562807,0.010403174,-0.272462,0.069590636,-0.29507956,0.11280104,0.1934479,0.27277753,1.7468579,1.5224806,1.5627577,2.4016795,-0.24441066,0.04587908,-0.26097888,0.37228,0.10784477,0.27302596,0.20808563,0.8243866,7.012742,6.0115895,-0.2859064,-0.100077055,0.1563228,-0.30629078,-0.343861,0.24584812,0.21958226,-0.6217452,5.1892834,-0.2704529,0.098300055,-1.906413,6.11239,5.30011,3.7653608,4.521769,4.286332,4.969235,4.793944,5.1036587,-0.14518988,0.7754166,0.59040797,0.05591409,-0.20611742,0.6569805,0.55499506,4.679825,0.8269448,-0.2148986,0.13518375,-0.2769389,-1.8188657,0.11120762,-0.18969686,0.15338644,-0.22284466,0.3238033,0.8413528,0.15776289,-0.25494727,-2.89587,-0.5130226,0.24100837,0.21650743,-0.24888363,0.19598435,-0.045115266,0.32415769,0.22749583,1.022206,-0.26734164,-0.18462433,-0.31018978,-0.066015854,-0.11656243,-1.7718785,-0.2497498,0.07798219,-0.42780432,5.0218635,-0.2710672,-2.36346,-0.63164145,0.16689375,-1.015996,-1.2271686,-1.6193461,7.148344,-0.22033755,0.46042037,-0.16082191,-0.27662078,-1.7461538,0.17295294,-1.5586424,1.2194471,3.141273,6.87383,0.30539298,4.7751718,0.64428127,-0.25480127,-0.19472443,-0.09254633,0.42544904,0.41230127,0.42035672,0.3751255,-0.2463606,-0.8724879],\"xaxis\":\"x\",\"y\":[-6.217767,-7.5561433,-6.473603,-1.7155519,-6.974803,-9.705827,-7.6177154,-7.6196733,-7.6520257,-7.350943,-9.863855,-7.3894033,-7.7913933,-6.6703296,-9.848872,-7.7838116,-7.7765393,-7.6434426,-7.0396237,-9.583009,-7.060062,-5.720884,-1.2262669,-4.733286,-6.2980914,-9.76841,-7.2929187,-9.846528,-7.460765,-7.7958694,-7.8854733,-4.2609596,-9.822274,-7.8156033,-7.872145,-7.6939974,-7.409687,-7.6360617,-7.003127,-9.798255,-7.646372,-7.643867,-7.065986,-4.972917,16.199219,0.47901738,-4.233702,-9.830962,-6.9195395,-7.2425084,-6.486312,-6.9011817,-6.6293774,-4.3570037,-9.780693,-7.4545393,-7.7127066,-7.9168463,-7.0205903,0.8075979,-6.098393,0.36553487,-4.108654,-4.3464966,0.18448962,-5.847595,-0.012143662,-1.6132703,-1.372616,-0.95370656,-1.4366151,-3.7901752,-3.9049373,-0.7207208,-9.183707,-7.620665,-7.517844,-7.665133,-6.81914,-9.813726,-6.293029,-6.2775493,-5.688287,-5.5740414,-7.139176,-5.80375,-5.1176376,-5.7924047,-7.5128407,-6.0355043,-6.1032314,-6.1013055,-6.500729,-3.4042754,-5.3493285,-6.0550933,-4.8784537,-5.4798956,-4.9861846,-5.3095317,-3.0950093,-3.262158,-9.570659,-7.407773,-7.165615,-4.6505275,-4.8870006,-4.779057,-3.284133,1.4705564,-4.4782324,-4.0139375,-4.773454,1.2254734,-5.609577,-4.7284746,-4.690274,-4.546591,-4.465115,-4.386231,-9.900678,-7.980585,-8.016868,-8.05375,-8.010286,-9.600588,-9.164915,-7.224919,-7.763058,-7.723826,-8.222783,-8.282893,-6.6789517,-9.877024,-7.650767,-7.8102136,-6.087582,-7.584868,-7.7357745,-7.4683833,-9.50913,-7.6093388,-7.6751113,-7.402972,-9.801996,-6.1745315,-8.123502,-6.538478,0.6985746,-4.2214127,-1.9692167,-5.9845676,0.31916058,-5.6603827,-6.0950065,-6.0708456,-8.698179,-8.303814,-8.257863,-8.466755,-9.772881,-7.909167,-7.9940457,-7.814408,-9.774803,-7.1224847,-7.234071,-7.105426,-7.290195,-9.854218,-6.565435,1.7891486,-6.8524165,-6.1014323,-7.0271564,-9.490037,-7.9149914,-8.238673,-8.380527,-6.614213,-9.861638,-7.67232,-7.591408,-1.4830903,-7.4322553,-7.4194546,-7.7189093,-9.856947,-7.8387814,-8.007353,-9.868706,-7.8808784,-9.804923,-8.160148,-8.273128,-9.434969,-6.845299,-7.2228055,-9.928041,-7.626161,-9.492092,-7.7245326,-8.082613,-8.277758,-7.70737,-7.314212,-8.815858,-8.361712,-8.286752,-4.0352187,-3.8883972,-9.491892,-7.644324,-7.268878,-9.858871,-7.687148,-5.816148,-6.29063,-6.280863,-6.2325654,2.4610753,-9.635459,-7.257318,-9.618908,2.2982302,-6.3030314,-8.142663,0.2944846,-1.6996844,-0.41498613,-9.784807,-7.663281,-7.6130404,-7.307639,-9.669418,-5.7819653,-6.37465,-9.788192,-7.666319,-6.9863334,-9.73113,-6.705125,-5.1829886,-4.536664,-5.053396,-9.883172,-7.546469,-7.9906073,-7.065996,-9.83227,-7.515453,-8.000936,-5.4671683,-6.4442663,-6.07738,-0.6920865,-0.4045715,-0.4999481,0.18120223,-9.7394495,-7.0651584,-9.584202,-6.703004,-7.1191244,-7.9568644,-8.355693,-5.8679495,-4.696916,-4.44841,-9.42143,-7.26623,-7.6554146,-8.172588,-9.738723,-7.16737,-7.523695,-7.788699,-4.5173316,-9.859958,-7.602195,-7.2787905,-0.45970774,-2.5758855,-1.0170032,-0.10601697,-0.15994127,-1.1633788,-0.4047723,-0.41382384,-9.781662,-7.049103,-7.893273,-6.8600683,-9.875724,-7.354396,-7.802684,-5.445222,-4.4221787,-9.787886,-8.015592,-9.630634,-5.6865892,-7.7216806,-9.74721,-7.617855,-9.846002,-4.3893785,-3.977013,-6.3694067,-9.818669,-5.036828,-7.640973,-7.6296773,-7.4445896,-9.759587,-7.9309454,-7.509785,-8.014452,-8.292782,-4.745846,-9.723608,-7.827781,-7.918103,-7.927779,-7.886722,-7.1971593,-9.7686615,-6.4046245,-6.1722646,-0.65040654,-9.851329,2.1168861,-7.232824,-8.377217,-7.3325343,-7.4072175,-7.273573,-2.0453372,-9.824735,-7.3823376,-6.952968,-9.6631,-5.7064166,-7.8109794,-6.1421566,-4.256652,-1.6227068,-2.2152066,-5.5633044,-0.64634997,-5.339074,-9.813858,-8.03795,-9.254558,-7.211562,-7.077075,-7.259405,-7.24555,-9.865978,-7.6642957],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"主要特点\\n\\n让我们来介绍一下 Gradio 最受欢迎的一些功能！这里是 Gradio 的主要特点：\\n\\n1. [添加示例输入](#example-inputs)\\n2. [传递自定义错误消息](#erro...\"],[\"继续了解示例，请参阅[更多示例](https:\\u002f\\u002fgradio.app\\u002fmore-on-examples)指南。\\n\\n## 错误\\n\\n您希望向用户传递自定义错误消息。为此，with `gr.Error(\\\"...\"],[\"另一个有用的关键字参数是 `label=`，它存在于每个 `Component` 中。这修改了每个 `Component` 顶部的标签文本。还可以为诸如 `Textbox` 或 `Radio` 之类的...\"],[\"```\\n\\n## 旗标\\n\\n默认情况下，\\\"Interface\\\" 将有一个 \\\"Flag\\\" 按钮。当用户测试您的 `Interface` 时，如果看到有趣的输出，例如错误或意外的模型行为，他们可以将输入标记为...\"],[\"```\\n\\n_flagged\\u002flogs.csv_\\n\\n```csv\\nim,Output\\nim\\u002f0.png,Output\\u002f0.png\\nim\\u002f1.png,Output\\u002f1.png\\n```\\n\\n如果您希望用户提供...\"],[\"```\\n\\n相反，这里我们保留图像的原始大小，但在将其转换为 numpy 数组之前反转颜色：\\n\\n```py\\nimg = gr.Image(invert_colors=True, type=\\\"numpy\\\"...\"],[\"```\\n\\n## 队列 (Queuing)\\n\\n如果您的应用程序预计会有大量流量，请 with `queue()` 方法来控制处理速率。这将排队处理调用，因此一次只处理一定数量的请求。队列使用 Webso...\"],[\"```\\n\\n## 迭代输出 (Iterative Outputs)\\n\\n在某些情况下，您可能需要传输一系列输出而不是一次显示单个输出。例如，您可能有一个图像生成模型，希望显示生成的每个步骤的图像，直到最终...\"],[\"```\\n\\n您以与常规函数相同的方式将生成器提供给 Gradio。例如，这是一个（虚拟的）图像生成模型，它在输出图像之前生成数个步骤的噪音：\\n\\n$code_fake_diffusion\\n$demo_fa...\"],[\"Gradio 支持传递*批处理*函数。批处理函数只是接受输入列表并返回预测列表的函数。\\n\\n例如，这是一个批处理函数，它接受两个输入列表（一个单词列表和一个整数列表），并返回修剪过的单词列表作为输出：\\n...\"],[\"```\\n\\n使用批处理函数的优点是，如果启用了队列，Gradio 服务器可以自动*批处理*传入的请求并并行处理它们，从而可能加快演示速度。以下是 Gradio 代码的示例（请注意 `batch=True...\"],[\"```\\n\\n在上面的示例中，可以并行处理 16 个请求（总推理时间为 5 秒），而不是分别处理每个请求（总推理时间为 80 秒）。许多 Hugging Face 的 `transformers` 和 `...\"],[\"## Gradio 笔记本 (Colab Notebooks)\\n\\nGradio 可以在任何运行 Python 的地方运行，包括本地 Jupyter 笔记本和协作笔记本，如[Google Colab](...\"],[\"Gradio Demo: blocks_random_slider\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n\\nimport gradio as gr\\n\\n\\ndef...\"],[\"State in Blocks\\n\\nWe covered [State in Interfaces](https:\\u002f\\u002fgradio.app\\u002finterface-state), this guide ta...\"],[\"如何使用地图组件绘制图表\\n\\nRelated spaces:\\nTags: PLOTS, MAPS\\n\\n## 简介\\n\\n本指南介绍如何使用 Gradio 的 `Plot` 组件在地图上绘制地理数据。Gradi...\"],[\"dataset = load_dataset(\\\"gradio\\u002fNYC-Airbnb-Open-Data\\\", split=\\\"train\\\")\\ndf = dataset.to_pandas()\\n\\ndef f...\"],[\"```\\n\\n在上面的代码中，我们先将 CSV 数据加载到一个 pandas dataframe 中。让我们首先定义一个函数，这将作为 gradio 应用程序的预测函数。该函数将接受最低价格、最高价格范围...\"],[\"```\\n\\n上面的代码中，我们通过传入经纬度列表来创建一个散点图。我们还传入了名称和价格的自定义数据，以便在鼠标悬停在每个标记上时显示额外的信息。接下来，我们使用 `update_layout` 来指定...\"],[\"```\\n\\n我们使用 `gr.Column` 和 `gr.Row` 布局这些组件，并为演示加载时和点击 \\\" 更新筛选 \\\" 按钮时添加了事件触发器，以触发地图更新新的筛选条件。\\n\\n以下是完整演示代码：\\n\\n...\"],[\"Gradio Demo: examples_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the...\"],[\"Gradio Demo: number_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr....\"],[\"Gradio Demo: map_airbnb\\n### Display an interactive map of AirBnB locations with Plotly. Data is host...\"],[\"```\\nimport gradio as gr\\nimport plotly.graph_objects as go\\nfrom datasets import load_dataset\\n\\ndataset...\"],[\"return fig\\n\\nwith gr.Blocks() as demo:\\n    with gr.Column():\\n        with gr.Row():\\n            min_p...\"],[\"Gradio Demo: question-answering\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\nimport gra...\"],[\"`@gradio\\u002fbutton`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { Button } from \\\"@gradio\\u002fbutton\\\";\\n\\u003c\\u002fscript\\u003e\\n\\n\\u003cbutton type...\"],[\"Gradio Demo: sales_projections\\n\\n\\n```\\n!pip install -q gradio pandas numpy matplotlib\\n```\\n\\n\\n```\\nimport...\"],[\"Gradio and W&B Integration\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fakhaliq\\u002fJoJoGAN\\nTags: WAND...\"],[\"Get started [here](https:\\u002f\\u002fgradio.app\\u002fgetting_started)\\n\\n### Hugging Face Spaces\\n\\nHugging Face Spaces...\"],[\"```\\n\\n3. Finetune StyleGAN and W&B experiment tracking\\n\\n   This next step will open a W&B dashboard t...\"],[\"for idx in tqdm(range(num_iter)):\\n       mean_w = generator.get_latent(torch.randn([latents.size(0),...\"],[\"```\\n\\nalpha = 1.0\\nalpha = 1-alpha\\n\\npreserve_color = True\\nnum_iter = 100\\nlog_interval = 50\\n\\nsamples = ...\"],[\"if preserve_color:\\nid_swap = [9,11,15,16,17]\\nelse:\\nid_swap = list(range(7, generator.n_latent))\\n\\nfor...\"],[\"g_optim.zero_grad()\\n    loss.backward()\\n    g_optim.step()\\n\\nout_table = wandb.Table(data=samples, co...\"],[\"```\\n\\n4. Save, Download, and Load Model\\n\\n    Here's how to save and download your model.\\n\\n```python\\n\\n...\"],[\"plt.rcParams['figure.dpi'] = 150\\n\\n\\n\\ntransform = transforms.Compose(\\n    [\\n        transforms.Resize(...\"],[\"```\\n\\n5. Build a Gradio Demo\\n\\n```python\\n\\nimport gradio as gr\\n\\ntitle = \\\"JoJoGAN\\\"\\ndescription = \\\"Gradio...\"],[\"```\\n\\n7. (Optional) Embed W&B plots in your Gradio App\\n\\n   It's also possible to embed W&B plots with...\"],[\"```\\n\\n## Conclusion\\n\\nWe hope you enjoyed this brief demo of embedding a Gradio demo to a W&B report! ...\"],[\"Gradio Demo: duplicatebutton_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n...\"],[\"Gradio Demo: upload_button_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    \\n    with gr.Row():\\n        with gr.Col...\"],[\"@gradio\\u002fimageeditor\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#6809](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f680...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"- Updated dependencies [[`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002f6a9151d5c9432c724098...\"],[\"[`5177132`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002f5177132d718c77f6d47869b4334afae6380394cb)]:...\"],[\"- @gradio\\u002fimage@0.5.0\\n  - @gradio\\u002fupload@0.5.3\\n  - @gradio\\u002fclient@0.9.0\\n  - @gradio\\u002fwasm@0.4.0\\n  - @...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`b639e04`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"A brand new component, completely separate from `Image` that provides simple editing capabilities.\\n\\n...\"],[\"```py\\n\\ndef fn(im):\\n    im[\\\"composite\\\"] # the full canvas\\n    im[\\\"background\\\"] # the background image...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Fixes\\n\\n- [#6502](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"Gradio Demo: chatinterface_system_prompt\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr...\"],[\"Gradio Demo: streaming_wav2vec\\n\\n\\n```\\n!pip install -q gradio torch transformers \\n```\\n\\n\\n```\\nfrom trans...\"],[\"component-styles\\n\\n## Textbox\\n\\n| name        | type                                 | description    ...\"],[\"## Checkbox\\n\\n| name        | type                                 | description                    |...\"],[\"## Dropdown\\n\\n| name        | type                                 | description                    |...\"],[\"## Audio\\n\\n| name      | type                                 | description         |\\n| --------- | -...\"],[\"## Label\\n\\n| name        | type   | description                    |\\n| ----------- | ------ | -------...\"],[\"## HTML\\n\\nNothing\\n\\n## Gallery\\n\\n| name        | type                                      | descriptio...\"],[\"## Plot\\n\\nNothing (yet)\\n\\n## Markdown\\n\\nNothing\\n\\n## Button\\n\\n| name         | type                      ...\"],[\"Gradio Demo: blocks_webcam\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport numpy as np\\n\\nimport gradio...\"],[\"Gradio Demo: on_listener_live\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.B...\"],[\"主题 Theming\\n\\nTags: THEMES\\n\\n## 介绍\\n\\nGradio 具有内置的主题引擎，可让您自定义应用的外观和感觉。您可以选择各种主题，或者创建自己的主题。要这样做，请将 `theme=...\"],[\"```\\n\\n$demo_theme_builder\\n\\n您可以使用上面的 Spaces 上运行的 Theme Builder，但通过 `gr.themes.builder()` 在本地启动时运行速度更快。...\"],[\"3 个颜色构造函数参数是：\\n\\n- `primary_hue`：这是主题中的主色。在默认主题中，此值设置为 `gradio.themes.colors.orange`。\\n- `secondary_hue...\"],[\"```\\n\\n或者直接使用 `Color` 对象，如下所示：\\n\\n```python\\nwith gr.Blocks(theme=gr.themes.Default(primary_hue=gr.themes...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-1.hf.space?__theme=light...\"],[\"```\\n\\n或者直接使用 `Size` 对象，如下所示：\\n\\n```python\\nwith gr.Blocks(theme=gr.themes.Default(spacing_size=gr.themes...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-2.hf.space?__theme=light...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-3.hf.space?__theme=light...\"],[\"```\\n\\n在上面的示例中，我们将 `loader_color` 和 `slider_color` 变量设置为`#FF0000`，尽管整体 `primary_color` 使用蓝色调色板。您可以以这种方...\"],[\"### CSS 变量组织\\n\\n虽然有数百个 CSS 变量，但并不需要为每个变量都指定单独的值。它们通过引用一组核心变量和彼此引用来获取值。这样做可以仅修改少量变量以改变整个主题的外观和感觉，同时也可以更...\"],[\"```\\n\\n在上面的示例中，我们将 `button_primary_background_fill` 和 `button_primary_background_fill_hover` 变量分别设置为`*...\"],[\"```\\n\\n现在，如果我们更改 `button_primary_background_fill` 变量，`button_primary_background_fill_hover` 和 `button_...\"],[\"```\\n\\n`button_primary_border_dark` 将从 `button_primary_background_fill_dark` 获取其值，因为暗模式总是使用变量的暗版本。\\n\\n##...\"],[\"\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-new-step-2.hf.space?__theme=light\\\"\\n\\tframebo...\"],[\"在创建主题后，您可以将其上传到 HuggingFace Hub，让其他人查看、使用和构建主题！\\n\\n### 上传主题\\n\\n有两种上传主题的方式，通过主题类实例或命令行。我们将使用之前创建的“seafoam...\"],[\"```\\n\\n- 通过命令行\\n\\n首先将主题保存到磁盘\\n\\n```python\\nseafoam.dump(filename=\\\"seafoam.json\\\")\\n```\\n\\n然后使用“upload_theme”命令：...\"],[\"```\\n\\n要上传主题，您必须拥有一个 HuggingFace 账户，并通过 `hf_token` 参数传递您的[访问令牌](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingfac...\"],[\"### 发现主题\\n\\n[主题库](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002ftheme-gallery)显示了所有公开的 gradio 主题。在发布主题之后，\\n它将在几分...\"],[\"```\\n\\n您也可以直接将主题字符串传递给 `Blocks` 或 `Interface`（`gr.Blocks(theme=\\\"gradio\\u002fseafoam\\\")`）\\n\\n您可以通过使用语义版本表达式将您的应...\"],[\"his demo shows how you can build an interactive dashboard with gradio. Click on a python library on ...\"],[\"gradio\\n\\n## 4.11.0\\n\\n### Features...\"],[\"- [#6842](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6842) [`846d52d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6809](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6809) [`1401d99`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6833](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6833) [`1b9d423`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"### Fixes\\n\\n- [#6829](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6829) [`50496f9`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 4.10.0\\n\\n### Features\\n\\n- [#6798](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6798) [`245d58e`](https...\"],[\"### Fixes\\n\\n- [#6799](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6799) [`c352811`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 4.9.1\\n\\n### Features\\n\\n- [#6781](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6781) [`a807ede`](https:...\"],[\"### Fixes\\n\\n- [#6525](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6525) [`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"- [#6726](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6726) [`21cfb0a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6745](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6745) [`3240d04`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6671](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6671) [`299f5e2`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6666](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6666) [`30c9fbb`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6704](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6704) [`24e0481`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6416](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6416) [`5177132`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"### Fixes...\"],[\"- [#6709](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6709) [`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6676](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6676) [`fe40308`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6639](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6639) [`9a6ff70`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6694](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6694) [`dfc61ec`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6759](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6759) [`28a7aa9`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"## 4.8.0\\n\\n### Features...\"],[\"- [#6624](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6624) [`1751f14`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6565](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6565) [`9bf1ad4`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6607](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6607) [`13ace03`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6572](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6572) [`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6551](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6551) [`8fc562a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"## 4.7.1\\n\\n### Features\\n\\n- [#6537](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6537) [`6d3fecfa4`](http...\"],[\"- [#6532](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6532) [`96290d304`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6523](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6523) [`63f466882`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6538](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6538) [`147926196`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6528](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6528) [`f53b01cbf`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6500](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6500) [`830b6c0e6`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.5.0\\n\\n### Highlights\\n\\n#### New `ImageEditor` component ([#6169](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"```\\n\\n Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Fixes\\n\\n- [#6497](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#6428](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6428) [`ac4ca59c9`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6455](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6455) [`179f5bcde`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6423](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6423) [`62d35c3d1`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6419](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6419) [`1959471a8`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6441](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6441) [`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6457](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6457) [`d00fcf89d`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.3.0\\n\\n### Features...\"],[\"- [#6395](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6395) [`8ef48f852`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6099](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6099) [`d84209703`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6412](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6412) [`649f3ceb6`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6383](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6383) [`324867f63`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6414](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6414) [`da1e31832`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.2.0\\n\\n### Features...\"],[\"- [#6333](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6333) [`42f76aeeb`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6356](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6356) [`854b482f5`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#6368](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6368) [`8a3f45c26`](https:\\u002f\\u002fgithub.co...\"],[\"## 4.1.2\\n\\n### Features\\n\\n- [#6318](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6318) [`d3b53a457`](http...\"],[\"- [#6310](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6310) [`dfdaf1092`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6317](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6317) [`19af2806a`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6311](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6311) [`176c4d140`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6309](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6309) [`c56128781`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.1.1\\n\\n### Fixes\\n\\n- [#6288](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6288) [`92278729e`](https:\\u002f...\"],[\"- [#6261](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6261) [`8bbeca0e7`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6240](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6240) [`dd901c1b0`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6232](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6232) [`ac4f2bcde`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6266](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6266) [`e32bac894`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6236](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6236) [`6bce259c5`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6249](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6249) [`2cffcf3c3`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6211](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6211) [`a4a931dd3`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.0.2\\n\\n### Fixes\\n\\n- [#6191](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6191) [`b555bc09f`](https:\\u002f...\"],[\"## 4.0.0\\n\\n### Highlights\\n\\n4.0 is a big release, so here are the main highlights:\\n\\n**1. Custom Compon...\"],[\"\\u003cimg style=\\\"width:50%\\\" src=\\\"https:\\u002f\\u002fi.imgur.com\\u002fewUIuUc.png\\\"\\u003e\\n\\n**4. Custom Share Servers**: \\n\\nGradio...\"],[\"Gradio 4.0 is a new major version, and includes breaking changes from 3.x. Here's a list of all the ...\"],[\"**Other changes related to the `gradio` library**:\\n\\n* Removes the deprecated `status_tracker` parame...\"],[\"### Migrating to Gradio 4.0\\n\\nHere are some concrete tips to help migrate to Gradio 4.0:\\n\\n#### **Usin...\"],[\"```\\n\\nIn order for the HTML component to be able to serve `image.png`, you will need to add `image.pn...\"],[\"```\\n\\n\\n#### **Using `concurrency_limit` instead of `concurrency_count`**\\n\\nPreviously, in Gradio 3.x, ...\"],[\"To summarize migration:\\n\\n* For events that execute quickly or don't use much CPU or GPU resources, y...\"],[\"```\\n\\nNow, you should write:\\n\\n```py\\ngr.ImageEditor(sources=(), brush=gr.Brush(colors=[\\\"#000000\\\"]))\\n``...\"],[\"```\\n\\nUnlike the `Image` component, which passes the input image as a single value into the predictio...\"],[\"- [#6184](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6184) [`86edc0199`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6153](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6153) [`1162ed621`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6152](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6152) [`982bff2fd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6135](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6135) [`bce37ac74`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6098](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6098) [`c3bc515bf`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6129](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6129) [`0d261c6ec`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6082](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6082) [`037e5af33`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6014](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6014) [`cad537aac`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6018](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6018) [`184834d02`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6114](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6114) [`39227b6fa`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6089](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6089) [`cd8146ba0`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6027](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6027) [`de18102b8`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6044](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6044) [`9053c95a1`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6146](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6146) [`40a171ea6`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5826](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5826) [`ce036c5d4`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6076](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6076) [`f3f98f923`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.45.0-beta.13\\n\\n### Features\\n\\n- [#5964](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5964) [`5fbda0b...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`d2314e53b`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5938](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5938) [`13ed8a485`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5894](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5894) [`fee3d527e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.48.0\\n\\n### Features...\"],[\"- [#5627](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5627) [`b67115e8e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5915](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5915) [`e24163e15`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5819](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5819) [`5f1cbc436`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5864](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5864) [`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5840](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5840) [`4e62b8493`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5904](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5904) [`891d42e9b`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5890](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5890) [`c4ba832b3`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5930](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5930) [`361823896`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.47.1\\n\\n### Fixes\\n\\n- [#5816](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5816) [`796145e2c`](https:...\"],[\"For more information check the [`FileExplorer` documentation](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002ffileexplorer)....\"],[\"- [#5798](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5798) [`a0d3cc45c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5794](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5794) [`f096c3ae1`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.46.1\\n\\n### Features\\n\\n- [#5124](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5124) [`6e56a0d9b`](htt...\"],[\"## 3.46.0\\n\\n### Features\\n\\n- [#5699](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5699) [`8f0fed857`](htt...\"],[\"- [#5735](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5735) [`abb5e9df4`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5731](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5731) [`c9af4f794`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.45.2\\n\\n### Features\\n\\n- [#5722](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5722) [`dba651904`](htt...\"],[\"- [#5714](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5714) [`a0fc5a296`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5705](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5705) [`78e7cf516`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5726](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5726) [`96c4b97c7`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.45.1\\n\\n### Fixes\\n\\n- [#5701](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5701) [`ee8eec1e5`](https:...\"],[\"- [#5675](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5675) [`b619e6f6e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5681](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5681) [`40de3d217`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5652](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5652) [`2e25d4305`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5660](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5660) [`d76555a12`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5240](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5240) [`da05e59a5`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5590](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5590) [`d1ad1f671`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5625](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5625) [`9ccc4794a`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5633](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5633) [`341402337`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5593](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5593) [`88d43bd12`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.44.4\\n\\n### Features\\n\\n- [#5514](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5514) [`52f783175`](htt...\"],[\"### Fixes\\n\\n- [#5587](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5587) [`e0d61b8ba`](https:\\u002f\\u002fgithub.co...\"],[\"- [#5562](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5562) [`50d9747d0`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5553](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5553) [`d1bf23cd2`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.44.2\\n\\n### Fixes\\n\\n- [#5537](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5537) [`301c7878`](https:\\u002f...\"],[\"- [#5516](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5516) [`c5fe8eba`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5525](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5525) [`21f1db40`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.44.0\\n\\n### Features...\"],[\"- [#5505](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5505) [`9ee20f49`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5488](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5488) [`8909e42a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5474](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5474) [`041560f9`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5459](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5459) [`bd2fda77`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5496](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5496) [`82ec4d26`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.43.2\\n\\n### Fixes\\n\\n- [#5456](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5456) [`6e381c4f`](https:\\u002f...\"],[\"- [#5165](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5165) [`c77f05ab`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5417](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5417) [`d14d63e3`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes\\n\\n- [#5412](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5412) [`26fef8c7`](https:\\u002f\\u002fgithub.com...\"],[\"Thanks [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82)!\\n\\n#### Added the ability to attach event lis...\"],[\"```\\n\\n Thanks [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94)!\\n\\n### Features...\"],[\"- [#5334](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5334) [`c5bf9138`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5370](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5370) [`61803c65`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5369](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5369) [`b8968898`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5304](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5304) [`05892302`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5394](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5394) [`4d94ea0a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.41.2\\n\\n### Features\\n\\n- [#5284](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5284) [`5f25eb68`](http...\"],[\"## 3.41.1\\n\\n### Fixes\\n\\n- [#5324](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5324) [`31996c99`](https:\\u002f...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"```\\n\\n Thanks [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton)!\\n\\n#### Add `render` function to `\\u003c...\"],[\"```\\n\\n Thanks [@hannahblair](https:\\u002f\\u002fgithub.com\\u002fhannahblair)!\\n\\n### Features...\"],[\"- [#5268](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5268) [`f49028cf`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5283](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5283) [`a7460557`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5280](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5280) [`a2f42e28`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#4943](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4943) [`947d615d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5188](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5188) [`b22e1888`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5305](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5305) [`15075241`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5264](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5264) [`46a2b600`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5256](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5256) [`933db53e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5179](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5179) [`6fb92b48`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5122](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5122) [`3b805346`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5231](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5231) [`87f1c2b4`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5235](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5235) [`1ecf88ac`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.40.0\\n\\n### Highlights\\n\\n#### Client.predict will now return the final output for streaming endpoi...\"],[\"```python\\nimport gradio as gr\\nfrom pydub import AudioSegment\\n\\ndef stream_audio(audio_file):\\n    audi...\"],[\"```\\n\\nFrom the backend, streamed outputs are served from the `\\u002fstream\\u002f` endpoint instead of the `\\u002ffil...\"],[\"- [#5081](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5081) [`d7f83823`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5125](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5125) [`80be7a1c`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5046) [`5244c587`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5047](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5047) [`883ac364`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5104](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5104) [`34f6b22e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5035](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5035) [`8b4eb8ca`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5080](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5080) [`37caa2e0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5062](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5062) [`7d897165`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5114](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5114) [`56d2609d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5039](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5039) [`620e4645`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5140](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5140) [`cd1353fa`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.39.0\\n\\n### Highlights\\n\\n#### Create Discord Bots from Gradio Apps 🤖 ([#4960](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"```\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fgradio-builds.s3.amazonaws.com\\u002fdemo-files\\u002fdiscordbots\\u002fguide\\u002fllama_chat.gif\\\"\\u003e\\n...\"],[\"But once again, you can deploy ANY `gr.ChatInterface` app exposed on the internet! So don't hesitate...\"],[\"- [#4995](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4995) [`3f8c210b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#4985](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4985) [`b74f8453`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes\\n\\n- [#4997](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4997) [`41c83070`](https:\\u002f\\u002fgithub.com...\"],[\"- Provide a parameter `animate` (`False` by default) in `gr.make_waveform()` which animates the over...\"],[\"- Add default sketch color argument `brush_color`. Also, masks drawn on images are now slightly tran...\"],[\"### Bug Fixes:\\n\\n- Fixes `cancels` for generators so that if a generator is canceled before it is com...\"],[\"## 3.37\\n\\n### New Features:\\n\\nIntroducing a new `gr.ChatInterface` abstraction, which allows Gradio us...\"],[\"```\\n\\nWhich produces:\\n\\n\\u003cimg width=\\\"1291\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fassets...\"],[\"- Chatbot messages now show hyperlinks to download files uploaded to `gr.Chatbot()` by [@dawoodkhan8...\"],[\"```\\n\\n```py\\nwith gr.Blocks() as demo:\\n    gr.Markdown(\\\"سلام\\\", rtl=True)\\ndemo.launch()...\"],[\"```\\n\\n- The `get_api_info` method of `Blocks` now supports layout output components [@freddyaboulton]...\"],[\"* Add missing `display: flex` property to `Row` so that flex styling is applied to children by [@han...\"],[\"### Other Changes:\\n\\n- Warning on mobile that if a user leaves the tab, websocket connection may brea...\"],[\"## 3.36.1\\n\\n### New Features:\\n\\n- Hotfix to support pydantic v1 and v2 by [@freddyaboulton](https:\\u002f\\u002fgi...\"],[\"No changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.36.0\\n\\n### New Featur...\"],[\"### Bug Fixes:...\"],[\"- Updated components with `info` attribute to update when `update()` is called on them. by [@jebarpg...\"],[\"- Fix `make_waveform` to work with paths that contain spaces [@akx](https:\\u002f\\u002fgithub.com\\u002fakx) in [PR 4...\"],[\"- Ensure that Gradio does not silently fail when running on a port that is occupied by [@abidlabs](h...\"],[\"- Removed uncessessary `type` deprecation warning by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyabou...\"],[\"- Fix bug where `show_label` was hiding the entire component for `gr.Label` by [@freddyaboulton](htt...\"],[\"### Other Changes:...\"],[\"- Add `.git-blame-ignore-revs` by [@akx](https:\\u002f\\u002fgithub.com\\u002fakx) in [PR 4586](https:\\u002f\\u002fgithub.com\\u002fgra...\"],[\"- Better errors when you define two Blocks and reference components in one Blocks from the events in...\"],[\"### Breaking Changes:\\n\\n[PR 4683](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4683) removes the explict...\"],[\"### Other Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3....\"],[\"- Min and max value for gr.Number by [@artegoser](https:\\u002f\\u002fgithub.com\\u002fartegoser) and [@dawoodkhan82](...\"],[\"- Add `latex_delimiters` parameter to `Chatbot` to control the delimiters used for LaTeX and to disa...\"],[\"Example:\\n\\n```python\\ndef start_process(name):\\n    gr.Info(\\\"Starting process\\\")\\n    if name is None:\\n  ...\"],[\"```\\n\\n### Bug Fixes:...\"],[\"- Add support for PAUSED state in the JS client by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 4...\"],[\"- Fix new line issue with `gr.Chatbot()` by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR ...\"],[\"- Fix dispatched errors from within components [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 478...\"],[\"### Other Changes:\\n\\n- Change styling of status and toast error components by [@hannahblair](https:\\u002f\\u002f...\"],[\"### Breaking Changes:\\n\\n- The behavior of the `Clear` button has been changed for `Slider`, `Checkbox...\"],[\"- Remove target=\\\"\\\\_blank\\\" override on anchor tags with internal targets by [@hannahblair](https:\\u002f\\u002fgi...\"],[\"- Fix video rendering in Safari by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 4433](https:\\u002f\\u002fg...\"],[\"### Other Changes:\\n\\n- When running on Spaces, handler functions will be transformed by the [PySpaces...\"],[\"No changes to highlight.\\n\\n## 3.33.1\\n\\n### New Features:\\n\\nNo changes to highlight.\\n\\n### Bug Fixes:\\n\\n- ...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.33.0\\n\\n### New Features:\\n\\n- Introduced `gradio ...\"],[\"- Fix bug where Label change event was triggering itself by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffre...\"],[\"- Do not send HF token to other domains via `\\u002fproxy` route by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlab...\"],[\"### Other Changes:\\n\\n- Remove flicker of loading bar by adding opacity transition, by [@aliabid94](ht...\"],[\"### Bug Fixes:\\n\\n- Fixed Gallery\\u002fAnnotatedImage components not respecting GRADIO_DEFAULT_DIR variable...\"],[\"### Other Changes:\\n\\n- Refactor web component `initial_height` attribute by [@whitphx](https:\\u002f\\u002fgithub...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.31.0\\n\\n### New Features:\\n\\n- The reloader comman...\"],[\"- Fix \\\"TypeError: issubclass() arg 1 must be a class\\\" When use Optional[Types] by [@lingfengchencn](...\"],[\"- Fixes a bug with typing.get_type_hints() on Python 3.9 by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs)...\"],[\"### Other Changes:\\n\\n- Change `gr.Chatbot()` markdown parsing to frontend using `marked` library and ...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.30.0\\n\\n### New Features:\\n\\n- Adds a `root_path` ...\"],[\"### Bug Fixes:\\n\\n- Records username when flagging by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR ...\"],[\"- Allow users to upload audio files in Audio component on iOS by by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"- Fix `gr.Slider` `release` event not triggering on mobile by [@space-nuko](https:\\u002f\\u002fgithub.com\\u002fspace...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"### New Features:\\n\\n- Add support for `visual-question-answering`, `document-question-answering`, and...\"],[\"### Bug Fixes:\\n\\n- Fixes issue with `matplotlib` not rendering correctly if the backend was not set t...\"],[\"### Full Changelog:\\n\\n- Safer version of `gr.HuggingFaceDatasetSaver` using HTTP methods instead of g...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\n- CI:...\"],[\"- Fix duplicate play commands in full-screen mode of 'video'. by [@tomchang25](https:\\u002f\\u002fgithub.com\\u002fto...\"],[\"- Fix issue in `gr.Gallery()` where setting height causes aspect ratio of images to collapse by [@da...\"],[\"- Fix bug where port was not reused if the demo was closed and then re-launched by [@freddyaboulton]...\"],[\"### Documentation Changes:\\n\\n- Make use of `gr` consistent across the docs by [@duerrsimon](https:\\u002f\\u002fg...\"],[\"### Full Changelog:\\n\\n- Add DESCRIPTION.md to image_segmentation demo by [@aliabd](https:\\u002f\\u002fgithub.com...\"],[\"![AnnotatedImage screenshot](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f7870876\\u002f232142720-86e0020f-be...\"],[\"```\\n\\nSee the [image_segmentation demo](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002ftree\\u002fmain\\u002fdemo\\u002fimage_seg...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Fix code markdown support in `gr.Chatbot()` component by [@dawoodkhan82](http...\"],[\"```python\\nwith gr.Blocks() as demo:\\n    img = gr.Image()\\n    textbox = gr.Textbox()\\n\\n    def select_...\"],[\"```\\n\\n![Recording 2023-04-08 at 17 44 39](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f7870876\\u002f230748572...\"],[\"- Increase timeout for sending analytics data by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in...\"],[\"- Fix bug where the upload button was not properly handling the `file_count='multiple'` case by [@fr...\"],[\"### Documentation Changes:\\n\\n- Fix invalid argument docstrings, by [@akx](https:\\u002f\\u002fgithub.com\\u002fakx) in ...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"```\\n\\n  ![Theme Builder](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f7870876\\u002f228204929-d71cbba5-69c2-45...\"],[\"- Fixed bug where text for altair plots was not legible in dark mode by [@freddyaboulton](https:\\u002f\\u002fgi...\"],[\"- Fixed bug where chatbot does not autoscroll inside of a tab, row or column by [@dawoodkhan82](http...\"],[\"- Fixes certain `_js` return values being double wrapped in an array, by [@space-nuko](https:\\u002f\\u002fgithu...\"],[\"- Fix items in `gr.Dropdown` besides the selected item receiving a checkmark, by [@space-nuko](https...\"],[\"### Documentation Changes:\\n\\n- Makes some fixes to the Theme Guide related to naming of variables, by...\"],[\"### Testing and Infrastructure Changes:\\n\\n- Removed heavily-mocked tests related to comet_ml, wandb, ...\"],[\"### Full Changelog:\\n\\n- Mobile responsive iframes in themes guide by [@aliabd](https:\\u002f\\u002fgithub.com\\u002fali...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.23.0\\n\\n### New Features:\\n\\n###### Theme Sha...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.22.1\\n\\n### New Features:\\n\\nNo changes to hi...\"],[\"###### `elem_classes`\\n\\nAdd keyword argument `elem_classes` to Components to control class names of c...\"],[\"### Testing and Infrastructure Changes:\\n\\n- Pinned `pyright==1.1.298` for stability by [@abidlabs](ht...\"],[\"###### Uploading\\n\\nThere are two ways to upload a theme, via the theme class instance or the command ...\"],[\"```\\n\\n2. Via the command line\\n\\nFirst save the theme to disk\\n\\n```python\\nmy_theme.dump(filename=\\\"my_the...\"],[\"```\\n\\nby [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 3428](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"```\\n\\n\\u003cimg width=\\\"1054\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f1778297\\u002f224116682-...\"],[\"```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    gallery = gr.Gallery([\\\"images\\u002f1.jpg\\\", \\\"...\"],[\"```\\n\\nBy [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 3399](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio...\"],[\"### Bug Fixes:\\n\\n- Use `huggingface_hub` to send telemetry on `interface` and `blocks`; eventually to...\"],[\"### Documentation Changes:\\n\\n- Added a section on security and access when sharing Gradio apps by [@a...\"],[\"### Testing and Infrastructure Changes:\\n\\n- Fixes tests that were failing locally but passing on CI b...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Prevent in-place updates of ...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.20.1\\n\\n### New Features:\\n\\n- Add `height` k...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Ensure uploaded images are always shown in the sketch tool by [@pngwn](https:...\"],[\"```\\n\\nby [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 3211](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- Updated image upload component to accept all image formats, including lossless formats like .webp ...\"],[\"- Allow developers to access the username of a logged-in user from the `gr.Request()` object using t...\"],[\"- Ensure `mirror_webcam` is always respected by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 3245](http...\"],[\"- Remove embed's `initial_height` when loading is complete so the embed finds its natural height onc...\"],[\"- Fix bug where `height` set in `Gallery.style` was not respected by the front-end by [@freddyaboult...\"],[\"- Fix error when using backen_fn and custom js at the same time by [@jialeicui](https:\\u002f\\u002fgithub.com\\u002fj...\"],[\"### Documentation Changes:\\n\\n- Added the `types` field to the dependency field in the config by [@fre...\"],[\"### Full Changelog:\\n\\n- Fixed comment typo in components.py by [@eltociear](https:\\u002f\\u002fgithub.com\\u002feltoci...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.19.0\\n\\n### New Features:\\n\\n###### Improved ...\"],[\"###### New `gr.BarPlot` component! 📊\\n\\nCreate interactive bar plots from a high-level interface with ...\"],[\"```\\n\\nBy [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 3157](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"```\\n\\nBy [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 3165](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- Fixes `gr.utils.delete_none` to only remove props whose values are `None` from the config by [@abi...\"],[\"- Fix bug where auth cookies where not sent when connecting to an app via http by [@freddyaboulton](...\"],[\"### Documentation Changes:\\n\\n- Sort components in docs by alphabetic order by [@aliabd](https:\\u002f\\u002fgithu...\"],[\"- Fix demos page css and add close demos button by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 3151]...\"],[\"- Fixed gradio share links so that they are persistent and do not reset if network\\n  connection is d...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.18.0\\n\\n### New Features:\\n\\n###### Revamped ...\"],[\"```\\n\\nBy [@maxaudron](https:\\u002f\\u002fgithub.com\\u002fmaxaudron) in [PR 3075](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio...\"],[\"- Fixes URL resolution on Windows by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 3108](https:\\u002f\\u002fg...\"],[\"- A share link will automatically be created when running on Sagemaker notebooks so that the front-e...\"],[\"### Documentation Changes:\\n\\n- Added a guide on the 4 kinds of Gradio Interfaces by [@yvrjsharma](htt...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.17.1\\n\\n### New Features:\\n\\n###### iOS image...\"],[\"### Bug Fixes:\\n\\n- Fix bug where examples were not rendered correctly for demos created with Blocks a...\"],[\"* Fix a broken link in the Quick Start guide, by [@cakiki](https:\\u002f\\u002fgithub.com\\u002fcakiki) in [PR 3109](h...\"],[\"```\\n\\n\\u003cimg width=\\\"1087\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f213260197...\"],[\"```\\n\\n![chatbot_load](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f213260220-3eaa25b7-a38b-48c6...\"],[\"- Fixes bug where interpretation event was not configured correctly by [@freddyaboulton](https:\\u002f\\u002fgit...\"],[\"- Fixes bug where temporary uploaded files were not being added to temp sets by [@abidlabs](https:\\u002f\\u002f...\"],[\"- Added better support for symlinks in the way absolute paths are resolved by [@abidlabs](https:\\u002f\\u002fgi...\"],[\"- Adding missing embedded components on docs by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 3027](ht...\"],[\"- Preserve selected image of Gallery through updated by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddya...\"],[\"### Documentation Changes:\\n\\n- SEO improvements to guides by[@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.16.2\\n\\n### New Features:\\n\\nNo changes to hi...\"],[\"- Fixed file upload fails for files with zero size by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan8...\"],[\"- Fix bug where outputs for examples where not being returned by the backend by [@freddyaboulton](ht...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"```\\n\\nProgress indicator bar by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 2750](https:\\u002f\\u002fgithu...\"],[\"```\\n\\n\\u003cimg width=\\\"610\\\" alt=\\\"Screenshot 2023-01-03 at 4 14 36 PM\\\" src=\\\"https:\\u002f\\u002fuser-images.githubuserc...\"],[\"- Fixed bug where an error opening an audio file led to a crash by [@FelixDombek](https:\\u002f\\u002fgithub.com...\"],[\"### Documentation Changes:\\n\\n- Added a Guide on using Google Sheets to create a real-time dashboard w...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- The `default_enabled` parame...\"],[\"With this component you can easily create time series visualizations with customizable\\nappearance fo...\"],[\"```\\n\\n![image](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f208711646-81ae3745-149b-46a3-babd-0...\"],[\"### Documentation Changes:\\n\\n- Added a Guide on using BigQuery with Gradio's `DataFrame` and `Scatter...\"],[\"No changes to highlight.\\n\\n## 3.14.0\\n\\n### New Features:\\n\\n###### Add Waveform Visual Support to Audio\\n...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Fixed issue where too many temporary files were created, all with randomly ge...\"],[\"No changes to highlight.\\n\\n### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.13.1\\n\\n### New F...\"],[\"```\\n\\nThese links are a more secure and scalable way to create shareable demos!\\n\\n### Bug Fixes:\\n\\n- Al...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Fixed typo in parameter `vis...\"],[\"The `gr.ScatterPlot` component accepts a pandas dataframe and some optional configuration parameters...\"],[\"```\\n\\n\\u003cimg width=\\\"404\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f206737726-...\"],[\"```\\n\\n\\u003cimg width=\\\"1366\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f204660697...\"],[\"```\\n\\n![label_bg_color_update](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f204400372-80e53857-...\"],[\"### Bug Fixes:\\n\\n- Fixed issue where image thumbnails were not showing when an example directory was ...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"Here's a simple example that references a local image `lion.jpg` that is in the same\\nfolder as the P...\"],[\"```\\n\\n![Alt text](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f1778297\\u002f204357455-5c1a4002-eee7-479d-9a1e...\"],[\"```\\n\\n###### Update Accordion properties from the backend\\n\\nYou can now update the Accordion `label` a...\"],[\"```\\n\\n![update_accordion](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f203164176-b102eae3-babe-...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"## 3.11.0\\n\\n### New Features:\\n\\n###### Upload Button\\n\\nThere is now a new component called the `UploadB...\"],[\"```\\n\\n###### Revamped API documentation page\\n\\nNew API Docs page with in-browser playground and update...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Fixed bug that limited files from being sent over websockets to 16MB. The new...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"### Bug Fixes:\\n\\n- Updated the minimum FastApi used in tests to version 0.87 by [@freddyaboulton](htt...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"When you load an upstream app with `gr.Blocks.load`, you can now specify which fn\\nto call with the `...\"],[\"```\\n\\nThe `api_name` parameter will take precedence over the `fn_index` parameter.\\n\\n### Bug Fixes:\\n\\n-...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"This can be used to:\\n\\n- Create live visualizations that show the most up to date data\\n- Refresh the ...\"],[\"```\\n\\n![live_demo](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f198357377-633ce460-4e31-47bd-82...\"],[\"### Bug Fixes:\\n\\n- Fix whitespace issue when using plotly. [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodk...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.7\\n\\n### New Features:\\n\\n###### Batched Func...\"],[\"```\\n\\nThe advantage of using batched functions is that if you enable queuing, the Gradio\\nserver can a...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Fixes issue where plotly animations, interactivity, titles, legends, were not...\"],[\"### Documentation Changes:\\n\\n- Added an example interactive dashboard to the \\\"Tabular & Plots\\\" sectio...\"],[\"- Fixes the error message if a user builds Gradio locally and tries to use `share=True` by [@abidlab...\"],[\"- Clearer error message when events are defined outside of a Blocks scope, and a warning if you\\n  tr...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.6\\n\\n### New Features:\\n\\n###### Cancelling R...\"],[\"cancel_on_change.change(None, None, None, cancels=[click_event, pred_event])\\n\\n\\ndemo.queue(concurrenc...\"],[\"```\\n\\nFor interfaces, a stop button will be added automatically if the function uses a `yield` statem...\"],[\"```\\n\\n![stop_interface_rl](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f195952883-e7ca4235-aae3...\"],[\"### Documentation Changes:\\n\\n- Adds a demo to show how a sound alert can be played upon completion of...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.5\\n\\n### Bug Fixes:\\n\\n- Ensure that Gradio d...\"],[\"### New Features:\\n\\n- When an `Image` component is set to `source=\\\"upload\\\"`, it is now possible to dr...\"],[\"- Speeds up Gallery component by using temporary files instead of base64 representation in the front...\"],[\"- Updated share link message to reference new Spaces Hardware [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlab...\"],[\"- Stops a gradio launch from hogging a port even after it's been killed [@aliabid94](https:\\u002f\\u002fgithub....\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.4.1\\n\\n### New Features:\\n\\n###### 1. See Pas...\"],[\"1. Fix typo in guide image path by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 2357]...\"],[\"7. Fix bug where new typeable slider doesn't respect the minimum and maximum values [@dawoodkhan82](...\"],[\"### Documentation Changes:\\n\\n1. New Guide: Connecting to a Database 🗄️\\n\\n   A new guide by [@freddyabo...\"],[\"- Create a guide on how to connect an app to a database hosted on the cloud by [@freddyaboulton](htt...\"],[\"- Catch the permission exception on the audio component by [@Ian-GL](https:\\u002f\\u002fgithub.com\\u002fIan-GL) in [...\"],[\"- Lets users provide a `gr.update()` dictionary even if post-processing is disabled [@abidlabs](http...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.4\\n\\n### New Features:\\n\\n###### 1. Gallery C...\"],[\"```\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f192399521-7360b1a9-7ce0-443e-8e94-8...\"],[\"```\\n\\n![color-sketch](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f192410500-3c8c3e64-a5fd-4df2-...\"],[\"```\\n\\n![webcam-sketch](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f192410820-0ffaf324-776e-4e1f...\"],[\"As well as other fixes\\n\\n### Bug Fixes:\\n\\n1. Fix bug where max concurrency count is not respected in q...\"],[\"### Documentation Changes:\\n\\n1. Adding a Playground Tab to the Website by [@aliabd](https:\\u002f\\u002fgithub.co...\"],[\"- Website fixes and refactoring by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 2280](https:\\u002f\\u002fgithub....\"],[\"- Respect Upstream Queue when loading interfaces\\u002fblocks from Spaces by [@freddyaboulton](https:\\u002f\\u002fgit...\"],[\"- Fix Web Tracker Script by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 2308](https:\\u002f\\u002fgithub.com\\u002fgra...\"],[\"### Contributors Shoutout:\\n\\n- [@SkyTNT](https:\\u002f\\u002fgithub.com\\u002fSkyTNT) made their first contribution in ...\"],[\"```\\n\\n![example](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f189086273-f5e7087d-71fa-4158-90a9-...\"],[\"```\\n\\n![187936493-5c90c01d-a6dd-400f-aa42-833a096156a1](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f902...\"],[\"- safari fixes by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 2138](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"- Fixed misleading log when server_name is '0.0.0.0' by [@lamhoangtung](https:\\u002f\\u002fgithub.com\\u002flamhoangt...\"],[\"- Preserve Labels In Interpretation Components by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulto...\"],[\"### Contributors Shoutout:\\n\\n- [@lamhoangtung](https:\\u002f\\u002fgithub.com\\u002flamhoangtung) made their first cont...\"],[\"```\\n\\n- Configure a maximum queue size\\n\\n```python\\ndemo = gr.Interface(...)\\ndemo.queue(max_size=100)\\nd...\"],[\"```\\n\\n- Automatic conversion of videos so they are playable in the browser (thanks to PR #2003). Grad...\"],[\"```\\n\\n###### 5. New Guide 🖊️\\n\\n- [Gradio and W&B Integration](https:\\u002f\\u002fgradio.app\\u002fGradio_and_Wandb_Inte...\"],[\"- Reset components to original state by setting value to None by [@freddyaboulton](https:\\u002f\\u002fgithub.co...\"],[\"- Encourage people to keep trying when queue full by [@apolinario](https:\\u002f\\u002fgithub.com\\u002fapolinario) in...\"],[\"- Allow frontend method execution on Block.load event by [@codedealer](https:\\u002f\\u002fgithub.com\\u002fcodedealer...\"],[\"- feat(samples table\\u002fgallery): Crop thumbs to square by [@ronvoluted](https:\\u002f\\u002fgithub.com\\u002fronvoluted)...\"],[\"### Contributors Shoutout:\\n\\n- [@chrisemezue](https:\\u002f\\u002fgithub.com\\u002fchrisemezue) made their first contri...\"],[\"```\\n\\nBut you can also embed demos that are running anywhere, you just need to link the demo to `src`...\"],[\"```\\n\\nIf you're working from a Jupyter or Colab Notebook, use these magic commands instead: `%load_ex...\"],[\"###### 5. `gr.Examples()` for Blocks 🧱\\n\\nWe've added the `gr.Examples` component helper to allow you ...\"],[\"### Full Changelog:...\"],[\"- File component: list multiple files and allow for download #1446 by [@dawoodkhan82](https:\\u002f\\u002fgithub...\"],[\"- Add python-3.7 tests by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 1818](https:\\u002f\\u002f...\"],[\"### Contributors Shoutout:\\n\\n- [@nhankiet](https:\\u002f\\u002fgithub.com\\u002fnhankiet) made their first contribution...\"],[\"```\\n\\n![hello-blocks](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f168684108-78cbd24b-e6bd-4a04-...\"],[\"###### 4. New Components: Model3D, Dataset, and More..\\n\\nWe've introduced a lot of new components in ...\"],[\"- Gradio dash fe by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 807](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- add test infra + add browser tests to CI by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 852](https:\\u002f...\"],[\"- backend_default_value_refactoring by [@FarukOzderim](https:\\u002f\\u002fgithub.com\\u002fFarukOzderim) in [PR 871](...\"],[\"- 3d Image Component by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 775](https:\\u002f\\u002fgithub....\"],[\"- Redesign 1 by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 918](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002f...\"],[\"- allow audio components to take a string value by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 930](ht...\"],[\"- add frontend for page load events by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 967](https:\\u002f\\u002fgithub...\"],[\"- State and variables by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 977](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"- added interactive parameter to components by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 992](...\"],[\"- release 2.9.4 by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 1006](https:\\u002f\\u002fgithub.com\\u002fgradio-a...\"],[\"- fixed failing test on main by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 1023](https:\\u002f\\u002fgithub...\"],[\"- GAN Gradio Guide: Adjustments to iframe heights by [@NimaBoscarino](https:\\u002f\\u002fgithub.com\\u002fNimaBoscari...\"],[\"- Update text by [@ronvoluted](https:\\u002f\\u002fgithub.com\\u002fronvoluted) in [PR 1050](https:\\u002f\\u002fgithub.com\\u002fgradio...\"],[\"- inputless-interfaces by [@FarukOzderim](https:\\u002f\\u002fgithub.com\\u002fFarukOzderim) in [PR 1038](https:\\u002f\\u002fgith...\"],[\"- Dark text by [@ronvoluted](https:\\u002f\\u002fgithub.com\\u002fronvoluted) in [PR 1049](https:\\u002f\\u002fgithub.com\\u002fgradio-a...\"],[\"- Website Reload: README in demos docker by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 1100](https:...\"],[\"- Interface types: handle input-only, output-only, and unified interfaces by [@abidlabs](https:\\u002f\\u002fgit...\"],[\"- Stacked form inputs css by [@gary149](https:\\u002f\\u002fgithub.com\\u002fgary149) in [PR 1134](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"- add select event for tabitems by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1154](https:\\u002f\\u002fgithub.co...\"],[\"- image gallery component + img css by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 1140](https...\"],[\"- enable flex grow for gr-box by [@radames](https:\\u002f\\u002fgithub.com\\u002fradames) in [PR 1165](https:\\u002f\\u002fgithub....\"],[\"- 962 dataframe by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1186](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- add copy functionality to json by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1205](https:\\u002f\\u002fgithub.c...\"],[\"- Hotfixes for course demos by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 1222](https:\\u002f\\u002fgithub....\"],[\"- ensure defaults height match for media inputs by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1236](h...\"],[\"- Labels spacing by [@gary149](https:\\u002f\\u002fgithub.com\\u002fgary149) in [PR 1254](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- Fixes to components by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 1260](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"- Add embedded demos to website by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 1270](https:\\u002f\\u002fg...\"],[\"- Create Streamables by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 1279](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"- Mobile responsive guides by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 1293](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"### Contributors Shoutout:\\n\\n- [@JefferyChiang](https:\\u002f\\u002fgithub.com\\u002fJefferyChiang) made their first co...\"],[\"@gradio\\u002fstatustracker\\n\\n## 0.4.3\\n\\n### Features\\n\\n- [#6814](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6...\"],[\"## 0.4.0\\n\\n### Highlights\\n\\n#### New `ImageEditor` component ([#6169](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.3.2\\n\\n### Patch Changes\\n\\n- Updated dependencies...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5938](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5938) [`13ed8a485...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5342](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5342) [`afac0006`](https...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5215) [`fbdad78a`](https...\"],[\"create-svelte\\n\\nEverything you need to build a Svelte project, powered by [`create-svelte`](https:\\u002f\\u002fg...\"],[\"imple image classification in Pytorch with Gradio's Image input and Label output....\"],[\"`@gradio\\u002fatoms`\\n\\n```html\\n\\u003cscript lang=\\\"ts\\\"\\u003e\\n\\timport { Block, BlockTitle, BlockLabel, IconButton, Emp...\"],[\"```\\n\\nShareButton:\\n```javascript\\n\\texport let formatter: (arg0: any) =\\u003e Promise\\u003cstring\\u003e;\\n\\texport let v...\"],[\"his text generation demo works like autocomplete. There's only one textbox and it's used for both th...\"],[\"Gradio Demo: sound_alert\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo r...\"],[\"Gradio Demo: theme_extended_step_2\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"实时语音识别\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fstreaming-asr-paused, https:\\u002f\\u002fhugging...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fabidlabs-streaming-asr-paused.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"350\\\" title=\\\"Gra...\"],[\"下面是构建实时语音识别（ASR）应用程序的步骤：\\n\\n1. [设置 Transformers ASR 模型](#1-set-up-the-transformers-asr-model)\\n2. [使用 T...\"],[\"```\\n\\n就是这样！默认情况下，自动语音识别模型管道会加载 Facebook 的 `facebook\\u002fwav2vec2-base-960h` 模型。\\n\\n## 2. 使用 Transformers 创建...\"],[\"```\\n\\n那么这里发生了什么？`transcribe` 函数接受一个参数 `audio`，它是用户录制的音频文件的文件路径。`pipeline` 对象期望一个文件路径，并将其转换为文本，然后返回到前端...\"],[\"好消息是，我们可以很容易地调整刚刚创建的演示，使其成为流式的，使用相同的 `Wav2Vec2` 模型。\\n\\n最大的变化是我们现在必须引入一个 `state` 参数，它保存到目前为止*转录的音频*。这样，...\"],[\"```\\n\\n请注意，我们还进行了另一个更改，即我们设置了 `live=True`。这使得 Gradio 接口保持持续运行，因此它可以自动转录音频，而无需用户反复点击提交按钮。\\n\\n让我们看看它的效果（在下...\"],[\"```python\\nfrom transformers import pipeline\\nimport gradio as gr\\nimport time\\n\\np = pipeline(\\\"automatic...\"],[\"```\\n\\n尝试下面的演示，查看差异（或[在新标签页中打开](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fstreaming-asr-paused)）！\\n\\n\\u003cifram...\"],[\"下面是一个完整的示例（在 Linux 上）：\\n\\n首先通过终端安装 DeepSpeech 库并下载预训练模型：\\n\\n```bash\\nwget https:\\u002f\\u002fgithub.com\\u002fmozilla\\u002fDeep...\"],[\"```\\n\\n然后，创建与之前相似的 `transcribe()` 函数：\\n\\n```python\\nfrom deepspeech import Model\\nimport numpy as np\\n\\nmode...\"],[\"```\\n\\n运行所有这些应该允许您使用一个漂亮的 GUI 部署实时 ASR 模型。尝试一下，看它在您那里运行得有多好。\\n\\n---\\n\\n你已经完成了！这就是构建用于 ASR 模型的基于 Web 的 GUI ...\"],[\"Gradio Demo: sine_curve\\n\\n\\n```\\n!pip install -q gradio plotly\\n```\\n\\n\\n```\\nimport math\\nimport gradio as g...\"],[\"his text generation demo takes in input text and returns generated text. It uses the Transformers li...\"],[\"Gradio Demo: color_generator\\n\\n\\n```\\n!pip install -q gradio opencv-python numpy\\n```...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\nimport cv2\\nimport numpy as np\\nimport random\\n\\n\\n# Convert decimal color ...\"],[\"@gradio\\u002fclient\\n\\n## 0.9.3\\n\\n### Features\\n\\n- [#6814](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6814) [`...\"],[\"## 0.9.0\\n\\n### Features\\n\\n- [#6398](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6398) [`67ddd40`](https:...\"],[\"## 0.8.1\\n\\n### Fixes\\n\\n- [#6383](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6383) [`324867f63`](https:\\u002f...\"],[\"## 0.7.1\\n\\n### Features\\n\\n- [#6137](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6137) [`2ba14b284`](http...\"],[\"## 0.7.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.7.0-beta.1\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.7.0-beta.0\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.5.2\\n\\n### Fixes\\n\\n- [#5840](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5840) [`4e62b8493`](https:\\u002f...\"],[\"Thanks to a new capability that allows components to communicate directly with the server _without_ ...\"],[\"### Fixes\\n\\n- [#5776](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5776) [`c0fef4454`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5682](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5682) [`c57f1b75e`](http...\"],[\"## 0.3.1\\n\\n### Fixes\\n\\n- [#5412](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5412) [`26fef8c7`](https:\\u002f\\u002f...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5133](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5133) [`61129052`](https...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- [#4717](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4717) [`ab5d1ea0`](...\"],[\"- [#4315](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4315) [`b525b122`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#4202](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4202) [`a26e9afd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#3605](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f3605) [`ae4277a9`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"@gradio\\u002flabel\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.2.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`3cdeabc68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Case Study: A Component to Display PDFs\\n\\nLet's work through an example of building a custom gradio c...\"],[\"```\\n\\n\\nTip: You should change the name of the component.\\nSome of the screenshots assume the component...\"],[\"The complete `package.json` should look like this:\\n\\n```json\\n{\\n  \\\"name\\\": \\\"gradio_pdf\\\",\\n  \\\"version\\\": \\\"...\"],[\"```\\n\\n\\nTip: Running `npm install` will install the latest version of the package available. You can i...\"],[\"```\\n\\n## Step 3: Frontend - Launching the Dev Server\\n\\nRun the `dev` command to launch the development...\"],[\"export let elem_id = \\\"\\\";\\n\\texport let elem_classes: string[] = [];\\n\\texport let visible = true;\\n\\texpor...\"],[\"```\\n\\n\\nTip: The `gradio`` object passed in here contains some metadata about the application as well ...\"],[\"```\\n\\nYou should see the following when you navigate to your app after saving your current changes:\\n\\n...\"],[\"```\\n\\nNow import `PdfUploadText.svelte` in your `\\u003cscript\\u003e` and pass it to the `Upload` component!\\n\\n``...\"],[\"```\\n\\nAlso create the following variables:\\n\\n```ts\\n    let pdfDoc;\\n    let numPages = 1;\\n    let curre...\"],[\"```\\n\\n\\nTip: The `$:` syntax in svelte is how you declare statements to be reactive. Whenever any of t...\"],[\"```\\n\\n\\nTip: The `gradio.dispatch` method is actually what is triggering the `change` or `upload` even...\"],[\"```\\n\\nCongratulations! You have a working pdf uploader!\\n\\n![upload-gif](https:\\u002f\\u002fgradio-builds.s3.amazo...\"],[\"```\\n\\nCongratulations! The frontend is almost complete 🎉\\n\\n![multipage-pdf-gif](https:\\u002f\\u002fgradio-builds....\"],[\"\\u003cdiv\\n\\tclass:table={type === \\\"table\\\"}\\n\\tclass:gallery={type === \\\"gallery\\\"}\\n\\tclass:selected\\n\\tstyle=\\\"jus...\"],[\"```\\n\\n\\nTip: Exercise for the reader - reduce the code duplication between `Index.svelte` and `Example...\"],[\"class PDF(Component):\\n\\n    EVENTS = [\\\"change\\\", \\\"upload\\\"]\\n\\n    data_model = FileData\\n\\n    def __init_...\"],[\"```\\n\\n## Step 10: Add a demo and publish!\\n\\nTo test our backend code, let's add a more complex demo th...\"],[\"```\\n\\nSee our demo in action below!\\n\\n\\u003cvideo autoplay muted loop\\u003e\\n  \\u003csource src=\\\"https:\\u002f\\u002fgradio-builds...\"],[\"```\\n\\n\\nI hope you enjoyed this tutorial!\\nThe complete source code for our component is [here](https:\\u002f...\"],[\"Gradio Demo: stream_audio_out\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the d...\"],[\"```\\nimport gradio as gr\\nfrom pydub import AudioSegment\\nfrom time import sleep\\n\\nwith gr.Blocks() as d...\"],[\"gradio-ui\\n\\nThis folder contains all of the Gradio UI and component source code.\\n\\n- [set up](#setup)\\n...\"],[\"```\\n\\nIf you have formatting failures then you can run the following command to fix them:\\n\\n```bash\\npn...\"],[\"Gradio Demo: gallery_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr...\"],[\"Gradio Demo: blocks_scroll\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndemo = gr.B...\"],[\"website\\n\\n## 0.20.3\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fcode@0.3.3\\n\\n## 0.20.2\\n...\"],[\"## 0.19.0\\n\\n### Features\\n\\n- [#5885](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5885) [`9919b8a`](https...\"],[\"## 0.17.0\\n\\n### Features\\n\\n- [#6533](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6533) [`e2810fcfc`](htt...\"],[\"## 0.15.0\\n\\n### Features\\n\\n- [#6436](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6436) [`58e3ca826`](htt...\"],[\"## 0.14.0\\n\\n### Features\\n\\n- [#6387](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6387) [`9d6d72f44`](htt...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fcode@0.2.3\\n\\n## 0.12.0\\n\\n### Features\\n\\n- [#6...\"],[\"## 0.11.1\\n\\n### Features\\n\\n- [#6189](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6189) [`345ddd888`](htt...\"],[\"- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.11.0-beta.0\\n\\n### Features...\"],[\"- [#6082](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6082) [`037e5af33`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6097](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6097) [`439efa39d`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.9.0\\n\\n### Features\\n\\n- [#5386](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5386) [`0312c990f`](http...\"],[\"## 0.7.0\\n\\n### Features\\n\\n- [#5643](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5643) [`f661c0733`](http...\"],[\"### Fixes\\n\\n- [#5608](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5608) [`eebf9d71f`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5423](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5423) [`bb31cd7d`](https...\"],[\"### Fixes\\n\\n- [#5304](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5304) [`05892302`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.2.1\\n\\n### Fixes\\n\\n- [#5324](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5324) [`31996c99`](https:\\u002f\\u002f...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5298](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5076](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5076) [`2745075a`](https...\"],[\"## 0.0.2\\n\\n### Features\\n\\n- [#5009](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5009) [`3e70fc81`](https...\"],[\"Gradio Demo: live_dashboard\\n### This demo shows how you can build a live interactive dashboard with ...\"],[\"```\\n!pip install -q gradio plotly\\n```\\n\\n\\n```\\nimport math\\n\\nimport pandas as pd\\n\\nimport gradio as gr\\nim...\"],[\"Image Classification in TensorFlow and Keras\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs...\"],[\"```\\n\\nThis line automatically downloads the MobileNet model and weights using the Keras library.\\n\\n## ...\"],[\"```\\n\\nLet's break this down. The function takes one parameter:\\n\\n- `inp`: the input image as a `numpy`...\"],[\"Gradio Demo: queue_full_e2e_test\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport ...\"],[\"Gradio Demo: blocks_simple_squares\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndemo...\"],[\"Gradio Demo: blocks_static\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndemo = gr.Bl...\"],[\"Gradio Demo: main_note\\n\\n\\n```\\n!pip install -q gradio scipy numpy matplotlib\\n```\\n\\n\\n```\\n# Downloading f...\"],[\"```\\nfrom math import log2, pow\\nimport os\\n\\nimport numpy as np\\nfrom scipy.fftpack import fft\\n\\nimport g...\"],[\"@gradio\\u002fatoms\\n\\n## 0.4.1\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`7326...\"],[\"## 0.3.0\\n\\n### Highlights\\n\\n#### New `ImageEditor` component ([#6169](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.2.2\\n\\n### Fixes\\n\\n- [#6254](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.2.0-beta.4\\n\\n### Features\\n\\n- [#5938](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5938) [`13ed8a485...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.1.2\\n\\n## 0.1.3\\n\\n### Patch...\"],[\"We now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highli...\"],[\"demo for predicting the depth of an image and generating a 3D model of it....\"],[\"Gradio Demo: video_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the de...\"],[\"TensorFlow 和 Keras 中的图像分类\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fkeras-image-classifier\\n标签：VIS...\"],[\"让我们开始吧！\\n\\n### 先决条件\\n\\n确保您已经[安装](\\u002fgetting_started)了 `gradio` Python 包。我们将使用一个预训练的 Keras 图像分类模型，因此您还应该安装了...\"],[\"```\\n\\n此行代码将使用 Keras 库自动下载 MobileNet 模型和权重。\\n\\n## 第二步 —— 定义 `predict` 函数\\n\\n接下来，我们需要定义一个函数，该函数接收*用户输入*作为参数...\"],[\"```\\n\\n让我们来详细了解一下。该函数接受一个参数：\\n\\n- `inp`：输入图像的 `numpy` 数组\\n\\n然后，函数添加一个批次维度，通过模型进行处理，并返回：\\n\\n- `confidences`：预...\"],[\"```\\n\\n这将生成以下界面，您可以在浏览器中立即尝试（尝试上传您自己的示例！）：\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fabidlabs-keras-image-classifier.hf.sp...\"],[\"his demo identifies if two speakers are the same person using Gradio's Audio and HTML components....\"],[\"Gradio Demo: image_classifier_interface_load\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading f...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\nimport pathlib\\n\\ncurrent_dir = pathlib.Path(__file__).parent\\n\\nimages = ...\"],[\"@gradio\\u002fimage\\n\\n## 0.5.3\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`7326...\"],[\"## 0.5.0\\n\\n### Features\\n\\n- [#6726](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6726) [`21cfb0a`](https:...\"],[\"### Fixes\\n\\n- [#6709](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6709) [`6a9151d`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.3.6\\n\\n### Fixes\\n\\n- [#6441](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.3.4\\n\\n### Features\\n\\n- [#6363](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6363) [`4d3aad33a`](http...\"],[\"## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`bca6c2c80`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2ba14b284`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.3.0-beta.9\\n\\n### Features...\"],[\"- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#6146](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6146) [`40a171ea6`](https:\\u002f\\u002fgithub.co...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5627](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5627) [`b67115e8e`](http...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.3\\n\\n### Fixes\\n\\n- [#5528](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5528) [`dc86e4a7`](https:\\u002f\\u002f...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`667875b2`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Gradio Demo: latex\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks() as ...\"],[\"Gradio Demo: video_identity\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the dem...\"],[\"@gradio\\u002ffallback\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`f816136a0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.0-beta.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`14fc612d8`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"## 0.2.0-beta.0\\n\\n### Features\\n\\n- [#5507](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5507) [`1385dc688...\"],[\"We now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highli...\"],[\"🚀 Creating Discord Bots from Gradio Apps 🚀\\n\\nTags: NLP, TEXT, CHAT\\n\\nWe're excited to announce that Gr...\"],[\"```\\n\\n### Step 2: Deploying our App\\n\\nIn order to create a discord bot for our app, it must be accessi...\"],[\"```\\n\\n### Step 5: Add the bot to your server\\n\\nVisit the space of your discord bot. You should see \\\"Ad...\"],[\"```\\n\\n## 🦾 Using State of The Art LLMs 🦾\\n\\nWe have created an organization on Hugging Face called [gra...\"],[\"```\\n\\n## 🦜 Additional LLMs 🦜\\n\\nIn addition to Meta's 70 billion Llama 2 model, we have prepared templa...\"],[\"Gradio Demo: blocks_essay\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\n\\ncountries_cities_dict = {\\n    \\\"USA\\\": [\\\"New York\\\", \\\"Los Angeles\\\", \\\"Chicago\\\"]...\"],[\"def reset_bounds(minimum, maximum):\\n        return gr.Number(minimum=minimum, maximum=maximum)\\n\\n    ...\"],[\"Gradio Demo: generate_tone\\n\\n\\n```\\n!pip install -q gradio numpy\\n```\\n\\n\\n```\\nimport numpy as np\\nimport gr...\"],[\"`@gradio\\u002fform`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { Form } from \\\"@gradio\\u002fform\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\nForm\\n```javasc...\"],[\"Gradio Demo: hello_world_3\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef greet(na...\"],[\"Gradio Components: The Key Concepts\\n\\nIn this section, we discuss a few important concepts when it co...\"],[\"```\\n\\nThe interactive version of the component is much more complex -- you can upload images or snap ...\"],[\"```python\\nimport numpy as np\\nimport gradio as gr\\n\\ndef sepia(input_img):\\n    sepia_filter = np.array(...\"],[\"```\\n\\nThis will create a Gradio app which has an `Image` component as the input and the output. \\nIn t...\"],[\"* As a component author, **YOU** control the format of the data displayed in the frontend as well as...\"],[\"### What you need to remember\\n\\n* If you expect your component to be used as input, it is important t...\"],[\"Real Time Speech Recognition\\n\\nTags: ASR, SPEECH, STREAMING\\n\\n## Introduction\\n\\nAutomatic speech recogn...\"],[\"Here's how to build a real time speech recognition (ASR) app:\\n\\n1. [Set up the Transformers ASR Model...\"],[\"```\\n\\nThat's it!\\n\\n## 2. Create a Full-Context ASR Demo with Transformers\\n\\nWe will start by creating a...\"],[\"Gradio Demo: unified_demo_text_generation\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\n...\"],[\"Gradio Demo: calculator_blocks_cached\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\n...\"],[\"Gradio Demo: image-simple\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo ...\"],[\"`@gradio\\u002fimageeditor`...\"],[\"Gradio Demo: chatbot_multimodal\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the...\"],[\"```\\nimport gradio as gr\\nimport os\\nimport time\\n\\n# Chatbot demo with multimodal input (text, markdown,...\"],[\"txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(\\n        bot, chatb...\"],[\"Gradio Demo: dataframe_block-ui-test\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwi...\"],[\"Gradio Demo: on_listener_test\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.B...\"],[\"The Backend 🐍\\n\\nThis guide will cover everything you need to know to implement your custom component'...\"],[\"```\\n\\n## The methods you need to implement\\n\\nWhen you inherit from any of these classes, the following...\"],[\"```\\n\\n### `api_info`\\n\\nA JSON-schema representation of the value that the `preprocess` expects. \\nThis ...\"],[\"```\\n\\n### `read_from_flag`\\nConvert from the format stored in the `csv` or `json` file used for flaggi...\"],[\"```\\n\\nBy adding these four lines of code, your component automatically implements the methods needed ...\"],[\"```\\n\\nEven if your component does not expect a \\\"complex\\\" JSON data structure it can be beneficial to ...\"],[\"Gradio Demo: blocks_flag\\n\\n\\n```\\n!pip install -q gradio numpy\\n```\\n\\n\\n```\\nimport numpy as np\\nimport grad...\"],[\"Gradio Demo: concurrency_without_queue\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\ni...\"],[\"Gradio Demo: dashboard\\n### This demo shows how you can build an interactive dashboard with gradio. C...\"],[\"```\\nimport gradio as gr\\nimport pandas as pd\\nimport plotly.express as px\\nfrom helpers import *\\n\\n\\nLIBR...\"],[\"def create_issue_plot(libraries, issue_choices):\\n    if \\\"Issue\\\" not in issue_choices:\\n        return...\"],[\"if __name__ == \\\"__main__\\\":\\n    demo.launch()...\"],[\"@gradio\\u002fslider\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd...\"],[\"## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"### Fixes\\n\\n- [#5984](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5984) [`66549d8d2`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5697](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5697) [`f4e4f82b5`](http...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"@gradio\\u002fmodel3d\\n\\n## 0.4.11\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.4.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.4.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`854b482f5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#6240](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6240) [`dd901c1b0`](http...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.3.0-beta.8\\n\\n### Features...\"],[\"- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.3.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5373](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5373) [`79d8f9d8`](https...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"# @gradio\\u002fmodel3D\\n\\n## 0.0.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`667875b2`](https:\\u002f\\u002fgithub....\"],[\"mport { Meta } from \\\"@storybook\\u002fblocks\\\";\\n\\n\\u003cMeta title=\\\"Introduction\\\" \\u002f\\u003e\\n\\n\\u003cstyle\\u003e\\n\\t{`\\n    img {\\n     ...\"],[\"\\u003cdiv class=\\\"divider\\\" \\u002f\\u003e\\n\\n\\u003cdiv class=\\\"subheading\\\"\\u003eResources\\u003c\\u002fdiv\\u003e\\n\\u003cul\\u003e\\n\\n  \\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgradio...\"],[\"simple demo showcasing the upload button used with its `upload` event trigger....\"],[\"`@gradio\\u002fhighlightedtext`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseStaticHighlightedText, BaseInteractiveH...\"],[\"Gradio Demo: image_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwit...\"],[\"Gradio Demo: reverse_audio\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo...\"],[\"Gradio Demo: blocks_page_load\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef prin...\"],[\"Gradio & LLM Agents 🤝\\n\\nLarge Language Models (LLMs) are very impressive but they can be made even mo...\"],[\"## gradio_tools - An end-to-end example\\n\\nTo get started with `gradio_tools`, all you need to do is i...\"],[\"agent = initialize_agent(tools, llm, memory=memory, agent=\\\"conversational-react-description\\\", verbos...\"],[\"```\\n\\nYou'll note that we are using some pre-built tools that come with `gradio_tools`. Please see th...\"],[\"```\\n\\nThe requirements are:\\n\\n1. The name for your tool\\n2. The description for your tool. This is cruc...\"],[\"And that's it!\\n\\nOnce you have created your tool, open a pull request to the `gradio_tools` repo! We ...\"],[\"```\\n\\nSome notes on this implementation:\\n\\n1. All instances of `GradioTool` have an attribute called `...\"],[\"his simple demo takes advantage of Gradio's HighlightedText, JSON and HTML outputs to create a clear...\"],[\"`@gradio\\u002fvideo`\\n\\n```javascript\\n\\u003cscript\\u003e\\n\\timport { BaseInteractiveVideo, BaseStaticVideo, BasePlayer ...\"],[\"utomatic speech recognition English. Record from your microphone and the app will transcribe the aud...\"],[\"Gradio Demo: longest_word\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef longest_...\"],[\"`@gradio\\u002fcolorpicker`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseColorPicker, BaseExample } from \\\"@gradio\\u002fco...\"],[\"Gradio Demo: annotatedimage_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nim...\"],[\"Running a Gradio App on your Web Server with Nginx\\n\\nTags: DEPLOYMENT, WEB SERVER, NGINX\\n\\n## Introduc...\"],[\"```\\n\\n2. Create a new file in the `\\u002fetc\\u002fnginx\\u002fsites-available` directory (create the directory if it ...\"],[\"```\\n\\n2. Start a `tmux` session by typing `tmux` and pressing enter (optional)\\n\\nIt's recommended that...\"],[\"@gradio\\u002fupload\\n\\n## 0.5.6\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`732...\"],[\"## 0.5.3\\n\\n### Fixes\\n\\n- [#6709](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6709) [`6a9151d`](https:\\u002f\\u002fg...\"],[\"## 0.5.0\\n\\n### Highlights\\n\\n#### New `ImageEditor` component ([#6169](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.4.2\\n\\n### Fixes\\n\\n- [#6441](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#6356](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6356) [`854b482f5`](http...\"],[\"## 0.3.2\\n\\n### Fixes\\n\\n- [#6234](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6234) [`aaa55ce85`](https:\\u002f...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.3.0-beta.5\\n\\n### Features\\n\\n- [#6044](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6044) [`9053c95a1...\"],[\"## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.2\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#5...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"## 0.0.3\\n\\n### Fixes\\n\\n- [#5077](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5077) [`667875b2`](https:\\u002f\\u002f...\"],[\"```\\n\\nFrom the backend, streamed outputs are served from the `\\u002fstream\\u002f` endpoint instead of the `\\u002ffil...\"],[\"Getting Started with the Gradio Python client\\n\\nTags: CLIENT, API, SPACES\\n\\nThe Gradio Python client m...\"],[\"```\\n\\n## Connecting to a running Gradio App\\n\\nStart by connecting instantiating a `Client` object and ...\"],[\"```\\n\\nIf you have previously duplicated a Space, re-running `duplicate()` will _not_ create a new Spa...\"],[\"```\\n\\nThis shows us that we have 1 API endpoint in this space, and shows us how to use the API endpoi...\"],[\"```\\n\\n## Running jobs asynchronously\\n\\nOe should note that `.predict()` is a _blocking_ operation as i...\"],[\"```\\n\\n## Status\\n\\nThe `Job` object also allows you to get the status of the running job by calling the...\"],[\"```\\n\\nIf the first job has started processing, then it will not be canceled. If the second job\\nhas no...\"],[\"Frequently Asked Questions\\n\\n## What do I need to install before using Custom Components?\\nBefore usin...\"],[\"A `data_model` defines the expected data format for your component, simplifying the component develo...\"],[\"Create Your Own Friends with a GAN\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fNimaBoscarino\\u002fcryp...\"],[\"Today we'll briefly look at the high-level intuition behind GANs, and then we'll build a small demo ...\"],[\"## Step 1 — Create the Generator model\\n\\nTo generate new images with a GAN, you only need the generat...\"],[\"```\\n\\nWe're taking the generator from [this repo by @teddykoker](https:\\u002f\\u002fgithub.com\\u002fteddykoker\\u002fcrypto...\"],[\"```\\n\\nWe're giving our `predict` function a `seed` parameter, so that we can fix the random tensor ge...\"],[\"```\\n\\nThe new input will be passed to our `predict()` function, so we have to make some changes to th...\"],[\"```\\n\\nThe `examples` parameter takes a list of lists, where each item in the sublists is ordered in t...\"],[\"```python\\nimport torch\\nfrom torch import nn\\nfrom huggingface_hub import hf_hub_download\\nfrom torchvi...\"],[\"def predict(seed, num_punks):\\n    torch.manual_seed(seed)\\n    z = torch.randn(num_punks, 100, 1, 1)\\n...\"],[\"```\\n\\n---\\n\\nCongratulations! You've built out your very own GAN-powered CryptoPunks generator, with a ...\"],[\"Gradio Demo: blocks_component_shortcut\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n...\"],[\"Gradio Demo: progress_component\\n\\n\\n```\\n!pip install -q gradio tqdm\\n```\\n\\n\\n```\\nimport gradio as gr\\nimpo...\"],[\"Gradio Demo: calculator\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo re...\"],[\"Gradio Demo: cancel_events\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport time\\nimport gradio as gr\\n\\n\\ndef fake_diffusion(steps):\\n    for i in range(steps):\\n        ...\"],[\"cancel_on_change.change(None, None, None, cancels=[click_event, pred_event])\\n    cancel_on_submit.su...\"],[\"Gradio Demo: live_with_vars\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndemo = gr.I...\"],[\"反应式界面 (Reactive Interfaces)\\n\\n本指南介绍了如何使 Gradio 界面自动刷新或连续流式传输数据。\\n\\n## 实时界面 (Live Interfaces)\\n\\n您可以通过在界面中...\"],[\"Gradio Demo: gpt2_xl\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ntitle = \\\"gpt2-xl\\\"\\n...\"],[\"Gradio Demo: uploadbutton_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef...\"],[\"Vision Transformers 图像分类\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fvision-transformer\\n标签：VISION, ...\"],[\"让我们开始吧！\\n\\n### 先决条件\\n\\n确保您已经[安装](\\u002fgetting_started)了 `gradio` Python 包。\\n\\n## 步骤 1 - 选择 Vision 图像分类模型\\n\\n首先，我...\"],[\"```python\\nimport gradio as gr\\n\\ngr.Interface.load(\\n             \\\"huggingface\\u002fgoogle\\u002fvit-base-patch16-...\"],[\"```\\n\\n请注意，我们添加了一个 `examples` 参数，允许我们使用一些预定义的示例预填充我们的界面。\\n\\n这将生成以下接口，您可以直接在浏览器中尝试。当您输入图像时，它会自动进行预处理并发送到 ...\"],[\"Gradio Demo: blocks_speech_text_sentiment\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\n...\"],[\"Gradio Demo: spectogram\\n\\n\\n```\\n!pip install -q gradio scipy numpy matplotlib\\n```\\n\\n\\n```\\nimport matplot...\"],[\"Gradio Demo: clustering\\n### This demo built with Blocks generates 9 plots based on the input.\\n      ...\"],[\"```\\nimport gradio as gr\\nimport math\\nfrom functools import partial\\nimport matplotlib.pyplot as plt\\nim...\"],[\"def get_moons(n_clusters):\\n    X, labels = make_moons(n_samples=N_SAMPLES, noise=0.05, random_state=...\"],[\"labels = np.zeros(N_SAMPLES, dtype=int)\\n    return normalize(X), labels\\n\\n\\nDATA_MAPPING = {\\n    'regu...\"],[\"def get_spectral(X, labels, n_clusters, **kwargs):\\n    model = SpectralClustering(\\n        n_cluster...\"],[\"def plot_clusters(ax, X, labels):\\n    set_clusters = set(labels)\\n    set_clusters.discard(-1)  # -1 ...\"],[\"def iter_grid(n_rows, n_cols):\\n    # create a grid using gradio Block\\n    for _ in range(n_rows):\\n  ...\"],[\"@gradio\\u002fhtml\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`f816136a0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.1.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.1.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.0.5\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.3\\n  - @gradio\\u002fstatustr...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"`@gradio\\u002fgallery`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseGallery } from \\\"@gradio\\u002fgallery\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\nB...\"],[\"命名实体识别 （Named-Entity Recognition）\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002frajistics\\u002fbiobert_ner_demo，htt...\"],[\"### 方法一：实体字典列表\\n\\n许多命名实体识别模型输出的是一个字典列表。每个字典包含一个*实体*，一个 \\\" 起始 \\\" 索引和一个 \\\" 结束 \\\" 索引。这就是 `transformers` 库中的 N...\"],[\"```\\n\\n输出结果：\\n\\n```bash\\n[{'entity': 'I-LOC',\\n  'score': 0.9988978,\\n  'index': 2,\\n  'word': 'Chicago',\\n  ...\"],[\"Blocks and Event Listeners\\n\\nWe briefly descirbed the Blocks class in the [Quickstart](\\u002fmain\\u002fguides\\u002fq...\"],[\"```\\n\\n_Note_: What happens if a Gradio component is neither an input nor an output? If a component is...\"],[\"1. as a list of arguments, or\\n2. as a single dictionary of values, keyed by the component\\n\\nLet's see...\"],[\"```\\n\\nAbove, each return statement returns two values corresponding to `food_box` and `status_box`, r...\"],[\"```\\n\\nNotice how when there is no food, we only update the `status_box` element. We skipped updating ...\"],[\"Here's an example showing how to use `gr.Examples` in a `gr.Blocks` app:\\n\\n$code_calculator_blocks\\n\\n*...\"],[\"## Gathering Event Data\\n\\nYou can gather specific data about an event by adding the associated event ...\"],[\"Gradio Demo: no_input\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport random\\n\\nsen...\"],[\"Gradio Demo: hello_world_2\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef greet(na...\"],[\"Gradio Demo: blocks_plug\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef change_ta...\"],[\"PyTorch 图像分类\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fpytorch-image-classifier, https...\"],[\"让我们开始吧！\\n\\n### 先决条件\\n\\n确保您已经[安装](\\u002fgetting_started)了 `gradio` Python 包。我们将使用一个预训练的图像分类模型，所以您还应该安装了 `torch...\"],[\"```\\n\\n由于我们将使用模型进行推断，所以我们调用了 `.eval()` 方法。\\n\\n## 第二步 - 定义 `predict` 函数\\n\\n接下来，我们需要定义一个函数，该函数接受*用户输入*，在本示例中...\"],[\"```\\n\\n让我们逐步来看一下这段代码。该函数接受一个参数：\\n\\n- `inp`：输入图片，类型为 `PIL` 图像\\n\\n然后，该函数将图像转换为 PIL 图像，最终转换为 PyTorch 的 `tenso...\"],[\"```\\n\\n这将产生以下界面，您可以在浏览器中直接尝试（试试上传自己的示例图片！）：\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fabidlabs-pytorch-image-classifier.hf...\"],[\"Gradio Demo: video_subtitle\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the dem...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\nimport os\\n\\na = os.path.join(os.path.abspath(''), \\\"files\\u002fa.mp4\\\")  # Vid...\"],[\"@gradio\\u002fstate\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`2...\"],[\"## 0.1.0-beta.1\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"分享您的应用\\n\\n如何分享您的 Gradio 应用：\\n\\n1. [使用 share 参数分享演示](#sharing-demos)\\n2. [在 HF Spaces 上托管](#hosting-on-hf-...\"],[\"```\\n\\n这将生成一个公开的可分享链接，您可以将其发送给任何人！当您发送此链接时，对方用户可以在其浏览器中尝试模型。因为处理过程发生在您的设备上（只要您的设备保持开启！），您不必担心任何打包依赖项的问...\"],[\"在您创建了一个免费的 Hugging Face 账户后，有三种方法可以将您的 Gradio 应用部署到 Hugging Face Spaces：\\n\\n1. 从终端：在应用目录中运行 `gradio de...\"],[\"![嵌入此空间下拉选项](\\u002fassets\\u002fguides\\u002fembed_this_space.png)\\n\\n### 使用 Web 组件嵌入\\n\\n与 IFrames 相比，Web 组件通常为用户提供更好的体验。...\"],[\"```\\n\\n2.  在您想放置应用的位置添加\\n    `html\\n&lt;gradio-app src=\\\"https:\\u002f\\u002f$your_space_host.hf.space\\\"\\u003e&lt;\\u002fgradio-a...\"],[\"```\\n\\n\\u003cscript\\u003e\\nfetch(\\\"https:\\u002f\\u002fpypi.org\\u002fpypi\\u002fgradio\\u002fjson\\\"\\n).then(r =\\u003e r.json()\\n).then(obj =\\u003e {\\n    let...\"],[\"您还可以使用传递给 `\\u003cgradio-app\\u003e` 标签的属性来自定义 Web 组件的外观和行为：\\n\\n- `src`：如前所述，`src` 属性链接到您想要嵌入的托管 Gradio 演示的 URL\\n- ...\"],[\"以下是使用这些属性创建一个懒加载且初始高度为 0px 的 Gradio 应用的示例。\\n\\n```html\\n&lt;gradio-app space=\\\"gradio\\u002fEchocardiogram-Segm...\"],[\"```\\n\\n_ 注意：Gradio 的 CSS 永远不会影响嵌入页面，但嵌入页面可以影响嵌入的 Gradio 应用的样式。请确保父页面中的任何 CSS 不是如此通用，以至于它也可能适用于嵌入的 Grad...\"],[\"```\\n\\n同样，您可以在“嵌入此空间”按钮中找到您的 Space 的嵌入 URL 的 `src=` 属性。\\n\\n注意：如果您使用 IFrames，您可能希望添加一个固定的 `height` 属性，并设置...\"],[\"```\\n\\n这将记录自动生成的 API 页面的端点 `\\u002fapi\\u002faddition\\u002f`。\\n\\n_注意_：对于启用了[队列功能](https:\\u002f\\u002fgradio.app\\u002fkey-features#queuing...\"],[\"```\\n\\n为了使身份验证正常工作，必须在浏览器中启用第三方 Cookie。\\n默认情况下，Safari、Chrome 隐私模式不会启用此功能。\\n\\n## 直接访问网络请求\\n\\n当用户向您的应用程序进行预测时...\"],[\"```\\n\\n注意：如果直接调用函数而不是通过 UI（例如在缓存示例时），则 `request` 将为 `None`。您应该明确处理此情况，以确保您的应用程序不会抛出任何错误。这就是为什么我们有显式检查 ...\"],[\"特别是，Gradio 应用程序允许用户访问以下三类文件：\\n\\n- **与 Gradio 脚本所在目录（或子目录）中的文件相同。** 例如，如果您的 Gradio 脚本的路径是 `\\u002fhome\\u002fusr\\u002fsc...\"],[\"- **点文件**（其名称以 '.' 开头的任何文件）或其名称以 '.' 开头的任何目录中的任何文件。\\n\\n- **通过 `launch()` 中的 `blocked_paths` 参数允许的文件。**...\"],[\"Gradio Demo: english_translator\\n\\n\\n```\\n!pip install -q gradio transformers torch\\n```\\n\\n\\n```\\nimport gra...\"],[\"@gradio\\u002frow\\n\\n## 0.1.1\\n\\n### Features\\n\\n- [#6399](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6399) [`053...\"],[\"## 0.1.0-beta.1\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"Gradio Demo: image_editor\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo ...\"],[\"分块状态 (State in Blocks)\\n\\n我们已经介绍了[接口状态](https:\\u002f\\u002fgradio.app\\u002finterface-state)，这篇指南将介绍分块状态，它的工作原理大致相同。\\n\\n#...\"],[\"$code_hangman\\n$demo_hangman\\n\\n让我们看看在这个游戏中如何完成上述的 3 个步骤：\\n\\n1. 我们将已使用的字母存储在 `used_letters_var` 中。在 `Stat...\"],[\"ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full c...\"],[\"Gradio Demo: matrix_transpose\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport numpy as np\\n\\nimport gra...\"],[\"Gradio Demo: blocks_textbox_max_lines\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\n...\"],[\"ecreate the viral AnimeGAN image transformation demo....\"],[\"Gradio Demo: calculator_list_and_dict\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nw...\"],[\"区块和事件监听器 (Blocks and Event Listeners)\\n\\n我们在[快速入门](https:\\u002f\\u002fgradio.app\\u002fquickstart\\u002f#blocks-more-flexibil...\"],[\"## 事件监听器与交互性 (Event Listeners and Interactivity)\\n\\n在上面的示例中，您会注意到可以编辑文本框 `name`，但无法编辑文本框 `output`。这是因为...\"],[\"```\\n\\n## 事件监听器的类型 (Types of Event Listeners)\\n\\n请查看下面的演示：\\n\\n$code_blocks_hello\\n$demo_blocks_hello\\n\\n`welc...\"],[\"1. 作为参数列表，或\\n2. 作为以组件为键的单个值字典\\n\\n让我们分别看一个例子：\\n$code_calculator_list_and_dict\\n\\n`add()` 和 `sub()` 都将 `a` 和...\"],[\"首先让我们看一个（1）的示例，其中我们通过返回两个值来设置两个输出组件的值：\\n\\n```python\\nwith gr.Blocks() as demo:\\n    food_box = gr.Number...\"],[\"```\\n\\n上面的每个返回语句分别返回与 `food_box` 和 `status_box` 相对应的两个值。\\n\\n除了返回与每个输出组件顺序相对应的值列表外，您还可以返回一个字典，其中键对应于输出组件，...\"],[\"```\\n\\n注意，在没有食物的情况下，我们只更新 `status_box` 元素。我们跳过更新 `food_box` 组件。\\n\\n字典返回在事件监听器影响多个组件的返回值或有条件地影响输出时非常有用。\\n\\n...\"],[\"$code_chatbot_simple\\n$demo_chatbot_simple\\n\\n事件监听器的 `.then()` 方法会执行后续事件，无论前一个事件是否引发任何错误。如果只想在前一个事件成功执行...\"],[\"在下面的双人井字游戏演示中，用户可以选择 `DataFrame` 中的一个单元格进行移动。事件数据参数包含有关所选单元格的信息。我们可以首先检查单元格是否为空，然后用用户的移动更新单元格。\\n\\n$cod...\"],[\"Gradio Demo: checkbox_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith g...\"],[\"@gradio\\u002ficons\\n\\n## 0.3.2\\n\\n### Features\\n\\n- [#6399](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6399) [`0...\"],[\"```\\n\\n Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.2.1\\n\\n### Fixes\\n\\n- [#6254](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 0.2.0-beta.3\\n\\n### Features\\n\\n- [#6094](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6094) [`c476bd5a5...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5699](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5699) [`8f0fed857`](http...\"],[\"Gradio Demo: audio_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        with gr.Column():\\n    ...\"],[\"output_video.play(lambda n: n + 1, output_num_play, output_num_play)\\n            output_video.pause(...\"],[\"Gradio Demo: chatinterface_random_response\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport random\\nimp...\"],[\"Gradio Demo: colorpicker_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwit...\"],[\"Gradio Demo: blocks_update\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Bloc...\"],[\"Getting Started with the Gradio JavaScript client\\n\\nTags: CLIENT, API, SPACES\\n\\nThe Gradio JavaScript ...\"],[\"```\\n\\nThe Gradio client works with any hosted Gradio app, whether it be an image generator, a text su...\"],[\"```\\n\\n## Duplicating a Space for private use\\n\\nWhile you can use any public Space as an API, you may g...\"],[\"```\\n\\n## Connecting a general Gradio app\\n\\nIf your app is running somewhere else, just provide the ful...\"],[\"```\\n\\nThis shows us that we have 1 API endpoint in this space, and shows us how to use the API endpoi...\"],[\"```\\n\\nFor certain inputs, such as images, you should pass in a `Buffer`, `Blob` or `File` depending o...\"],[\"```\\n\\n## Status\\n\\nThe event interface also allows you to get the status of the running job by listenin...\"],[\"```\\n\\nIf the first job has started processing, then it will not be canceled but the client will no lo...\"],[\"更多示例 (More on Examples)\\n\\n本指南介绍了有关示例的更多内容：从目录中加载示例，提供部分示例和缓存。如果你对示例还不熟悉，请查看 [关键特性](..\\u002fkey-features\\u002f#e...\"],[\"```csv\\nnum,operation,num2\\n5,\\\"add\\\",3\\n4,\\\"divide\\\",2\\n5,\\\"multiply\\\",3...\"],[\"```\\n\\n当浏览标记数据时，这将非常有用。只需指向标记目录，`Interface` 将从标记数据加载示例。\\n\\n### 提供部分示例\\n\\n有时你的应用程序有许多输入组件，但你只想为其中的一部分提供示例。为...\"],[\"从 Supabase 数据创建仪表盘\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n\\n[Supabase](https:\\u002f\\u002fsupabase.com\\u002f) 是一个基于云的开源后端，提...\"],[\"1\\\\. 在 Supabase 中创建一个新项目。一旦您登录，点击 \\\"New Project\\\" 按钮\\n\\n2\\\\. 给您的项目命名并设置数据库密码。您还可以选择定价计划（对于我们来说，免费计划已足够！）\\n\\n...\"],[\"```\\n\\n7\\\\. 获取项目 URL 和 API 密钥。点击左侧窗格上的设置（齿轮图标），然后点击 'API'。URL 列在项目 URL 框中，API 密钥列在项目 API 密钥（带有 `service...\"],[\"```\\n\\n返回 Supabase 仪表板并刷新页面，您将看到 10 行数据填充到 `Product` 表中！\\n\\n## 在实时 Gradio 仪表盘中可视化数据\\n\\n最后，我们将使用相同的 `supaba...\"],[\"```\\n\\n请注意，通过将函数传递给 `gr.BarPlot()`，我们可以在网络应用加载时查询数据库（然后每 60 秒查询一次，因为有 `every` 参数）。您的最终仪表盘应如下所示：\\n\\n\\u003cgrad...\"],[\"Gradio Demo: model3d_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr...\"],[\"@gradio\\u002fmarkdown\\n\\n## 0.6.0\\n\\n### Features\\n\\n- [#6842](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6842) ...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.3.1\\n  - @gradio\\u002fstatustracker@0.4....\"],[\"## 0.3.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`f816136a0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.3.0-beta.7\\n\\n### Features\\n\\n- [#6071](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6071) [`f08da1a6f...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.3.2\\n\\n### Fixes\\n\\n- [#5897](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5897) [`0592c301d`](https:\\u002f...\"],[\"## 0.3.0\\n\\n### Fixes\\n\\n- [#5755](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5755) [`e842a561a`](https:\\u002f...\"],[\"### Fixes\\n\\n- [#5604](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5604) [`faad01f8e`](https:\\u002f\\u002fgithub.co...\"],[\"- [#5304](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5304) [`05892302`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5368](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5368) [`b27f7583`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 0.1.1\\n\\n### Fixes\\n\\n- [#5324](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5324) [`31996c99`](https:\\u002f\\u002f...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5268](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Gradio Demo: video_identity_2\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef video...\"],[\"`gradio_client`: Use a Gradio app as an API -- in 3 lines of Python\\n\\nThis directory contains the sou...\"],[\"```\\n\\nYou can also connect to private Spaces by passing in your HF token with the `hf_token` paramete...\"],[\"```\\n\\n### Inspecting the API endpoints\\n\\nOnce you have connected to a Gradio app, you can view the API...\"],[\"```\\n\\nFor certain inputs, such as images, you should pass in the filepath or URL to the file. Likewis...\"],[\"Gradio Demo: image_classification\\n### Simple image classification in Pytorch with Gradio's Image inp...\"],[\"Gradio Demo: label_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.B...\"],[\"自定义的 JS 和 CSS\\n\\n本指南介绍了如何更灵活地为 Blocks 添加样式，并添加 JavaScript 代码到事件监听器中。\\n\\n**警告**：在自定义的 JS 和 CSS 中使用查询选择器不能...\"],[\"```\\n\\n如果您想在您的 CSS 中引用外部文件，请使用 `\\\"file=\\\"` 作为文件路径的前缀（可以是相对路径或绝对路径），例如：\\n\\n```python\\nwith gr.Blocks(css=\\\".g...\"],[\"```\\n\\nCSS `#warning` 规则集仅针对第二个文本框，而 `.feedback` 规则集将同时作用于两个文本框。请注意，在针对类时，您可能需要使用 `!important` 选择器来覆盖默...\"],[\"Gradio Demo: bokeh_plot\\n\\n\\n```\\n!pip install -q gradio bokeh\\u003e=3.0 xyzservices\\n```...\"],[\"```\\nimport gradio as gr\\nimport xyzservices.providers as xyz\\nfrom bokeh.models import ColumnDataSourc...\"],[\"p.circle(\\n            jitter(\\\"class\\\", 0.3, range=p.x_range),\\n            \\\"hwy\\\",\\n            source=d...\"],[\"# JavaScript Client Library\\n\\nA javascript (and typescript) client to call Gradio APIs.\\n\\n## Installat...\"],[\"```\\n\\n##### `status_callback`\\n\\nThis should be a function which will notify your of the status of a sp...\"],[\"```\\n\\nThe gradio client returns an object with a number of methods and properties:\\n\\n#### `predict`\\n\\nT...\"],[\"```\\n\\nThe `submit` method accepts the same [`endpoint`](#endpoint) and [`payload`](#payload) argument...\"],[\"```\\n\\n##### `off`\\n\\nThe `off` method unsubscribes from a specific event of the submitted job and works...\"],[\"```\\n\\n#### `view_api`\\n\\nThe `view_api` method provides details about the API you are connected to. It ...\"],[\"```\\n\\nThis function accepts two arguments: `source` and `options`:\\n\\n#### `source`\\n\\nThe space to dupli...\"],[\"```\\n\\n##### `hardware`\\n\\nThis is an optional property specific to `duplicate`'s options object and wil...\"],[\"iles in this directory are used in:\\n\\n- tests for the gradio library\\n- example inputs in the view API...\"],[\"How to Use the 3D Model Component\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fModel3D, htt...\"],[\"```python\\nimport gradio as gr\\nimport os\\n\\n\\ndef load_mesh(mesh_file_name):\\n    return mesh_file_name\\n\\n...\"],[\"```\\n\\nLet's break down the code above:\\n\\n`load_mesh`: This is our 'prediction' function and for simpli...\"],[\"@gradio\\u002fdataset\\n\\n## 0.1.13\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.1.10\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.1.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`854b482f5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`aaa55ce85`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.1.0-beta.2\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.1.0-beta.0\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.0.5-beta.3\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.2.0-beta.3\\n\\n## 0....\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.1\\n\\n## 0.0.2\\n\\n### Features\\n\\n- [#5...\"],[\"Gradio Demo: blocks_kinematics\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport pandas as pd\\nimport nu...\"],[\"Gradio Demo: blocks_neural_instrument_coding\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading f...\"],[\"```\\n# A Blocks implementation of https:\\u002f\\u002ferlj.notion.site\\u002fNeural-Instrument-Cloning-from-very-few-sa...\"],[\"io4 = gr.Interface(\\n    lambda x, y, z: os.path.join(os.path.abspath(''),\\\"sax2.wav\\\"),\\n    [\\n        ...\"],[\"m(\\n        \\\"\\\"\\\"\\\\n\\n        Here is a **generated** saxophone recordings:\\\"\\\"\\\"\\n    )\\n    a = gr.Audio(os....\"],[\"Gradio Demo: image_classifier\\n\\n\\n```\\n!pip install -q gradio numpy tensorflow\\n```\\n\\n\\n```\\n# Downloading ...\"],[\"```\\n\\n\\n```\\nimport os\\nimport requests\\nimport tensorflow as tf\\n\\nimport gradio as gr\\n\\ninception_net = tf...\"],[\"Using Gradio Blocks Like Functions\\n\\nTags: TRANSLATION, HUB, SPACES\\n\\n**Prerequisite**: This Guide bui...\"],[\"Option 1 technically always works, but it often introduces unwanted complexity.\\n\\nOption 2 lets you b...\"],[\"```\\n\\nThe `api_name` gives this function a unique name in our app. You can use this name to tell grad...\"],[\"Gradio Demo: animeganv2\\n### Recreate the viral AnimeGAN image transformation demo.\\n        \\n\\n\\n```\\n!p...\"],[\"```\\nimport gradio as gr\\nimport torch\\n\\nmodel2 = torch.hub.load(\\n    \\\"AK391\\u002fanimegan2-pytorch:main\\\",\\n ...\"],[\"demo = gr.Interface(\\n    fn=inference, \\n    inputs=[gr.Image(type=\\\"pil\\\"),gr.Radio(['version 1 (🔺 sty...\"],[\"Gradio Demo: image_segmentation\\n### Simple image segmentation using gradio's AnnotatedImage componen...\"],[\"```\\nimport gradio as gr\\nimport numpy as np\\nimport random\\n\\nwith gr.Blocks() as demo:\\n    section_labe...\"],[\"section_btn = gr.Button(\\\"Identify Sections\\\")\\n    selected_section = gr.Textbox(label=\\\"Selected Secti...\"],[\"Gradio Demo: stable-diffusion\\n### Note: This is a simplified version of the code needed to create th...\"],[\"```\\nimport gradio as gr\\nimport torch\\nfrom diffusers import StableDiffusionPipeline\\nfrom PIL import I...\"],[\"advanced_button = gr.Button(\\\"Advanced options\\\", elem_id=\\\"advanced-btn\\\")\\n\\n        with gr.Row(elem_id...\"],[\"`@gradio\\u002fstatustracker`\\n\\n```html\\n\\u003cscript\\u003e\\n    import {StatusTracker, Toast, Loader} from `@gradio\\u002fst...\"],[\"@gradio\\u002fsimpletextbox\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgith...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`3cdeabc68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0-beta.2\\n\\n### Features\\n\\n- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd...\"],[\"Gradio Demo: chatbot_consecutive\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport ...\"],[\"Gradio Demo: file_explorer\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\nfrom pathlib import Path\\n\\ncurrent_file_path = Path(__file__).resolve()\\nrelat...\"],[\"code = gr.Code(lines=30, scale=2, language=\\\"python\\\")\\n\\n    file_3.change(get_file_content, file_3, co...\"],[\"运行后台任务\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002ffreddyaboulton\\u002fgradio-google-forms\\nTags: TASKS...\"],[\"我们将使用 `sqlite3` 库来连接我们的 sqlite 数据库，但 gradio 可以与任何库一起使用。\\n\\n代码如下 :\\n\\n```python\\nDB_FILE = \\\".\\u002freviews.db\\\"\\n...\"],[\"```\\n\\n让我们还写一个函数，在 gradio 应用程序加载时加载最新的评论 :\\n\\n```python\\ndef load_data():\\n    db = sqlite3.connect(DB_FIL...\"],[\"```\\n\\n## 第三步 - 与 HuggingFace 数据集同步 🤗\\n\\n在第 2 步后我们可以调用 `demo.launch()` 来运行一个完整功能的应用程序。然而，我们的数据将存储在本地机器上。...\"],[\"```\\n\\n请注意，您需要从 HuggingFace 的“设置”选项卡中获取访问令牌，以上代码才能正常工作。在脚本中，通过环境变量安全访问令牌。\\n\\n![access_token](\\u002fassets\\u002fgui...\"],[\"```\\n\\n## 第四步（附加）- 部署到 HuggingFace Spaces\\n\\n您可以使用 HuggingFace [Spaces](https:\\u002f\\u002fhuggingface.co\\u002fspaces) 平...\"],[\"Gradio Demo: file_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Bl...\"],[\"Gradio Demo: blocks_group\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\n\\ndef greet(name):\\n    return \\\"Hello \\\" + name + \\\"!\\\"\\n\\nwith gr.Blocks() as demo...\"],[\"gr.Markdown(\\\"### Several columns grouped together. If columns are uneven, there is a gray group back...\"],[\"with gr.Column():\\n        gr.Radio([1,2,3], container=False)\\n        gr.Slider(0, 20, container=Fals...\"],[\"通过自动重载实现更快的开发\\n\\n**先决条件**：本指南要求您了解块的知识。请确保[先阅读块指南](https:\\u002f\\u002fgradio.app\\u002fquickstart\\u002f#blocks-more-flexibil...\"],[\"inp.change(fn=lambda x: f\\\"欢迎，{x}！\\\",\\n               inputs=inp,\\n               outputs=out)\\n\\nif __nam...\"],[\"```\\n\\n问题在于，每当您想要更改布局、事件或组件时，都必须通过编写 `python run.py` 来关闭和重新运行应用程序。\\n\\n而不是这样做，您可以通过更改 1 个单词来以**重新加载模式**运行...\"],[\"```\\n\\n这里最重要的一行是 `正在观察 ...`。这里发生的情况是 Gradio 将观察 `run.py` 文件所在的目录，如果文件发生更改，它将自动为您重新运行文件。因此，您只需专注于编写代码，G...\"],[\"```\\n\\n那么您可以这样启动它：`gradio run.py my_demo.app`。\\n\\nGradio默认使用UTF-8编码格式。对于**重新加载模式**，如果你的脚本使用的是除UTF-8以外的编码...\"],[\"```\\n\\n您可以像这样运行它：`gradio run.py --name Gretel`\\n\\n作为一个小提示，只要更改了 `run.py` 源代码或 Gradio 源代码，自动重新加载就会发生。这意味着...\"],[\"```\\n\\n请注意：\\n\\n- 您不需要放置样板代码 `with gr.Blocks() as demo:` 和 `demo.launch()` — Gradio 会自动为您完成！\\n\\n- 每次重新运行单元格...\"],[\"Gradio Demo: sentiment_analysis\\n### This sentiment analaysis demo takes in input text and returns it...\"],[\"Gradio Demo: image_mod\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo rep...\"],[\"Gradio Demo: hello_blocks\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef greet(nam...\"],[\"Installing Gradio in a Virtual Environment\\n\\nTags: INSTALLATION\\n\\nIn this guide, we will describe step...\"],[\"```\\n\\n5. **Verification**:\\n   To verify the installation, run `python` and then type:\\n\\n   ```python\\n ...\"],[\"Gradio Demo: score_tracker\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nscores = []\\n...\"],[\"`@gradio\\u002ffile`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseFile, BaseFileUpload, FilePreview, BaseExample } from...\"],[\"Gradio Demo: scatterplot_component\\n\\n\\n```\\n!pip install -q gradio vega_datasets\\n```\\n\\n\\n```\\nimport gradi...\"],[\"Gradio Demo: xgboost-income-prediction-with-explainability\\n### This demo takes in 12 inputs from the...\"],[\"```\\nimport gradio as gr\\nimport random\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport sha...\"],[\"def interpret(*args):\\n    df = pd.DataFrame([args], columns=X_train.columns)\\n    df = df.astype({col...\"],[\"with gr.Blocks() as demo:\\n    gr.Markdown(\\\"\\\"\\\"\\n    **Income Classification with XGBoost 💰**:  This de...\"],[\")\\n            relationship = gr.Dropdown(\\n                label=\\\"Relationship Status\\\",\\n             ...\"],[\"relationship,\\n                    sex,\\n                    capital_gain,\\n                    capital...\"],[\"demo.launch()...\"],[\"his demo built with Blocks generates 9 plots based on the input....\"],[\"Gradio & LLM Agents 🤝\\n\\n非常强大的大型语言模型（LLM），如果我们能赋予它们完成专门任务的技能，它们将变得更加强大。\\n\\n[gradio_tools](https:\\u002f\\u002fgithub...\"],[\"### Gradio是什么？\\n\\n[Gradio](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio)是用于构建机器学习Web应用程序并与全球共享的事实上的标准框架-完全由Pyt...\"],[\"from langchain.memory import ConversationBufferMemory\\n\\nllm = OpenAI(temperature=0)\\nmemory = Conversa...\"],[\"```\\n\\n您会注意到我们正在使用一些与`gradio_tools`一起提供的预构建工具。请参阅此[文档](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton\\u002fgradio-tools#...\"],[\"```\\n\\n需要满足的要求是：...\"],[\"1. 工具的名称\\n2. 工具的描述。这非常关键！代理根据其描述决定使用哪个工具。请确切描述输入和输出应该是什么样的，最好包括示例。\\n3. Gradio应用程序的url或space id，例如`fred...\"],[\"5. postprocess - 给定作业的结果，将其转换为LLM可以向用户显示的字符串。\\n6. _Optional可选_ - 某些库，例如[MiniChain](https:\\u002f\\u002fgithub.com...\"],[\"就是这样！\\n\\n一旦您创建了自己的工具，请在`gradio_tools`存储库上发起拉取请求！我们欢迎所有贡献。\\n\\n## 示例工具 - 稳定扩散\\n\\n以下是作为示例的稳定扩散工具代码：\\n\\nfrom gra...\"],[\"```\\n关于此实现的一些注意事项：\\n1. 所有的 `GradioTool` 实例都有一个名为 `client` 的属性，它指向底层的 [gradio 客户端](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"Gradio Demo: barplot_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport pa...\"],[\"Gradio Demo: blocks_hello\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef welcome(n...\"],[\"his is a fake GAN that shows how to create a text-to-image interface for image generation. Check out...\"],[\"Gradio Demo: loginbutton_component\\n\\n\\n```\\n!pip install -q gradio gradio[oauth]\\n```\\n\\n\\n```\\nimport gradi...\"],[\"@gradio\\u002fgallery\\n\\n## 0.4.14\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.12\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.4.10\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`b639e04`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.4.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`324867f63`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`bca6c2c80`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.2\\n\\n### Fixes\\n\\n- [#6277](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6277) [`5fe091367`](https:\\u002f...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.4.0-beta.9\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.4.0-beta.8\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"- Updated dependencies []:\\n  - @gradio\\u002fimage@0.3.0-beta.7\\n\\n## 0.4.0-beta.6\\n\\n### Features\\n\\n- [#5960](...\"],[\"## 0.5.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`b67115e8e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.5.0\\n\\n### Features\\n\\n- [#5780](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5780) [`ed0f9a21b`](http...\"],[\"## 0.4.1\\n\\n### Fixes\\n\\n- [#5735](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5735) [`abb5e9df4`](https:\\u002f...\"],[\"## 0.3.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e0d61b8ba`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5025](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5025) [`6693660a`](https...\"],[\"在 Web 服务器上使用 Nginx 运行 Gradio 应用\\n\\n标签：部署，Web 服务器，Nginx\\n\\n## 介绍\\n\\nGradio 是一个 Python 库，允许您快速创建可定制的 Web 应用程...\"],[\"```\\n\\n2. 在 `\\u002fetc\\u002fnginx\\u002fsites-available` 目录中创建一个新文件（如果目录不存在则创建），文件名表示您的应用，例如：`sudo nano \\u002fetc\\u002fnginx\\u002fsit...\"],[\"```\\n\\n2. 通过键入 `tmux` 并按回车键（可选）启动 `tmux` 会话\\n\\n推荐在 `tmux` 会话中运行 Gradio 应用，以便可以轻松地在后台运行它\\n\\n3. 然后，启动您的 Grad...\"],[\"@gradio\\u002fhighlightedtext\\n\\n## 0.4.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgi...\"],[\"## 0.4.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.4.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`3cdeabc68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.4.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.4.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.3.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.1\\n\\n### Fixes\\n\\n- [#5602](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5602) [`54d21d3f1`](https:\\u002f...\"],[\"## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5046) [`5244c587`](https...\"],[\"Gradio Demo: stream_frames\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport numpy ...\"],[\"@gradio\\u002futils\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`2...\"],[\"## 0.2.0-beta.5\\n\\n### Features\\n\\n- [#5966](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5966) [`9cad2127b...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e4a307ed6`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"从 Google Sheets 创建实时仪表盘\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n[Google Sheets](https:\\u002f\\u002fwww.google.com\\u002fshee...\"],[\"```\\n\\n2. 现在，修改此网址并使用它从 Google Sheets 读取数据到 Pandas DataFrame 中。 (在下面的代码中，用您的公开 Google Sheet 的网址替换 `URL...\"],[\"```\\n\\n到此为止！您现在拥有一个仪表盘，每 5 秒刷新一次，从 Google Sheets 中获取数据。\\n\\n## 私有 Google Sheets\\n\\n对于私有 Google Sheets，流程需要更...\"],[\"```json\\n{\\n\\t\\\"type\\\": \\\"service_account\\\",\\n\\t\\\"project_id\\\": \\\"your project\\\",\\n\\t\\\"private_key_id\\\": \\\"your privat...\"],[\"```\\n\\n### 查询\\n\\n在获得凭据的 `.json` 文件后，可以按照以下步骤查询您的 Google Sheet：\\n\\n1. 单击 Google Sheet 右上角的“共享”按钮。使用身份验证子部分第...\"],[\"```\\n\\n4\\\\. 数据查询是一个函数，这意味着可以使用 `gr.DataFrame` 组件实时显示数据，或使用 `gr.LinePlot` 组件实时绘制数据（当然，根据数据的不同，可能需要使用不同的图...\"],[\"@gradio\\u002fradio\\n\\n## 0.3.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`f816136a0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.1\\n\\n### Fixes\\n\\n- [#6262](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6262) [`afb72bd19`](https:\\u002f...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.3.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.3\\n  - @gradio\\u002fstatustr...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Gradio Demo: markdown_example\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\n\\ncss = (\\n    \\\"footer {display: none !important;} .gradio-container {min-heig...\"],[\"## Tech\\n\\nDillinger uses a number of open source projects to work properly:\\n\\n- [AngularJS] - HTML enh...\"],[\"```\\n\\nFor production environments...\\n\\n```bash\\nnpm install --production\\nNODE_ENV=production node app\\n`...\"],[\"```\\n\\nThis will create the dillinger image and pull in the necessary dependencies.\\nBe sure to swap ou...\"],[\"```\\n\\n## License\\n\\nMIT\\n\\n**Free Software, Hell Yeah!**\\n\\n[\\u002f\\u002f]: # (These are reference links used in the ...\"],[\"[PlDb]: \\u003chttps:\\u002f\\u002fgithub.com\\u002fjoemccann\\u002fdillinger\\u002ftree\\u002fmaster\\u002fplugins\\u002fdropbox\\u002fREADME.md\\u003e\\n   [PlGh]: \\u003ch...\"],[\"gradio_client\\n\\n## 0.7.3\\n\\n### Fixes\\n\\n- [#6693](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6693) [`34f9...\"],[\"## 0.7.2\\n\\n### Features\\n\\n- [#6598](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6598) [`7cbf96e`](https:...\"],[\"### Fixes\\n\\n- [#6556](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6556) [`d76bcaa`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub...\"],[\"## 0.7.0-beta.2\\n\\n### Features\\n\\n- [#6094](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6094) [`c476bd5a5...\"],[\"## 0.7.0-beta.1\\n\\n### Features\\n\\n- [#6082](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6082) [`037e5af33...\"],[\"## 0.7.0-beta.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13...\"],[\"## 0.6.0\\n\\n### Highlights\\n\\n#### new `FileExplorer` component ([#5672](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fg...\"],[\"Thanks [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94)!\\n\\n## 0.5.3\\n\\n### Features\\n\\n- [#5721](https:\\u002f\\u002fgithub...\"],[\"## 0.5.0\\n\\n### Highlights\\n\\n#### Enable streaming audio in python client ([#5248](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"```\\n\\n Thanks [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton)!\\n\\n### Fixes\\n\\n- [#5295](https:\\u002f\\u002fgit...\"],[\"Thanks [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton)!\\n\\n### Features\\n\\n- [#5076](https:\\u002f\\u002fgithub...\"],[\"We're excited to announce that Gradio can now automatically create a discord bot from any `gr.ChatIn...\"],[\"But once again, you can deploy ANY `gr.ChatInterface` app exposed on the internet! So don't hesitate...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Pinned dependencies to major...\"],[\"No changes to highlight.\\n\\n### Full Changelog:\\n\\nNo changes to highlight.\\n\\n# 0.2.7\\n\\n### New Features:\\n...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\nNo changes to highlight.\\n\\n# 0....\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n# 0.2.3\\n\\n### New Features:\\n\\nNo changes to high...\"],[\"No changes to highlight.\\n\\n# 0.2.1\\n\\n### New Features:\\n\\nNo changes to highlight.\\n\\n### Bug Fixes:\\n\\nRemo...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"Using the `gradio_client` library, we can easily use the Gradio as an API to transcribe audio files ...\"],[\"```\\n\\nRead more about how to use the `gradio_client` library here: https:\\u002f\\u002fgradio.app\\u002fgetting-started...\"],[\"Gradio Demo: blocks_form\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks...\"],[\"Gradio Demo: tictactoe\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks()...\"],[\"Gradio Demo: fake_gan_2\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo re...\"],[\"```\\n\\n\\n```\\n# This demo needs to be run from the repo folder.\\n# python demo\\u002ffake_gan\\u002frun.py\\nimport ran...\"],[\"simple dashboard ranking spaces by number of likes....\"],[\"@gradio\\u002fjson\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`3cdeabc68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.1.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.0.5\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"`gradio\\u002fmodel3d`\\n\\n```html\\n\\u003cscript\\u003e\\n    import {BaseModel3D, BaseModel3DUpload, BaseExample } from `@...\"],[\"@gradio\\u002ftheme\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`2...\"],[\"Thanks to a new capability that allows components to communicate directly with the server _without_ ...\"],[\"## 0.0.3\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.0.2\\n\\n### Fixes\\n\\n- [#4997](https:\\u002f\\u002fgithub.com\\u002fgradio...\"],[\"Gradio Demo: automatic-speech-recognition\\n### Automatic speech recognition English. Record from your...\"],[\"Gradio Demo: event_trigger\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo...\"],[\"```\\n# %%\\nimport gradio as gr\\n\\n\\nTEST_VIDEO_A = \\\"mp4\\u002fa.mp4\\\"\\nTEST_VIDEO_B = \\\"mp4\\u002fb.mp4\\\"\\n\\nTEST_IMAGE_A =...\"],[\"def change_video(index):\\n            if index == 0:\\n                return TEST_VIDEO_A\\n            ...\"],[\"file_btn = gr.Button(\\\"Change interactive\\\")\\n\\n            with gr.Column():\\n                file1 = gr...\"],[\"`@gradio\\u002fbutton`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseChatBot } from \\\"@gradio\\u002fchatbot\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\n\\nB...\"],[\"div align=\\\"center\\\"\\u003e\\n\\n[\\u003cimg src=\\\"readme_files\\u002fgradio.svg\\\" alt=\\\"gradio\\\" width=400\\u003e](https:\\u002f\\u002fgradio.app...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\n# Gradio: Build Machine Learning Web Apps — in Python\\n\\n$getting_started\\n\\n## Questions?\\n\\nIf y...\"],[\"If you like Gradio, please leave us a ⭐ on GitHub!\\n\\n## Open Source Stack\\n\\nGradio is built on top of ...\"],[\"## License\\n\\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in ...\"],[\"```\\n@article{abid2019gradio,\\n  title = {Gradio: Hassle-Free Sharing and Testing of ML Models in the ...\"],[\"Gradio Demo: theme_new_step_3\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nfrom __future__ import annotations\\nfrom typing import Iterable\\nimport gradio as gr\\nfrom gradio.t...\"],[\"class Seafoam(Base):\\n    def __init__(\\n        self,\\n        *,\\n        primary_hue: colors.Color | ...\"],[\"text_size=text_size,\\n            font=font,\\n            font_mono=font_mono,\\n        )\\n        super...\"],[\"seafoam = Seafoam()\\n\\nwith gr.Blocks(theme=seafoam) as demo:\\n    textbox = gr.Textbox(label=\\\"Name\\\")\\n ...\"],[\"Gradio Demo: dataframe_datatype\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport p...\"],[\"Gradio Demo: Echocardiogram-Segmentation\\n\\n\\n```\\n!pip install -q gradio -f https:\\u002f\\u002fdownload.pytorch.or...\"],[\"```\\nimport os\\nimport numpy as np\\nimport torch\\nimport torchvision\\nimport wget \\n\\n\\ndestination_folder =...\"],[\"print(\\\"loading weights from \\\", os.path.join(destination_for_weights, \\\"deeplabv3_resnet50_random\\\"))\\n\\n...\"],[\"import gradio as gr\\n\\ni = gr.Image(label=\\\"Echocardiogram\\\")\\no = gr.Image(label=\\\"Segmentation Mask\\\")\\n\\ne...\"],[\"`@gradio\\u002fbutton`\\n\\n```javascript\\n\\u003cscript\\u003e\\n\\timport { BaseButton } from \\\"@gradio\\u002fbutton\\\";\\n\\timport { cre...\"],[\"Gradio Demo: reverse_audio_2\\n\\n\\n```\\n!pip install -q gradio numpy\\n```\\n\\n\\n```\\nimport gradio as gr\\nimport...\"],[\"Gradio Demo: model3d_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nw...\"],[\"Gradio Demo: sentence_builder\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\n\\n\\ndef sentence_builder(quantity, animal, countries, place, activity_list, mo...\"],[\"demo = gr.Interface(\\n    sentence_builder,\\n    [\\n        gr.Slider(2, 20, value=4, label=\\\"Count\\\", in...\"],[\"@gradio\\u002faudio\\n\\n## 0.6.3\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`7326...\"],[\"## 0.6.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.5.5\\n\\n### Fixes\\n\\n- [#6551](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6551) [`8fc562a`](https:\\u002f\\u002fg...\"],[\"## 0.5.2\\n\\n### Features\\n\\n- [#6419](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6419) [`1959471a8`](http...\"],[\"## 0.5.1\\n\\n### Fixes\\n\\n- [#6382](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6382) [`2090aad73`](https:\\u002f...\"],[\"## 0.4.3\\n\\n### Fixes\\n\\n- [#6317](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6317) [`19af2806a`](https:\\u002f...\"],[\"## 0.4.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2ba14b284`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.4.0-beta.9\\n\\n### Features...\"],[\"- [#6153](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6153) [`1162ed621`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.4.0-beta.8\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.4.0-beta.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`174b73619`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"## 0.4.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5627](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5627) [`b67115e8e`](http...\"],[\"## 0.3.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.3\\n\\n### Fixes\\n\\n- [#5459](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5459) [`bd2fda77`](https:\\u002f\\u002f...\"],[\"## 0.3.0\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5149) [`144df459`](https...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#4993](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4993) [`dc07a9f9`](https...\"],[\"Gradio Demo: waveform\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport random\\n\\n\\nCO...\"],[\"Gradio Demo: hello_login\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport argparse...\"],[\"@gradio\\u002ffileexplorer\\n\\n## 0.3.13\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgith...\"],[\"## 0.3.11\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.3.9\\n\\n### Fixes\\n\\n- [#6550](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6550) [`3156598`](https:\\u002f\\u002fg...\"],[\"## 0.3.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.5\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`324867f63`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`bca6c2c80`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2ba14b284`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.0-beta.2\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.3.0-beta.0\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`796145e2c`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"Using Gradio and Comet\\n\\nTags: COMET, SPACES\\nContributed by the Comet team\\n\\n## Introduction\\n\\nIn this ...\"],[\"```\\n\\nNext, you will need to [sign up for a Comet Account](https:\\u002f\\u002fwww.comet.com\\u002fsignup?utm_source=gr...\"],[\"```\\n\\n## 1. Logging Gradio UI's to your Comet Experiments\\n\\n[![Open In Colab](https:\\u002f\\u002fcolab.research.g...\"],[\"inputs = gr.Image()\\noutputs = gr.Label(num_top_classes=3)\\n\\nio = gr.Interface(\\n    fn=predict, inputs...\"],[\"```\\n\\nThe last line in this snippet will log the URL of the Gradio Application to your Comet Experime...\"],[\"Go to your Comet Project page, and head over to the Panels tab. Click the `+ Add` button to bring up...\"],[\"## 3. Embedding Hugging Face Spaces directly into your Comet Projects\\n\\n\\u003ciframe width=\\\"560\\\" height=\\\"3...\"],[\"## 4. Logging Model Inferences to Comet\\n\\n\\u003ciframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https:\\u002f\\u002fwww.youtube.c...\"],[\"MODEL_NAME = \\\"gpt2\\\"\\n\\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\\n\\n# set model decoder t...\"],[\"return plot\\n\\n\\nwith gr.Blocks() as demo:\\n    start_experiment_btn = gr.Button(\\\"Start New Experiment\\\")...\"],[\"```\\n\\nInferences from this snippet will be saved in the HTML tab of your experiment.\\n\\n\\u003cvideo width=\\\"5...\"],[\"`@gradio\\u002fmarkdown`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseMarkdown, MarkdownCode, BaseExample } from `@g...\"],[\"Gradio Demo: calculator_live\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef calcul...\"],[\"Named-Entity Recognition\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002frajistics\\u002fbiobert_ner_demo, ...\"],[\"```\\n\\nOutput:\\n\\n```bash\\n[{'entity': 'I-LOC',\\n  'score': 0.9988978,\\n  'index': 2,\\n  'word': 'Chicago',\\n...\"],[\"Gradio Demo: fraud_detector\\n\\n\\n```\\n!pip install -q gradio pandas\\n```\\n\\n\\n```\\n# Downloading files from t...\"],[\"!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides\\u002f1)getting_start...\"],[\"[Website](https:\\u002f\\u002fgradio.app)\\n| [Documentation](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002f)\\n| [Guides](https:\\u002f\\u002fgradio....\"],[\"```\\npip install gradio\\n```\\n\\n\\n\\u003e [!TIP]\\n \\u003e it is best to install Gradio in a virtual environment. Deta...\"],[\"```\\n\\n\\n\\n\\u003e [!TIP]\\n \\u003e We shorten the imported name from \\u003ccode\\u003egradio\\u003c\\u002fcode\\u003e to \\u003ccode\\u003egr\\u003c\\u002fcode\\u003e for bett...\"],[\"The `Interface` class has three core arguments:\\n\\n- `fn`: the function to wrap a user interface (UI) ...\"],[\"### Sharing Your Demo\\n\\nWhat good is a beautiful demo if you can't share it? Gradio lets you easily s...\"],[\"```\\n\\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, so...\"],[\"You can build very custom and complex applications using `gr.Blocks()`. For example, the popular ima...\"],[\"Or, if you already know the basics and are looking for something specific, you can search the more [...\"],[\"Building a FastAPI App with the Gradio Python Client\\n\\nTags: CLIENT, API, WEB APP\\n\\nIn this blog post,...\"],[\"```\\n\\nYou will also need to have ffmpeg installed. You can check to see if you already have ffmpeg by...\"],[\"```\\n\\nThat's all the code that's needed -- notice that the API endpoints returns two audio files (one...\"],[\"```\\n\\nEverything else remains the same!\\n\\n---\\n\\nNow, of course, we are working with video files, so we ...\"],[\"```\\n\\nYou can read up on [ffmpeg documentation](https:\\u002f\\u002fffmpeg.org\\u002fffmpeg.html) if you'd like to unde...\"],[\"```\\n\\nIn this example, the FastAPI app has two routes: `\\u002f` and `\\u002fuploadvideo\\u002f`.\\n\\nThe `\\u002f` route return...\"],[\"```\\n\\nWrite the following as the contents of `home.html`:...\"],[\"```html\\n&lt;!DOCTYPE html\\u003e &lt;html\\u003e &lt;head\\u003e &lt;title\\u003eVideo Gallery&lt;\\u002ftitle\\u003e\\n&lt;style\\u003e body { ...\"],[\"&lt;div class=\\\"gallery\\\"\\u003e {% for video in videos %} &lt;div class=\\\"video\\\"\\u003e\\n&lt;video controls\\u003e &lt;so...\"],[\"```\\n\\n## Step 4: Run your FastAPI app\\n\\nFinally, we are ready to run our FastAPI app, powered by the G...\"],[\"Developing Faster with Auto-Reloading\\n\\n**Prerequisite**: This Guide requires you to know about Block...\"],[\"```\\n\\nThe problem is that anytime that you want to make a change to your layout, events, or component...\"],[\"```\\n\\nThe important part here is the line that says `Watching...` What's happening here is that Gradi...\"],[\"```\\n\\nThen you would launch it in reload mode like this: `gradio run.py my_demo`.\\n\\nBy default, the Gr...\"],[\"```\\n\\nWhich you could run like this: `gradio run.py --name Gretel`\\n\\nAs a small aside, this auto-reloa...\"],[\"```\\n\\nNotice that:\\n\\n- You do not need to launch your demo — Gradio does that for you automatically!\\n\\n...\"],[\"Running Background Tasks\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002ffreddyaboulton\\u002fgradio-google...\"],[\"The code will look like this:\\n\\n```python\\nDB_FILE = \\\".\\u002freviews.db\\\"\\ndb = sqlite3.connect(DB_FILE)\\n\\n# C...\"],[\"```\\n\\nLet's also write a function to load the latest reviews when the gradio application loads:\\n\\n```p...\"],[\"```\\n\\n## Step 3 - Synchronize with HuggingFace Datasets 🤗\\n\\nWe could call `demo.launch()` after step 2...\"],[\"```\\n\\nNote that you'll have to get an access token from the \\\"Settings\\\" tab of your HuggingFace for th...\"],[\"```\\n\\n## Step 4 (Bonus) - Deployment to HuggingFace Spaces\\n\\nYou can use the HuggingFace [Spaces](http...\"],[\"How to Use the Plot Component for Maps\\n\\nTags: PLOTS, MAPS\\n\\n## Introduction\\n\\nThis guide explains how ...\"],[\"```\\n\\nIn the code above, we first load the csv data into a pandas dataframe. Let's begin by defining ...\"],[\"```\\n\\nAbove, we create a scatter plot on mapbox by passing it our list of latitudes and longitudes to...\"],[\"```\\n\\nWe layout these components using the `gr.Column` and `gr.Row` and we'll also add event triggers...\"],[\"Gradio Demo: blocks_style\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks(title=\\\"Styling Examples\\\") as demo:\\n    with gr.Column(...\"],[\"Using Flagging\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fcalculator-flagging-crowdsource...\"],[\"There are [four parameters](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002f#interface-header) in `gradio.Interface` that co...\"],[\"## What happens to flagged data?\\n\\nWithin the directory provided by the `flagging_dir` argument, a CS...\"],[\"```\\n\\n\\u003cgradio-app space=\\\"gradio\\u002fcalculator-flag-basic\\u002f\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\nWhen you click the flag button...\"],[\"```\\n\\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to th...\"],[\"```\\n\\n## The HuggingFaceDatasetSaver Callback\\n\\nSometimes, saving the data to a local CSV file doesn't...\"],[\"```\\n\\nNotice that we define our own\\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Fa...\"],[\"At the same time, you might want to use an existing `FlaggingCallback` to avoid writing extra code.\\n...\"],[\"his demo shows how you can build a live interactive dashboard with gradio.\\nThe current time is refre...\"],[\"Gradio Demo: plot_component\\n\\n\\n```\\n!pip install -q gradio matplotlib numpy\\n```\\n\\n\\n```\\nimport gradio as...\"],[\"Gradio Demo: request_ip_headers\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef pr...\"],[\"连接到数据库\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fchicago-bike-share-dashboard\\n标签：TABULAR, PLOTS\\n\\n##...\"],[\"**重要提示**：如果您计划在 HuggingFace Spaces 上托管此演示，请确保数据库在 **8080** 端口上。Spaces\\n将阻止除端口 80、443 或 8080 之外的所有外部连接...\"],[\"```python\\nimport os\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\nDB_USER = os.getenv(\\\"DB_USE...\"],[\"def get_most_popular_stations():\\n\\n    df = pd.read_sql(\\n        \\\"\\\"\\\"\\n    SELECT COUNT(ride_id) as n, ...\"],[\"```\\n\\n如果您在本地运行我们的脚本，可以像下面这样将凭据作为环境变量传递：\\n\\n```bash\\nDB_USER='username' DB_PASSWORD='password' DB_HOST='h...\"],[\"```\\n\\n## 步骤 3 - 部署\\n\\n如果您运行上述代码，您的应用程序将在本地运行。\\n您甚至可以通过将 `share=True` 参数传递给 `launch` 来获得一个临时共享链接。\\n\\n但是如果您想...\"],[\"Gradio Demo: chatinterface_streaming_echo\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport time\\nimport...\"],[\"Gradio Demo: hangman\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nsecret_word = \\\"gra...\"],[\"The 4 Kinds of Gradio Interfaces\\n\\nSo far, we've always assumed that in order to build an Gradio demo...\"],[\"$code_fake_gan_no_input\\n$demo_fake_gan_no_input\\n\\n## Input-only demos\\n\\nSimilarly, to create a demo th...\"],[\"`@gradio\\u002fdataframe`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseDataFrame, BaseExample } from \\\"@gradio\\u002fdatafr...\"],[\"Gradio Demo: line_plot\\n\\n\\n```\\n!pip install -q gradio vega_datasets pandas\\n```...\"],[\"```\\nimport gradio as gr\\nfrom vega_datasets import data\\n\\nstocks = data.stocks()\\ngapminder = data.gapm...\"],[\"def line_plot_fn(dataset):\\n    if dataset == \\\"stocks\\\":\\n        return gr.LinePlot(\\n            stock...\"],[\"if __name__ == \\\"__main__\\\":\\n    line_plot.launch()...\"],[\"Contributing to Gradio\\n\\n![GitHub issues by-label](https:\\u002f\\u002fimg.shields.io\\u002fgithub\\u002fissues\\u002fgradio-app\\u002fgr...\"],[\"```\\n\\n### 📦 Using dev containers\\n\\nYou can alternatively use dev containers. This is supported on all ...\"],[\"If you're a newcomer to Gradio, we recommend getting familiar with the overall structure of the repo...\"],[\"```\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n   gr.Button()\\n    \\nif __name__ == \\\"__main__\\\":\\n  ...\"],[\"```\\npnpm test:browser\\n```\\n\\nTo build the frontend code before running browser tests:\\n\\n```\\npnpm test:b...\"],[\"```\\npnpm storybook\\n```\\n\\n\\n## 📮 Submitting PRs\\n\\nAll PRs should be against `main`, and ideally should a...\"],[\"```\\nbash scripts\\u002fformat_backend.sh\\n```\\n\\n```\\nbash scripts\\u002fformat_frontend.sh\\n```\\n\\nThank you for takin...\"],[\"Gradio Demo: leaderboard\\n### A simple dashboard ranking spaces by number of likes.\\n        \\n\\n\\n```\\n!p...\"],[\"```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport requests\\nimport pandas as pd\\nfrom h...\"],[\"Gradio 界面的 4 种类型\\n\\n到目前为止，我们一直假设构建 Gradio 演示需要同时具备输入和输出。但对于机器学习演示来说，并不总是如此：例如，*无条件图像生成模型*不需要任何输入，但会生成一...\"],[\"我们来看一下如何使用 `Interface` 类构建每种类型的演示，以及示例：\\n\\n## 标准演示 (Standard demos)\\n\\n要创建具有输入和输出组件的演示，只需在 `Interface()`...\"],[\"Gradio Demo: calculator_blocks\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef cal...\"],[\"@gradio\\u002fdataframe\\n\\n## 0.4.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`846d52d`](https:\\u002f\\u002fgithub.c...\"],[\"## 0.4.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.3.9\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`46f13f496`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.4\\n\\n### Features\\n\\n- [#6318](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6318) [`d3b53a457`](http...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5877](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5877) [`a55b80942`](http...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5569](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5569) [`2a5b9e03b`](http...\"],[\"## 0.2.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`ee8eec1e5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.1\\n\\n### Fixes\\n\\n- [#5445](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5445) [`67bb7bcb`](https:\\u002f\\u002f...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`05892302`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`31996c99`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"- [#5268](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5268) [`f49028cf`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5283](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5283) [`a7460557`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes\\n\\n- [#5256](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5256) [`933db53e`](https:\\u002f\\u002fgithub.com...\"],[\"Gradio Demo: rows_and_columns\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the d...\"],[\"Gradio Demo: lineplot_component\\n\\n\\n```\\n!pip install -q gradio vega_datasets\\n```\\n\\n\\n```\\nimport gradio a...\"],[\"@gradio\\u002fbox\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`73268ee`](https:\\u002f\\u002fgithub.com\\u002fgra...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`3cdeabc68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0-beta.6\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.1.0-beta.5\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.0.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"Gradio Demo: webcam\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n\\nimport gradio as gr\\n\\n\\ndef snap(image, v...\"],[\"Gradio Demo: theme_new_step_2\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nfrom __future__ import annotations\\nfrom typing import Iterable\\nimport gradio as gr\\nfrom gradio.t...\"],[\"def repeat(name, count):\\n        time.sleep(3)\\n        return name * count\\n\\n    button.click(repeat,...\"],[\"Gradio 和 ONNX 在 Hugging Face 上\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fonnx\\u002fEfficientNet-Lite...\"],[\"## ONNX 模型仓库是什么？\\n\\nOpen Neural Network Exchange（[ONNX](https:\\u002f\\u002fonnx.ai\\u002f)）是一种表示机器学习模型的开放标准格式。ONNX 由一个实...\"],[\"在此处开始[https:\\u002f\\u002fgradio.app\\u002fgetting_started](https:\\u002f\\u002fgradio.app\\u002fgetting_started)\\n\\n### Hugging Face Spac...\"],[\"## ONNX Runtime 的作用是什么？\\n\\nONNX Runtime 是一个跨平台的推理和训练机器学习加速器。它使得在 Hugging Face 上使用 ONNX 模型仓库中的模型进行实时 Gr...\"],[\"在这里，我们将演示如何使用 Gradio 为 EfficientNet-Lite4 设置示例演示\\n\\n首先，我们导入所需的依赖项并下载和载入来自 ONNX 模型仓库的 efficientnet-lite...\"],[\"# 使用等比例缩放调整图像尺寸\\ndef resize_with_aspectratio(img, out_height, out_width, scale=87.5, inter_pol=cv2.IN...\"],[\"title = \\\"EfficientNet-Lite4\\\"\\ndescription = \\\"EfficientNet-Lite 4是最大的变体，也是EfficientNet-Lite模型集合中最准确的。它...\"],[\"```\\n\\n## 如何使用 ONNX 模型在 HF Spaces 上贡献 Gradio 演示\\n\\n- 将模型添加到[onnx model zoo](https:\\u002f\\u002fgithub.com\\u002fonnx\\u002fmode...\"],[\"Gradio Demo: slider_release\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef identi...\"],[\"How to add support for more languages\\n\\nWe would love to support more languages for Gradio 🌎\\n\\nTo add ...\"],[\"Setting Up a Demo for Maximum Performance\\n\\nTags: CONCURRENCY, LATENCY, PERFORMANCE\\n\\nLet's say that y...\"],[\"(2) They allow the server to send multiple updates to the frontend. This means, for example, that th...\"],[\"```\\n\\n**How Requests are Processed from the Queue**\\n\\nWhen a Gradio server is launched, a pool of thre...\"],[\"```\\n\\nInitially, 3 workers will get dispatched to handle requests 1, 2, and 5 (corresponding to funct...\"],[\"### The `concurrency_limit` parameter in events\\n\\nYou can also set the number of requests that can be...\"],[\"**Recommendation**: For a better user experience, set a `max_size` that is reasonable given your exp...\"],[\"```\\n\\nHere's the same function rewritten to take in a batch of samples:\\n\\n```py\\nimport time\\n\\ndef trim_...\"],[\"Gradio Demo: blocks_outputs\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\n\\n\\ndef make_markdown():\\n    return [\\n        [\\n            \\\"# hello again\\\",\\n ...\"],[\"with gr.Blocks() as demo:\\n    with gr.Column():\\n        txt = gr.Textbox(label=\\\"Small Textbox\\\", line...\"],[\"highlight = gr.HighlightedText(show_label=False)\\n        gr.Dataframe(interactive=True, col_count=(3...\"],[\"[\\n                    \\\"## hello\\\",\\n                    \\\"Hello my name is frank, I am liking the small...\"],[\"Gradio Demo: slider_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr....\"],[\"`@gradio\\u002fradio`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseRadio, BaseExample } from \\\"@gradio\\u002fradio\\\"; \\n\\u003c\\u002fscr...\"],[\"Gradio Demo: clearbutton_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\nwit...\"],[\"`@gradio\\u002fhtml`\\n\\n```javascript\\nimport { BaseHTML } from \\\"@gradio\\u002fhtml\\\";\\n```\\n\\nBaseHTML\\n```javascript\\n\\t...\"],[\"Gradio Demo: hello_world\\n### The simplest possible Gradio demo. It wraps a 'Hello {name}!' function ...\"],[\"Gradio Demo: ner_pipeline\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\nfrom transformer...\"],[\"接口状态 (Interface State)\\n\\n本指南介绍了 Gradio 中如何处理状态。了解全局状态和会话状态的区别，以及如何同时使用它们。\\n\\n## 全局状态 (Global State)\\n\\n您的...\"],[\"聊天机器人就是需要会话状态的一个例子 - 您希望访问用户之前的提交，但不能将聊天记录存储在全局变量中，因为这样聊天记录会在不同用户之间混乱。\\n\\n$code_chatbot_dialogpt\\n$demo...\"],[\"使用 Hugging Face 集成\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fhelsinki_translation_en_es\\n标签：HUB，SPAC...\"],[\"🤗 transformers 库有一个非常易于使用的抽象层，[`pipeline()`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fv4.16.2\\u002fen\\u002fmai...\"],[\"```\\n\\n但是，`gradio` 实际上使将 `pipeline` 转换为演示更加容易，只需使用 `gradio.Interface.from_pipeline` 方法，无需指定输入和输出组件：\\n\\n`...\"],[\"```\\n\\n请注意，我们只需指定模型名称并说明 `src` 应为 `models`（Hugging Face 的 Model Hub）。由于您不会在计算机上加载模型，因此无需安装任何依赖项（除了 `gr...\"],[\"```python\\nfrom huggingface_hub import (\\n    create_repo,\\n    get_full_repo_name,\\n    upload_file,\\n)\\n...\"],[\"```\\n\\n在这里，`create_repo` 使用特定帐户的 Write Token 在特定帐户下创建一个带有目标名称的 gradio repo。`repo_name` 获取相关存储库的完整存储库名称...\"],[\"```\\n\\n请注意，我们使用了 `gr.load()`，这与使用推理 API 加载模型所使用的方法相同。但是，在这里，我们指定 `src` 为 `spaces`（Hugging Face Spaces）...\"],[\"Gradio Demo: chatbot_dialogpt\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\nimport gradi...\"],[\"Gradio Demo: blocks_joined\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo...\"],[\"Gradio Demo: zip_to_json\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nfrom zipfile import ZipFile\\n\\nimport...\"],[\"@gradio\\u002fgroup\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`2...\"],[\"## 0.0.2-beta.0\\n\\n### Features\\n\\n- [#5648](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5648) [`c573e2339...\"],[\"Gradio Demo: titanic_survival\\n\\n\\n```\\n!pip install -q gradio scikit-learn numpy pandas\\n```\\n\\n\\n```\\n# Dow...\"],[\"```\\nimport os\\n\\nimport pandas as pd\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn....\"],[\"def encode_df(df):\\n    df = encode_age(df)\\n    df = encode_fare(df)\\n    sex_mapping = {\\\"male\\\": 0, \\\"f...\"],[\"clf = RandomForestClassifier()\\nclf.fit(X_train, y_train)\\npredictions = clf.predict(X_test)\\n\\n\\ndef pre...\"],[\"`@gradio\\u002fuploadbutton`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseUploadButton } from \\\"@gradio\\u002fuploadbutton\\\"...\"],[\"Gradio Demo: translation\\n### This translation demo takes in the text, source and target languages, a...\"],[\"```\\n!pip install -q gradio git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers gradio torch\\n```\\n\\n\\n```\\nim...\"],[\"Using Hugging Face Integrations\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fen2es\\nTags: HU...\"],[\"```\\n\\nFor any Hugging Face model supported in the Inference API, Gradio automatically infers the expe...\"],[\"```\\n\\nHere, `create_repo` creates a gradio repo with the target name under a specific account using t...\"],[\"```\\n\\nNotice that we use `gr.load()`, the same method we used to load models using the Inference API....\"],[\"```\\n\\nThe previous code produces the following interface, which you can try right here in your browse...\"],[\"使用 Gradio 块像函数一样\\n\\nTags: TRANSLATION, HUB, SPACES\\n\\n**先决条件**: 本指南是在块介绍的基础上构建的。请确保[先阅读该指南](https:\\u002f\\u002fgrad...\"],[\"现在，假设你有一个生成英文文本的应用程序，但你还想额外生成德文文本。\\n\\n你可以选择：\\n\\n1. 将我的英德翻译的源代码复制粘贴到你的应用程序中。\\n\\n2. 在你的应用程序中加载我的英德翻译，并将其当作普通...\"],[\"translate_btn.click(translate, inputs=english, outputs=german, api_name=\\\"translate-to-german\\\")\\n\\n这个 `...\"],[\"### 开始构建！⚒️\\n\\n## Parting Remarks\\n\\n我们展示了如何将 Blocks 应用程序视为常规 Python 函数，以便在不同的应用程序之间组合功能。\\n任何 Blocks 应用程序...\"],[\"Gradio Demo: unispeech-speaker-verification\\n\\n\\n```\\n!pip install -q gradio git+https:\\u002f\\u002fgithub.com\\u002fhugg...\"],[\"```\\nimport gradio as gr\\nimport torch\\nfrom torchaudio.sox_effects import apply_effects_file\\nfrom tran...\"],[\"STYLE = \\\"\\\"\\\"\\n\\u003clink rel=\\\"stylesheet\\\" href=\\\"https:\\u002f\\u002fcdn.jsdelivr.net\\u002fnpm\\u002fbootstrap@5.1.3\\u002fdist\\u002fcss\\u002fboots...\"],[\"\\u003cdiv class=\\\"container\\\"\\u003e\\n        \\u003cdiv class=\\\"row\\\"\\u003e\\u003ch1 style=\\\"text-align: center\\\"\\u003eThe speakers are\\u003c\\u002fh1...\"],[\"EFFECTS = [\\n    [\\\"remix\\\", \\\"-\\\"],\\n    [\\\"channels\\\", \\\"1\\\"],\\n    [\\\"rate\\\", \\\"16000\\\"],\\n    [\\\"gain\\\", \\\"-1.0\\\"],\\n...\"],[\"with torch.no_grad():\\n        emb1 = model(input1).embeddings\\n        emb2 = model(input2).embedding...\"],[\"description = (\\n    \\\"This demo will compare two speech samples and determine if they are from the sa...\"],[\"Gradio Demo: white_noise_vid_not_playable\\n\\n\\n```\\n!pip install -q gradio opencv-python\\n```\\n\\n\\n```\\nimpor...\"],[\"Gradio Demo: logoutbutton_component\\n\\n\\n```\\n!pip install -q gradio gradio[oauth]\\n```\\n\\n\\n```\\nimport grad...\"],[\"Gradio Demo: chicago-bikeshare-dashboard\\n\\n\\n```\\n!pip install -q gradio psycopg2 matplotlib SQLAlchemy...\"],[\"```\\nimport os\\nimport gradio as gr\\nimport pandas as pd\\n\\nDB_USER = os.getenv(\\\"DB_USER\\\")\\nDB_PASSWORD = ...\"],[\"If data were added to the database, the plots in this demo would update\\n    whenever the webpage is ...\"],[\"Gradio Demo: textbox_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr...\"],[\"使用标记\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fcalculator-flagging-crowdsourced, https:\\u002f\\u002fhuggingfac...\"],[\"## 在 `gradio.Interface` 中使用**标记**按钮\\n\\n使用 Gradio 的 `Interface` 进行标记特别简单。默认情况下，在输出组件下方有一个标记为**标记**的按钮。当...\"],[\"- `allow_flagging`：此参数可以设置为 `\\\"manual\\\"`（默认值），`\\\"auto\\\"` 或 `\\\"never\\\"`。\\n  - `manual`：用户将看到一个标记按钮，只有在点击按钮时样...\"],[\"## 标记的数据会发生什么？\\n\\n在 `flagging_dir` 参数提供的目录中，将记录标记的数据的 CSV 文件。\\n\\n以下是一个示例：下面的代码创建了嵌入其中的计算器界面：\\n\\n```python\\n...\"],[\"```\\n\\n\\u003cgradio-app space=\\\"gradio\\u002fcalculator-flag-basic\\u002f\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\n当您点击上面的标记按钮时，启动界面的目录将包括一个新的标记子...\"],[\"```\\n\\n如果您希望用户为标记提供一个原因，您可以将字符串列表传递给 Interface 的 `flagging_options` 参数。用户在标记时必须选择其中一项，选项将作为附加列保存在 CSV ...\"],[\"```\\n\\n## HuggingFaceDatasetSaver 回调\\n\\n有时，将数据保存到本地 CSV 文件是不合理的。例如，在 Hugging Face Spaces 上\\n，开发者通常无法访问托管 ...\"],[\"```\\n\\n注意，我们使用我们的 Hugging Face 令牌和\\n要保存样本的数据集的名称，定义了我们自己的\\n`gradio.HuggingFaceDatasetSaver` 的实例。此外，我们还将 ...\"],[\"同时，您可能希望使用现有的 `FlaggingCallback` 来避免编写额外的代码。\\n这需要两个步骤：\\n\\n1. 您必须在代码中的某个位置运行您的回调的 `.setup()` 方法\\n   在第一次标...\"],[\"Image Classification in PyTorch\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fpytorch-imag...\"],[\"```\\n\\nBecause we will be using the model for inference, we have called the `.eval()` method.\\n\\n## Step...\"],[\"```\\n\\nLet's break this down. The function takes one parameter:\\n\\n- `inp`: the input image as a `PIL` i...\"],[\"Theming\\n\\nTags: THEMES\\n\\n## Introduction\\n\\nGradio features a built-in theming engine that lets you cust...\"],[\"```\\n\\n$demo_theme_builder\\n\\nYou can use the Theme Builder running on Spaces above, though it runs much...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-1.hf.space?__theme=light...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-2.hf.space?__theme=light...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-3.hf.space?__theme=light...\"],[\"```\\n\\nIn the example above, we've set the `loader_color` and `slider_color` variables to `#FF0000`, d...\"],[\"#### Referencing Core Variables\\n\\nTo reference one of the core constructor variables, precede the var...\"],[\"```\\n\\nIn the example above, we've set the `button_primary_background_fill` and `button_primary_backgr...\"],[\"```\\n\\nNow, if we change the `button_primary_background_fill` variable, the `button_primary_background...\"],[\"```\\n\\n`button_primary_border_dark` will draw its value from `button_primary_background_fill_dark`, be...\"],[\"$code_theme_new_step_2\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-new-step-2.hf.space...\"],[\"- Via the class instance\\n\\nEach theme instance has a method called `push_to_hub` we can use to upload...\"],[\"```\\n\\n- Via the command line\\n\\nFirst save the theme to disk\\n\\n```python\\nseafoam.dump(filename=\\\"seafoam....\"],[\"```\\n\\nIn order to upload a theme, you must have a HuggingFace account and pass your [Access Token](ht...\"],[\"You can sort the themes by the number of likes on the space and from most to least recently created ...\"],[\"```\\n\\nYou can also pass the theme string directly to `Blocks` or `Interface` (`gr.Blocks(theme=\\\"gradi...\"],[\"@gradio\\u002ftabs\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`28...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.1.2\\n\\n## 0.0.6\\n\\n### Features\\n\\n- [#5...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"如何创建一个新组件\\n\\n## 简介\\n\\n本指南旨在说明如何添加一个新组件，你可以在 Gradio 应用程序中使用该组件。该指南将通过代码片段逐步展示如何添加[ColorPicker](https:\\u002f\\u002fgr...\"],[\"## 1. 创建一个新的 Python 类并导入它\\n\\n首先要做的是在[components.py](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fblob\\u002fmain\\u002fgra...\"],[\"def __init__(\\n        self,\\n        value: str = None,\\n        *,\\n        label: Optional[str] = Non...\"],[\"# 输入功能\\n    def preprocess(self, x: str | None) -\\u003e Any:\\n        \\\"\\\"\\\"\\n        Any preprocessing needed ...\"],[\"```\\n\\n一旦定义完，就需要在[\\\\_\\\\_init\\\\_\\\\_](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fblob\\u002fmain\\u002fgradio\\u002f__init__.py)模块类中...\"],[\"```\\n\\n### 1.1 为 Python 类编写单元测试\\n\\n在开发新组件时，还应为其编写一套单元测试。这些测试应该放在[gradio\\u002ftest\\u002ftest_components.py](https:\\u002f...\"],[\"color_picker_input.interpretation_replacement = \\\"unknown\\\"\\n\\n        self.assertEqual(\\n            col...\"],[\"```\\n\\n## 2. 创建一个新的 Svelte 组件\\n\\n让我们来看看创建新组件的前端并将其与其 Python 代码映射起来的步骤：\\n\\n- 在 [js 文件夹](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"$: value;\\n\\t$: handle_change(value);\\n\\n\\tconst dispatch = createEventDispatcher\\u003c{\\n\\t\\tchange: string;\\n\\t\\ts...\"],[\"```\\n\\n- 通过执行 `export { default as FileName } from \\\".\\u002fFileName.svelte\\\"`，在您将 Svelte 组件放置的包的 index.ts 文件...\"],[\"export let label: string = \\\"ColorPicker\\\";\\n\\texport let elem_id: string = \\\"\\\";\\n\\texport let visible: boo...\"],[\"```\\n\\n第二个文件包含了前端的测试，例如 ColorPicker 组件的测试：\\n\\n```typescript\\nimport { test, describe, assert, afterEach }...\"],[\"```\\n\\n- `directory.ts` 文件中添加组件的映射。复制并粘贴任何组件的映射行，并编辑其文本。键名必须是 Python 库中实际组件名称的小写版本。例如，对于 ColorPicker 组...\"],[\"```\\n\\n### 2.1 为 Svelte 组件编写单元测试\\n\\n在开发新组件时，您还应该为其编写一套单元测试。测试应该放置在新组件的文件夹中，文件名为 MyAwesomeComponent.test....\"],[\"要测试应用程序：\\n\\n- 在终端上运行 `python path\\u002fdemo\\u002frun.py`，它会在地址 [http:\\u002f\\u002flocalhost:7860](http:\\u002f\\u002flocalhost:7860) 启动...\"],[\"Gradio Demo: concurrency_with_queue\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpo...\"],[\"Gradio Demo: blocks_essay_simple\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef c...\"],[\"@gradio\\u002fform\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`73268ee`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.0.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.0.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`fe057300`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Gradio Demo: blocks_chained_events\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time\\n\\n\\ndef failure():\\n    raise gr....\"],[\"Gradio Demo: dataframe_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith g...\"],[\"Gradio-Lite: Serverless Gradio Running Entirely in Your Browser\\n\\nTags: SERVERLESS, BROWSER, PYODIDE\\n...\"],[\"```\\n\\nAnd that's it! You should now be able to open your HTML page in the browser and see the Gradio ...\"],[\"```\\n\\n**Try it out**: You can see this example running in [this Hugging Face Static Space](https:\\u002f\\u002fhu...\"],[\"控制布局 (Controlling Layout)\\n\\n默认情况下，块中的组件是垂直排列的。让我们看看如何重新排列组件。在幕后，这种布局结构使用了[Web 开发的 flexbox 模型](https:\\u002f...\"],[\"```\\n\\n可以通过每个组件中存在的 `scale` 和 `min_width` 参数来控制行中元素的宽度。\\n\\n- `scale` 是一个整数，定义了元素在行中的占用空间。如果将 scale 设置为 `...\"],[\"```\\n\\n- `min_width` 将设置元素的最小宽度。如果没有足够的空间满足所有的 `min_width` 值，行将换行。\\n\\n在[文档](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002f#row...\"],[\"例如：\\n\\n$code_blocks_flipper\\n$demo_blocks_flipper\\n\\n还请注意本示例中的 `gr.Accordion('label')`。手风琴是一种可以切换打开或关闭的布局...\"],[\"例如：\\n\\n$code_variable_outputs\\n$demo_variable_outputs\\n\\n## 分开定义和渲染组件 (Defining and Rendering Components ...\"],[\"Gradio Demo: blocks_kitchen_sink\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\nimport time\\nfrom os.path import abspath, join, pardir\\n\\nKS_FILES = abspath(jo...\"],[\"var link_elem = document.createElement('link');\\n                link_elem.classList.add('link-css');...\"],[\"with gr.Row():\\n        slider1 = gr.Slider(label=\\\"Slider 1\\\")\\n        slider2 = gr.Slider(label=\\\"Slid...\"],[\"def go(*args):\\n                    time.sleep(3)\\n                    return \\\"https:\\u002f\\u002fi.ibb.co\\u002f6BgKdS...\"],[\"gr.Markdown(\\\"## Media Files\\\")\\n\\n    with gr.Tabs() as tabs:\\n        with gr.Tab(\\\"Audio\\\"):\\n           ...\"],[\"with gr.Row():\\n        with gr.Column(scale=2):\\n            highlight = gr.HighlightedText(\\n        ...\"],[\"gr.Markdown(\\\"## Dataset Examples\\\")\\n\\n    component_example_set = [\\n        (gr.Audio(render=False), j...\"],[\"isplay an interactive map of AirBnB locations with Plotly. Data is hosted on HuggingFace Datasets....\"],[\"使用 Gradio 和 Comet\\n\\nTags: COMET, SPACES\\n由 Comet 团队贡献\\n\\n## 介绍\\n\\n在这个指南中，我们将展示您可以如何使用 Gradio 和 Comet。我们将介绍...\"],[\"```\\n\\n接下来，您需要[注册一个 Comet 账户](https:\\u002f\\u002fwww.comet.com\\u002fsignup?utm_source=gradio&utm_medium=referral&utm_c...\"],[\"```\\n\\n## 1. 将 Gradio UI 记录到您的 Comet 实验中\\n\\n[![在 Colab 中打开](https:\\u002f\\u002fcolab.research.google.com\\u002fassets\\u002fcol...\"],[\"# 为 ImageNet 下载可读的标签。\\nresponse = requests.get(\\\"https:\\u002f\\u002fgit.io\\u002fJJkYN\\\")\\nlabels = response.text.split(\\\"...\"],[\"```\\n\\n此片段中的最后一行将将 Gradio 应用程序的 URL 记录到您的 Comet 实验中。您可以在实验的文本选项卡中找到该 URL。\\n\\n\\u003cvideo width=\\\"560\\\" height=\\\"...\"],[\"转到您的 Comet 项目页面，转到面板选项卡。单击“+ 添加”按钮以打开面板搜索页面。\\n\\n\\u003cimg width=\\\"560\\\" alt=\\\"adding-panels\\\" src=\\\"https:\\u002f\\u002fuser...\"],[\"## 3. 直接将 Hugging Face Spaces 嵌入到您的 Comet 项目中\\n\\n\\u003ciframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https:\\u002f\\u002fwww.you...\"],[\"## 4. 记录模型推断结果到 Comet\\n\\n\\u003ciframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https:\\u002f\\u002fwww.youtube.com\\u002fembed\\u002fKZnpH7msP...\"],[\"if torch.cuda.is_available():\\n    device = \\\"cuda\\\"\\nelse:\\n    device = \\\"cpu\\\"\\n\\nMODEL_NAME = \\\"gpt2\\\"\\n\\nmod...\"],[\"if experiment is not None:\\n        experiment.log_other(\\\"message\\\", message)\\n        experiment.log_h...\"],[\"```\\n\\n该代码段中的推断结果将保存在实验的 HTML 选项卡中。\\n\\n\\u003cvideo width=\\\"560\\\" height=\\\"315\\\" controls\\u003e\\n    \\u003csource src=\\\"https:...\"],[\"@gradio\\u002fcolumn\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`...\"],[\"## 0.1.0-beta.2\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"Gradio Demo: asr\\n\\n\\n```\\n!pip install -q gradio torch torchaudio transformers\\n```\\n\\n\\n```\\nimport gradio ...\"],[\"he simplest possible Gradio demo. It wraps a 'Hello {name}!' function in an Interface that accepts a...\"],[\"his  demo converts text to speech in 14 languages....\"],[\"Contributing a Guide\\n\\nWant to help teach Gradio? Consider contributing a Guide! 🤗\\n\\nBroadly speaking,...\"],[\"## How to Contribute a Guide\\n\\n1. Clone or fork this `gradio` repo\\n2. Add a new markdown document wit...\"],[\"Gradio Demo: blocks_xray\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time\\n\\ndi...\"],[\"Gradio Demo: fake_gan\\n### This is a fake GAN that shows how to create a text-to-image interface for ...\"],[\"```\\n\\n\\n```\\n# This demo needs to be run from the repo folder.\\n# python demo\\u002ffake_gan\\u002frun.py\\nimport ran...\"],[\"如何使用 3D 模型组件\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fdawood\\u002fModel3D, https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fradam...\"],[\"```python\\nimport gradio as gr\\n\\ndef load_mesh(mesh_file_name):\\n    return mesh_file_name\\n\\ndemo = gr.I...\"],[\"```\\n\\n让我们来解析上面的代码：\\n\\n`load_mesh`：这是我们的“预测”函数，为简单起见，该函数将接收 3D 模型网格并返回它。\\n\\n创建界面：\\n\\n- `fn`：当用户点击提交时使用的预测函数。...\"],[\"下面是一个使用 PIFu 模型将穿着衣物的人的图像转换为 3D 数字化模型的演示。查看[spaces.py](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fradames\\u002fPIFu-Cl...\"],[\"@gradio\\u002faccordion\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.c...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.1\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.4\\n  - @gradio\\u002fstatustracker@0.2.2\\n\\n## 0.1.1\\n\\n### Pa...\"],[\"## 0.0.2\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5215) [`fbdad78a`](https...\"],[\"Gradio Demo: highlightedtext_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n...\"],[\"Gradio Demo: on_listener_decorator\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith...\"],[\"Gradio Demo: zip_files\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo rep...\"],[\"his demo uses a fake model to showcase iterative output. The Image output will update every time a g...\"],[\"Gradio Demo: color_picker\\n\\n\\n```\\n!pip install -q gradio Pillow\\n```\\n\\n\\n```\\n# Downloading files from the...\"],[\"Gradio Demo: tax_calculator\\n### Calculate taxes using Textbox, Radio, and Dataframe components\\n     ...\"],[\"Customizing your demo with CSS and Javascript\\n\\nGradio allows you to customize your demo in several w...\"],[\"```\\n\\nNote: By default, files in the host machine are not accessible to users running the Gradio app....\"],[\"```\\n\\nThe CSS `#warning` ruleset will only target the second Textbox, while the `.feedback` ruleset w...\"],[\"```python\\nhead = f\\\"\\\"\\\"\\n\\u003cscript async src=\\\"https:\\u002f\\u002fwww.googletagmanager.com\\u002fgtag\\u002fjs?id={google_analyti...\"],[\"```\\n\\nNote: The `head` parameter accepts any HTML tags you would normally insert into the `\\u003chead\\u003e` of...\"],[\"The `Interface` class\\n\\nAs mentioned in the [Quickstart](\\u002fmain\\u002fguides\\u002fquickstart), the `gr.Interface`...\"],[\"Just as each component in the `inputs` list corresponds to one of the parameters of the function, in...\"],[\"```\\n\\nAlso note that our input `Image` component comes with an edit button 🖉, which allows for croppi...\"],[\"![annotated](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fblob\\u002fmain\\u002fguides\\u002fassets\\u002fannotated.png?raw=true)\\n\\nI...\"],[\"```\\n\\n## Flagging\\n\\nBy default, an `Interface` will have \\\"Flag\\\" button. When a user testing your `Inte...\"],[\"Gradio Demo: variable_outputs\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nmax_textb...\"],[\"Security Policy\\n\\n## Reporting a Vulnerability\\n\\nIf you discover a security vulnerability, we would be...\"],[\"Configuring Your Custom Component\\n\\nThe custom components workflow focuses on [convention over config...\"],[\"```\\n\\n\\nTip: Remember to change the import statement in `demo\\u002fapp.py`!\\n\\n## Top Level Python Exports\\n\\nB...\"],[\"```\\n\\n## Directory Structure\\n\\nBy default, the CLI will place the Python code in `backend` and the Jav...\"],[\"Gradio Demo: blocks_gpt\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\napi = gr.load(\\\"...\"],[\"imple image segmentation using gradio's AnnotatedImage component....\"],[\"Gradio Demo: save_file_no_output\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport random\\nimport string...\"],[\"@gradio\\u002fcolorpicker\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.3\\n  - @gradio\\u002fstatustr...\"],[\"## 0.0.2\\n\\n### Fixes\\n\\n- [#5118](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5118) [`1b017e68`](https:\\u002f\\u002f...\"],[\"Gradio Demo: code_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blo...\"],[\"Gradio Demo: fake_diffusion\\n### This demo uses a fake model to showcase iterative output. The Image ...\"],[\"Gradio Demo: altair_plot\\n\\n\\n```\\n!pip install -q gradio altair vega_datasets\\n```...\"],[\"```\\nimport altair as alt\\nimport gradio as gr\\nimport numpy as np\\nimport pandas as pd\\nfrom vega_datase...\"],[\"pts = alt.selection(type=\\\"single\\\", encodings=['x'])\\n\\n        rect = alt.Chart(data.movies.url).mark_...\"],[\"c2 = base.mark_text(radiusOffset=10).encode(text=\\\"values:Q\\\")\\n\\n        return c1 + c2\\n    elif plot_t...\"],[\"Gradio Demo: native_plots\\n\\n\\n```\\n!pip install -q gradio vega_datasets\\n```\\n\\n\\n```\\n# Downloading files f...\"],[\"Gradio Demo: same-person-or-different\\n### This demo identifies if two speakers are the same person u...\"],[\"```\\nimport gradio as gr\\nimport torch\\nfrom torchaudio.sox_effects import apply_effects_file\\nfrom tran...\"],[\"OUTPUT_OK = (\\n    \\\"\\\"\\\"\\n    \\u003cdiv class=\\\"container\\\"\\u003e\\n        \\u003cdiv class=\\\"row\\\"\\u003e\\u003ch1 style=\\\"text-align: ce...\"],[\"EFFECTS = [\\n    [\\\"remix\\\", \\\"-\\\"],\\n    [\\\"channels\\\", \\\"1\\\"],\\n    [\\\"rate\\\", \\\"16000\\\"],\\n    [\\\"gain\\\", \\\"-1.0\\\"],\\n...\"],[\"with torch.no_grad():\\n        emb1 = model(input1).embeddings\\n        emb2 = model(input2).embedding...\"],[\"interface = gr.Interface(\\n    fn=similarity_fn,\\n    inputs=inputs,\\n    outputs=output,\\n    layout=\\\"h...\"],[\"How to Create a Custom Chatbot with Gradio Blocks\\n\\nTags: NLP, TEXT, CHAT\\nRelated spaces: https:\\u002f\\u002fhug...\"],[\"$code_chatbot_simple\\n\\nThere are three Gradio components here:\\n\\n- A `Chatbot`, whose value stores the...\"],[\"2. The second method, `bot()` updates the chatbot history with the bot's response. Instead of creati...\"],[\"```py\\nimport gradio as gr\\n\\ndef greet(history, input):\\n    return history + [(input, \\\"Hello, \\\" + inpu...\"],[\"```\\n\\n## Adding Markdown, Images, Audio, or Videos\\n\\nThe `gr.Chatbot` component supports a subset of m...\"],[\"Gradio Demo: video_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwit...\"],[\"Gradio Demo: bar_plot\\n\\n\\n```\\n!pip install -q gradio pandas\\n```...\"],[\"```\\nimport gradio as gr\\nimport pandas as pd\\nimport random\\n\\nsimple = pd.DataFrame(\\n    {\\n        \\\"a\\\":...\"],[\"def bar_plot_fn(display):\\n    if display == \\\"simple\\\":\\n        return gr.BarPlot(\\n            simple,...\"],[\"with gr.Blocks() as bar_plot:\\n    with gr.Row():\\n        with gr.Column():\\n            display = gr....\"],[\"Gradio Demo: progress_simple\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time...\"],[\"@gradio\\u002fuploadbutton\\n\\n## 0.3.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithu...\"],[\"## 0.3.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#6584](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6584) [`9bcb1da`](https:...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#6461](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6461) [`6b53330a5`](http...\"],[\"## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`bca6c2c80`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub...\"],[\"## 0.1.0-beta.7\\n\\n### Features...\"],[\"## 0.1.0-beta.6\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.1.0-beta.5\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.0.11\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`796145e2c`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 0.0.8\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fupload@0.3.1\\n  - @gradio\\u002fbutton@...\"],[\"## 0.0.5\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`26fef8c7`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.0.3\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.0.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6...\"],[\"The Frontend 🌐⭐️\\n\\nThis guide will cover everything you need to know to implement your custom compone...\"],[\"```\\n\\n* `elem_id` and `elem_classes` allow Gradio app developers to target your component with custom...\"],[\"```\\n\\n## The Example.svelte file\\n\\nThe `Example.svelte` file should expose the following props:\\n\\n```ty...\"],[\"```\\n\\n## Handling Files\\n\\nIf your component deals with files, these files **should** be uploaded to th...\"],[\"```\\n\\nThe component exposes a prop named `root`. \\nThis is passed down by the parent gradio app and it...\"],[\"```\\n\\n## Leveraging Existing Gradio Components\\n\\nMost of Gradio's frontend components are published on...\"],[\"```\\n\\nYou can also combine existing Gradio components to create entirely unique experiences.\\nLike ren...\"],[\"@gradio\\u002fsimpledropdown\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgit...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`3cdeabc68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0-beta.3\\n\\n### Features\\n\\n- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd...\"],[\"Test Coverage\\n\\nJust a little reference docs to understand what is tested\\u002f needs testing. Perhaps tem...\"],[\"| Component       | `value` | `visible` | `elem_id` | `elem_classes` | `container` | `label` | `show...\"],[\"| ColorPicker     | `❌`    | `✅`      | `✅`      | `✅`           | `❌`        | `✅`    | `❌`        ...\"],[\"| Number          | `❌`    | `✅`      | `✅`      | `✅`           | `❌`        | `✅`    | `❌`        ...\"],[\"### Events...\"],[\"| Component       | `value` | `visible` | `elem_id` | `elem_classes` | `container` | `label` | `show...\"],[\"| ColorPicker     | `❌`    | `❌`      | `❌`      | `❌`           | `❌`        | `❌`    | `❌`        ...\"],[\"| Markdown        | `❌`    | `❌`      | `❌`      | `❌`           | `❌`        | `❌`    | `❌`        ...\"],[\"### `AnnotatedImage`\\n\\n### `Audio`\\n\\n### `BarPlot`\\n\\n### `Button`\\n\\n### `Chatbot`\\n\\n### `Checkbox`\\n\\n### `...\"],[\"### `Request`\\n\\n### `mount_gradio_app`\\n\\n## Clients\\n\\n### Python (`gradio_client`)\\n\\n### JavaScript (`@g...\"],[\"@gradio\\u002fcode\\n\\n## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.2.9\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`3cdeabc68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fupload@0.3.1\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"## 0.0.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`667875b2`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Gradio Demo: text_analysis\\n### This simple demo takes advantage of Gradio's HighlightedText, JSON an...\"],[\"Gradio Demo: diff_texts\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nfrom difflib import Differ\\n\\nimport g...\"],[\"Gradio Demo: code\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo repo\\nimp...\"],[\"Reactive Interfaces\\n\\nFinally, we cover how to get Gradio demos to refresh automatically or continuou...\"],[\"`@gradio\\u002ftheme`\\n\\ncss for gradio...\"],[\"Gradio Demo: chatbot_simple\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport rando...\"],[\"his sentiment analaysis demo takes in input text and returns its classification for either positive,...\"],[\"Gradio Demo: count_generator\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time...\"],[\"Creating a Real-Time Dashboard from Google Sheets\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n\\n[Google Sheets](...\"],[\"```\\n\\n2\\\\. Now, let's modify this URL and then use it to read the data from the Google Sheets into a P...\"],[\"```\\n\\nAnd that's it! You have a dashboard that refreshes every 5 seconds, pulling the data from your ...\"],[\"6\\\\. After selecting the service account, select the \\\"JSON\\\" key type and then click on the \\\"Create\\\" b...\"],[\"```\\n\\n### Querying\\n\\nOnce you have the credentials `.json` file, you can use the following steps to qu...\"],[\"```\\n\\n4\\\\. The data query is a function, which means that it's easy to display it real-time using the ...\"],[\"@gradio\\u002fnumber\\n\\n## 0.3.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.3\\n  - @gradio\\u002fstatustr...\"],[\"## 0.2.0\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5047](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5047) [`883ac364`](https...\"],[\"@gradio\\u002fplot\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`3cdeabc68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0-beta.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`667802a6c`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.2\\n\\n### Fixes\\n\\n- [#5795](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5795) [`957ba5cfd`](https:\\u002f...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5642](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5642) [`21c7225bd`](http...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Gradio Demo: chatbot_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr....\"],[\"Gradio Demo: interface_random_slider\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\nd...\"],[\"his demo identifies musical instruments from an audio file. It uses Gradio's Audio and Label compone...\"],[\"Gradio Demo: blocks_js_load\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef welcome...\"],[\"Gradio Demo: image_mod_default_image\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files fro...\"],[\"Gradio Demo: on_listener_basic\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr....\"],[\"Gradio Demo: reversible_flow\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef increa...\"],[\"@gradio\\u002fcheckboxgroup\\n\\n## 0.3.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgith...\"],[\"## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`f816136a0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"Gradio Demo: digit_classifier\\n\\n\\n```\\n!pip install -q gradio tensorflow\\n```\\n\\n\\n```\\nfrom urllib.request ...\"],[\"@gradio\\u002fwasm\\n\\n## 0.4.0\\n\\n### Features\\n\\n- [#6398](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6398) [`67...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#6099](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6099) [`d84209703`](http...\"],[\"## 0.2.0-beta.1\\n\\n### Features\\n\\n- [#5963](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5963) [`174b73619...\"],[\"## 0.2.0-beta.0\\n\\n### Features\\n\\n- [#5956](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5956) [`f769876e0...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5868](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5868) [`4e0d87e9c`](http...\"],[\"### Fixes\\n\\n- [#5919](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5919) [`1724918f0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.0.2\\n\\n### Fixes\\n\\n- [#5538](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5538) [`b5c6f7b08`](https:\\u002f...\"],[\"@gradio\\u002fcheckbox\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.3\\n  - @gradio\\u002fstatustr...\"],[\"## 0.1.1\\n\\n### Fixes\\n\\n- [#5340](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5340) [`df090e89`](https:\\u002f\\u002f...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Gradio Demo: input_output\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef image_mo...\"],[\"Key Features\\n\\nLet's go through some of the key features of Gradio. This guide is intended to be a hi...\"],[\"**Preprocessing and Postprocessing**\\n\\nWhen a component is used as an input, Gradio automatically han...\"],[\"```\\n\\nPostprocessing is even simpler! Gradio automatically recognizes the format of the returned data...\"],[\"```\\n\\nThis limits the number of requests processed for this event listener at a single time to 5. By ...\"],[\"```\\n\\nYou supply a generator into Gradio the same way as you would a regular function. For example, h...\"],[\"```\\n\\nGradio comes with a set of prebuilt themes which you can load from `gr.themes.*`. You can exten...\"],[\"```\\n\\nThe advantage of using batched functions is that if you enable queuing, the Gradio server can a...\"],[\"Gradio Demo: file_explorer_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nw...\"],[\"Gradio Demo: timeseries-forecasting-with-prophet\\n### A simple dashboard showing pypi stats for pytho...\"],[\"```\\nimport gradio as gr\\nimport pypistats\\nfrom datetime import date\\nfrom dateutil.relativedelta impor...\"],[\"plt = gr.Plot()\\n\\n    lib.change(get_forecast, [lib, time], plt, queue=False)\\n    time.change(get_for...\"],[\"Custom Components in 5 minutes\\n\\nGradio 4.0 introduces Custom Components -- the ability for developer...\"],[\"Each of these steps is done via the Custom Component CLI. You can invoke it with `gradio cc` or `gra...\"],[\"```\\n\\nInstead of `MyComponent`, give your component any name.\\n\\nInstead of `SimpleTextbox`, you can us...\"],[\"```\\n\\nThis will create a `tar.gz` and `.whl` file in a `dist\\u002f` subdirectory.\\nIf you or anyone install...\"],[\"Test Strategy\\n\\nVery brief, mildly aspirational test strategy document. This isn't where we are but i...\"],[\"## Types of testing\\n\\nOur tests will broadly fall into one of three categories:\\n\\n- Static Quality che...\"],[\"## Testing tools\\n\\nWe currently use the following tools:\\n\\n### Static quality checks\\n\\n- Python type-ch...\"],[\"## Test execution\\n\\nTests need to be executed in a number of environments and at different stages of ...\"],[\"Gradio Demo: audio_debugger\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the dem...\"],[\"`@gradio\\u002fupload`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { Upload, ModifyUpload, normalise_file, get_fetchable_...\"],[\"```\\n\\nModifyUpload\\n```javascript\\n    export let editable = false;\\n\\texport let undoable = false;\\n\\texpo...\"],[\"Gradio Demo: hello_blocks_decorator\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\nwi...\"],[\"Connecting to a Database\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fchicago-bikeshare-das...\"],[\"Once your database is created, download the dataset from Kaggle and upload it to your database.\\nFor ...\"],[\"connection_string = f\\\"postgresql:\\u002f\\u002f{DB_USER}:{DB_PASSWORD}@{DB_HOST}?port={PORT}&dbname={DB_NAME}\\\"\\n\\n...\"],[\"```\\n\\nIf you were to run our script locally, you could pass in your credentials as environment variab...\"],[\"```\\n\\n## Step 3 - Deployment\\n\\nIf you run the code above, your app will start running locally.\\nYou can...\"],[\"Gradio Demo: image_classifier_2\\n\\n\\n```\\n!pip install -q gradio pillow torch torchvision\\n```\\n\\n\\n```\\n# Do...\"],[\"Gradio Demo: scatter_plot\\n\\n\\n```\\n!pip install -q gradio vega_datasets pandas\\n```...\"],[\"```\\nimport gradio as gr\\nfrom vega_datasets import data\\n\\ncars = data.cars()\\niris = data.iris()\\n\\n# # O...\"],[\"# cars = pd.DataFrame(cars_data)\\n# iris = pd.DataFrame(iris_data)\\n\\n\\ndef scatter_plot_fn(dataset):\\n  ...\"],[\"使用Gradio Python客户端构建FastAPI应用\\n\\nTags: CLIENT, API, WEB APP\\n\\n在本博客文章中，我们将演示如何使用 `gradio_client` [Python...\"],[\"```\\n\\n否则，通过按照这些说明安装ffmpeg [链接](https:\\u002f\\u002fwww.hostinger.com\\u002ftutorials\\u002fhow-to-install-ffmpeg)。\\n\\n## 步骤1：编写...\"],[\"```\\n\\n所需的代码仅如上所示--请注意，API端点返回一个包含两个音频文件（一个没有音乐，一个只有音乐）的列表，因此我们只返回列表的第一个元素。\\n\\n---\\n\\n**注意**：由于这是一个公共Space...\"],[\"```\\n\\n其他的代码保持不变！\\n\\n---\\n\\n现在，当然，我们正在处理视频文件，所以我们首先需要从视频文件中提取音频。为此，我们将使用`ffmpeg`库，它在处理音频和视频文件时做了很多艰巨的工作。使用...\"],[\"```\\n\\n如果您想了解所有命令行参数的详细信息，请阅读[ffmpeg文档](https:\\u002f\\u002fffmpeg.org\\u002fffmpeg.html)，因为它们超出了本教程的范围。\\n\\n## 步骤2: 创建一个Fa...\"],[\"```\\n\\n在这个示例中，FastAPI应用程序有两个路由：`\\u002f` 和 `\\u002fuploadvideo\\u002f`。\\n\\n`\\u002f` 路由返回一个显示所有上传视频的画廊的HTML模板。\\n\\n`\\u002fuploadvideo\\u002f` ...\"],[\"```\\n\\n将以下内容写入`home.html`文件中：...\"],[\"```html\\n&lt;!DOCTYPE html\\u003e &lt;html\\u003e &lt;head\\u003e &lt;title\\u003e 视频库 &lt;\\u002ftitle\\u003e &lt;style\\u003e\\nbody { font-fam...\"],[\"&lt;div class=\\\"gallery\\\"\\u003e {% for video in videos %} &lt;div class=\\\"video\\\"\\u003e\\n&lt;video controls\\u003e &lt;so...\"],[\"```\\n\\n## 第4步：运行 FastAPI 应用\\n\\n最后，我们准备好运行由 Gradio Python 客户端提供支持的 FastAPI 应用程序。\\n\\n打开终端并导航到包含 `main.py` 文件...\"],[\"Gradio Demo: theme_extended_step_3\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"@gradio\\u002ftabitem\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [...\"],[\"## 0.1.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.0.5\\n\\n### Features\\n\\n- [#5590](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5590) [`d1ad1f671`](http...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"从 BigQuery 数据创建实时仪表盘\\n\\nTags: 表格 , 仪表盘 , 绘图\\n\\n[Google BigQuery](https:\\u002f\\u002fcloud.google.com\\u002fbigquery) 是一个基...\"],[\"## 设置 BigQuery 凭据\\n\\n要使用 Gradio 和 BigQuery，您需要获取您的 BigQuery 凭据，并将其与 [BigQuery Python 客户端](https:\\u002f\\u002fpypi...\"],[\"6. 在选择服务帐号后，选择“JSON”密钥类型，然后单击“创建”按钮。这将下载包含您凭据的 JSON 密钥文件到您的计算机。它的外观类似于以下内容：\\n\\n```json\\n{\\n\\t\\\"type\\\": \\\"ser...\"],[\"```\\n\\n## 使用 BigQuery 客户端\\n\\n获得凭据后，您需要使用 BigQuery Python 客户端使用您的凭据进行身份验证。为此，您需要在终端中运行以下命令安装 BigQuery Pyt...\"],[\"```\\n\\n## 构建实时仪表盘\\n\\n一旦您有了查询数据的函数，您可以使用 Gradio 库的 `gr.DataFrame` 组件以表格形式显示结果。这是一种检查数据并确保查询正确的有用方式。\\n\\n以下是如...\"],[\"```\\n\\n也许您想在我们的仪表盘中添加一个可视化效果。您可以使用 `gr.ScatterPlot()` 组件将数据可视化为散点图。这可以让您查看数据中不同变量（例如病例数和死亡数）之间的关系，并可用于...\"],[\"Gradio Demo: progress\\n\\n\\n```\\n!pip install -q gradio tqdm datasets\\n```...\"],[\"```\\nimport gradio as gr\\nimport random\\nimport time\\nimport tqdm\\nfrom datasets import load_dataset\\nimpo...\"],[\"# track iterable of unknown length\\n    def load_random(data, progress=gr.Progress()):\\n        def yi...\"],[\"def bind_internal_tqdm(data, progress=gr.Progress(track_tqdm=True)):\\n        outdir = \\\"__tmp\\u002f\\\" + str...\"],[\"Gradio Demo: theme_new_step_1\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nfrom gradi...\"],[\"Interface State\\n\\nSo far, we've assumed that your demos are *stateless*: that they do not persist inf...\"],[\"Notice how the state persists across submits within each page, but if you load this demo in another ...\"],[\"Gradio Demo: tabbed_interface_lite\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nhell...\"],[\"Gradio Demo: theme_extended_step_1\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"使用 GAN 创建您自己的朋友\\n\\nspaces\\u002fNimaBoscarino\\u002fcryptopunks, https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fnateraw\\u002fcryptopunks...\"],[\"今天我们将简要介绍 GAN 的高级直觉，然后我们将围绕一个预训练的 GAN 构建一个小型演示，看看这一切都是怎么回事。下面是我们将要组合的东西的一瞥：\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fni...\"],[\"生成器不断训练以创建对鉴别器更难以识别的图像，而鉴别器每次正确检测到伪造图像时，都会为生成器设置更高的门槛。随着网络之间的这种竞争（**adversarial 对抗性！**），生成的图像改善到了对人眼...\"],[\"```python\\nfrom torch import nn\\n\\nclass Generator(nn.Module):\\n    # 有关nc，nz和ngf的解释，请参见下面的链接\\n    # http...\"],[\"```\\n\\n我们正在使用来自[此 repo 的 @teddykoker](https:\\u002f\\u002fgithub.com\\u002fteddykoker\\u002fcryptopunks-gan\\u002fblob\\u002fmain\\u002ftrain.py...\"],[\"```\\n\\n## 步骤 2 - 定义“predict”函数\\n\\n`predict` 函数是使 Gradio 工作的关键！我们通过 Gradio 界面选择的任何输入都将通过我们的 `predict` 函数传...\"],[\"```\\n\\n我们给 `predict` 函数一个 `seed` 参数，这样我们就可以使用一个种子固定随机张量生成。然后，我们可以通过传入相同的种子再次查看生成的 punks。\\n\\n_注意！_ 我们的模型需...\"],[\"```\\n\\n启动界面后，您应该会看到像这样的东西 :\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fnimaboscarino-cryptopunks-1.hf.space\\\" frameBorder=\\\"0...\"],[\"```\\n\\n新的输入将传递给我们的 `predict()` 函数，所以我们必须对该函数进行一些更改，以接受一个新的参数 :\\n\\n```python\\ndef predict(seed, num_punks)...\"],[\"```\\n\\n`examples` 参数接受一个列表的列表，其中子列表中的每个项目的顺序与我们列出的 `inputs` 的顺序相同。所以在我们的例子中，`[seed, num_punks]`。试一试吧！\\n...\"],[\"```python\\nimport torch\\nfrom torch import nn\\nfrom huggingface_hub import hf_hub_download\\nfrom torchvi...\"],[\"```\\n\\n---\\n\\n恭喜！你已经成功构建了自己的基于 GAN 的 CryptoPunks 生成器，配备了一个时尚的 Gradio 界面，使任何人都能轻松使用。现在你可以在 Hub 上[寻找更多的 GA...\"],[\"Gradio Demo: blocks_flipper\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport numpy as np\\nimport gradio...\"],[\"@gradio\\u002fannotatedimage\\n\\n## 0.3.13\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgi...\"],[\"## 0.3.11\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.3.10\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.3.9\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.3.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`854b482f5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`aaa55ce85`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub...\"],[\"## 0.3.0-beta.2\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.3.0-beta.0\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"## 0.0.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`667875b2`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Gradio Demo: stt_or_tts\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ntts_examples = ...\"],[\"Gradio and W&B Integration\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fakhaliq\\u002fJoJoGAN\\n标签：WANDB, SPACES\\n由 Gr...\"],[\"\\u003cimg alt=\\\"Screen Shot 2022-08-01 at 5 54 59 PM\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f81195...\"],[\"让我们开始吧！\\n\\n1. 创建 W&B 账号\\n\\n   如果您还没有 W&B 账号，请按照[这些快速说明](https:\\u002f\\u002fapp.wandb.ai\\u002flogin)创建免费账号。这不应该超过几分钟的时间。一...\"],[\"```\\n\\n3. 微调 StyleGAN 和 W&B 实验跟踪\\n\\n   下一步将打开一个 W&B 仪表板，以跟踪实验，并显示一个 Gradio 演示提供的预训练模型，您可以从下拉菜单中选择。这是您需要的...\"],[\"for idx in tqdm(range(num_iter)):\\n       mean_w = generator.get_latent(torch.randn([latents.size(0),...\"],[\"```\\n\\n4. 保存、下载和加载模型\\n\\n   以下是如何保存和下载您的模型。\\n\\n   ```python\\n\\n   from PIL import Image\\n   import torch\\n   to...\"],[\"generator = deepcopy(original_generator)\\n\\n   ckpt = torch.load(\\\"\\u002fcontent\\u002fJoJoGAN\\u002fyour-model-name.pt\\\"...\"],[\"```\\n\\n5. 构建 Gradio 演示\\n\\n   ```python\\n\\n   import gradio as gr\\n\\n   title = \\\"JoJoGAN\\\"\\n   description = \\\"J...\"],[\"```\\n\\n7.（可选）在 Gradio 应用程序中嵌入 W&B 图\\n\\n    也可以在 Gradio 应用程序中嵌入 W&B 图。为此，您可以创建一个 W&B 报告，并在一个 `gr.HTML` 块中...\"],[\"```\\n\\n## 结论\\n\\n希望您喜欢此嵌入 Gradio 演示到 W&B 报告的简短演示！感谢您一直阅读到最后。回顾一下 :\\n\\n- 仅需要一个单一参考图像即可对 JoJoGAN 进行微调，通常在 GPU...\"],[\"Gradio Demo: dataset\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo repo\\n...\"],[\"```\\nimport gradio as gr\\nimport os\\nimport numpy as np\\n\\n\\ntxt = \\\"the quick brown fox\\\"\\nnum = 10\\n\\nimg = o...\"],[\"c_2 = gr.CheckboxGroup(visible=False, choices=['a', 'b', 'c'])\\n    gr.Dataset(\\n        label=\\\"Checkb...\"],[\"[np.random.randint(0, 10, (10, 10)).tolist()],\\n        ],\\n    )\\n    d_2 = gr.Dropdown(visible=False,...\"],[\")\\n    i = gr.Image(visible=False)\\n    gr.Dataset(\\n        components=[i],\\n        label=\\\"Image\\\",\\n   ...\"],[\")\\n    s = gr.Slider(visible=False)\\n    gr.Dataset(\\n        label=\\\"Slider\\\",\\n        components=[s],\\n ...\"],[\"`@gradio\\u002fdropdown`\\n\\n```html\\n\\u003cscript\\u003e\\n    import {BaseDropdown, BaseMultiselect, BaseExample } from \\\"...\"],[\"Gradio Demo: fake_diffusion_with_gif\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files fro...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\nimport numpy as np\\nimport time\\nimport os\\nfrom PIL import Image\\nimport ...\"],[\"Gradio Demo: theme_soft\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time\\n\\nwit...\"],[\"Gradio Demo: radio_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.B...\"],[\"Gradio Demo: image_selections\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\nimport numpy as np\\n\\nwith gr.Blocks() as demo:\\n    tolerance = gr.Slider(labe...\"],[\"out = img.copy() * 0.2\\n        out = out.astype(np.uint8)\\n        for pixel in pixels_in_segment:\\n  ...\"],[\"使用 Gradio Python 客户端入门\\n\\nTags: CLIENT, API, SPACES\\n\\nGradio Python 客户端使得将任何 Gradio 应用程序作为 API 使用变得非常容易...\"],[\"```\\n\\nGradio 客户端适用于任何托管在 Hugging Face Spaces 上的 Gradio 应用程序，无论是图像生成器、文本摘要生成器、有状态聊天机器人、税金计算器还是其他任何应用程序...\"],[\"```\\n\\n你还可以通过在 `hf_token` 参数中传递你的 HF 令牌来连接到私有空间。你可以在这里获取你的 HF 令牌：https:\\u002f\\u002fhuggingface.co\\u002fsettings\\u002ftoken...\"],[\"```\\n\\n\\u003e \\u003e \\\" 这是 Whisper 语音识别模型的测试。\\\"\\n\\n如果之前已复制了一个空间，重新运行 `duplicate()` 将*不会*创建一个新的空间。相反，客户端将连接到之前创建的空间。因...\"],[\"```\\n\\n这显示了在此空间中有 1 个 API 端点，并显示了如何使用 API 端点进行预测：我们应该调用 `.predict()` 方法（我们将在下面探讨），提供类型为 `str` 的参数 `inp...\"],[\"```\\n\\n如果有多个参数，那么你应该将它们作为单独的参数传递给 `.predict()`，就像这样：\\n\\n````python\\nfrom gradio_client import Client\\n\\ncli...\"],[\"```\\n\\n## 添加回调 （Adding callbacks）\\n\\n或者，可以添加一个或多个回调来在作业完成后执行操作，像这样：\\n\\n```python\\nfrom gradio_client import...\"],[\"```\\n\\n_注意_：`Job`类还有一个`.done()`实例方法，返回一个布尔值，指示作业是否已完成。\\n\\n## 取消作业 （Cancelling Jobs）\\n\\n`Job`类还有一个`.cancel(...\"],[\"```\\n\\n请注意，在生成器端点上运行`job.result()`只会获得端点返回的*第一个*值。\\n\\n`Job`对象还是可迭代的，这意味着您可以使用它按照从端点返回的结果逐个显示生成器函数的结果。以下是...\"],[\"Gradio Demo: markdown_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr...\"],[\"Gradio Demo: gpt2_xl_unified\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ncomponent ...\"],[\"How to Create a Chatbot with Gradio\\n\\nTags: NLP, TEXT, CHAT\\n\\n## Introduction\\n\\nChatbots are a popular ...\"],[\"```\\n\\nNow, we can plug this into `gr.ChatInterface()` and call the `.launch()` method to create the w...\"],[\"```\\n\\nNotice that we've [enabled queuing](\\u002fguides\\u002fkey-features#queuing), which is required to use gen...\"],[\"```\\n\\n## Additional Inputs\\n\\nYou may want to add additional parameters to your chatbot and expose them...\"],[\"```\\n\\nIf you need to create something even more custom, then its best to construct the chatbot UI usi...\"],[\"os.environ[\\\"OPENAI_API_KEY\\\"] = \\\"sk-...\\\"  # Replace with your key\\n\\nllm = ChatOpenAI(temperature=1.0, ...\"],[\"```\\n\\n## A streaming example using `openai`\\n\\nOf course, we could also use the `openai` library direct...\"],[\"```\\n\\n## Example using a local, open-source LLM with Hugging Face\\n\\nOf course, in many cases you want ...\"],[\"model_inputs = tokenizer([messages], return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n    streamer = TextIteratorStre...\"],[\"```\\n\\nWith those examples, you should be all set to create your own Gradio Chatbot demos soon! For bu...\"],[\"Create a Dashboard from Supabase Data\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n\\n[Supabase](https:\\u002f\\u002fsupabase....\"],[\"3\\\\. You'll be presented with your API keys while the database spins up (can take up to 2 minutes).\\n\\n...\"],[\"```\\n\\n7\\\\. Get your project URL and API key. Click the Settings (gear icon) on the left pane and click...\"],[\"```\\n\\nReturn to your Supabase dashboard and refresh the page, you should now see 10 rows populated in...\"],[\"```\\n\\nNotice that by passing in a function to `gr.BarPlot()`, we have the BarPlot query the database ...\"],[\"Gradio Demo: hello_world_4\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef greet(na...\"],[\"Gradio Demo: fake_gan_no_input\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport time\\n\\nimport gradio as...\"],[\"gradio_test\\n\\n## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgra...\"],[\"## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"@gradio\\u002fpreview\\n\\n## 0.6.0\\n\\n### Features\\n\\n- [#6738](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6738) [...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#6532](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6532) [`96290d304`](http...\"],[\"```\\n\\n Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.2.2\\n\\n### Features\\n\\n- [#6467](https:\\u002f\\u002fgithub.c...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#6261](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6261) [`8bbeca0e7`](http...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.1.0-beta.8\\n\\n### Features\\n\\n- [#6094](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6094) [`c476bd5a5...\"],[\"## 0.1.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5938](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5938) [`13ed8a485`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5962](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5962) [`d298e7695`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.1.0-beta.3\\n\\n### Features\\n\\n- [#5648](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5648) [`c573e2339...\"],[\"## 0.1.0-beta.0\\n\\n### Features\\n\\n- [#5507](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5507) [`1385dc688...\"],[\"Gradio Demo: blocks_js_methods\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nblocks = gr.Blocks()\\n\\nwith blocks as demo...\"],[\"Gradio Demo: theme_extended_step_4\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"@gradio\\u002ftextbox\\n\\n## 0.4.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.4.5\\n\\n### Fixes\\n\\n- [#6635](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6635) [`b639e04`](https:\\u002f\\u002fg...\"],[\"## 0.4.2\\n\\n### Fixes\\n\\n- [#6323](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6323) [`55fda81fa`](https:\\u002f...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.4.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5652](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5652) [`2e25d4305`](http...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5417](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5417) [`d14d63e3`](https...\"],[\"## 0.1.1\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"### Fixes\\n\\n- [#5114](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5114) [`56d2609d`](https:\\u002f\\u002fgithub.com...\"],[\"Gradio Demo: dataset_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr....\"],[\"Gradio Demo: blocks_layout\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndemo = gr.B...\"],[\"@gradio\\u002fapp\\n\\n## 1.17.0\\n\\n### Features\\n\\n- [#6831](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6831) [`f3...\"],[\"## 1.16.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`245d58e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 1.16.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 1.16.0\\n\\n### Features\\n\\n- [#6398](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6398) [`67ddd40`](https...\"],[\"## 1.15.0\\n\\n### Features\\n\\n- [#6512](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6512) [`4f040c7`](https...\"],[\"## 1.13.1\\n\\n### Fixes\\n\\n- [#6536](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6536) [`1bbd6cab3`](https:...\"],[\"\\u003cvideo src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f12937446\\u002f284027169-31188926-fd16-4a1c-8718-998...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 1.12.0\\n\\n### Features\\n\\n- [#6427](https:\\u002f\\u002fgithub.c...\"],[\"## 1.11.0\\n\\n### Features\\n\\n- [#6099](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6099) [`d84209703`](htt...\"],[\"- Updated dependencies [[`6204ccac5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002f6204ccac5967763e0e...\"],[\"- @gradio\\u002ftextbox@0.4.2\\n  - @gradio\\u002faudio@0.5.0\\n  - @gradio\\u002fclient@0.8.0\\n  - @gradio\\u002fgallery@0.4.5\\n ...\"],[\"## 1.10.2\\n\\n### Patch Changes...\"],[\"- Updated dependencies [[`4b1011bab`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002f4b1011bab03c0b6a09...\"],[\"- @gradio\\u002fdataframe@0.3.4\\n  - @gradio\\u002fatoms@0.2.1\\n  - @gradio\\u002fupload@0.3.3\\n  - @gradio\\u002fvideo@0.1.3\\n ...\"],[\"- @gradio\\u002flabel@0.2.1\\n  - @gradio\\u002fmarkdown@0.3.1\\n  - @gradio\\u002fnumber@0.3.1\\n  - @gradio\\u002fplot@0.2.1\\n  -...\"],[\"## 1.10.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`92278729e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"- [#6266](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6266) [`e32bac894`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6236](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6236) [`6bce259c5`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 1.9.2\\n\\n### Fixes\\n\\n- [#6191](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6191) [`b555bc09f`](https:\\u002f...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.co...\"],[\"- [#6124](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6124) [`a7435ba9e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6118](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6118) [`88bccfdba`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6069](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6069) [`bf127e124`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 1.9.0-beta.2\\n\\n### Features...\"],[\"- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6107](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6107) [`9a40de7bf`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5990](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5990) [`85056de5c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6065](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6065) [`7d07001e8`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 1.9.0-beta.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`174b73619`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"## 1.9.0-beta.0\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"### Fixes\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.co...\"],[\"## 1.7.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`796145e2c`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"For more information check the [`FileExplorer` documentation](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002ffileexplorer)....\"],[\"- Updated dependencies [[`abb5e9df4`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002fabb5e9df47989b2c56...\"],[\"- @gradio\\u002faudio@0.3.6\\n  - @gradio\\u002fcode@0.2.1\\n  - @gradio\\u002fdropdown@0.3.1\\n  - @gradio\\u002ffile@0.1.5\\n  - @...\"],[\"## 1.6.2\\n\\n### Features\\n\\n- [#5721](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5721) [`84e03fe50`](http...\"],[\"## 1.6.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`ee8eec1e5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 1.5.4\\n\\n### Features\\n\\n- [#5514](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5514) [`52f783175`](http...\"],[\"## 1.5.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`a0cc9ac9`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 1.5.0\\n\\n### Features\\n\\n- [#5505](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5505) [`9ee20f49`](https...\"],[\"## 1.4.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6e381c4f`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002fafac0006337ce2840cf...\"],[\"- @gradio\\u002fdropdown@0.1.3\\n  - @gradio\\u002ffile@0.1.2\\n  - @gradio\\u002fgallery@0.3.2\\n  - @gradio\\u002fhighlightedtex...\"],[\"## 1.4.0\\n\\n### Features\\n\\n- [#5267](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5267) [`119c8343`](https...\"],[\"## 1.3.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5f25eb68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 1.3.0\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"We now have an event `render` on the \\u003cgradio-app\\u003e web component, which is triggered once the embedde...\"],[\"```\\n\\nThanks [@hannahblair](https:\\u002f\\u002fgithub.com\\u002fhannahblair)!\\n\\n### Features...\"],[\"- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5215) [`fbdad78a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5264](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5264) [`46a2b600`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes\\n\\n- [#5285](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5285) [`cdfd4217`](https:\\u002f\\u002fgithub.com...\"],[\"## 1.2.0\\n\\n### Highlights\\n\\n#### Client.predict will now return the final output for streaming endpoin...\"],[\"- [#5025](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5025) [`6693660a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5005](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5005) [`f5539c76`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 1.1.0\\n\\n### Features\\n\\n- [#4995](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4995) [`3f8c210b`](https...\"],[\"Gradio Demo: kitchen_sink\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo ...\"],[\"```\\nimport os\\nimport json\\n\\nimport numpy as np\\n\\nimport gradio as gr\\n\\nCHOICES = [\\\"foo\\\", \\\"bar\\\", \\\"baz\\\"]\\n...\"],[\"def fn(\\n    text1,\\n    text2,\\n    num,\\n    slider1,\\n    slider2,\\n    single_checkbox,\\n    checkboxes...\"],[\"[\\n            (\\\"The\\\", \\\"art\\\"),\\n            (\\\"quick brown\\\", \\\"adj\\\"),\\n            (\\\"fox\\\", \\\"nn\\\"),\\n       ...\"],[\"json.loads(JSONOBJ),  # JSON\\n        \\\"\\u003cbutton style='background-color: red'\\u003eClick Me: \\\"\\n        + ra...\"],[\"demo = gr.Interface(\\n    fn,\\n    inputs=[\\n        gr.Textbox(value=\\\"Lorem ipsum\\\", label=\\\"Textbox\\\"),\\n...\"],[\"gr.Audio(label=\\\"Audio\\\"),\\n        gr.Audio(label=\\\"Microphone\\\", sources=[\\\"microphone\\\"]),\\n        gr.Fi...\"],[\"# os.path.join(os.path.abspath(''), \\\"files\\u002fcheetah1.jpg\\\"),\\n            # os.path.join(os.path.abspat...\"],[\"`@gradio\\u002futils`\\n\\nGeneral functions for handling events in Gradio Svelte components\\n\\n\\n```javascript\\ne...\"],[\"Gradio Demo: audio_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Bl...\"],[\"Gradio Demo: musical_instrument_identification\\n### This demo identifies musical instruments from an ...\"],[\"```\\nimport gradio as gr\\nimport torch\\nimport torchaudio\\nfrom timeit import default_timer as timer\\nfro...\"],[\"def predict(audio_path):\\n    start_time = timer()\\n    wavform, sample_rate = torchaudio.load(audio_p...\"],[\"Gradio Demo: change_vs_input\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the de...\"],[\"```\\nimport os\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    set_button = gr.Button(\\\"Set Values\\\"...\"],[\"with gr.Column(min_width=200):\\n            gr.Markdown(\\\"# ON:CHANGE\\\")\\n            text_ch = gr.Textb...\"],[\"counter = gr.Number(label=\\\"Change counter\\\")\\n\\n    lion = os.path.join(os.path.abspath(''), \\\"files\\u002flio...\"],[\"text.change(lambda x,y:(x,y+1), [text, counter], [text_ch, counter])\\n    num.change(lambda x,y:(x, y...\"],[\"text_ch.change(lambda x:x, text_ch, text_ch2)\\n    num_ch.change(lambda x:x, num_ch, num_ch2)\\n    sli...\"],[\"@gradio\\u002fchatbot\\n\\n## 0.5.5\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`846d52d`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.5.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.5.1\\n\\n### Fixes\\n\\n- [#6574](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6574) [`2b625ad`](https:\\u002f\\u002fg...\"],[\"## 0.4.8\\n\\n### Features\\n\\n- [#6296](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6296) [`46f13f496`](http...\"],[\"## 0.4.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`854b482f5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2ba14b284`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6135](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6135) [`bce37ac74`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.4.0-beta.8\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.4.0-beta.7\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.5.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e4a307ed6`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"### Fixes\\n\\n- [#5755](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5755) [`e842a561a`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5671](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5671) [`6a36c3b78`](http...\"],[\"### Fixes\\n\\n- [#5604](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5604) [`faad01f8e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"### Fixes\\n\\n- [#5304](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5304) [`05892302`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`31996c99`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"### Fixes\\n\\n- [#5242](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5242) [`2b397791`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5125](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5125) [`80be7a1c`](https...\"],[\"@gradio\\u002fbutton\\n\\n## 0.2.13\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.2.10\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.2.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`854b482f5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2ba14b284`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub...\"],[\"## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.2.0-beta.6\\n\\n### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](...\"],[\"- Updated dependencies []:\\n  - @gradio\\u002fupload@0.3.3\\n\\n## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependen...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"Gradio Demo: dataframe_colorful\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport pandas as pd \\nimport ...\"],[\"How to Style the Gradio Dataframe\\n\\nTags: DATAFRAME, STYLE, COLOR\\n\\n## Introduction\\n\\nData visualizatio...\"],[\"# Applying style to highlight the maximum value in each row\\nstyler = df.style.highlight_max(color = ...\"],[\"```\\n\\nThe Styler class can be used to apply conditional formatting and styling to dataframes, making ...\"],[\"```\\n\\nHere's how it looks:\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"```\\n\\nIn this script, we define a custom function highlight_cols that changes the text color to purpl...\"],[\"```\\n\\nIn this script, the format method of the Styler object is used to set the precision of numbers ...\"],[\"Image Classification with Vision Transformers\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlab...\"],[\"## Step 2 — Loading the Vision Transformer Model with Gradio\\n\\nWhen using a model from the Hugging Fa...\"],[\"```\\n\\nNotice that we have added one more parameter, the `examples`, which allows us to prepopulate ou...\"],[\"Gradio Demo: text_generation\\n### This text generation demo takes in input text and returns generated...\"],[\"Gradio Demo: diffusers_with_batching\\n\\n\\n```\\n!pip install -q gradio torch transformers diffusers\\n```\\n\\n...\"],[\"Gradio Demo: depth_estimation\\n### A demo for predicting the depth of an image and generating a 3D mo...\"],[\"```\\nimport gradio as gr\\nfrom transformers import DPTFeatureExtractor, DPTForDepthEstimation\\nimport t...\"],[\"def create_3d_obj(rgb_image, depth_image, image_path, depth=10):\\n    depth_o3d = o3d.geometry.Image(...\"],[\"print('run Poisson surface reconstruction')\\n    with o3d.utility.VerbosityContextManager(o3d.utility...\"],[\"iface = gr.Interface(fn=process_image,\\n                     inputs=[gr.Image(\\n                      ...\"],[\"Using Gradio for Tabular Data Science Workflows\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fsciki...\"],[\"model = joblib.load(\\\"model.pkl\\\")\\n\\n# we will give our dataframe as example\\ndf = datasets.load_dataset...\"],[\"```\\n\\nLet's break down above code.\\n\\n- `fn`: the inference function that takes input dataframe and ret...\"],[\"def plot(df):\\n  plt.scatter(df.measurement_13, df.measurement_15, c = df.loading,alpha=0.5)\\n  plt.sa...\"],[\"```\\n\\n\\u003cgradio-app space=\\\"gradio\\u002fgradio-analysis-dashboard-minimal\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\nWe will use the sam...\"],[\"Controlling Layout\\n\\nBy default, Components in Blocks are arranged vertically. Let's take a look at h...\"],[\"```\\n\\n- `min_width` will set the minimum width the element will take. The Row will wrap if there isn'...\"],[\"```\\n\\nIn this example, the Column layout component is given a height of 100% of the viewport height (...\"],[\"$code_blocks_form\\n$demo_blocks_form\\n\\n## Variable Number of Outputs\\n\\nBy adjusting the visibility of c...\"],[\"Build a Custom Multimodal Chatbot - Part 1\\n\\nThis is the first in a two part series where we build a ...\"],[\"```\\n\\nAnd we're ready to go!\\n\\nTip: Make sure to modify the `Author` key in the `pyproject.toml` file....\"],[\"```\\n\\n\\nTip: The `data_model`s are implemented using `Pydantic V2`. Read the documentation [here](http...\"],[\"```\\n\\nBefore we wrap up with the backend code, let's modify the `example_inputs` method to return a v...\"],[\"```\\n\\nWe need to normalize each message to make sure each file has a proper URL to fetch its contents...\"],[\"```\\n\\nNow for the fun part, actually rendering the text and files in the same message!\\n\\nYou should se...\"],[\"```\\n\\nWe will modify this code to always display the text message and then loop through the files and...\"],[\"```\\n\\nWe did it! 🎉\\n\\n## Part 4 - The demo\\n\\nFor this tutorial, let's keep the demo simple and just disp...\"],[\"```\\n\\n\\nTip: Change the filepaths so that they correspond to files on your machine. Also, if you are r...\"],[\"Creating a Real-Time Dashboard from BigQuery Data\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n\\n[Google BigQuery...\"],[\"## Setting up your BigQuery Credentials\\n\\nTo use Gradio with BigQuery, you will need to obtain your B...\"],[\"6. After selecting the service account, select the \\\"JSON\\\" key type and then click on the \\\"Create\\\" bu...\"],[\"```\\n\\n## Using the BigQuery Client\\n\\nOnce you have the credentials, you will need to use the BigQuery ...\"],[\"```\\n\\n## Building the Real-Time Dashboard\\n\\nOnce you have a function to query the data, you can use th...\"],[\"Gradio Demo: sepia_filter\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport numpy as np\\nimport gradio a...\"],[\"`@gradio\\u002fimage`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseImageUploader, BaseStaticImage, Webcam, BaseExample ...\"],[\"`@gradio\\u002faudio`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseStaticAudio, BaseInteractiveAudio, BasePlayer, BaseE...\"],[\"```\\n\\nBasePlayer:\\n```javascript\\n\\texport let value: null | { name: string; data: string } = null;\\n\\texp...\"],[\"Gradio Demo: blocks_multiple_event_triggers\\n\\n\\n```\\n!pip install -q gradio plotly pypistats\\n```...\"],[\"```\\n!pip install -q gradio plotly pypistats\\n```\\n\\n\\n```\\nimport gradio as gr\\nimport pypistats\\nfrom date...\"],[\"!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides\\u002f1)getting_start...\"],[\"[官网](https:\\u002f\\u002fgradio.app)\\n| [文档](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002f)\\n| [指南](https:\\u002f\\u002fgradio.app\\u002fguides\\u002f)\\n| [开始](...\"],[\"**依赖**: Gradio只需要Python 3.7及以上版本！\\n\\n#### Gradio能做什么？\\n\\n与他人共享机器学习模型、API或数据科学工作流程的最佳方法之一就是创建一个**交互式应用**，...\"],[\"```\\n\\n2\\\\. 用Python脚本或在Jupyter Notebook中运行下面的代码 （或者使用 [Google Colab](https:\\u002f\\u002fcolab.research.google.com\\u002f...\"],[\"```\\n\\n3\\\\. 下面的演示会自动出现在Jupyter Notebook中，如果使用脚本运行则会在浏览器[http:\\u002f\\u002flocalhost:7860](http:\\u002f\\u002flocalhost:7860)弹出...\"],[\"```python\\nimport gradio as gr\\n\\ndef greet(name):\\n    return \\\"Hello \\\" + name + \\\"!\\\"\\n\\ndemo = gr.Interfac...\"],[\"```\\n\\n![`hello_world_2` demo](..\\u002f..\\u002fdemo\\u002fhello_world_2\\u002fscreenshot.gif)\\n\\n#### 多输入和输出组件\\n\\n假设您有一个更复杂的函数，有...\"],[\"```\\n\\n![`hello_world_3` demo](..\\u002f..\\u002fdemo\\u002fhello_world_3\\u002fscreenshot.gif)\\n\\n您只需将组件包装在列表中。输入列表`inputs`中的每个...\"],[\"```\\n\\n![`sepia_filter` demo](..\\u002f..\\u002fdemo\\u002fsepia_filter\\u002fscreenshot.gif)\\n\\n当使用`Image`组件作为输入时，您的函数将接收一个形状为 ...\"],[\"```\\n\\n还要注意，我们的输入 `Image` 组件带有一个编辑按钮 🖉，它允许裁剪和放大图像。以这种方式操作图像可以帮助揭示机器学习模型中的偏见或隐藏的缺陷！\\n\\n您可以在[Gradio文档](htt...\"],[\"```\\n\\n![`hello_blocks` demo](..\\u002f..\\u002fdemo\\u002fhello_blocks\\u002fscreenshot.gif)\\n\\n注意事项：\\n\\n- `Blocks` 由 `with` 子句组成...\"],[\"```\\n\\n![`blocks_flipper` demo](..\\u002f..\\u002fdemo\\u002fblocks_flipper\\u002fscreenshot.gif)\\n\\n还有很多事情可以做！我们将在[使用blocks构建](...\"],[\"## 开源栈\\n\\nGradio是由许多很棒的开源库构建的，请一并支持它们!\\n\\n[\\u003cimg src=\\\"..\\u002fhuggingface_mini.svg\\\" alt=\\\"huggingface\\\" height=4...\"],[\"More on Examples\\n\\nIn the [previous Guide](\\u002fmain\\u002fguides\\u002fthe-interface-class), we discussed how to pro...\"],[\"```\\n\\nThis can be helpful when browsing flagged data. Simply point to the flagged directory and the `...\"],[\"Backend Testing Guidelines\\n\\n- All the tests should test Backend functionalities. Frontend functional...\"],[\"Gradio and ONNX on Hugging Face\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fonnx\\u002fEfficientNet-Lit...\"],[\"Get started [here](https:\\u002f\\u002fgradio.app\\u002fgetting_started)\\n\\n### Hugging Face Spaces\\n\\nHugging Face Spaces...\"],[\"ONNX Runtime is a cross-platform inference and training machine-learning accelerator. It makes live ...\"],[\"# loads ONNX model from ONNX Model Zoo\\nmodel = hub.load(\\\"efficientnet-lite4\\\")\\n# loads the labels tex...\"],[\"# crops the image around the center based on given height and width\\ndef center_crop(img, out_height,...\"],[\"```\\n\\n## How to contribute Gradio demos on HF spaces using ONNX models\\n\\n- Add model to the [onnx mode...\"],[\"# 使用 Gradio 进行表格数据科学工作流\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fscikit-learn\\u002fgradio-skops-int...\"],[\"inputs = [gr.Dataframe(row_count = (2, \\\"dynamic\\\"), col_count=(4,\\\"dynamic\\\"), label=\\\"Input Data\\\", inte...\"],[\"```\\n\\n让我们来解析上述代码。\\n\\n- `fn`：推理函数，接受输入数据帧并返回预测结果。\\n- `inputs`：我们使用 `Dataframe` 组件作为输入。我们将输入定义为具有 2 行 4 列的...\"],[\"```\\n\\n\\u003cgradio-app space=\\\"gradio\\u002fgradio-analysis-dashboard-minimal\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\n我们将使用与训练模型相同的数据集，但这...\"],[\"```\\n\\n\\u003cgradio-app space=\\\"gradio\\u002fgradio-skops-integration\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\n使用 `skops` 将 `sklearn` 模型推送到...\"],[\"his translation demo takes in the text, source and target languages, and returns the translation. It...\"],[\"Gradio Demo: model3D\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo repo\\n...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\nimport os\\n\\n\\ndef load_mesh(mesh_file_name):\\n    return mesh_file_name\\n\\n...\"],[\"@gradio\\u002flite\\n\\n## 0.4.4\\n\\n## 0.4.4-beta.0\\n\\n### Features\\n\\n- [#6147](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.4.1\\n\\n### Fixes\\n\\n- [#5988](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5988) [`bea931c31`](https:\\u002f...\"],[\"## 0.3.1\\n\\n### Features\\n\\n- [#5226](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5226) [`64039707`](https...\"],[\"- [#4826](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4826) [`f0150c62`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#4785](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4785) [`da0e9447`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- [#4731](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4731) [`f9171288`](...\"],[\"@gradio\\u002fvideo\\n\\n## 0.2.3\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`7326...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#6726](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6726) [`21cfb0a`](https:...\"],[\"## 0.1.9\\n\\n### Fixes\\n\\n- [#6566](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6566) [`d548202`](https:\\u002f\\u002fg...\"],[\"## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6204ccac5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.2\\n\\n### Fixes\\n\\n- [#6234](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6234) [`aaa55ce85`](https:\\u002f...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.1.0-beta.9\\n\\n### Features...\"],[\"- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.1.0-beta.8\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.1.0-beta.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`174b73619`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5627](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5627) [`b67115e8e`](http...\"],[\"## 0.0.10\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 0.0.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.0.4\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"## 0.0.3\\n\\n### Fixes\\n\\n- [#5140](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5140) [`cd1353fa`](https:\\u002f\\u002f...\"],[\"Gradio Demo: outbreak_forecast\\n### Generate a plot based on 5 inputs.\\n        \\n\\n\\n```\\n!pip install -q...\"],[\"```\\nimport altair\\n\\nimport gradio as gr\\nfrom math import sqrt\\nimport matplotlib.pyplot as plt\\nimport ...\"],[\"inputs = [\\n    gr.Dropdown([\\\"Matplotlib\\\", \\\"Plotly\\\", \\\"Altair\\\"], label=\\\"Plot Type\\\"),\\n    gr.Slider(1, ...\"],[\"@gradio\\u002ffile\\n\\n## 0.4.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.4.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#6511](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6511) [`71f1a1f99`](http...\"],[\"## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`854b482f5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`aaa55ce85`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.2.0-beta.8\\n\\n### Features...\"],[\"## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`796145e2c`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"Thanks [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94)!\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependenc...\"],[\"## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`c57f1b75e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`119c8343`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5215) [`fbdad78a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5265](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5265) [`06982212`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes\\n\\n- [#5253](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5253) [`ddac7e4d`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.0.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`61129052`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Gradio Demo: blocks_flashcards\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport random\\n\\nimport gradio as gr\\n\\ndemo = gr.Blocks()\\n\\nwith demo:\\n    gr.Markdown(\\n        \\\"Loa...\"],[\"flip_btn.click(flip_card, [selected_card], [back, answer_col])\\n\\n    def mark_correct(card, results):...\"],[\"`@gradio\\u002fjson`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseJSON } from \\\"@gradio\\u002fjson\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\nBaseJSON\\n`...\"],[\"使用Gradio JavaScript客户端快速入门\\n\\nTags: CLIENT, API, SPACES\\n\\nGradio JavaScript客户端使得使用任何Gradio应用作为API非常简单。例...\"],[\"```\\n\\nGradio客户端适用于任何托管的Gradio应用，无论是图像生成器、文本摘要生成器、有状态的聊天机器人、税收计算器还是其他任何应用！Gradio客户端通常与托管在[Hugging Face...\"],[\"```\\n\\n## 为私人使用复制一个Space\\n\\n虽然您可以将任何公共Space用作API，但是如果您发出的请求过多，Hugging Face可能会对您进行速率限制。为了无限制使用Space，只需复制S...\"],[\"```\\n\\n如果您之前复制过一个Space，则重新运行`duplicate`不会创建一个新的Space。而是客户端将连接到先前创建的Space。因此，可以安全地多次使用相同的Space重新运行`dupl...\"],[\"```\\n\\n## 检查API端点\\n\\n一旦连接到Gradio应用程序，可以通过调用`client`的`view_api`方法来查看可用的API端点。\\n\\n对于Whisper Space，我们可以这样做：\\n\\n...\"],[\"```\\n\\n这告诉我们该Space中有1个API端点，并显示了如何使用API端点进行预测：我们应该调用`.predict()`方法（下面将进行更多探索），并提供类型为`string`的参数`input_...\"],[\"```\\n\\n对于某些输入，例如图像，您应该根据所需要的方便程度传入`Buffer`、`Blob`或`File`。在Node.js中，可以使用`Buffer`或`Blob`；在浏览器环境中，可以使用`Bl...\"],[\"```\\n\\n## 状态\\n\\n事件接口还可以通过监听`\\\"status\\\"`事件来获取运行作业的状态。这将返回一个对象，其中包含以下属性：`status`（当前作业的人类可读状态，`\\\"pending\\\" | \\\"g...\"],[\"```\\n\\n如果第一个作业已经开始处理，那么它将不会被取消，但客户端将不再监听更新（丢弃该作业）。如果第二个作业尚未启动，它将被成功取消并从队列中移除。\\n\\n## 生成器端点\\n\\n某些Gradio API端...\"],[\"@gradio\\u002ftootils\\n\\n## 0.1.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com...\"],[\"- Updated dependencies []:\\n  - @gradio\\u002fstatustracker@0.3.1\\n\\n## 0.1.1\\n\\n### Fixes\\n\\n- [#6234](https:\\u002f\\u002fg...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.1.0-beta.6\\n\\n### Features\\n\\n- [#6044](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6044) [`9053c95a1...\"],[\"## 0.1.0-beta.4\\n\\n### Features\\n\\n- [#5648](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5648) [`c573e2339...\"],[\"- Updated dependencies []:\\n  - @gradio\\u002futils@0.2.0-beta.1\\n\\n## 0.1.0-beta.0\\n\\n### Features\\n\\n- [#5507](...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!...\"],[\"Quickstart\\n\\nGradio is an open-source Python package that allows you to quickly **build** a demo or w...\"],[\"```\\n\\n\\nTip: it is best to install Gradio in a virtual environment. Detailed installation instructions...\"],[\"$demo_hello_world_4\\n\\nType your name in the textbox on the left, drag the slider, and then press the ...\"],[\"The `input` and `output` arguments take one or more Gradio components. As we'll see, Gradio includes...\"],[\"```\\n\\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, so...\"],[\"You can build very custom and complex applications using `gr.Blocks()`. For example, the popular ima...\"],[\"Or, if you already know the basics and are looking for something specific, you can search the more [...\"],[\"如何创建一个聊天机器人\\n\\nTags: NLP, TEXT, CHAT\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fchatbot_stre...\"],[\"## 简单聊天机器人演示\\n\\n让我们从重新创建上面的简单演示开始。正如您可能已经注意到的，我们的机器人只是随机对任何输入回复 \\\" 你好吗？\\\"、\\\" 我爱你 \\\" 或 \\\" 我非常饿 \\\"。这是使用 Gradio...\"],[\"$ 演示 \\\\_ 简单聊天机器人\\n\\n## 为聊天机器人添加流式响应\\n\\n我们可以通过几种方式来改进上述聊天机器人的用户体验。首先，我们可以流式传输响应，以便用户不必等待太长时间才能生成消息。其次，我们可以...\"],[\"3. 第三个方法使输入字段再次可以交互，以便用户可以向机器人发送另一条消息。\\n\\n当然，实际上，您会用自己更复杂的函数替换 `bot()`，该函数可能调用预训练模型或 API 来生成响应。\\n\\n最后，我们...\"],[\"```\\n\\n此外，它还可以处理图片、音频和视频等媒体文件。要传递媒体文件，我们必须将文件作为两个字符串的元组传递，如`(filepath, alt_text)` 所示。`alt_text` 是可选的，因...\"],[\"Gradio Demo: chatbot_streaming\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport ra...\"],[\"Gradio Demo: upload_button\\n### A simple demo showcasing the upload button used with its `upload` eve...\"],[\"Gradio Demo: button_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.B...\"],[\"Gradio Demo: image_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.B...\"],[\"`@gradio\\u002fcheckbox`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseCheckbox } from \\\"@gradio\\u002fcheckbox\\\";\\n\\u003c\\u002fscript\\u003e\\n...\"],[\"Gradio Demo: dropdown_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith g...\"],[\"Gradio Demo: filter_records\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef filter...\"],[\"@gradio\\u002fdropdown\\n\\n## 0.4.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.3.2\\n\\n### Fixes\\n\\n- [#6425](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6425) [`b3ba17dd1`](https:\\u002f...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.3.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.3.2\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.1.2\\n  - @gradio\\u002fatoms@0....\"],[\"## 0.2.1\\n\\n### Fixes\\n\\n- [#5525](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5525) [`21f1db40`](https:\\u002f\\u002f...\"],[\"## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 0.1.0\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 0.0.2\\n\\n### Fixes\\n\\n- [#5062](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5062) [`7d897165`](https:\\u002f\\u002f...\"],[\"Gradio Demo: autocomplete\\n### This text generation demo works like autocomplete. There's only one te...\"],[\"Gradio Demo: generate_english_german\\n\\n\\n```\\n!pip install -q gradio transformers torch\\n```\\n\\n\\n```\\nimpor...\"],[\"Gradio Demo: gallery_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\n...\"],[\"Gradio Demo: theme_builder\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndemo = gr.th...\"],[\"Gradio Demo: blocks_inputs\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo...\"],[\"Gradio Demo: html_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Bl...\"],[\"Gradio Demo: checkboxgroup_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nw...\"],[\"Gradio Demo: state_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.B...\"],[\"`@gradio\\u002ftextbox`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseTextbox, BaseExample } from \\\"@gradio\\u002ftextbox\\\";\\n...\"],[\"Gradio Demo: clear_components\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the d...\"],[\"```\\nimport gradio as gr\\nfrom datetime import datetime\\nimport os\\nimport random\\nimport string\\nimport p...\"],[\"images = [\\n    \\\"https:\\u002f\\u002fimages.unsplash.com\\u002fphoto-1507003211169-0a1dd7228f2d?ixlib=rb-1.2.1&ixid=Mnw...\"],[\"\\\"index\\\": 2,\\n        \\\"word\\\": \\\"Chicago\\\",\\n        \\\"start\\\": 5,\\n        \\\"end\\\": 12,\\n    },\\n    {\\n        \\\"...\"],[\"highlighted_text = \\\"Does Chicago have any Pakistani restaurants\\\"\\n\\n\\ndef random_model3d():\\n    model_3...\"],[\"components = [\\n    gr.Textbox(value=lambda: datetime.now(), label=\\\"Current Time\\\"),\\n    gr.Number(val...\"],[\"gr.Audio(value=lambda: os.path.join(file_dir, \\\"cantina.wav\\\")),\\n    gr.File(\\n        value=lambda: ra...\"],[\"value=lambda: images\\n    ),\\n    gr.Model3D(value=random_model3d),\\n    gr.Plot(value=random_plot),\\n  ...\"],[\"def evaluate_values(*args):\\n    are_false = []\\n    for a in args:\\n        if isinstance(a, (pd.DataF...\"],[\"快速开始\\n\\n**先决条件**：Gradio 需要 Python 3.8 或更高版本，就是这样！\\n\\n## Gradio 是做什么的？\\n\\n与他人分享您的机器学习模型、API 或数据科学流程的*最佳方式之一...\"],[\"```\\n\\n2. 将下面的代码作为 Python 脚本运行或在 Jupyter Notebook 中运行（或者 [Google Colab](https:\\u002f\\u002fcolab.research.google....\"],[\"```\\n\\n注意：您也可以运行 `python app.py`，但它不会提供自动重新加载机制。\\n\\n## `Interface` 类\\n\\n您会注意到为了创建演示，我们创建了一个 `gr.Interface`...\"],[\"$code_hello_world_2\\n$demo_hello_world_2\\n\\n## 多个输入和输出组件\\n\\n假设您有一个更复杂的函数，具有多个输入和输出。在下面的示例中，我们定义了一个接受字符串、布...\"],[\"```\\n\\n还要注意，我们的输入 `Image` 组件附带有一个编辑按钮🖉，允许裁剪和缩放图像。通过这种方式操作图像可以帮助揭示机器学习模型中的偏见或隐藏的缺陷！\\n\\n您可以在[Gradio 文档](ht...\"],[\"$code_hello_blocks\\n$demo_hello_blocks\\n\\n需要注意的事项：\\n\\n- `Blocks` 可以使用 `with` 子句创建，此子句中创建的任何组件都会自动添加到应用程序中...\"],[\"@gradio\\u002fstorybook\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#6451](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6451)...\"],[\"## 0.1.0-beta.0\\n\\n### Features\\n\\n- [#5966](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5966) [`9cad2127b...\"],[\"### Fixes\\n\\n- [#6065](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6065) [`7d07001e8`](https:\\u002f\\u002fgithub.co...\"],[\"Sharing Your App\\n\\nHow to share your Gradio app:\\n\\n1. [Sharing demos with the share parameter](#sharin...\"],[\"```\\n\\nThis generates a public, shareable link that you can send to anybody! When you send this link, ...\"],[\"After you have [created a free Hugging Face account](https:\\u002f\\u002fhuggingface.co\\u002fjoin), you have two meth...\"],[\"### Embedding with Web Components\\n\\nWeb components typically offer a better experience to users than ...\"],[\"```\\n\\n2. Add\\n\\n```html\\n\\u003cgradio-app src=\\\"https:\\u002f\\u002f$your_space_host.hf.space\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n```\\n\\nelement ...\"],[\"```\\n\\n\\u003cscript\\u003e\\nfetch(\\\"https:\\u002f\\u002fpypi.org\\u002fpypi\\u002fgradio\\u002fjson\\\"\\n).then(r =\\u003e r.json()\\n).then(obj =\\u003e {\\n    let...\"],[\"You can also customize the appearance and behavior of your web component with attributes that you pa...\"],[\"```\\n\\nHere's another example of how to use the `render` event. An event listener is used to capture t...\"],[\"```\\n\\nAgain, you can find the `src=` attribute to your Space's embed URL, which you can find in the \\\"...\"],[\"```\\n\\nThis will add and document the endpoint `\\u002fapi\\u002faddition\\u002f` to the automatically generated API pag...\"],[\"```\\n\\nFor authentication to work properly, third party cookies must be enabled in your browser.\\nThis ...\"],[\"def hello(profile: gr.OAuthProfile | None) -\\u003e str:\\n    if profile is None:\\n        return \\\"I don't k...\"],[\"```\\n\\nWhen the user clicks on the login button, they get redirected in a new page to authorize your S...\"],[\"```\\n\\nNote: if your function is called directly instead of through the UI (this happens, for\\nexample,...\"],[\"- **Cached examples created by Gradio.** These are files that are created by Gradio as part of cachi...\"],[\"`@gradio\\u002ftooltip`\\n\\n```javascript\\nimport { Tooltip } from \\\"@gradio\\u002ftooltip\\\";\\n```\\n\\n```javascript\\n\\texpo...\"],[\"enerate a plot based on 5 inputs....\"],[\"Gradio Demo: stock_forecast\\n\\n\\n```\\n!pip install -q gradio numpy matplotlib\\n```\\n\\n\\n```\\nimport matplotli...\"],[\"simple dashboard showing pypi stats for python libraries. Updates on load, and has no buttons!...\"],[\"Gradio Demo: neon-tts-plugin-coqui\\n### This  demo converts text to speech in 14 languages.\\n        \\n...\"],[\"@gradio\\u002ftooltip\\n\\n## 0.1.0\\n\\n## 0.1.0-beta.2\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"`@gradio\\u002fcode`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseCode, BaseCopy, BaseDownload, BaseWidget, BaseExam...\"],[\"Gradio Demo: gif_maker\\n\\n\\n```\\n!pip install -q gradio opencv-python\\n```\\n\\n\\n```\\nimport cv2\\nimport gradio...\"],[\"Gradio Demo: file_explorer_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading fi...\"],[\"```\\nimport gradio as gr\\nfrom pathlib import Path\\n\\nbase_root = Path(__file__).parent.resolve()\\n\\nwith ...\"],[\"with gr.Row():\\n        a = gr.Textbox(elem_id=\\\"input-box\\\")\\n        a.change(lambda x: x, inputs=[a])...\"],[\"Gradio Demo: stream_audio\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport numpy a...\"],[\"Gradio Demo: gallery_selections\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport n...\"],[\"Gradio Demo: stream_asr\\n\\n\\n```\\n!pip install -q gradio torch torchaudio transformers\\n```\\n\\n\\n```\\nimport ...\"],[\"his demo takes in 12 inputs from the user in dropdowns and sliders and predicts income. It also has ...\"],[\"`@gradio\\u002flabel`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseLabel } from \\\"@gradio\\u002flabel\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\nBaseLab...\"],[\"alculate taxes using Textbox, Radio, and Dataframe components...\"],[\"Gradio Demo: json_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Bl...\"]],\"hovertemplate\":\"source=gradio\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"gradio, circle\",\"marker\":{\"color\":\"#ab63fa\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"gradio, circle\",\"showlegend\":true,\"x\":[10.328652,10.411032,9.945257,10.327137,10.29379,10.3972435,10.104107,10.748952,10.383308,10.533918,10.547337,10.410363,10.851086,11.809264,11.615391,9.991137,10.298326,10.044318,10.028333,10.082539,11.755753,11.929986,11.087016,10.396946,9.985645,11.408855,12.175573,10.972063,11.02039,10.294493,10.953111,0.17368168,0.53234977,0.79436666,-0.12367461,0.99692637,0.76512307,11.449766,11.537837,10.854105,11.913299,11.918753,11.350069,15.045872,17.985437,17.178368,14.592372,15.153239,17.83928,9.313967,9.784754,14.760316,12.481706,11.224438,7.9930305,7.779564,8.009054,6.9439464,7.95576,7.9141717,9.069713,11.65453,11.853964,10.306153,10.272859,10.219445,10.072384,10.221953,9.990212,10.3812895,10.316929,9.910487,9.72728,9.772034,9.808897,10.254783,10.239138,7.354952,9.190112,9.377064,10.345736,10.240143,10.795984,15.890632,14.610269,14.766799,14.426202,14.867362,15.046306,14.992758,15.918943,14.720133,14.90869,14.634195,14.696774,14.573331,13.182523,14.606667,16.195442,14.848718,14.616373,14.403974,14.5236435,14.102068,16.309586,14.683045,14.693139,14.430068,14.401563,13.814878,16.361712,14.199699,14.729766,13.931626,14.935929,13.299727,15.474029,15.030332,14.410124,14.13062,13.89153,14.573246,15.042407,14.781781,16.345213,14.389159,14.493456,14.41173,14.758334,14.633859,16.350338,14.687258,15.107433,15.134679,15.550764,14.511144,14.826779,14.612463,14.972738,15.991259,14.781126,14.754221,14.589218,15.004889,14.999508,15.047241,14.571126,16.372847,15.381579,14.71691,15.346302,15.184667,12.518988,11.556314,11.745384,11.600084,10.2335205,11.37065,14.942956,14.496142,14.637298,15.127794,14.457368,14.926737,14.671554,14.914754,14.680302,14.333974,14.78625,15.052019,15.845102,14.461295,14.64724,15.194896,14.685112,14.802999,14.89226,15.403518,14.529991,13.647478,14.357911,14.685474,15.123962,14.260962,14.817681,14.84699,14.768397,15.03742,16.33323,14.497872,13.856119,14.382784,14.75019,14.512584,14.640143,11.789947,14.746893,14.745609,16.290033,14.338785,14.972629,14.771233,14.351704,14.829344,14.427019,15.034137,14.665723,14.297029,14.540257,14.570649,14.632072,15.264411,15.644571,14.609394,14.557058,14.869599,14.68458,14.1359625,14.773832,15.355263,14.945963,14.240916,14.430985,14.994046,14.690687,13.763192,14.734108,14.883333,14.753843,15.383006,15.264025,14.73808,14.367867,14.917817,14.611362,14.776571,16.23786,14.439399,14.706933,13.816197,14.800981,13.583781,15.47665,14.614123,14.308058,14.60106,11.914483,6.065298,14.124231,14.590683,14.61121,14.566631,14.72902,14.965627,16.468048,15.20701,11.545022,6.1182017,15.705604,14.263574,14.687904,14.6436,14.737227,13.451604,14.959208,14.874283,15.281941,14.458183,15.01674,14.771404,12.753325,11.481772,7.490283,14.329863,14.395186,14.483749,14.269502,14.677023,14.866421,14.031177,14.943344,14.660824,14.69617,14.785318,13.13705,12.775874,13.105243,14.402999,13.434622,14.730452,13.687681,11.483454,14.94066,12.9054165,13.124356,13.185792,10.375653,14.977203,15.095288,15.405085,15.599322,15.568811,16.170979,14.93923,14.8229885,14.903859,14.86014,14.915164,16.168848,14.288336,14.758608,15.481793,15.340161,14.634202,13.908289,2.1389327,15.977363,14.95821,14.738824,14.809522,15.325236,15.500774,14.761639,14.8549595,15.228887,15.593813,15.5821905,14.820096,14.017572,15.765606,15.092783,15.472589,15.57974,14.262578,14.162406,15.43472,15.570301,15.227753,15.219859,14.982338,15.572898,15.621382,14.994795,15.045293,15.637521,15.541179,14.889499,14.927286,15.06216,15.236889,15.385322,11.036484,12.143189,12.740384,10.960203,16.065186,15.100372,14.540864,15.523489,15.612855,15.069841,14.966918,15.1520195,14.641912,14.914565,15.348488,15.380908,14.937386,15.622754,15.625905,14.4399805,15.543877,5.555758,6.2101746,12.972876,11.378123,11.320484,14.716762,15.051192,15.17018,15.345202,15.567104,15.410063,11.649867,13.518212,15.025605,12.910013,15.001275,14.715931,14.896563,14.235838,15.261457,15.497818,15.453735,10.79302,14.431376,5.5719724,15.034886,14.796256,15.567082,14.344662,14.57037,15.549408,14.990947,14.8302355,14.714836,15.242531,15.632408,15.01883,15.296685,12.264208,14.298943,15.049074,15.079015,15.269578,14.779049,14.935463,15.360175,15.834285,15.020217,14.930487,15.5833645,12.295515,6.6330996,15.029316,15.248498,14.849713,10.358081,14.532776,15.331405,15.334028,14.774776,15.512631,13.460741,15.1789875,10.738531,11.120907,11.663408,14.389861,15.491314,15.520266,11.244221,12.0474415,12.682497,15.396358,15.502548,11.723433,12.266653,14.549286,15.527276,15.548277,15.507773,15.547174,15.497506,11.629331,14.472854,15.55448,10.564078,15.378799,15.168699,12.11517,11.500932,14.9128065,15.444344,14.902989,14.157391,15.060663,9.99221,11.185295,15.0338745,15.186344,15.566267,15.39922,15.05521,14.005955,14.872771,15.602145,14.993281,14.867375,15.480768,14.686194,15.149874,14.962006,15.396662,11.009085,11.2351885,6.9787364,14.671524,15.066137,14.736974,14.532077,14.661606,11.898434,11.892969,16.12307,14.492148,14.597218,13.714439,14.175141,10.809941,11.027955,14.779138,14.713474,14.819862,14.722274,14.068798,13.486432,11.526366,11.491278,10.943864,16.245365,13.220096,12.664858,15.426645,14.951786,15.23975,14.391354,12.980037,14.686665,14.557923,14.801389,15.039064,14.5492525,14.984868,14.531121,14.244751,14.970691,14.431476,14.272531,14.648176,14.3287115,14.084968,14.016699,13.490997,14.41783,15.001424,14.886745,13.586748,15.104353,14.142167,13.965549,14.537227,14.648977,14.086207,14.015938,14.252107,14.04728,17.489277,15.398314,17.700056,17.5031,17.782217,16.300797,16.07901,4.694231,11.32003,12.00106,8.415494,8.213721,11.517993,11.884299,10.084639,10.365053,8.050326,10.596222,9.83847,9.953694,10.241756,11.053383,10.146874,9.06864,10.217132,9.678421,10.922193,-3.8927443,11.621921,10.541266,14.259013,14.989533,15.310943,16.065197,16.0838,16.202972,16.17704,14.776334,13.330537,14.9537325,15.475384,15.179731,16.383104,17.465107,14.679273,14.788915,14.616277,18.01752,18.663538,18.351507,16.290176,16.218525,17.474937,18.370026,17.248823,15.936938,12.308162,12.651619,12.710094,12.775678,12.461289,10.9176235,11.999569,12.241607,12.08671,11.46364,11.106713,11.630606,12.07535,12.143776,7.6586437,12.140098,11.548423,11.732279,12.6331005,11.757628,11.509826,11.535015,12.66167,3.7877872,11.779842,11.884985,17.353191,15.331427,16.459211,14.807332,16.155691,17.43021,17.371515,16.00876,16.383057,14.815571,14.189565,15.457582,16.113043,16.208538,15.3596,16.755754,14.904284,17.007555,14.650541,16.149473,15.286465,10.810086,10.870421,11.333613,-0.5652034,11.204021,11.90488,11.743974,11.870146,11.607463,11.456189,17.037788,15.445236,15.178255,16.276146,16.40013,16.49821,17.288054,16.768429,-3.8062935,11.823928,10.1673,10.384385,-0.5154802,10.468463,10.14675,10.998227,11.717773,11.124549,17.372078,15.622895,16.854708,15.095104,15.494556,18.094946,17.81459,15.473091,14.667984,16.389082,15.643198,15.29299,15.37705,17.375587,16.895325,15.98754,17.492638,17.929585,17.27135,16.305035,15.989383,11.537986,11.751565,18.07765,18.402842,17.125338,16.22591,17.344107,17.883259,16.686808,16.802809,13.079124,12.917693,13.04721,12.73999,12.975272,11.989674,10.882687,0.51714647,11.413919,12.110732,12.110253,11.922547,11.642056,10.964933,11.327896,11.955909,12.150584,11.284121,-7.278131,-7.598664,11.630114,11.642365,11.787462,14.669584,12.47234,12.499053,12.1612215,11.440796,12.069987,12.475078,1.7665645,11.05914,10.88661,11.450381,11.611295,11.507303,12.070046,10.817746,10.552864,9.730654,9.646255,18.03911,16.671585,16.237026,16.058918,15.377863,18.28243,16.04129,17.395752,16.13154,17.550913,18.351273,18.526218,17.873718,18.40553,16.042503,15.398141,16.338247,14.584407,16.105299,17.351435,18.491484,15.922694,16.932953,17.821697,12.378956,11.956183,9.626507,12.11691,11.774507,11.482531,11.960969,11.537212,11.733833,0.5939626,11.435662,11.645285,11.518475,11.523274,11.720737,12.048972,-7.500522,11.643962,12.025676,11.626263,11.913811,11.903503,11.891963,17.308239,17.58425,15.50575,15.119606,16.33732,16.31663,16.107319,15.969021,16.843464,18.406471,16.575695,16.106676,14.165299,8.036113,11.374355,10.727176,8.168498,12.095002,11.247875,11.075079,11.301831,12.30441,11.891202,-6.0596857,-8.911747,0.4079749,1.3586687,11.074754,11.233427,10.180964,1.3950572,0.29717296,11.398446,11.950665,11.892685,11.552798,11.966499,11.147057,9.556661,11.956534,10.189921,11.663126,11.918783,10.201561,10.323493,11.587973,10.208117,11.384668,11.250601,11.115705,10.133766,0.96616125,1.006485,0.8326766,1.2133079,10.249122,18.075617,18.485918,16.752289,16.150185,17.530542,17.68719,16.911947,12.011146,8.851017,3.3032868,9.695943,11.515801,11.682397,0.6341139,0.4450617,11.510003,11.705475,11.537731,11.96451,12.064186,11.94982,9.936371,10.2072,-0.043101605,10.48381,10.270705,11.832294,11.39569,15.889294,16.735416,10.425927,10.324509,10.185995,10.448877,10.446299,10.70744,10.387349,10.388228,10.358286,9.619802,10.581559,10.913266,10.905194,11.421505,11.18253,11.31902,14.728875,16.915693,11.64011,10.182644,9.983526,-0.58577925,11.393829,11.8766775,-2.2852237,11.69788,9.993453,10.118086,10.038863,8.741609,10.135796,9.800352,9.926002,10.007351,9.6113,11.996018,14.72602,15.387645,15.967023,15.572506,11.698359,11.2959585,9.485802,12.62029,11.909847,11.853548,11.518388,11.577605,9.819973,11.892783,12.008041,11.265501,11.303183,11.652862,9.651879,0.5950462,10.252217,10.134641,9.303036,9.570694,10.1182995,10.415928,11.77067,16.63283,17.938692,18.433926,16.716024,16.169212,15.825811,16.860735,15.843411,15.192384,15.890759,14.631352,14.714632,17.016642,15.759436,11.77569,11.2851,10.22095,11.91965,11.464275,11.186292,11.836199,10.552502,10.444501,10.511466,11.198435,10.335901,9.748626,11.767291,10.531904,11.967203,11.659268,11.557872,11.364971,9.5614395,7.905178,13.837458,11.218401,11.374192,11.311608,17.635275,18.159765,18.382391,18.447105,17.91267,15.48213,14.256408,16.266575,16.744968,18.29316,17.86004,11.067297,11.631538,11.10166,-7.571093,-7.623619,11.324038,-0.59337765,11.489556,6.8291607,11.284246,11.531488,10.889509,10.336244,11.5203905,10.818356,9.789753,-0.072239965,0.71788377,10.127905,11.925561,17.773657,18.54718,18.414299,16.27956,12.695494,11.941093,11.812865,9.742019,10.189086,10.062988,10.415109,6.971269,8.8638735,9.9542885,11.932681,11.977352,11.841653,10.049864,9.904265,10.747131,9.475497,10.971481,10.971653,10.858093,10.740179,10.404217,11.5753145,11.721459,12.069364,12.016917,12.163073,11.637566,12.067101,11.080346,11.141352,10.530349,1.3462776,11.337128,9.872692,1.6529887,9.308553,-3.4561648,10.673407,10.75675,12.272843,10.979828,5.5295415,10.422321,10.515853,10.51692,10.763944,11.159311,12.094828,-2.0404856,11.961268,18.014471,17.734324,18.28762,18.456373,18.023243,18.072662,16.669958,14.882687,15.951207,16.21015,17.473076,18.348862,15.385494,15.797743,18.115707,17.496801,16.152273,15.177113,10.652813,11.133469,11.160139,17.975449,18.62957,18.453009,16.14213,16.329525,16.28686,18.476084,17.53682,17.357588,15.136601,11.607115,15.5035925,16.11666,16.432487,16.944157,10.208189,10.22388,9.70128,4.595797,9.902612,10.366972,18.005636,18.41353,15.076564,16.177563,16.11444,17.471153,17.746655,16.114424,15.881977,11.983219,6.081197,5.565959,4.7223134,4.654332,5.9866104,6.1622763,14.152197,15.898378,14.867771,15.1929245,16.196842,16.124367,16.305906,14.426913,14.384378,12.398894,12.779522,13.515593,13.041227,13.293885,15.572348,15.507734,15.5619,15.588129,15.658353,15.65122,11.360664,11.703213,11.887311,11.747642,11.863621,11.247063,-4.8054934,18.002419,17.702278,16.188255,16.974947,18.369677,17.442568,16.083048,12.042469,14.561426,12.326964,17.050581,14.751104,11.218287,11.864306,10.986086,9.780864,9.993346,12.044622,12.95151,12.048014,12.245791,12.025686,-6.0772004,11.973039,10.689568,9.357605,7.7518687,10.112855,11.034586,11.368153,0.43508282,-0.30630353,11.645967,11.8648615,11.449105,11.695863,11.918426,9.846927,9.67345,15.076839,16.850487,16.235464,15.809082,15.6752825,16.15896,17.772795,15.461792,14.830473,16.338505,15.07314,15.896424,15.99851,17.620743,16.873362,15.50035,18.224792,16.227121,17.106152,15.80928,16.254467,15.5360155,11.476477,12.081675,18.090122,17.600836,15.857991,18.58278,18.10397,18.005018,17.715862,16.231398,17.50919,14.28565,11.169466,11.448982,11.240939,10.598118,11.639701,9.99969,8.769966,11.408419,0.6710527,10.109149,10.223769,12.082785,11.640518,-4.2686253,1.032064,11.202413,12.843318,11.858113,12.071518,11.695992,11.463763,11.502799,12.862249,12.001722,12.550703,11.45228,10.034587,10.978207,9.339774,9.330562,9.318486,6.197664,7.9077773,8.7654915,10.877597,11.760802,11.771192,11.405273,11.42463,11.44722,11.660348,11.068927,2.9475064,10.803116,3.8728025,10.911615,10.531298,10.794375,10.658969,10.307775,10.174338,11.982683,10.858806,11.286255,11.23359,10.955189,11.740129,10.752186,10.848767,10.950001,10.009834,10.873325,11.168166,11.959042,10.322656,8.7358885,9.900396,3.6486094,10.587196,10.153345,12.791462,11.956569,11.447585,9.232876,12.130592,11.109666,10.689488,9.858409,9.677983,12.334875,12.691867,12.358001,12.200292,12.504702,5.0156407,13.276445,11.123862,11.401402,10.347615,9.62521,11.721212,16.89817,18.353878,18.173168,18.376698,16.458563,16.931753,16.315582,16.007532,16.291803,16.133009,16.186543,16.663277,16.411558,18.379293,17.108545,15.82668,12.902155,16.155207,11.692296,11.24551,18.121965,18.262165,16.133904,17.048836,18.02771,11.740546,11.927626,10.844205,9.710229,10.188728,10.247519,10.35173,-2.1804662,10.36677,9.254783,9.221651,10.054735,11.83538,13.954966,11.733845,11.340841,11.897445,11.746449,10.897887,-0.42881557,-0.5507093,11.994817,11.812972,10.143947,10.0907135,6.764223,11.979725,12.081338,11.983602,12.0584135,12.053345,11.498332,10.329836,9.830354,8.043657,7.857155,11.037304,10.171556,5.468958,10.632789,10.260322,12.879531,11.771417,11.855286,16.018782,16.685333,11.864195,10.680958,0.09011215,0.17520179,11.992982,11.567462,11.306883,10.006719,10.602066,11.407712,10.832159,11.030354,10.255184,9.0266,7.5943875,7.8567233,11.307659,11.031662,6.720242,7.1241646,-7.4071426,-7.41924,-7.3154387,11.673447,11.983469,11.093622,10.680277,10.052998,11.945638,10.286063,10.37643,10.057744,10.363532,10.576089,10.289928,10.12269,10.149376,10.157724,11.140703,0.013488636,11.213658,11.123673,10.842483,10.421938,10.32901,10.78915,9.848079,9.954301,9.9394655,10.051559,10.800088,10.998039,5.212973,5.7571015,7.0068383,10.777,10.885941,16.8002,16.356644,15.451502,16.907902,10.473517,10.250415,8.880014,9.608493,10.505343,10.291084,9.573866,10.5616045,9.541807,10.489632,8.727827,9.738567,6.7605705,10.516228,10.537516,12.074658,11.799381,18.13016,18.44927,16.396164,16.569391,18.459085,18.410643,11.977946,11.998435,11.395774,12.085146,12.292944,12.018416,9.715793,9.880096,10.342043,10.090293,10.092151,11.949498,11.834906,7.723515,10.068954,10.165125,10.155052,10.238525,10.08477,7.133955,10.301111,10.578657,10.45353,-0.2962693,10.357482,10.226167,10.077385,10.256693,-0.87098974,10.088954,10.329159,16.183168,16.816494,11.216033,12.068101,-7.0813794,12.064111,12.889616,11.5706415,11.455011,11.234504,10.413952,11.34811,10.438395,10.090601,18.030218,17.17352,16.415918,17.836693,16.03698,12.040151,12.009375,11.928271,-1.0422534,11.324126,11.378476,11.6021385,12.115275,11.941402,10.983316,6.208768,11.567079,11.468352,11.950791,11.884983,10.485807,11.826611,13.809314,12.496896,12.649714,5.7630672,11.828498,11.850185,11.703561,17.84178,16.913208,16.240248,17.45525,17.67853,15.226074,12.012285,11.356061,11.206783,10.226471,9.586596,9.847307,11.126712,11.151646,11.181247,6.329194,-7.3948717,-7.4271164,11.061658,12.860279,12.770911,12.793604,12.194053,12.545464,11.727101,11.123401,10.870422,9.6676855,9.993966,11.878381,17.754652,17.96961,17.107037,17.62922,17.786854,15.683828,16.358795,16.13633,17.158787,18.037306,18.089714,18.04945,17.072954,17.569345,12.251122,11.611488,8.026862,11.770535,11.627025,12.029647,12.100513,17.84153,18.376463,16.308798,5.2781076,8.373448,7.7808146,6.7756166,7.1002746,8.398873,7.6561007,6.8639193,10.146728,12.0811,18.177044,18.547018,18.005198,18.394207,17.359589,17.377565,18.252415,17.390863,16.108438,18.433243,11.498929,11.768346,11.935179,11.376723,12.875323,12.914353,11.577319,11.750568,10.572191,10.448889,4.4711814,4.4216447,4.293022,10.591434,18.14256,16.88891,18.002562,17.098448,16.063667,15.014893,17.642458,17.824013,17.70812,17.500034,17.81334,16.579132,15.9335575,12.889622,11.690688,11.359007,11.986923,11.732511,12.084036,11.790143,17.940643,18.241053,11.310653,14.785189,15.076998,16.365156,15.121311,15.529941,15.1956835,16.395273,18.028563,16.302101,16.147232,18.07869,17.057549,16.079685,11.81737,11.774666,11.491825,11.508962,11.280117,11.436917,11.232748,11.518413,11.92113,10.933319,10.949402,10.19497,12.8614,12.491775,12.486529,12.622747,12.340961,3.683053,3.6572986,12.89718,11.616651,11.931763,9.322329,12.033288,10.469295,4.239269,9.5035715,10.934533,10.618399,11.185667,11.032085,10.678473,10.079292,10.923905,10.344612,10.129823,9.736502,9.527871,9.375958,9.104805,8.853351,9.099614,11.043557,11.9599085,16.583767,17.059486,15.486334,16.752226,10.237076,10.3462925,9.514166,9.875027,10.474507,10.313707,11.695619,10.842534,10.779123,1.6405189,11.899669,11.617704,9.229831,11.995814,11.847889,8.989886,10.094364,9.07223,0.7220011,7.2373834,10.716775,10.618345,10.253677,10.637022,10.23089,1.6614399,10.036609,11.627254,17.790703,17.96383,18.36546,18.592093,18.435871,18.456593,17.92496,15.72868,16.18594,17.51967,16.223806,17.859793,11.65402,10.393086,10.212299,9.995737,9.96374,0.040556736,1.4653621,0.1934177,10.542498,10.308271,10.153252,11.824096,10.646385,9.857273,9.86747,9.897513,9.911276,12.0548935,11.478555,11.241421,11.9364395,11.892904,11.717438,10.925362,10.658637,10.678304,10.60084,10.197782,10.4752035,10.987319,10.794138,10.600291,10.560815,10.631574,12.046249,11.723318,12.920054,12.736415,12.8386755,12.67747,12.922422,11.975481,11.933101,-3.5758758,-1.8909041,12.9801035,10.304501,3.9791448,4.4261403,10.337417,10.55131,12.089389,11.521155,17.963861,18.431993,16.005129,16.227032,15.756111,16.017094,15.441539,16.407133,14.156775,14.496934,16.333931,16.302107,16.02832,14.381887,13.624608,14.094232,15.658976,16.856098,16.388943,12.045407,11.905773,11.907221,17.901379,17.952267,17.282364,16.928988,16.265795,16.536394,18.4143,14.960324,17.04247,17.051065,14.867613,11.556935,11.843599,15.807038,17.518421,17.39755,15.3557,16.631483,15.5377035,8.317022,15.071802,15.270203,15.834775,13.897502,16.7095,17.091928,14.48156,12.466271,16.915974,15.040833,14.834213,16.173382,15.073941,14.701105,14.932209,15.438109,14.8417635,14.670175,16.351107,15.546339,14.708307,14.154833,14.900507,17.156294,16.415312,15.12687,14.53677,14.737703,17.131306,14.621894,15.553259,17.119154,16.814466,17.475496,16.568003,16.472225,16.161217,14.2062025,16.034216,17.352322,16.92685,11.342085,6.2019687,16.055048,14.873939,15.202248,12.417165,14.064818,14.506957,15.389641,11.909048,10.945838,8.840292,6.6000905,5.8908706,10.06193,10.120597,11.735344,11.751807,11.701587,11.404341,11.157689,-7.426291,11.980877,10.707356,9.906573,10.752422,9.582387,9.124816,16.943228,17.268614,15.138301,18.264267,18.377466,16.998278,17.857807,15.476111,15.393087,16.26215,16.991034,16.29175,15.807996,16.360062,14.999078,16.12368,14.731774,17.194715,15.714237,15.117871,16.159311,17.886656,18.089605,18.174152,18.184723,17.781843,15.6833105,15.951771,16.857275,17.686602,17.3104,16.199799,11.029936,10.987434,10.763092,10.765062,10.167118,10.508552,10.898278,-8.8744955,11.233225,11.1851845,11.580888,0.3078673,11.4528475,0.43460456,-4.537302,-4.681985,10.576516,10.964299,0.4121455,10.883647,9.59622,10.981828,9.05788,10.50824,9.415749,11.365234,12.879994,12.487284,12.242801,12.198907,8.266136,7.5577416,8.245849,12.506469,12.692371,10.482893,10.280881,4.4605045,3.883166,10.689676,11.4703455,12.011072,11.752766,8.7304945,11.693202,10.968609,11.379873,10.5809145,10.8763275,10.552154,10.151764,11.978842,10.354964,10.447428,10.682162,10.392112,10.268613,10.13343,10.541176,10.307553,11.098895,12.31815,10.569889,9.691958,-2.087776,0.6075695,-11.157238,9.975033,10.448206,0.4264526,10.361142,10.477287,10.389979,-4.7858467,11.796019,11.483248,16.13065,16.507418,15.8342705,13.5429735,14.815923,15.018976,16.882107,15.54737,17.087624,17.24583,17.656782,16.325119,15.326618,16.333076,14.84207,16.233006,17.391068,15.832087,18.326174,18.356518,17.057976,16.63751,10.816414,10.227052,9.89075,18.245138,18.31499,16.725996,18.481693,18.398067,17.82196,14.952568,16.345282,16.182789,17.579823,14.671316,17.956656,18.181349,18.37785,17.329248,15.966253,14.82851,14.914104,18.339571,11.984316,10.81192,0.23668362,12.223387,10.610007,10.814137,10.376922,9.837567,10.601689,10.720439,10.629065,10.680219,10.519097,18.034496,15.741018,16.926304,16.733946,17.300781,16.809048,6.905002,11.749663,11.855311,11.535995,11.457479,12.887162,12.042731,12.409406,10.035452,9.891709,9.776073,9.828879,9.3729,12.848043,11.857225,11.972887,11.815255,12.102716,11.995748,11.390673,16.957504,18.360266,17.677763,16.271753,16.176422,16.40292,17.712574,15.245334,17.954393,17.082201,15.905005,14.878437,11.557924,11.408446,11.75985,11.946109,11.819296,12.126812,11.92823,11.949795,12.15846,11.998533,10.811849,5.698553,0.8331954,7.4080377,9.98311,9.930446,9.8154745,0.97496575,10.984033,10.793292,10.057323,10.314872,10.445157,10.223401,15.196665,15.8616295,15.255027,11.402083,11.548678,10.017818,11.280445,9.59391,12.420654,10.826582,10.951501,10.532216,5.2299266,6.5614195,5.589051,10.271293,12.118786,12.463698,12.168238,9.793753,10.988185,4.404724,11.390869,16.129713,12.1003065,11.648802,11.886655,11.144048,10.049762,11.522718,11.622929,11.261879,-6.6020327,12.011083,0.8785685,11.928581],\"xaxis\":\"x\",\"y\":[-3.159042,-3.2018287,-3.819418,-3.884049,-3.814101,-3.265451,-3.4045787,-4.0229826,-3.06673,-3.4582856,-3.507266,-3.244458,-3.6183465,-6.7669826,-4.656546,-3.2024322,-5.8962493,-3.5502129,-3.4789095,-3.1267369,-6.6270313,-6.7923512,-4.731506,-5.901706,-5.6330705,-5.5423017,-3.769845,-5.9452896,-4.4078965,-3.8391232,-4.730166,-1.5846441,-1.8351237,-2.1037786,-1.3739607,-0.36773312,-2.316183,-4.7725873,-4.715873,-4.2358155,-6.761414,-6.633871,-5.1874957,-4.597237,-4.493235,-4.6036677,-4.754335,-4.6170673,-4.478415,-4.635672,-4.6656966,-4.6014,-6.3420644,-5.696237,-4.5494165,-4.7895226,-4.6791697,-5.1358433,-4.692466,-4.710943,-5.446118,-6.2382503,-6.672969,-3.0632265,-3.210943,-3.3303807,-3.4878528,-3.2603872,-3.4522576,-2.9273713,-3.2555826,-3.357217,-3.2939334,-3.4987056,-3.284587,-3.1621993,-3.1242802,-1.2362946,-3.1421678,-2.785949,-3.1199,-3.2255921,-5.2625775,-4.678668,-4.577097,-4.6874843,-4.7846885,-4.820949,-4.633387,-4.5894804,-4.674066,-4.5605745,-4.5481677,-4.7345657,-4.684524,-4.7109284,-4.5701556,-4.7115307,-2.1914258,-5.023237,-4.730412,-4.7742143,-4.869843,-4.5689464,-1.955364,-4.659701,-4.897395,-4.649031,-4.705309,-5.0332375,-4.8445234,-4.686281,-4.7194023,-4.8850603,-4.758683,-4.801068,-4.4453945,-4.6851335,-4.760947,-4.8046927,-4.4123316,-4.7697754,-4.839161,-4.6554346,-1.9420334,-4.819883,-4.6660933,-4.927846,-4.6707377,-4.6374087,-1.979348,-4.658196,-5.0107207,-4.6740313,-4.373402,-4.6100545,-4.9129243,-4.7048106,-4.6644497,-4.592761,-4.67553,-4.6403213,-4.524764,-4.7786393,-4.6439743,-4.779958,-4.7582917,-4.827508,-4.1123023,-4.4912953,-4.251229,-4.178023,-4.801359,-5.0382466,-4.458942,-4.820172,-4.9086275,-4.9398775,-4.9415307,-4.783138,-4.7643876,-5.041901,-4.930451,-4.8234773,-4.6247,-5.088836,-4.819665,-4.737683,-4.8057303,-4.8757763,-5.4648147,-4.823929,-4.766393,-4.841501,-4.8154383,-4.800876,-4.6081376,-5.148632,-4.8074822,-4.725842,-4.738932,-4.7111974,-4.7990446,-4.787362,-4.6402936,-4.664696,-4.6716347,-4.587519,-5.0967183,-4.8752055,-4.710202,-4.938238,-4.7686353,-4.8172293,-4.7839804,-5.030433,-4.6728187,-4.750353,-1.97011,-4.7459626,-5.0038185,-4.746878,-4.6075926,-4.752317,-4.71599,-4.471421,-4.5497794,-4.616667,-4.6580725,-4.6551566,-4.7301483,-4.2106376,-4.0141253,-4.677647,-4.703868,-4.497991,-4.559804,-4.5654697,-4.650737,-4.42973,-4.557523,-4.597853,-4.4732165,-4.519717,-4.8586845,-4.415702,-4.719264,-4.484797,-4.635121,-4.618121,-5.019324,-4.6097794,-4.7968616,-4.7320695,-4.6655846,-4.739617,-2.0265136,-4.719085,-4.6890507,-4.472663,-4.830116,-4.7089677,-4.3107467,-4.4912667,-4.6070504,-4.691013,-5.3705473,-4.018547,-4.702075,-4.693186,-4.7172475,-4.5360594,-4.5124,-4.5799894,-3.8643417,-3.7315183,-4.143177,-4.0788302,-4.17522,-4.5695243,-4.655032,-4.662456,-4.6943183,-4.6516843,-4.932912,-4.5810065,-5.031833,-4.6317215,-4.937862,-4.882379,-4.319938,-5.273657,-3.6449716,-4.827562,-4.733276,-4.6357613,-4.776954,-4.850657,-4.7820387,-4.7286654,-4.658887,-4.619026,-4.6731257,-4.9308453,-5.6201806,-5.566811,-5.659633,-4.72725,-5.318533,-4.3120174,-4.8354025,-4.9159474,-4.1285386,-5.7754564,-5.5073175,-5.30652,-5.0559087,-4.3522735,-4.255333,-4.091203,-3.5246444,-3.4132993,-2.2559369,-4.4906373,-4.629214,-4.365689,-4.7933965,-4.4257956,-2.2755206,-4.6280265,-4.603495,-3.8371716,-3.802233,-4.745143,-4.8171806,-1.4985626,-2.5404088,-4.5352235,-4.3341217,-4.656157,-3.930265,-3.6887257,-4.4859357,-4.702471,-3.7557385,-3.3991678,-3.2932782,-4.4144263,-4.7665486,-3.976139,-4.4482174,-3.7251275,-3.4294093,-4.716701,-4.550601,-3.77089,-3.4231076,-4.2501864,-4.8156366,-4.474449,-3.303197,-3.2587779,-4.109263,-3.8703573,-3.669718,-3.418944,-4.2034473,-4.5126667,-4.5951753,-3.9743454,-3.9638023,-5.156403,-5.1276245,-4.9133477,-5.330041,-2.3033373,-4.1991405,-4.5917125,-3.553378,-3.3778193,-4.5438185,-4.26366,-4.0935035,-4.6364665,-4.5766087,-4.278621,-3.9341023,-4.4803286,-3.3562756,-3.2844472,-4.593529,-3.3874032,-1.4976788,-1.9065655,-5.356018,-5.2546306,-5.578529,-4.6991158,-4.2943716,-4.1933937,-3.767616,-3.508498,-3.7924454,-5.196813,-4.7713385,-4.481637,-4.533876,-4.375469,-4.4680657,-4.4392242,-4.5909214,-3.926445,-3.7698586,-3.6280222,-6.265242,-4.3709116,-4.3353987,-4.243113,-4.4761953,-3.5706933,-4.52788,-4.583728,-3.4570618,-4.840814,-4.5169115,-4.3239903,-4.0551844,-3.2767715,-4.255428,-3.7229936,-5.7151766,-4.5820208,-4.5028124,-4.2253876,-4.108408,-4.4793854,-4.497679,-3.8755238,-2.9080267,-4.509287,-4.4816675,-3.3830974,-4.6689687,-4.1451125,-4.2090487,-3.8289862,-3.8912182,-5.7701964,-4.559993,-3.7233202,-3.7071986,-4.4454684,-3.3090913,-4.4914684,-4.034312,-6.0580573,-5.4533544,-4.907129,-4.6719666,-3.7363017,-3.6553156,-5.2552233,-5.4244366,-4.888685,-4.39609,-3.4666994,-4.6361346,-4.5832486,-4.503991,-3.3647118,-3.4866657,-3.7965465,-3.49501,-3.5343325,-4.6306963,-4.3393764,-3.4654653,-5.542576,-3.7085838,-3.7353497,-4.652879,-4.5229654,-4.46768,-3.5267987,-4.5155044,-4.8023286,-3.4871082,-4.758352,-4.745832,-4.617389,-3.7907014,-3.4518228,-4.0209675,-4.45642,-4.376601,-4.6249175,-3.369114,-4.854328,-4.6196394,-3.6496277,-4.492973,-4.7723308,-4.347142,-3.4386108,-4.9501224,-5.1540265,-4.138146,-4.4216647,-4.208674,-4.4743376,-4.446297,-4.440718,-4.8313756,-4.963876,-2.376418,-4.4391418,-4.492753,-4.6010475,-4.340935,-4.975563,-4.862515,-4.2786207,-4.493804,-4.2965007,-4.4820743,-4.773436,-4.4858084,-4.6371055,-5.2133474,-4.953783,-2.1497555,-4.743113,-4.7777596,-4.095678,-4.128672,-4.028815,-4.6998987,-4.6730423,-4.597465,-4.5194445,-4.6307454,-4.8326116,-4.5503173,-4.4918294,-4.499283,-4.615825,-4.552469,-4.5115137,-4.6111546,-4.5418425,-4.606607,-4.546158,-4.5507426,-4.542189,-4.4121943,-4.4796815,-4.4943385,-4.7052045,-4.73725,-4.515635,-4.6289344,-4.550485,-4.488853,-4.5773425,-4.5757794,-4.6267867,-4.5540757,-4.43075,-4.498849,-4.5286613,-4.897834,-4.774116,-4.0368614,-3.936439,-1.421044,-5.381684,-3.4439359,-3.686298,-3.8345485,-6.064971,-6.730115,-3.205792,-3.2491527,-1.3036077,-4.2539954,-3.3471687,-3.7242048,-3.2741082,-5.511712,-3.132254,-3.3882027,-3.447282,-3.672441,-6.107017,-2.4118693,-6.2058315,-5.0387216,-4.5553555,-4.6174803,-4.7153635,-5.3829184,-5.342217,-5.4424415,-5.7212605,-4.776019,-4.4928756,-4.609425,-4.690915,-4.7993937,-4.9086566,-4.4906616,-4.8300815,-4.72763,-4.6727905,-4.463303,-4.4911203,-4.580742,-5.249169,-5.733155,-5.399058,-4.510843,-3.7657275,-4.0018253,-4.6340966,-4.453189,-4.4090285,-4.348673,-4.5577855,-3.519221,-4.3486176,-4.4259267,-4.042945,-3.9353378,-3.9705665,-4.325418,-4.2827306,-4.2206793,-4.2531877,-4.454067,-4.679167,-5.1791935,-5.0359044,-4.455306,-6.1008334,-5.266269,-4.7756815,-1.5281823,-6.543282,-6.6381073,-4.4984417,-4.3651342,-4.8969436,-4.182185,-4.8904986,-4.6169386,-4.792268,-5.5052977,-1.8646477,-4.787208,-4.6398697,-4.7488546,-4.3330517,-4.751651,-4.392675,-5.0011377,-4.5160646,-3.5560167,-4.649169,-4.918627,-4.347541,-5.5067315,-6.1959906,-5.2808614,-1.8353082,-4.99096,-6.679218,-6.4542317,-6.663389,-6.225336,-5.53915,-4.3958893,-4.4098973,-4.9547997,-5.077735,-5.5145564,-5.110551,-3.8816955,-3.542511,7.1520977,-6.300312,-3.2815902,-3.3365245,-1.8695037,-3.4630418,-3.1492248,-5.338498,-6.30967,-5.3589983,-4.4639306,-4.5087504,-4.5173173,-4.7333617,-4.507635,-4.4962525,-4.516148,-5.296927,-4.963888,-1.8381629,-5.370655,-4.9315386,-4.758269,-4.801753,-5.256134,-4.614141,-4.563895,-4.505685,-3.823687,-3.585909,-4.281165,-6.2403326,-6.222909,-4.4543447,-4.5242467,-5.2535524,-5.534844,-5.1625476,-4.708628,-3.9000568,-3.4994242,-5.756307,-5.63273,-5.7790594,-5.7196674,-5.7151833,-6.698284,-5.6013403,-3.9541025,-5.8317966,-3.3716528,-6.5334425,-4.687177,-4.714154,-5.1755986,-4.810735,-4.6521363,-4.553864,-5.6141133,-6.314275,-6.7826777,-5.839623,-6.603082,-6.6785154,-4.6961646,-5.7995644,-5.7227674,-5.8072786,-6.4118886,-6.632525,-4.6016335,-3.1319816,-4.518113,-4.555999,-4.5544496,-4.30449,-6.05744,-6.625176,-5.268889,-6.1408043,-5.945339,-4.7921906,-4.460283,-5.492664,-5.1133776,-5.702424,-4.5670104,-4.4735794,-4.6220756,-3.8932776,-3.635792,-4.5051875,-4.5493174,-4.5053916,-4.4984856,-4.5241427,-4.343249,-5.3056173,-1.9218465,-4.784265,-5.7176976,-5.3588576,-4.5264344,-4.086052,-3.4520931,-4.5134377,-4.57981,-4.2817926,-4.4164357,-3.3310945,-6.478662,-6.039807,-6.551462,-4.8381214,-4.9426594,-2.5028996,-4.7002954,-4.65784,-5.1612616,-4.605057,-5.2070923,-3.669666,-6.2685165,-6.091257,-3.2838988,-6.1077476,-4.6200333,-4.5211935,-4.5570316,-4.479747,-4.480659,-4.463323,-4.6576,-4.757025,-4.79816,-5.4990525,-5.5546813,-5.061195,-4.4885325,-4.11157,-3.6886427,-4.74838,-3.8404734,-4.447733,-3.9600523,-2.6700902,-4.2228355,-4.4667444,-4.2651052,-4.2467475,-4.5967836,-4.4248476,2.4271674,-0.49869323,8.28506,-0.98077655,-4.4472957,-4.8884115,-4.138465,-0.7652582,-2.2068727,-4.8020654,-6.4113526,-6.705721,-6.568135,-6.828299,-4.994071,-4.6474075,-6.5444117,-3.146083,-6.251787,-6.3384566,-3.1501675,-3.2975135,-5.5792527,-3.164056,-5.7213,-6.101114,-5.930166,-5.561694,-4.83466,-5.453906,-4.8848968,-5.1416273,-5.852269,-4.4658113,-4.5319495,-5.4031696,-5.6054864,-5.3055086,-4.1698503,-3.5013855,-3.3655396,-3.4547658,0.5516643,-3.3510041,-4.9119644,-4.7734327,-3.367795,-3.7653685,-4.7898927,-4.7586536,-4.72221,-6.419392,-6.7414646,-6.69618,-3.2678115,-3.3429961,-2.4005432,-3.8082733,-3.0931854,-6.362796,-5.79477,-5.2361226,-5.4116273,-3.2791615,-3.1442535,-2.9220583,-3.1153724,-3.231202,-3.4093778,-3.0568836,-3.2009041,-3.0432127,-3.4547207,-3.4500275,-3.5828521,-3.5157073,-4.3502955,-3.8996596,-5.4616356,-4.459111,-5.5305905,-6.419192,-3.0627723,-3.0807643,7.269938,-6.270613,-6.527258,7.4565763,-6.582843,-3.0739255,-3.684755,-3.061569,-3.0741951,-3.5404112,-3.5561059,-3.135363,-3.2435,-3.5217204,-6.751316,-4.474605,-4.9514112,-4.942759,-4.087513,-6.430449,-5.4583464,-4.7823715,-6.0042577,-6.614968,-6.633462,-4.049644,-4.098792,-3.275306,-3.9159358,-4.0890465,-4.046487,-3.9199302,-4.0524178,-3.6604066,-4.350351,-3.3751576,-3.6703498,-3.2588902,-3.3917205,-3.6229112,-3.4950235,-6.500682,-4.172973,-4.422133,-4.5098495,-5.390131,-5.425279,-5.25776,-5.243407,-4.358731,-4.517877,-4.044568,-4.4543486,-4.5664454,-3.5978081,-4.1345353,-6.2600207,-4.4146323,-3.7661235,-4.125278,-4.615359,-5.288279,-6.7552943,-3.2173836,-3.1927845,-3.258948,-6.3709087,-6.1822734,-5.9648857,-3.9326181,-3.7780545,-4.060038,-4.0641947,-4.16162,-3.85248,-3.1894197,-2.6117947,-4.7114706,-4.8697934,-5.457261,-5.054048,-4.45822,-4.474543,-4.495778,-4.493805,-4.4434276,-5.300404,-4.9092746,-5.6759515,-5.3718967,-4.572863,-4.604238,-6.0880504,-6.1768537,-5.2150307,-6.80954,-6.777194,-5.566422,-1.65587,-4.938982,-2.753057,-4.9237,-5.848913,-5.3013444,-5.0634985,-5.5123935,-5.3891954,-5.547035,7.3726306,8.24544,-5.463034,-3.452731,-4.46944,-4.5183134,-4.555098,-5.1300244,-6.0774717,-6.503126,-5.3859477,-4.900852,-3.3027027,-3.4517252,-3.7595332,-1.0631407,-2.5144656,-2.9902081,-6.700746,-6.8691473,-5.510312,-5.4469438,-5.491852,-3.6159794,-4.6151223,-3.8461845,-3.8378417,-3.7719789,-3.697018,-3.259726,-5.7353034,-6.7149205,-6.65076,-5.4434547,-4.990007,-6.1980276,-3.386805,-6.441557,-5.7447767,-5.9448752,-4.6153407,-5.2780986,-5.6287875,-4.806881,-4.4487333,6.951026,-3.6110928,-3.6016722,-5.565393,-4.023593,-4.422139,-3.3772383,-3.4800584,-3.414486,-3.6548512,-6.50667,-6.754276,7.665763,-6.7186856,-4.491219,-4.528471,-4.5006614,-4.514084,-4.473184,-4.5012655,-4.423331,-5.098608,-5.436625,-5.6123905,-4.59642,-4.538625,-4.5932913,-4.4211516,-4.4979725,-3.8624158,-3.6715748,-4.5259814,-3.3151872,-3.8836863,-3.8042505,-4.475015,-4.5233026,-4.587627,-5.2226143,-5.5432587,-5.7070785,-4.5014367,-4.4233723,-4.454893,-4.289138,-6.124149,-5.075509,-4.982089,-4.2176523,-3.4081223,-3.5034468,-3.7813373,-3.4394016,-4.422764,-3.4538853,-3.5766637,-4.447434,-4.4892488,-4.707993,-5.5610785,-5.7323885,-5.3056607,-4.3432417,-3.6921086,-4.0648923,-6.5236983,-3.2169259,-2.8376632,-1.6422038,-1.336098,-3.213663,-3.3165464,-4.507858,-4.695041,-4.6267233,-4.7350683,-5.1631355,-4.948653,-4.899271,-4.6841025,-4.5660825,-4.4623704,-4.303141,-5.5455008,-5.7067084,-5.4960966,-3.7145278,-3.43819,-3.4321063,-3.3498797,-3.2026675,-3.8920565,-5.330868,-4.685929,-6.640881,-6.5433083,-6.460093,-5.4260497,-3.0757673,-4.5273075,-4.8368363,-5.6854157,-5.233553,-4.4811764,-3.7829878,-3.6603663,-3.3610222,-4.755011,-4.3541055,-3.4991028,-4.3630433,-5.261771,-6.4906263,-5.1708803,-4.7579455,-4.8648963,-3.4088938,-4.5255775,-4.6207933,-4.508,-4.484909,2.3422973,-6.6368055,-4.9612966,-4.60527,-4.596936,-5.193661,-6.1369886,-5.6130376,-1.0705394,-1.638723,-5.1593995,-4.0543265,-5.960112,-6.3812523,-6.3686604,-5.0331345,-5.321408,-4.7407126,-4.693636,-4.8890734,-4.970442,-5.1154227,-4.8839536,-4.4947147,-5.270014,-4.9645686,-1.9173841,-4.953373,-5.4310493,-5.4310718,-4.660845,-5.1716948,-4.6990323,-4.506659,-4.788793,-3.475157,-4.179402,-4.952048,-4.9765563,-5.8937745,-6.6438274,-4.482158,-4.5634775,-4.708308,-4.502324,-4.471273,-4.483102,-4.593215,-5.597315,-5.130144,-4.5991535,-4.569826,-4.4458556,-5.0351367,-5.043952,-4.3796077,-3.7411466,-3.2787354,-4.6411366,-1.3880098,-5.457377,-4.157046,-3.3278065,-6.5605025,-2.5711215,-4.365912,-6.1712914,-4.7128634,-4.613338,-6.421343,-4.851708,-4.909188,-4.546258,-5.601683,-4.8488975,-4.539188,-4.381798,-4.4699326,-4.7598834,-4.2776713,-4.2080374,-4.092086,-3.2577765,-4.3452954,-3.9186537,-4.69206,-4.6321707,-4.6120195,-4.534794,-4.764636,-4.752229,-4.613597,-4.5107565,-3.6153738,-4.8232017,-2.536852,-4.742925,-3.9361098,-5.6695433,-5.9173875,-5.768131,-3.8120253,-6.839001,-5.385875,-4.662354,-4.524335,-4.4499025,-4.6251197,-4.362861,-4.3792777,-4.5378227,-4.218031,-5.240372,-6.54228,-5.1443005,-3.4644647,-2.333696,-5.8860803,-4.7810516,-3.964058,-3.0349958,-6.112947,-6.617308,-4.8296704,-4.2295594,-3.1958678,-6.4890976,-6.0014005,-5.9568095,-4.978962,-4.838172,-4.5412793,-4.8738437,-4.6054163,-4.597224,-1.5264279,-4.691104,-5.127996,-5.774489,-3.203872,-3.4037678,-6.5304847,-4.4179516,-4.481479,-4.362124,-4.519017,-4.374448,-5.539692,-5.6169147,-4.930279,-4.349058,-4.250823,-4.266432,-4.3273644,-4.3107953,-4.4709125,-3.607861,-4.173236,-4.688202,-4.4696846,-6.514749,-6.5102906,-4.445677,-4.6012273,-5.6297846,-5.2460313,-4.499072,-6.146307,-6.576371,-5.032965,-4.890991,-3.0685148,-3.1993067,-2.8184464,1.3624011,-3.3720684,-3.3478084,-3.4937313,-2.9018357,-6.810561,-4.6416855,-4.457913,-4.5792193,-4.36479,-4.4961324,-4.553094,1.6129392,-2.5548909,-6.862768,-5.080924,-5.5162354,-5.690896,-4.3471713,-6.8026686,-3.3856776,-6.7103677,-3.388699,-6.606463,-5.692879,-3.0804274,-3.301106,-1.4234889,-1.3751593,-4.5780945,-2.972436,-0.8889841,-3.6055405,-3.0116022,-5.8894663,-6.648854,-6.3157454,-5.276735,-5.4807196,-6.391607,-5.883325,-4.248064,-4.6372185,-3.5075514,-5.45405,-5.41261,-4.092369,-4.1861634,-4.419022,-4.8556924,-4.63206,-3.2675433,-2.993522,-2.632958,-1.3829985,-5.8361416,-5.6376877,-4.6914067,-4.5198407,-7.364346,-7.078581,-6.280472,-6.174228,-6.705462,-5.729811,-5.2410736,-5.4571824,-6.660545,-3.3483427,-3.2243943,-3.6852083,-3.700419,-3.5708456,-3.5501304,-3.4227786,-3.1323645,-3.7078688,-5.584747,-2.3337462,-5.052401,-4.341247,-4.4344125,-4.346411,-4.049274,-4.32079,-4.3126044,-4.5023704,-4.344692,-4.3077707,-4.4719286,-4.3998437,-1.3218813,-1.6193347,-2.1357832,-4.3153324,-4.5069265,-5.129322,-4.9340363,-4.1774564,-3.5144901,-3.367128,-3.3175573,-4.4304476,-3.604038,-3.25349,-3.4146497,-3.9430325,-3.3113778,-4.0495896,-3.3464174,-3.8357923,-3.4792786,-2.9886801,-3.3198993,-3.5078623,-6.641941,-6.311239,-4.471046,-4.5100822,-5.504929,-5.0499516,-4.518059,-4.4608674,-6.8069563,-6.6962132,-6.5125003,-4.7574043,-4.9491935,-4.7231145,-3.589717,-3.6521184,-3.48046,-3.0956013,-3.5541894,-6.784726,-5.02906,-4.174483,-5.557019,-4.9384184,-5.460141,-5.6113563,-5.526737,-3.121964,-3.088207,-3.5970347,-3.3903081,-1.9922243,-3.158687,-3.0916252,-2.8707626,-3.1847198,-2.1761441,-5.3437276,-3.0401468,-5.27318,-5.4148216,-5.7426987,-5.419597,-5.9599886,-4.657323,-4.5003967,-6.382959,-5.5569997,-5.336095,-3.2772992,-5.3898597,-3.2693837,-2.899373,-4.4617467,-4.880909,-5.5277224,-4.459855,-4.3724256,-6.386283,-6.671127,-6.2630396,7.69753,-5.5381546,-6.3127017,-4.514441,-4.5513396,-4.3991637,-4.6902905,-2.89023,-4.919792,-4.883124,-4.838022,-5.20276,-4.2818794,-6.6306357,-4.5871954,-4.514649,-4.844771,-2.5269005,-6.5027704,-5.431436,-6.2293506,-4.457344,-5.4656653,-5.5910015,-5.2799916,-4.167029,-4.3140907,-6.6777167,-5.4678216,-6.399165,-5.9406443,-5.8914943,-5.6630697,-6.3976526,-5.6281395,-5.7384205,-4.6769457,-7.3441157,-7.122307,-4.793819,-5.759239,-5.744747,-5.785952,-5.6791883,-5.860551,-6.2321153,-6.445662,-5.920115,-5.931363,-5.634406,-6.668193,-4.4925175,-4.4639726,-4.5541263,-4.5126452,-4.502025,-5.3644695,-1.8922415,-5.775669,-5.2037683,-4.4657664,-4.448903,-4.458787,-3.5379844,-4.5827346,-4.4990563,-4.371615,-3.572705,-4.036896,-3.9856393,-4.258064,-4.516017,-4.416158,-4.538131,-5.157387,-1.8066012,-4.7351003,-4.5847206,-4.9075346,-4.2765074,-4.730634,-4.702243,-4.829638,-5.264492,-4.2515116,-4.502457,-4.5023727,-4.472646,-4.5249333,-4.9186134,-5.3269176,-4.5206776,-3.81497,-3.6574786,-4.465076,-5.481423,-6.020204,-5.9608226,-4.7439365,-4.397216,-5.9163904,-5.5112123,-6.6016054,-4.9633646,-4.793614,-1.8113661,-1.8334254,-2.683931,-5.0981245,-4.4672394,-5.488683,-4.428746,-3.4908535,-4.018063,-4.7479725,-4.39975,-4.82949,-4.8227596,-5.337637,-4.544191,-4.4074535,-4.0441055,-5.857344,-6.491985,-5.835106,-6.2333765,-6.529908,-6.663825,-6.8364687,-4.445928,-4.494178,-5.2950544,-4.723913,-4.703782,-5.1938553,-4.6218357,-4.8296423,-4.723376,-4.917862,-4.459388,-5.5842013,-5.69124,-4.417729,-3.5739708,-4.112063,-6.645676,-4.6417117,-4.9118624,-4.790934,-4.6429796,-4.8585253,-4.588999,-4.7841845,-6.6624775,-5.9785423,-5.931758,-5.3765574,-4.678931,-4.6387076,-4.571827,-4.709233,-4.6132536,-1.7629641,-0.6585784,-4.595552,-6.1188183,-3.5900795,-3.7208755,-6.611307,-4.542157,-3.873616,-5.7903643,-4.95465,-3.8874598,-5.460502,-6.501189,-6.0833874,-5.9907427,-3.6857996,-3.4522219,-3.2658846,-3.6914794,-3.6120324,-3.8376367,-3.5268493,-3.9074006,-3.6993122,-3.9259121,-6.68703,-5.1947885,-5.203373,-4.4287477,-3.5448623,-3.3681555,-3.393512,-3.337506,-4.1191387,-3.802903,-3.62711,-6.498775,-5.0982556,-5.0191274,-0.5031017,-6.7287736,-4.818115,-4.357514,-6.305738,-6.72413,-3.3562214,-3.1225188,-2.6072373,-1.33583,-1.0157338,-4.239104,-3.7744932,-3.1480896,-3.7485116,-3.2319171,-0.86753994,-3.2547214,-6.4078884,-4.5034013,-4.4793234,-4.482758,-4.474972,-4.498226,-4.5108223,-4.4678106,-5.37821,-5.57959,-5.160069,-3.6621354,-4.4994254,-6.0895514,-3.2340386,-2.936245,-3.2722816,-3.473889,-1.1559399,-0.5508039,8.05142,-3.5046878,-3.1876307,-2.9503057,-6.4400206,-5.424952,-5.6424985,-5.666565,-5.575995,-5.601341,-3.361974,-5.8152065,-5.447659,-6.7016654,-6.7589574,-6.3932824,-5.5418315,-5.300973,-3.4836059,-3.3823533,-3.3405225,-3.3594625,-3.5962539,-3.5741138,-3.4968867,-3.572318,-3.617986,-6.6253285,-6.2915273,-5.82414,-5.6986,-5.710191,-5.703053,-5.7111044,-5.3386745,-4.8634377,-0.7059131,-2.8473058,-5.5271373,-4.803263,-3.1353216,-2.6811724,-4.8999224,-5.087073,-6.6816616,-5.8171234,-4.5282683,-4.5119734,-5.030461,-5.5272865,-4.2241883,-4.843895,-4.8950634,-5.1045713,-5.002256,-4.800211,-5.1843066,-5.752508,-4.9325624,-4.8025393,-4.841741,-4.8531866,-4.8146524,-5.468551,-5.189691,-6.9265356,-6.416153,-6.6598268,-4.502994,-4.5023212,-4.504052,-5.5084934,-5.5764976,-5.0063443,-4.503551,-4.7478943,-4.4395924,-3.5052755,-4.6691985,-6.531531,-6.686029,-4.261049,-4.4816794,-4.4957113,-4.315137,-4.9428315,-4.425228,-4.582937,-4.692404,-4.65999,-4.836128,-4.7562413,-2.5914268,-4.488339,-4.6393385,-4.7828894,-4.439386,-4.7398577,-4.7264624,-5.07079,-5.0566883,-4.859223,-4.688764,-4.897076,-4.888432,-4.7964544,-1.929972,-5.276533,-4.662426,-4.6800733,-4.671748,-4.7360616,-5.0877247,-4.7492013,-4.670639,-4.75292,-4.5063457,-4.722744,-4.518069,-4.3451085,-4.8668942,-4.4805093,-4.706686,-4.306514,-4.4788985,-4.6955023,-4.085486,-4.3942733,-3.5298996,-4.093661,-4.101066,-4.112696,-4.9133945,-4.4743576,-4.145028,-4.7300763,-4.687147,-4.708002,-6.4617605,-5.203179,-4.7127256,-4.6096473,-4.3168173,-5.5189104,-5.4359,-4.978299,-4.031437,-6.386657,-5.925352,-5.6522427,-7.1262593,-6.485932,-5.4802322,-5.563024,-5.3454485,-5.3593855,-4.931312,-4.207996,-4.4571614,-4.454725,-4.580281,-4.4559917,-4.3627186,-4.5066605,-5.253268,-4.9004693,-5.6633053,-5.150604,-4.0069056,-4.249189,-5.1130524,-4.4672956,-4.1768413,-4.461407,-3.6451745,-4.28469,-4.388047,-4.3986807,-4.4703927,-4.4586315,-4.4637456,-4.4598866,-4.503649,-5.414381,-5.446144,-5.1656895,-4.483913,-3.7992277,-3.6365545,-6.1571207,-5.602255,-5.5333886,-5.587777,-5.722339,-5.7598624,-5.5327,-0.3420191,-4.9313884,-4.7881246,-5.617347,7.6076603,-5.480406,-1.744781,7.0859003,7.062437,-5.1691647,-5.014271,-3.880514,-5.699837,-6.007864,-4.9912233,-4.465539,-4.792148,-4.764125,-4.979534,-5.647425,-5.7786813,-5.8253202,-5.6605325,-3.5920253,-3.460017,-3.7071104,-5.5601726,-4.994219,-4.889888,-4.301739,-1.9605387,-3.8345723,-4.9623675,-5.6879144,-3.4786353,-3.5781872,-3.8051627,-6.594057,-6.0732913,-3.7979796,-3.4897442,-3.6925051,-3.6834455,-3.45049,-5.961882,-3.565883,-3.5639791,-4.389777,-3.3374586,-3.351839,-3.0463262,-3.364453,-4.441441,-4.5460234,-4.657422,-4.2023625,-3.5973964,1.4707193,8.404434,-0.2006006,-3.7650385,-3.6082354,-4.4026155,-3.646213,-3.7131865,-3.5266826,-1.5994563,-6.094667,-5.3120403,-5.0011663,-5.0205436,-4.1888533,-4.7673545,-4.489231,-4.6886787,-4.416364,-4.6110725,-4.4394155,-4.478924,-4.50775,-4.836239,-5.3021946,-1.9258715,-4.935005,-5.67752,-5.0026026,-4.5650206,-4.500183,-4.498926,-3.5317895,-4.496668,-6.087914,-6.100371,-5.834854,-4.466516,-4.5114317,-4.4594193,-4.4942226,-4.499801,-4.479414,-4.71392,-1.8653184,-5.7115145,-5.21823,-4.6621313,-4.487485,-4.455297,-4.510445,-3.842796,-4.076871,-4.887973,-4.7113147,-4.4952664,-6.7461176,-5.4985805,-4.177704,-3.6632426,-3.2647567,-3.480769,-3.2881553,-3.0613313,-3.2745771,-3.2886546,-3.4031909,-3.3756578,-3.268214,-4.4639244,-4.7540493,-5.389532,-4.9816337,-5.0401673,-3.5901015,-4.1172276,-4.548035,-4.974175,-4.9096155,-4.9704876,-5.609124,-4.7227674,-4.4863443,-3.177943,-3.1443799,-3.1180747,-3.2737176,-3.3705745,-5.99139,-6.275616,-6.7519293,-6.585008,-3.3882208,-6.738915,-6.2790704,-4.335862,-4.499224,-4.4389644,-5.3746405,-5.671501,-4.8704567,-4.450189,-4.576461,-4.4386134,-3.5452027,-4.0274315,-4.7224836,-5.530351,-5.671497,-6.465742,-6.595362,-6.7013793,-6.2701645,-6.7393503,-6.7349195,-3.259455,-6.570874,-6.1205153,-4.02203,-5.625893,-4.387362,-5.50417,-5.5369825,-5.4395556,-4.4260325,-3.7953234,-3.66541,-3.5896642,-3.4826636,-3.3015695,-3.1810622,-4.994978,-4.894213,-4.6599174,-4.3946776,-4.3167577,-3.7773695,-4.1574836,-3.4781485,-4.1888056,-4.011998,-4.046504,-3.9059217,-2.229865,-1.3812572,-2.2152848,-3.5931284,-4.440849,-4.447809,-3.2726254,-6.0726266,-6.0857835,-2.7371845,-5.7018013,-5.0949674,-3.3450239,-5.9360948,-6.409467,-5.2508407,-5.2616277,-5.8655076,-6.1505823,-5.6171956,4.1116734,-3.303511,-4.553739,-6.7865596],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### What kinds of TPU are available?\\n\\nNew users are often very confused by the range of TPUs, and th...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThe second way to access a TPU is via a **TPU VM.** When using a TPU VM, you connect directl...\"],[\"### I keep hearing about this XLA thing. What’s XLA, and how does it relate to TPUs?\\n\\nXLA is an opti...\"],[\"\\u003cTip\\u003e\\n\\n**🤗Specific HuggingFace Tip🤗:** We’ve put a lot of effort into rewriting our TensorFlow model...\"],[\"```\\n\\nThis might seem very restrictive at first, but most neural net code doesn’t need to do this. Yo...\"],[\"```\\n\\nThis code is totally fine in NumPy or PyTorch, but it breaks in XLA! Why? Because the shape of ...\"],[\"```\\n\\nHere, we avoid data-dependent shapes by computing the loss for every position, but zeroing out ...\"],[\"\\u003cTip\\u003e\\n\\n**🤗Specific HuggingFace Tip🤗:** Our tokenizers and data collators have methods that can help ...\"],[\"### Summary\\n\\nThere was a lot in here, so let’s summarize with a quick checklist you can follow when ...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\n👀 The resulting model can be found here: https:\\u002f\\u002fhuggingface.co\\u002fnielsr\\u002flayoutlmv3-finetuned-fun...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## VisionTextDualEncoderConfig\\n\\n[[autodoc]] VisionTextDualEncoderConfig\\n\\n## VisionTextDualEncoderPro...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"With this approach, the model can detect objects based on textual descriptions without prior trainin...\"],[\"```\\n\\n## Zero-shot object detection pipeline\\n\\nThe simplest way to try out inference with OWL-ViT is t...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```py\\n\\u003e\\u003e\\u003e predictions = detector(\\n...     image,\\n...     candidate_labels=[\\\"human face\\\", \\\"rocket\\\", \\\"...\"],[\"```\\n\\nLet's visualize the predictions:\\n\\n```py\\n\\u003e\\u003e\\u003e from PIL import ImageDraw\\n\\n\\u003e\\u003e\\u003e draw = ImageDraw.Dra...\"],[\"```\\n\\nLet's take a different image to switch things up.\\n\\n```py\\n\\u003e\\u003e\\u003e import requests\\n\\n\\u003e\\u003e\\u003e url = \\\"https:...\"],[\"```\\n\\nPass the inputs through the model, post-process, and visualize the results. Since the image pro...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\nPreviously for post-processing you passed the single image's size as a tensor, but you can also...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fcircleci.com\\u002fgh\\u002fhuggingface\\u002ftransformers\\\"\\u003e\\n        \\u003cimg alt=...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fmain\\u002fR...\"],[\"\\u003ch3 align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fhf.co\\u002fcourse\\\"\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhug...\"],[\"Модели transformers также могут выполнять несколько задач, такие как ответы на табличные вопросы, ра...\"],[\"🤗 Transformers опирается на три самые популярные библиотеки глубокого обучения - [Jax](https:\\u002f\\u002fjax.r...\"],[\"В области NLP ( Обработка текстов на естественном языке ):\\n- [Маскированное заполнение слов с помощь...\"],[\"- [Обобщение с помощью BART](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fbart-large-cnn?text=The+tower+is+324+me...\"],[\"- [Ответы на вопросы с помощью...\"],[\"DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+u...\"],[\"s+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portugu...\"],[\"8Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon...\"],[\"C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud...\"],[\"egenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest...\"],[\"f+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000...\"],[\"00%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%28...\"],[\"tres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belongi...\"],[\"+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+t...\"],[\"%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amount...\"],[\"r+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+depa...\"],[\"+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+ha...\"],[\"+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+...\"],[\"diverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+...\"],[\"l+trees+divided+into+16%2C000+species)...\"],[\"- [Перевод с помощью T5](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berli...\"],[\"В области компьютерного зрения:\\n- [Классификация изображений с помощью ViT](https:\\u002f\\u002fhuggingface.co\\u002fg...\"],[\"В области звука:\\n- [Автоматическое распознавание речи с помощью Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002ffac...\"],[\"В мультимодальных задачах:\\n- [Ответы на вопросы по таблице с помощью TAPAS](https:\\u002f\\u002fhuggingface.co\\u002fg...\"],[\"## 100 проектов, использующих Transformers\\n\\nTransformers - это не просто набор инструментов для испо...\"],[\"Если вы являетесь владельцем или пользователем проекта, который, по вашему мнению, должен быть включ...\"],[\"## Быстрый гайд\\n\\nДля использования модели на заданном входе (текст, изображение, звук, ...) мы предо...\"],[\"```\\n\\nВторая строка кода загружает и кэширует предварительно обученную модель, используемую конвейеро...\"],[\"# Выделение конвейера для обнаружения объектов\\n\\u003e\\u003e\\u003e object_detector = pipeline('object-detection')\\n\\u003e\\u003e...\"],[\"```\\n\\nЗдесь мы получаем список объектов, обнаруженных на изображении, с рамкой вокруг объекта и оценк...\"],[\"\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"bert-base-uncased\\\")\\n\\u003e\\u003e\\u003e model = AutoModel.from_pretra...\"],[\"```\\n\\nА вот эквивалентный код для TensorFlow:\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, T...\"],[\"```\\n\\nТокенизатор отвечает за всю предварительную обработку, которую ожидает предварительно обученная...\"],[\"Сама модель представляет собой обычный [Pytorch `nn.Module`](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fnn.html...\"],[\"## Почему необходимо использовать transformers?\\n\\n1. Простые в использовании современные модели:\\n    ...\"],[\"1. Более низкие вычислительные затраты, меньший \\\"углеродный след\\\":\\n    - Исследователи могут обменив...\"],[\"1. Выбор подходящего фреймворка для каждого этапа жизни модели:\\n    - Обучение самых современных мод...\"],[\"1. Легко настроить модель или пример под свои нужды:\\n    - Мы предоставляем примеры для каждой архит...\"],[\"- Данная библиотека не является модульным набором строительных блоков для нейронных сетей. Код в фай...\"],[\"- Несмотря на то, что мы стремимся представить как можно больше примеров использования, скрипты в на...\"],[\"## Установка\\n\\n### С помощью pip\\n\\nДанный репозиторий протестирован на Python 3.8+, Flax 0.4.1+, PyTor...\"],[\"Затем необходимо установить хотя бы один бекенд из Flax, PyTorch или TensorFlow.\\nПожалуйста, обратит...\"],[\"```\\n\\nЕсли вы хотите поиграть с примерами или вам нужен самый современный код и вы не можете ждать но...\"],[\"```\\n\\nО том, как установить Flax, PyTorch или TensorFlow с помощью conda, читайте на страницах, посвя...\"],[\"## Модельные архитектуры\\n\\n**[Все контрольные точки моделей](https:\\u002f\\u002fhuggingface.co\\u002fmodels)**, предос...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (from Google Research and...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BARTpho](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbartpho)** (from VinAI Research) r...\"],[\"1. **[BigBird-Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbigbird_pegasus)** (from G...\"],[\"1. **[BiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbit)** (from Google AI) released with ...\"],[\"1. **[BLIP-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblip-2)** (from Salesforce) release...\"],[\"1. **[ByT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbyt5)** (from Google Research) releas...\"],[\"1. **[CLAP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclap)** (from LAION-AI) released with...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (from MetaAI) rele...\"],[\"1. **[ConvNeXT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconvnext)** (from Facebook AI) re...\"],[\"1. **[CTRL](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fctrl)** (from Salesforce) released wi...\"],[\"1. **[DeBERTa-v2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta-v2)** (from Microsoft) ...\"],[\"1. **[DePlot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeplot)** (from Google AI) released...\"],[\"1. **[DiNAT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdinat)** (from SHI Labs) released wi...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (from HuggingFace...\"],[\"1. **[DPR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdpr)** (from Facebook) released with t...\"],[\"1. **[ELECTRA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002felectra)** (from Google Research\\u002fS...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (from Baidu) released wi...\"],[\"1. **[Falcon](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffalcon)** (from Technology Innovati...\"],[\"1. **[FLAN-UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-ul2)** (from Google AI) rele...\"],[\"1. **[FNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffnet)** (from Google Research) releas...\"],[\"1. **[GIT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgit)** (from Microsoft Research) relea...\"],[\"1. **[GPT NeoX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_neox)** (from EleutherAI) rel...\"],[\"1. **[GPT-Sw3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt-sw3)** (from AI-Sweden) releas...\"],[\"1. **[GPTSAN-japanese](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgptsan-japanese)** release...\"],[\"1. **[Hubert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fhubert)** (from Facebook) released ...\"],[\"1. **[Informer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002finformer)** (from Beihang Univers...\"],[\"1. **[LayoutLMv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv2)** (from Microsoft R...\"],[\"1. **[LeViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flevit)** (from Meta AI) released wit...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (from The FAIR team of Me...\"],[\"1. **[Longformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flongformer)** (from AllenAI) re...\"],[\"1. **[M-CTC-T](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmctct)** (from Facebook) released ...\"],[\"1. **[MarkupLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmarkuplm)** (from Microsoft Resea...\"],[\"1. **[mBART](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (from Facebook) released wi...\"],[\"1. **[Megatron-GPT2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmegatron_gpt2)** (from NVIDI...\"],[\"1. **[MobileBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilebert)** (from CMU\\u002fGoogle ...\"],[\"1. **[MobileViTV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilevitv2)** (from Apple) re...\"],[\"1. **[MusicGen](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmusicgen)** (from Meta) released ...\"],[\"1. **[NLLB-MOE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnllb-moe)** (from Meta) released ...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (from Google AI) release...\"],[\"1. **[Persimmon](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fmodel_doc\\u002fpersimmon)** (from ADEPT) r...\"],[\"1. **[Pix2Struct](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpix2struct)** (from Google) rel...\"],[\"1. **[ProphetNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fprophetnet)** (from Microsoft R...\"],[\"1. **[REALM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frealm.html)** (from Google Research)...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (from Facebook), releas...\"],[\"1. **[RWKV](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frwkv)** (from Bo Peng), released on [...\"],[\"1. **[SEW-D](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsew_d)** (from ASAPP) released with ...\"],[\"1. **[Splinter](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsplinter)** (from Tel Aviv Univer...\"],[\"1. **[Swin Transformer V2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswinv2)** (from Micros...\"],[\"1. **[T5v1.1](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5v1.1)** (from Google AI) released...\"],[\"1. **[Time Series Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftime_series_transf...\"],[\"1. **[TVLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftvlt)** (from UNC Chapel Hill) releas...\"],[\"1. **[UniSpeechSat](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funispeech-sat)** (from Micros...\"],[\"1. **[ViLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvilt)** (from NAVER AI Lab\\u002fKakao Ente...\"],[\"1. **[ViT Hybrid](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_hybrid)** (from Google AI) ...\"],[\"1. **[ViTMSN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_msn)** (from Meta AI) released ...\"],[\"1. **[Wav2Vec2-Conformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2-conformer)** (...\"],[\"1. **[X-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxclip)** (from Microsoft Research) ...\"],[\"1. **[XLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm)** (from Facebook) released togeth...\"],[\"1. **[XLM-V](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-v)** (from Meta AI) released wit...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (from Faceb...\"],[\"Чтобы проверить, есть ли у каждой модели реализация на Flax, PyTorch или TensorFlow, или связанный с...\"],[\"| Секция | Описание |\\n|-|-|\\n| [Документация](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002f) | Полная док...\"],[\"| [Совместное использование и загрузка моделей](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_shari...\"],[\"## Цитирование\\n\\nТеперь у нас есть [статья](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f), ко...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nTo share a model with the community, you need an account on [huggingface.co](https:\\u002f\\u002fhuggingf...\"],[\"```\\n\\nFiles are also easily edited in a repository, and you can view the commit history as well as th...\"],[\"```\\n\\n## Convert a model for all frameworks\\n\\nTo ensure your model can be used by someone working with...\"],[\"```\\n\\u003c\\u002fjax\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Push a model during training\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\u003cYoutube id...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nShare a model to the Hub with [`PushToHubCallback`]. In the [`PushToHubCallback`] fun...\"],[\"```\\n\\nThe `push_to_hub` function can also be used to add other files to a model repository. For examp...\"],[\"```\\n\\nNow when you navigate to your Hugging Face profile, you should see your newly created model rep...\"],[\"* Manually creating and uploading a `README.md` file.\\n* Clicking on the **Edit model card** button i...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We present a convolution-free approach to video clas...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large-scale NLP models have been shown to significan...\"],[\"A list of official Hugging Face and community (indicated by 🌎) resources to help you get started wit...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We re-evaluate the standard practice of sharing weig...\"],[\"## RemBertConfig\\n\\n[[autodoc]] RemBertConfig\\n\\n## RemBertTokenizer\\n\\n[[autodoc]] RemBertTokenizer\\n    -...\"],[\"## TFRemBertForMaskedLM\\n\\n[[autodoc]] TFRemBertForMaskedLM\\n    - call\\n\\n## TFRemBertForCausalLM\\n\\n[[aut...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model augments the Transformer as a deep decomposition architecture, which can progressively de...\"],[\"- Check out the Autoformer blog-post in HuggingFace blog: [Yes, Transformers are Effective for Time ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Vision-and-language reasoning requires an understand...\"],[\"## Usage tips\\n\\n- Bounding boxes are not necessary to be used in the visual feature embeddings, any k...\"],[\"[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n...\"],[\"Author: [@vasudevgupta7](https:\\u002f\\u002fgithub.com\\u002fthevasudevgupta\\u002f)\\n\\n## Intro\\n\\nIn this project, we fine-tu...\"],[\"```\\n\\n## Evaluation\\n\\nOur evaluation script is different from the original script and we are evaluatin...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [Preprocessing data](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002ftransformers_doc\\u002fen\\u002fprepro...\"],[\"| [Summary of the tokenizers](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002ftransformers_doc\\u002fen...\"],[\"### PyTorch Examples\\n\\n#### Natural Language Processing[[pytorch-nlp]]...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a model on text classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"| [How to fine-tune a model on token classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fm...\"],[\"| [How to fine-tune a model on multiple choice](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fe...\"],[\"| [How to fine-tune a model on summarization](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fexa...\"],[\"| [How to generate text](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fblog\\u002fblob\\u002fmain\\u002fnotebooks\\u002f02_how_to_generate....\"],[\"| [Reformer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fblog\\u002fblob\\u002fmain\\u002fnotebooks\\u002f03_reformer.ipynb)| How Reforme...\"],[\"#### Computer Vision[[pytorch-cv]]...\"],[\"| Notebook                                                                                          ...\"],[\"|:--------------------------------------------------------------------------------------------------...\"],[\"| [How to fine-tune a model on image classification (Torchvision)](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fno...\"],[\"| [How to fine-tune a model on image classification (Kornia)](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteboo...\"],[\"| [How to fine-tune an image captioning model](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fex...\"],[\"| [How to fine-tune a SegFormer model on semantic segmentation](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteb...\"],[\"#### Audio[[pytorch-audio]]...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a speech recognition model in any language](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteb...\"],[\"#### Biological Sequences[[pytorch-bio]]...\"],[\"| Notebook     | Description                                                                        ...\"],[\"| [How to generate protein folds](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fexamples\\u002fprotei...\"],[\"| [Fine-tune a Nucleotide Transformer model with LoRA](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob...\"],[\"#### Other modalities[[pytorch-other]]\\n\\n| Notebook     | Description                                ...\"],[\"#### Utility notebooks[[pytorch-utility]]\\n\\n| Notebook     |      Description      |   |   |\\n|:------...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a model on text classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"| [How to fine-tune a model on token classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fm...\"],[\"| [How to fine-tune a model on multiple choice](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fe...\"],[\"| [How to fine-tune a model on summarization](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fexa...\"],[\"#### Computer Vision[[tensorflow-cv]]...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to fine-tune a model on image classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fm...\"],[\"#### Biological Sequences[[tensorflow-bio]]\\n\\n| Notebook     |      Description      |   |   |\\n|:----...\"],[\"#### Utility notebooks[[tensorflow-utility]]\\n\\n| Notebook     |      Description      |   |          ...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to quantize a model with Intel Neural Compressor for text classification](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"| [How to fine-tune a model on text classification with ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"| [How to fine-tune a model on summarization with ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteb...\"],[\"## Community notebooks:\\n\\nMore notebooks developed by the community are available [here](https:\\u002f\\u002fhf.c...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nThe [`Trainer`] class is optimized for 🤗 Transformers models and can have surp...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers-based models, such as BERT, have been o...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"## BigBirdForMultipleChoice\\n\\n[[autodoc]] BigBirdForMultipleChoice\\n    - forward\\n\\n## BigBirdForTokenC...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In terms of model details, the work outlines the architecture and training methodology of Persimmon-...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\nTips:\\n\\n- To convert the model, you need to clone the original repository using `git clone h...\"],[\"```\\n\\nFor the chat model:\\n```bash\\nwget https:\\u002f\\u002faxtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-o...\"],[\"*TEMPLATE**\\n=====================================\\n\\n*search & replace the following keywords, e.g.:*\\n...\"],[\"To start, let's try to get a general overview of the Transformers\\nlibrary.\\n\\nGeneral overview of 🤗 Tr...\"],[\"Let's take a look:\\n\\n![image](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolv...\"],[\"```\\n\\nSimilar to the model, the configuration inherits basic serialization and\\ndeserialization functi...\"],[\"From experience, we can tell you that the most important things to keep\\nin mind when adding a model ...\"],[\"7.  [ ] Successfully ran forward pass in Transformers that gives\\n    identical output to original ch...\"],[\"-   What type of model is *[camelcase name of model]*? BERT-like encoder-only\\n    model? GPT2-like d...\"],[\"If any of the mentioned aspects above are **not** clear to you, now is a great time to talk to [name...\"],[\"```\\n\\n3.  Set up a development environment, for instance by running the\\n    following command:\\n\\n    `...\"],[\"```\\n\\nNow you have set up a development environment to port *[camelcase name of model]*\\nto 🤗 Transfor...\"],[\"-   Where to find the pretrained weights?\\n-   How to load the pretrained weights into the correspond...\"],[\"Jupyter notebooks have the advantage that they allow for cell-by-cell\\nexecution which can be helpful...\"],[\"```\\n\\nNext, regarding the debugging strategy, there are generally a few from\\nwhich to choose from:\\n\\n-...\"],[\"However, if the original code-base is very complex or only allows\\nintermediate components to be run ...\"],[\"The outputs of the following layers often consist of multi-dimensional\\nfloat arrays and can look lik...\"],[\"```\\n\\nWe expect that every model added to 🤗 Transformers passes a couple of\\nintegration tests, meanin...\"],[\"-   Find the best way of debugging intermediate results. Is the original\\n    repository written in P...\"],[\"`autoregressive_sample`, `generate`.\\n-   Try to separate the tokenization from the model's\\n    forwa...\"],[\"#### More details on how to create a debugging environment for [camelcase name of model] \\n\\n[TODO FIL...\"],[\"```\\n    git checkout -b add_[lowercase name of model]\\n```\\n\\n2.  Commit the automatically generated co...\"],[\"```\\n\\n5.  Once you are satisfied, go to the webpage of your fork on GitHub.\\n    Click on \\\"Pull reques...\"],[\"**5. Adapt the generated models code for [camelcase name of model]**\\n\\nAt first, we will focus only o...\"],[\"```python\\nfrom transformers import [camelcase name of model]Model, [camelcase name of model]Config\\nm...\"],[\"```\\n\\nThe above command will create a model according to the default\\nparameters as defined in `[camel...\"],[\"In the following, we'll quickly explain how PyTorch models store layer\\nweights and define layer name...\"],[\"```\\n\\nNow we can create an instance of this model definition which will fill\\nall weights: `dense`, `i...\"],[\"```\\n\\nto see that the weights were randomly initialized...\"],[\"```bash\\ntensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\\n         -0...\"],[\"[-0.1173,  0.1561,  0.2945,  0.0595, -0.1996,  0.2988, -0.0802,  0.0407,\\n          0.1829, -0.1568],...\"],[\"```\\n\\nIn the conversion script, you should fill those randomly initialized\\nweights with the exact wei...\"],[\"```\\n\\nIf either the shape or the name doesn't match, you probably assigned\\nthe wrong checkpoint weigh...\"],[\"```\\n\\n[TODO FILL: Here the mentor should add very specific information on what exactly has to be done...\"],[\"```\\n\\nIt is very likely that the 🤗 Transformers implementation and the\\noriginal model implementation ...\"],[\"The best way to fix the problem is usually to look at the forward pass\\nof the original implementatio...\"],[\"```\\n\\n[TODO FILL: Here the mentor should add very specific information on what tests are likely to fa...\"],[\"```\\n\\n**Note:** In case you are using Windows, you should replace `RUN_SLOW=1` with `SET RUN_SLOW=1`\\n...\"],[\"```\\n\\nYou might have to take a deeper look again into the original repository\\nto find the correct tok...\"],[\"```\\n\\nWhen both `input_ids` yield the same values, as a final step a tokenizer\\ntest file should also ...\"],[\"**11. Add Docstring**\\n\\nNow, all the necessary functionality for *[camelcase name of model]* is added...\"],[\"```\\n\\nand verify that your coding style passes the quality check:\\n\\n```bash\\nmake quality...\"],[\"```\\n\\nThere are a couple of other very strict design tests in 🤗 Transformers\\nthat might still be fail...\"],[\"### Share your work!!\\n\\nNow, it's time to get some credit from the community for your work!\\nHaving co...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nFor Chinese models, we need to generate a reference files (which requires the ltp library), bec...\"],[\"```\\n\\nThen you can run the script like this: \\n\\n\\n```bash\\nexport TRAIN_FILE=\\u002fpath\\u002fto\\u002ftrain\\u002ffile\\nexport ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Implementation Notes\\n\\n- Each model is about 298 MB on disk, there are more than 1,000 models.\\n- T...\"],[\"## Examples\\n\\n- Since Marian models are smaller than many other translation models available in the l...\"],[\"\\u003e\\u003e\\u003e model_name = \\\"Helsinki-NLP\\u002fopus-mt-en-roa\\\"\\n\\u003e\\u003e\\u003e tokenizer = MarianTokenizer.from_pretrained(model...\"],[\"```\\n\\nHere is the code to see all available pretrained models on the hub:\\n\\n```python\\nfrom huggingface...\"],[\"```\\n\\n## Old Style Multi-Lingual Models\\n\\nThese are the old style multi-lingual models ported from the...\"],[\"```python no-style\\n['Helsinki-NLP\\u002fopus-mt-NORTH_EU-NORTH_EU',\\n 'Helsinki-NLP\\u002fopus-mt-ROMANCE-en',\\n '...\"],[\"'ROMANCE': ['fr', 'fr_BE', 'fr_CA', 'fr_FR', 'wa', 'frp', 'oc', 'ca', 'rm', 'lld', 'fur', 'lij', 'lm...\"],[\"```\\n\\nExample of translating english to many romance languages, using old-style 2 character language ...\"],[\"```\\n\\n## Resources\\n\\n- [Translation task guide](..\\u002ftasks\\u002ftranslation)\\n- [Summarization task guide](..\\u002f...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n- Wav2Vec2-Conformer follows the same architecture as Wav2Vec2, but replaces the *Att...\"],[\"## Wav2Vec2ConformerForSequenceClassification\\n\\n[[autodoc]] Wav2Vec2ConformerForSequenceClassificatio...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"TAPEX has been fine-tuned on several datasets: \\n- [SQA](https:\\u002f\\u002fwww.microsoft.com\\u002fen-us\\u002fdownload\\u002fdet...\"],[\"## Usage tips\\n\\n- TAPEX is a generative (seq2seq) model. One can directly plug in the weights of TAPE...\"],[\"\\u003e\\u003e\\u003e # prepare table + question\\n\\u003e\\u003e\\u003e data = {\\\"Actors\\\": [\\\"Brad Pitt\\\", \\\"Leonardo Di Caprio\\\", \\\"George Clo...\"],[\"```\\n\\nNote that [`TapexTokenizer`] also supports batched inference. Hence, one can provide a batch of...\"],[\"```\\n\\nIn case one wants to do table verification (i.e. the task of determining whether a given senten...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"```\\n\\nYou will use [PyTorchVideo](https:\\u002f\\u002fpytorchvideo.org\\u002f) (dubbed `pytorchvideo`) to process and p...\"],[\"```\\n\\nAt a high level, the dataset is organized like so:\\n\\n```bash\\nUCF101_subset\\u002f\\n    train\\u002f\\n        B...\"],[\"```\\n\\nYou will notice that there are video clips belonging to the same group \\u002f scene where group is d...\"],[\"```\\n\\nThere are 10 unique classes. For each class, there are 30 videos in the training set.\\n\\n## Load ...\"],[\"```\\n\\nWhile the model is loading, you might notice the following warning:\\n\\n```bash\\nSome weights of th...\"],[\"```\\n\\nThe warning is telling us we are throwing away some weights (e.g. the weights and bias of the `...\"],[\"```\\n\\nFor the training dataset transformations, use a combination of uniform temporal subsampling, pi...\"],[\"```\\n\\nNow, define the dataset-specific transformations and the datasets respectively. Starting with t...\"],[\"```\\n\\nThe same sequence of workflow can be applied to the validation and evaluation sets: \\n\\n```py \\n\\u003e\\u003e...\"],[\"```\\n\\n**Note**: The above dataset pipelines are taken from the [official PyTorchVideo example](https:...\"],[\"```\\n\\n## Visualize the preprocessed video for better debugging \\n\\n```py \\n\\u003e\\u003e\\u003e import imageio\\n\\u003e\\u003e\\u003e import...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"\\u003e\\u003e\\u003e args = TrainingArguments(\\n...     new_model_name,\\n...     remove_unused_columns=False,\\n...     e...\"],[\"```\\n\\nThe dataset returned by `pytorchvideo.data.Ucf101()` doesn't implement the `__len__` method. As...\"],[\"```\\n\\nThen you just pass all of this along with the datasets to `Trainer`:\\n\\n```py \\n\\u003e\\u003e\\u003e trainer = Trai...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nYou can also manually replicate the results of the `pipeline` if you'd like.\\n\\n\\n```py\\n\\u003e\\u003e\\u003e def ru...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Natural language understanding comprises a wide rang...\"],[\"Note:\\n\\nIf you want to reproduce the original tokenization process of the *OpenAI GPT* paper, you wil...\"],[\"```\\n\\nIf you don't install `ftfy` and `SpaCy`, the [`OpenAIGPTTokenizer`] will default to tokenize\\nus...\"],[\"- A blog on how to [Finetune a non-English GPT-2 Model with Hugging Face](https:\\u002f\\u002fwww.philschmid.de\\u002f...\"],[\"- [`OpenAIGPTLMHeadModel`] is supported by this [causal language modeling example script](https:\\u002f\\u002fgi...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- A course material on [Byte-Pair Encoding tokenizat...\"],[\"## TFOpenAIGPTDoubleHeadsModel\\n\\n[[autodoc]] TFOpenAIGPTDoubleHeadsModel\\n    - call\\n\\n## TFOpenAIGPTFo...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-...\"],[\"\\u003c\\u002fa\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fmain\\u002fCODE_OF_CONDUCT.md\\\"\\u003e\\n       ...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"🤗 Transformers 提供了数以千计的预训练模型，支持 100 多种语言的文本分类、信息抽取、问答、摘要、翻译、文本生成。它的宗旨是让最先进的 NLP 技术人人易用。\\n\\n🤗 Transform...\"],[\"这里是一些例子：\\n- [用 BERT 做掩码填词](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+F...\"],[\"- [用 RoBERTa 做自然语言推理](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+a...\"],[\"- [用 DistilBERT...\"],[\"做问答](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+...\"],[\"used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+...\"],[\"uese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa...\"],[\"n%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+...\"],[\"d%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+c...\"],[\"t+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square...\"],[\"0+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100...\"],[\"82%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+n...\"],[\"ing+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rain...\"],[\"the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Ve...\"],[\"ts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments...\"],[\"artments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+t...\"],[\"alf+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+o...\"],[\"+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided...\"],[\"+divided+into+16%2C000+species)...\"],[\"- [用 T5 做翻译](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"**[Write With Transformer](https:\\u002f\\u002ftransformer.huggingface.co)**，由抱抱脸团队打造，是一个文本生成的官方 demo。\\n\\n## 如果你在寻...\"],[\"```\\n\\n第二行代码下载并缓存了流水线使用的预训练模型，而第三行代码则在给定的文本上进行了评估。这里的答案“正面” (positive) 具有 99 的置信度。\\n\\n许多的 NLP 任务都有开箱即用的预...\"],[\"```\\n这里是等效的 TensorFlow 代码：\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, TFAutoModel\\n\\n\\u003e\\u003e\\u003e tok...\"],[\"```\\n\\n词符化器 (tokenizer) 为所有的预训练模型提供了预处理，并可以直接对单个字符串进行调用（比如上面的例子）或对列表 (list) 调用。它会输出一个你可以在下游代码里使用或直接通过 ...\"],[\"1. 对于模型生命周期的每一个部分都面面俱到：\\n    - 训练先进的模型，只需 3 行代码\\n    - 模型在不同深度学习框架间任意转移，随你心意\\n    - 为训练、评估和生产选择最适合的框架，衔...\"],[\"这个仓库已在 Python 3.8+、Flax 0.4.1+、PyTorch 1.10+ 和 TensorFlow 2.6+ 下经过测试。\\n\\n你可以在[虚拟环境](https:\\u002f\\u002fdocs.pytho...\"],[\"```\\n\\n如果你想要试试用例或者想在正式发布前使用最新的开发中代码，你得[从源代码安装](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002finstallation#i...\"],[\"```\\n\\n要通过 conda 安装 Flax、PyTorch 或 TensorFlow 其中之一，请参阅它们各自安装页的说明。\\n\\n## 模型架构\\n\\n🤗 Transformers 支持的[**所有的模型...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (来自 Google Research and t...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BARTpho](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbartpho)** (来自 VinAI Research) 伴随论...\"],[\"1. **[BigBird-Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbigbird_pegasus)** (来自 Goo...\"],[\"1. **[BiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbit)** (来自 Google AI) 伴随论文 [Big Transf...\"],[\"1. **[BLIP-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblip-2)** (来自 Salesforce) 伴随论文 [BLI...\"],[\"1. **[BROS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbros)** (来自 NAVER CLOVA) 伴随论文 [BROS: ...\"],[\"1. **[Chinese-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fchinese_clip)** (来自 OFA-Sys) ...\"],[\"1. **[CLVP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclvp)** released with the paper [Bett...\"],[\"1. **[Conditional DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconditional_detr)** (来自 M...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (来自 Tsinghua University) 伴随论文 [...\"],[\"1. **[Data2Vec](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdata2vec)** (来自 Facebook) 伴随论文 [D...\"],[\"1. **[Deformable DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeformable_detr)** (来自 Sen...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (来自 Facebook) 伴随论文 [End-to-En...\"],[\"1. **[DINOv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdinov2)** (来自 Meta AI) 伴随论文 [DINOv2...\"],[\"1. **[DiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdit)** (来自 Microsoft Research) 伴随论文 [D...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (来自 Sna...\"],[\"1. **[EncoderDecoder](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fencoder-decoder)** (来自 Goog...\"],[\"1. **[ESM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fesm)** (from Meta AI) are transformer ...\"],[\"1. **[FLAN-T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-t5)** (from Google AI) releas...\"],[\"1. **[FlauBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflaubert)** (来自 CNRS) 伴随论文 [FlauB...\"],[\"1. **[Funnel Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffunnel)** (来自 CMU\\u002fGoogl...\"],[\"1. **[GPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopenai-gpt)** (来自 OpenAI) 伴随论文 [Improv...\"],[\"1. **[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt2)** (来自 OpenAI) 伴随论文 [Language M...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (来自 BigCode) 伴随论...\"],[\"1. **[GroupViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgroupvit)** (来自 UCSD, NVIDIA) 伴随论...\"],[\"1. **[IDEFICS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fidefics)** (from HuggingFace) rele...\"],[\"1. **[InstructBLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002finstructblip)** (来自 Salesforc...\"],[\"1. **[LayoutLMv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv2)** (来自 Microsoft Res...\"],[\"1. **[LeViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flevit)** (来自 Meta AI) 伴随论文 [LeViT: A...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (来自 The FAIR team of Meta...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (来自 Microsoft Research & Un...\"],[\"1. **[LXMERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flxmert)** (来自 UNC Chapel Hill) 伴随论文...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[MaskFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmaskformer)** (from Meta and UI...\"],[\"1. **[MEGA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmega)** (来自 Facebook) 伴随论文 [Mega: Mov...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MMS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmms)** (来自 Facebook) 伴随论文 [Scaling Spe...\"],[\"1. **[MobileNetV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v2)** (来自 Google Inc...\"],[\"1. **[MRA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmra)** (来自 the University of Wisconsin...\"],[\"1. **[NAT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnat)** (来自 SHI Labs) 伴随论文 [Neighborhoo...\"],[\"1. **[Nyströmformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnystromformer)** (来自 the Uni...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (来自 Google AI) 伴随论文 [Sim...\"],[\"1. **[Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus)** (来自 Google) 伴随论文 [PEGAS...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft) released with ...\"],[\"1. **[PLBart](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fplbart)** (来自 UCLA NLP) 伴随论文 [Unifi...\"],[\"1. **[PVT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpvt)** (来自 Nanjing University, The Uni...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (来自 Google Research) ...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (来自 Facebook), 伴随论文 [Ro...\"],[\"1. **[RWKV](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frwkv)** (来自 Bo Peng) 伴随论文 [this repo]...\"],[\"1. **[Segment Anything](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsam)** (来自 Meta AI) 伴随论文 ...\"],[\"1. **[SpeechToTextTransformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text)** ...\"],[\"1. **[SwiftFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswiftformer)** (来自 MBZUAI) 伴随论...\"],[\"1. **[SwitchTransformers](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswitch_transformers)** ...\"],[\"1. **[TAPAS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftapas)** (来自 Google AI) 伴随论文 [TAPAS:...\"],[\"1. **[Transformer-XL](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftransfo-xl)** (来自 Google\\u002fCM...\"],[\"1. **[UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ful2)** (from Google Research) released...\"],[\"1. **[UnivNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funivnet)** (from Kakao Corporation...\"],[\"1. **[ViLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvilt)** (来自 NAVER AI Lab\\u002fKakao Enterp...\"],[\"1. **[ViT Hybrid](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_hybrid)** (来自 Google AI) 伴随...\"],[\"1. **[ViTMSN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_msn)** (来自 Meta AI) 伴随论文 [Maske...\"],[\"1. **[Wav2Vec2-Conformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2-conformer)** (...\"],[\"1. **[X-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxclip)** (来自 Microsoft Research) 伴随...\"],[\"1. **[XLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm)** (来自 Facebook) 伴随论文 [Cross-lingu...\"],[\"1. **[XLM-V](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-v)** (来自 Meta AI) 伴随论文 [XLM-V: O...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (来自 Faceboo...\"],[\"要检查某个模型是否已有 Flax、PyTorch 或 TensorFlow 的实现，或其是否在 🤗 Tokenizers 库中有对应词符化器（tokenizer），敬请参阅[此表](https:\\u002f\\u002fh...\"],[\"## 引用\\n\\n我们已将此库的[论文](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)正式发表，如果你使用了 🤗 Transformers 库...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n[Activation-aware Weight Quantization (AWQ)](https:\\u002f\\u002fhf.co\\u002fpapers\\u002f2306.00978) doesn't quanti...\"],[\"```\\n\\nAWQ-quantized models can be identified by checking the `quantization_config` attribute in the m...\"],[\"```\\n\\nAWQ quantization can also be combined with [FlashAttention-2](perf_infer_gpu_one#flashattention...\"],[\"```\\n\\n### Fused modules\\n\\nFused modules offers improved accuracy and performance and it is supported o...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"unsupported architectures\\\"\\u003e\\n\\nFor architectures that don't support fus...\"],[\"```\\n\\nThe parameter `modules_to_fuse` should include:\\n\\n- `\\\"attention\\\"`: The names of the attention la...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThe [AutoGPTQ](https:\\u002f\\u002fgithub.com\\u002fPanQiWei\\u002fAutoGPTQ) library implements the GPTQ algorithm, ...\"],[\"```\\n\\nTo quantize a model (currently only supported for text models), you need to create a [`GPTQConf...\"],[\"```\\n\\nIf you're running out of memory because a dataset is too large, disk offloading is not supporte...\"],[\"```\\n\\nYou could also save your quantized model locally with the [`~PreTrainedModel.save_pretrained`] ...\"],[\"```\\n\\n### ExLlama\\n\\n[ExLlama](https:\\u002f\\u002fgithub.com\\u002fturboderp\\u002fexllama) is a Python\\u002fC++\\u002fCUDA implementatio...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nOnly 4-bit models are supported, and we recommend deactivating the ExLlam...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"4-bit\\\"\\u003e\\n\\n```bash\\npip install bitsandbytes\\u003e=0.39.0\\npip install --upgra...\"],[\"```\\n\\nOnce a model is quantized to 8-bit, you can't push the quantized weights to the Hub unless you'...\"],[\"```\\n\\nOnce a model is quantized to 4-bit, you can't push the quantized weights to the Hub.\\n\\n\\u003c\\u002fhfoptio...\"],[\"```\\n\\nDesign a custom device map to fit everything on your GPU except for the `lm_head`, which you'll...\"],[\"```\\n\\n#### Skip module conversion\\n\\nFor some models, like [Jukebox](model_doc\\u002fjukebox), you don't need...\"],[\"```\\n\\n#### Finetuning\\n\\nWith the [PEFT](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fpeft) library, you can finetune...\"],[\"```\\n\\n#### Normal Float 4 (NF4)\\n\\nNF4 is a 4-bit data type from the [QLoRA](https:\\u002f\\u002fhf.co\\u002fpapers\\u002f2305....\"],[\"```\\n\\nFor inference, the `bnb_4bit_quant_type` does not have a huge impact on performance. However, t...\"],[\"```\\n\\n## Optimum\\n\\nThe [Optimum](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002findex) library supports quantizat...\"],[\"\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhu...\"],[\"The benchmarks indicate AWQ quantization is the fastest for inference, text generation, and has the ...\"],[\"\\u003cfigcaption class=\\\"text-center text-gray-500 text-lg\\\"\\u003eUnfused module\\u003c\\u002ffigcaption\\u003e\\n\\n|   Batch Size | ...\"],[\"\\u003cfigcaption class=\\\"text-center text-gray-500 text-lg\\\"\\u003eFused module\\u003c\\u002ffigcaption\\u003e\\n\\n|   Batch Size |   ...\"],[\"The speed and throughput of fused and unfused modules were also tested with the [optimum-benchmark](...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The model is trained end-to-end with a combination of losses derived from variational lower bound an...\"],[\"## Usage examples\\n\\nBoth the VITS and MMS-TTS checkpoints can be used with the same API. Since the fl...\"],[\"```\\n\\nThe resulting waveform can be saved as a `.wav` file:\\n\\n```python\\nimport scipy\\n\\nscipy.io.wavfile...\"],[\"```\\n\\nYou can then pre-process the text input using the following code snippet. You can either rely o...\"],[\"```\\n\\n## VitsConfig\\n\\n[[autodoc]] VitsConfig\\n\\n## VitsTokenizer\\n\\n[[autodoc]] VitsTokenizer\\n    - __call...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Overview\\n\\nThe XLM-RoBERTa model was proposed in [Unsupervised Cross-lingual Representation Learni...\"],[\"This model was contributed by [stefan-it](https:\\u002f\\u002fhuggingface.co\\u002fstefan-it). The original code can b...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- A blog post on how to [finetune XLM RoBERTa for mul...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- [`XLMRobertaForTokenClassification`] is supported ...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`XLMRobertaForMaskedLM`] is supported by this [example scrip...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`XLMRobertaForQuestionAnswering`] is supported by t...\"],[\"**Multiple choice**\\n\\n- [`XLMRobertaForMultipleChoice`] is supported by this [example script](https:\\u002f...\"],[\"[[autodoc]] XLMRobertaTokenizerFast\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## XLMRobertaModel\\n\\n[[autodoc]] XLMRob...\"],[\"[[autodoc]] TFXLMRobertaForSequenceClassification\\n    - call\\n\\n## TFXLMRobertaForMultipleChoice\\n\\n[[au...\"],[\"## FlaxXLMRobertaForQuestionAnswering\\n\\n[[autodoc]] FlaxXLMRobertaForQuestionAnswering\\n    - __call__...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fimagegpt_arc...\"],[\"- ImageGPT is almost exactly the same as [GPT-2](gpt2), with the exception that a different activati...\"],[\"easily obtained by first forwarding the image through the model, then specifying `output_hidden_stat...\"],[\"| **Model variant** | **Depths** | **Hidden sizes** | **Decoder hidden size** | **Params (M)** | **I...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Pop2Piano is an encoder-decoder Transformer model based on [T5](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f1910.10683.pdf...\"],[\"```\\npip install pretty-midi==0.2.9 essentia==2.1b6.dev1034 librosa scipy\\n```\\nPlease note that you ma...\"],[\"```\\n\\n- Example using your own audio file:\\n\\n```python\\n\\u003e\\u003e\\u003e import librosa\\n\\u003e\\u003e\\u003e from transformers import...\"],[\"```\\n\\n- Example of processing multiple audio files in batch:\\n\\n```python\\n\\u003e\\u003e\\u003e import librosa\\n\\u003e\\u003e\\u003e from t...\"],[\"```\\n\\n\\n- Example of processing multiple audio files in batch (Using `Pop2PianoFeatureExtractor` and `...\"],[\"\\u003e\\u003e\\u003e # Since we now have 2 generated MIDI files\\n\\u003e\\u003e\\u003e tokenizer_output[0].write(\\\".\\u002fOutputs\\u002fmidi_output1...\"],[\"```\\n\\n\\n## Pop2PianoConfig\\n\\n[[autodoc]] Pop2PianoConfig\\n\\n## Pop2PianoFeatureExtractor\\n\\n[[autodoc]] Pop...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdeformable_d...\"],[\"[[autodoc]] DeformableDetrImageProcessor\\n    - preprocess\\n    - post_process_object_detection\\n\\n## De...\"],[\"### Fine-tuning BERT on SQuAD1.0 with relative position embeddings\\n\\nThe following examples show how ...\"],[\"##### Base models fine-tuning\\n\\n```bash\\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\\ntorchrun --nproc_...\"],[\"```\\nTraining with the above command leads to the following results. It boosts the BERT default from ...\"],[\"```\\nTraining with the above command leads to the f1 score of 93.52, which is slightly better than th...\"],[\"```\\n\\n##### Results for SQuAD2.0 with the previously defined hyper-parameters:\\n\\n```python\\n{\\n\\\"exact\\\": ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Detection Transformer (DETR) directly transforms que...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large Transformer models routinely achieve state-of-...\"],[\"### Axial Positional Encodings\\n\\nAxial Positional Encodings were first implemented in Google's [trax ...\"],[\"with:\\n\\n$$d = d^1 + d^2 \\\\text{ and } n_s = n_s^1 \\\\times n_s^2 .$$\\n\\nTherefore the following holds:\\n\\n$$...\"],[\"In practice, the parameter `config.axial_pos_embds_dim` is set to a tuple \\\\\\\\((d^1, d^2)\\\\\\\\) which sum...\"],[\"For more information, see the [original Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2001.04451) or this great [blog...\"],[\"### Local Self Attention\\n\\nLocal self attention is essentially a \\\"normal\\\" self attention layer with k...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Question ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## {{cookiecutter.camelcase_modelname}}TokenizerFast\\n\\n[[autodoc]] {{cookiecutter.camelcase_modelname...\"],[\"[[autodoc]] {{cookiecutter.camelcase_modelname}}ForQuestionAnswering\\n    - forward\\n\\n{%- else %}\\n## {...\"],[\"[[autodoc]] TF{{cookiecutter.camelcase_modelname}}ForCausalLM\\n    - call\\n\\n\\n## TF{{cookiecutter.camel...\"],[\"{% if cookiecutter.is_encoder_decoder_model == \\\"False\\\" %}\\n## Flax{{cookiecutter.camelcase_modelname}...\"],[\"[[autodoc]] Flax{{cookiecutter.camelcase_modelname}}ForQuestionAnswering\\n    - call\\n\\n\\n## Flax{{cooki...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-trained language models have attracted increasin...\"],[\"## Usage tips\\n\\n- BioGPT is a model with absolute position embeddings so it's usually advised to pad ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recently, plain vision Transformers (ViTs) have show...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Randomly initializing `SpeechEncoderDecoderModel` from model configurations.\\n\\n[`SpeechEncoderDeco...\"],[\"```\\n\\n## Initialising `SpeechEncoderDecoderModel` from a pretrained encoder and a pretrained decoder....\"],[\"```\\n\\n## Loading an existing `SpeechEncoderDecoderModel` checkpoint and perform inference.\\n\\nTo load f...\"],[\"```\\n\\n## Training\\n\\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other e...\"],[\"\\u003e\\u003e\\u003e # load its corresponding transcription and tokenize to generate labels\\n\\u003e\\u003e\\u003e labels = tokenizer(ds...\"],[\"```\\n\\n## SpeechEncoderDecoderConfig\\n\\n[[autodoc]] SpeechEncoderDecoderConfig\\n\\n## SpeechEncoderDecoderM...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Backward\\n\\nThe last addition is to replace the typical `loss.backward()` in your training loo...\"],[\"```\\n\\nAs you can see in the following code, you only need to add four additional lines of code to you...\"],[\"```\\n\\nThen launch your training with:\\n\\n```bash\\naccelerate launch train.py\\n```\\n\\n### Train with a noteb...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We propose focal modulation networks (FocalNets in s...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr).\\nThe original code can be foun...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Pipeline\\n\\n\\u003cYoutube id=\\\"tiZFewofSLM\\\"\\u002f\\u003e\\n\\nThe [`pipeline`] is the eas...\"],[\"| **Task**                     | **Description**                                                    ...\"],[\"| Audio classification         | assign a label to some audio data                                  ...\"],[\"Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this ...\"],[\"```\\n\\nThe [`pipeline`] downloads and caches a default [pretrained model](https:\\u002f\\u002fhuggingface.co\\u002fdisti...\"],[\"```\\n\\nLoad an audio dataset (see the 🤗 Datasets [Quick Start](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fqu...\"],[\"```\\n\\nFor larger datasets where the inputs are big (like in speech or vision), you'll want to pass a ...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nUse [`TFAutoModelForSequenceClassification`] and [`AutoTokenizer`] to load the pretra...\"],[\"```\\n\\nIf you can't find a model for your use-case, you'll need to finetune a pretrained model on your...\"],[\"```\\n\\nPass your text to the tokenizer:\\n\\n```py\\n\\u003e\\u003e\\u003e encoding = tokenizer(\\\"We are very happy to show you...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [preprocess](.\\u002fpreprocessing) tutorial for more ...\"],[\"```\\n\\nThe model outputs the final activations in the `logits` attribute. Apply the softmax function t...\"],[\"```\\n\\nThe model outputs the final activations in the `logits` attribute. Apply the softmax function t...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nOnce your model is fine-tuned, you can save it with its tokenizer using [`TFPreTraine...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Custom model builds\\n\\nYou can modify the model's configuration clas...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nTake a look at the [Create a custom architecture](.\\u002fcreate_a_model) g...\"],[\"```\\n\\n3. Load a preprocessing class like a tokenizer, image processor, feature extractor, or processo...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nFor tasks - like translation or summarization - that use a sequence-to-sequence model, u...\"],[\"```py\\n   \\u003e\\u003e\\u003e from transformers import TFAutoModelForSequenceClassification\\n\\n   \\u003e\\u003e\\u003e model = TFAutoMod...\"],[\"```\\n\\n2. Load a preprocessing class like a tokenizer, image processor, feature extractor, or processo...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Randomly initializing `EncoderDecoderModel` from model configurations.\\n\\n[`EncoderDecoderModel`] c...\"],[\"```\\n\\n## Initialising `EncoderDecoderModel` from a pretrained encoder and a pretrained decoder.\\n\\n[`En...\"],[\"```\\n\\n## Loading an existing `EncoderDecoderModel` checkpoint and perform inference.\\n\\nTo load fine-tu...\"],[\"```\\n\\n## Loading a PyTorch checkpoint into `TFEncoderDecoderModel`.\\n\\n[`TFEncoderDecoderModel.from_pre...\"],[\"```\\n\\n## Training\\n\\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other e...\"],[\"\\u003e\\u003e\\u003e labels = tokenizer(\\n...     \\\"the eiffel tower surpassed the washington monument to become the ta...\"],[\"```\\n\\nDetailed [colab](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1WIk2bxglElfZewOHboPFNj8H44_VAyKE?usp=...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-attention has become a defacto choice for captu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"--\\u003e\\n\\n# Llama2\\n\\n## Overview\\n\\nThe Llama2 model was proposed in [LLaMA: Open Foundation and Fine-Tuned ...\"],[\"The abstract from the paper is the following:\\n\\n*In this work, we develop and release Llama 2, a coll...\"],[\"The `dtype` of the online weights is mostly irrelevant unless you are using `torch_dtype=\\\"auto\\\"` whe...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nTips:\\n\\n- Weights for the Llama2 models can be obtained by filling out [this form](https:\\u002f\\u002fai...\"],[\"```\\n\\n- After conversion, the model and tokenizer can be loaded via:\\n\\n```python\\nfrom transformers imp...\"],[\"```\\n\\nNote that executing the script requires enough CPU RAM to host the whole model in float16 preci...\"],[\"\\u003cPipelineTag pipeline=\\\"text-generation\\\"\\u002f\\u003e\\n\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1P...\"],[\"⚡️ Inference\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1TC56ArKerXUpbgRy5vM3woRsbTEVNq7...\"],[\"[[autodoc]] LlamaModel\\n    - forward\\n\\n\\n## LlamaForCausalLM\\n\\n[[autodoc]] LlamaForCausalLM\\n    - forwa...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In this work, we present a new network design paradi...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-supervised learning (SSL) achieves great succes...\"],[\"- [Audio classification task guide](..\\u002ftasks\\u002faudio_classification)\\n- [Automatic speech recognition t...\"],[\"Performer fine-tuning\\n\\nExample authors: @TevenLeScao, @Patrickvonplaten\\n\\nPaper authors: Krzysztof Ch...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Deeper neural networks are more difficult to train. ...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"End-to-End finetuning of RAG (including DPR retriever) for Question Answering.\\n\\nThis finetuning scri...\"],[\"To start training, use the bash script (finetune_rag_ray_end2end.sh) in this folder. This script als...\"],[\"We conducted a simple experiment to investigate the effectiveness of this end2end training extension...\"],[\"!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\\n\\nLicensed under ...\"],[\"This model was contributed by [shangz](https:\\u002f\\u002fhuggingface.co\\u002fshangz).\\n\\n## Usage tips\\n\\n- QDQBERT mod...\"],[\"Before creating QDQBERT model, one has to set the default `QuantDescriptor` defining default tensor ...\"],[\"```\\n\\n### Calibration\\n\\nCalibration is the terminology of passing data samples to the quantizer and de...\"],[\"```\\n\\n### Export to ONNX\\n\\nThe goal of exporting to ONNX is to deploy inference by [TensorRT](https:\\u002f\\u002f...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token cla...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [patrickvonplaten](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten). The Autho...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Depth estimation pipeline\\n\\nThe simplest way to try out inference with a model supporting dep...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\nVisualize the results:\\n\\n```py\\n\\u003e\\u003e\\u003e import numpy as np\\n\\n\\u003e\\u003e\\u003e # interpolate to original size\\n\\u003e\\u003e\\u003e pr...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Most neural vocoders employ band-limited mel-spectro...\"],[\"Tips:\\n\\n- The `noise_sequence` argument for [`UnivNetModel.forward`] should be standard Gaussian nois...\"],[\"ds = load_dataset(\\\"hf-internal-testing\\u002flibrispeech_asr_dummy\\\", \\\"clean\\\", split=\\\"validation\\\")\\n# Resamp...\"],[\"```\\n\\nThis model was contributed by [dg845](https:\\u002f\\u002fhuggingface.co\\u002fdg845).\\nTo the best of my knowledg...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Understanding document images (e.g., invoices) is a ...\"],[\"## Usage tips\\n\\n- The quickest way to get started with Donut is by checking the [tutorial\\n  notebooks...\"],[\"\\u003e\\u003e\\u003e device = \\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\"\\n\\u003e\\u003e\\u003e model.to(device)  # doctest: +IGNORE...\"],[\"```\\n\\n- Step-by-step Document Parsing\\n\\n```py\\n\\u003e\\u003e\\u003e import re\\n\\n\\u003e\\u003e\\u003e from transformers import DonutProcess...\"],[\"\\u003e\\u003e\\u003e pixel_values = processor(image, return_tensors=\\\"pt\\\").pixel_values\\n\\n\\u003e\\u003e\\u003e outputs = model.generate(...\"],[\"```\\n\\n- Step-by-step Document Visual Question Answering (DocVQA)\\n\\n```py\\n\\u003e\\u003e\\u003e import re\\n\\n\\u003e\\u003e\\u003e from trans...\"],[\"\\u003e\\u003e\\u003e pixel_values = processor(image, return_tensors=\\\"pt\\\").pixel_values\\n\\n\\u003e\\u003e\\u003e outputs = model.generate(...\"],[\"```\\n\\nSee the [model hub](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=donut) to look for Donut checkpoints.\\n...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformer based models, like BERT and RoBERTa, hav...\"],[\"## IBertConfig\\n\\n[[autodoc]] IBertConfig\\n\\n## IBertModel\\n\\n[[autodoc]] IBertModel\\n    - forward\\n\\n## IBe...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While the Transformer architecture has become the de...\"],[\"- [BEiT](beit) (BERT pre-training of Image Transformers) by Microsoft Research. BEiT models outperfo...\"],[\"- To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-o...\"],[\"use a higher resolution than pre-training [(Touvron et al., 2019)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1906.06423)...\"],[\"## Resources\\n\\nDemo notebooks regarding inference as well as fine-tuning ViT on custom data can be fo...\"],[\"⚗️ Optimization\\n\\n- A blog post on how to [Accelerate Vision Transformer (ViT) with Quantization usin...\"],[\"[[autodoc]] ViTForImageClassification\\n    - forward\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n## TFViTModel\\n\\n[[autodoc]] TFViTMod...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [Patrick von Platen](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten).\\n\\nThe or...\"],[\"- Step-by-step Speech Translation\\n\\n```python\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transformers import Speech2Te...\"],[\"```\\n\\n- Speech Translation via Pipelines\\n\\n  The automatic speech recognition pipeline can also be use...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The `padding` argument controls padding. It can be a boolean or a string:\\n\\n  - `True` or `'longest'`...\"],[\"The `max_length` argument controls the length of the padding and truncation. It can be an integer or...\"],[\"| Truncation                           | Padding                           | Instruction            ...\"],[\"|                                      |                                   | `tokenizer(batch_senten...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nwhere task name can be one of cola, sst2, mrpc, stsb, qqp, mnli, qnli, rte, wnli.\\n\\nWe get the f...\"],[\"The following example fine-tunes BERT on the `imdb` dataset hosted on our [hub](https:\\u002f\\u002fhuggingface....\"],[\"```\\n\\n\\u003e If your model classification head dimensions do not fit the number of labels in the dataset, ...\"],[\"```\\nTraining for 1 epoch results in acc of around 0.5958 for review_body only and 0.659 for title+bo...\"],[\"```\\n It results in a Micro F1 score of around 0.82 without any text and label filtering. Note that y...\"],[\"Using mixed precision training usually results in 2x-speedup for training with the same final result...\"],[\"Like `run_glue.py`, this script allows you to fine-tune any of the models on the [hub](https:\\u002f\\u002fhuggi...\"],[\"```\\n\\nthen\\n\\n```bash\\nexport TASK_NAME=mrpc\\n\\npython run_glue_no_trainer.py \\\\\\n  --model_name_or_path ber...\"],[\"```\\n\\nThis command is the same and will work for:\\n\\n- a CPU-only setup\\n- a setup with one GPU\\n- a dist...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Vision Transformers (ViT) have shown rapid progress ...\"],[\"## Documentation resources\\n\\n- [Image classification task guide](..\\u002ftasks\\u002fimage_classification)\\n\\n## E...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Fine-tuning ViLT\\n\\nViLT model incorporates text embeddings into a Vision Transformer (ViT), allowi...\"],[\"```\\n\\nWe encourage you to share your model with the community. Log in to your Hugging Face account to...\"],[\"```\\n\\nLet's take a look at an example to understand the dataset's features:\\n\\n```py\\n\\u003e\\u003e\\u003e dataset[0]\\n{'q...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\nNow that we have the mappings, we can replace the string answers with their ids, and flatten th...\"],[\"```\\n\\nTo preprocess the data we need to encode the images and questions using the [`ViltProcessor`]. ...\"],[\"```\\n\\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.map`] f...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\nThe model in this guide has only been trained on 200 examples, so don't expect a lot from it. L...\"],[\"```\\n\\n## Zero-shot VQA\\n\\nThe previous model treated VQA as a classification task. Some recent models, ...\"],[\"```\\n\\nNow we need to preprocess the image\\u002fprompt with the model's processor, pass the processed input...\"],[\"!---\\nCopyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\\n\\nLicense...\"],[\"```\\n\\nwhere task name can be one of cola, mnli, mnli_mismatched, mnli_matched, mrpc, qnli, qqp, rte, ...\"],[\"```\\n\\nor directly on the hub under *Training metrics*.\\n\\n### Accuracy Evaluation\\n\\nWe train five replic...\"],[\"| Task  | Metric                       | Acc (best run) | Acc (avg\\u002f5runs) | Stdev     | Metrics     ...\"],[\"| QQP   | Accuracy\\u002fF1                  | 90.81\\u002f87.58    | 90.76\\u002f87.51     | 0.05\\u002f0.06 | [tfhub.dev](...\"],[\"Some of these results are significantly different from the ones reported on the test set of GLUE ben...\"],[\"| Task  | TPU v3-8  | 8 GPU      | [1 GPU](https:\\u002f\\u002ftensorboard.dev\\u002fexperiment\\u002fmkPS4Zh8TnGe1HB6Yzwj4Q...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We propose an efficient design of Transformer-based ...\"],[\"## PatchTSTConfig\\n\\n[[autodoc]] PatchTSTConfig\\n\\n## PatchTSTModel\\n\\n[[autodoc]] PatchTSTModel\\n    - for...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"🤗 Transformers는 분류, 정보 추출, 질문 답변, 요약, 번역, 문장 생성 등을 100개 이상의 언어로 수행할 수 있는 수천개의 사전학습된 모델을 제공합니다. 우리의 목...\"],[\"🤗 Transformers는 가장 유명한 3개의 딥러닝 라이브러리를 지원합니다. 이들은 서로 완벽히 연동됩니다 — [Jax](https:\\u002f\\u002fjax.readthedocs.io\\u002fen\\u002f...\"],[\"예시:\\n- [BERT로 마스킹된 단어 완성하기](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+...\"],[\"- [BART를 이용한 요약](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fbart-large-cnn?text=The+tower+is+324+metres+%281%2C...\"],[\"- [DistilBERT를 이용한 질문...\"],[\"답변](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+d...\"],[\"- [T5로 번역하기](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"**[Transformer와 글쓰기](https:\\u002f\\u002ftransformer.huggingface.co)** 는 이 저장소의 텍스트 생성 능력에 관한 Hugging Face 팀의 공식...\"],[\"## 퀵 투어\\n\\n원하는 텍스트에 바로 모델을 사용할 수 있도록, 우리는 `pipeline` API를 제공합니다. Pipeline은 사전학습 모델과 그 모델을 학습할 때 적용한 전처...\"],[\"```\\n\\n코드의 두번째 줄은 pipeline이 사용하는 사전학습 모델을 다운로드하고 캐시로 저장합니다. 세번째 줄에선 그 모델이 주어진 텍스트를 평가합니다. 여기서 모델은 99.9...\"],[\"```\\n\\n답변뿐만 아니라, 여기에 사용된 사전학습 모델은 확신도와 토크나이즈된 문장 속 답변의 시작점, 끝점까지 반환합니다. [이 튜토리얼](https:\\u002f\\u002fhuggingface.c...\"],[\"```\\n\\n토크나이저는 사전학습 모델의 모든 전처리를 책임집니다. 그리고 (위의 예시처럼) 1개의 스트링이나 리스트도 처리할 수 있습니다. 토크나이저는 딕셔너리를 반환하는데, 이는 ...\"],[\"모델 자체는 일반적으로 사용되는 [Pytorch `nn.Module`](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fnn.html#torch.nn.Module)나 [T...\"],[\"1. 더 적은 계산 비용, 더 적은 탄소 발자국:\\n    - 연구자들은 모델을 계속 다시 학습시키는 대신 학습된 모델을 공유할 수 있습니다.\\n    - 실무자들은 학습에 필요한 시...\"],[\"1. 필요한 대로 모델이나 예시를 커스터마이즈하세요:\\n    - 우리는 저자가 공개한 결과를 재현하기 위해 각 모델 구조의 예시를 제공합니다.\\n    - 모델 내부 구조는 가능한 ...\"],[\"## 왜 transformers를 사용하지 말아야 할까요?\\n\\n- 이 라이브러리는 신경망 블록을 만들기 위한 모듈이 아닙니다. 연구자들이 여러 파일을 살펴보지 않고 바로 각 모델을 ...\"],[\"## 설치\\n\\n### pip로 설치하기\\n\\n이 저장소는 Python 3.8+, Flax 0.4.1+, PyTorch 1.10+, TensorFlow 2.6+에서 테스트 되었습니다.\\n\\n...\"],[\"이들 중 적어도 하나가 설치되었다면, 🤗 Transformers는 다음과 같이 pip을 이용해 설치할 수 있습니다:\\n\\n```bash\\npip install transformers...\"],[\"```\\n\\n예시들을 체험해보고 싶거나, 최최최첨단 코드를 원하거나, 새로운 버전이 나올 때까지 기다릴 수 없다면 [라이브러리를 소스에서 바로 설치](https:\\u002f\\u002fhuggingfac...\"],[\"```\\n\\nFlax, PyTorch, TensorFlow 설치 페이지에서 이들을 conda로 설치하는 방법을 확인하세요.\\n\\n## 모델 구조\\n\\n**🤗 Transformers가 제공하는...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (from Google Research and...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbit)** (from Google AI) released with ...\"],[\"1. **[BLIP-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblip-2)** (Salesforce 에서 제공)은 Junna...\"],[\"1. **[BROS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbros)** (NAVER CLOVA 에서 제공)은 Teakgyu ...\"],[\"1. **[CANINE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcanine)** (Google Research 에서) Jona...\"],[\"1. **[CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclip)** (OpenAI 에서) Alec Radford, Jon...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (MetaAI 에서 제공)은 Ba...\"],[\"1. **[ConvNeXT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconvnext)** (Facebook AI 에서) Zhua...\"],[\"1. **[CPM-Ant](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpmant)** (from OpenBMB) released ...\"],[\"1. **[DeBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta)** (Microsoft 에서) Pengchen...\"],[\"1. **[DeiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeit)** (Facebook 에서) Hugo Touvron, M...\"],[\"1. **[DialoGPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdialogpt)** (Microsoft Research 에...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (HuggingFace 에서) ...\"],[\"1. **[DPR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdpr)** (Facebook 에서) Vladimir Karpukhi...\"],[\"1. **[ELECTRA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002felectra)** (Google Research\\u002fStanfo...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (Baidu 에서 제공)은 Xuan Ouya...\"],[\"1. **[FNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffnet)** (from Google Research) releas...\"],[\"1. **[GPT NeoX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_neox)** (EleutherAI 에서) Sid B...\"],[\"1. **[GPT-Sw3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt-sw3)** (AI-Sweden 에서) Ariel Ek...\"],[\"1. **[GPTSAN-japanese](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgptsan-japanese)** release...\"],[\"1. **[Hubert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fhubert)** (Facebook 에서) Wei-Ning Hs...\"],[\"1. **[Informer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002finformer)** (from Beihang Univers...\"],[\"1. **[KOSMOS-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fkosmos-2)** (from Microsoft Resea...\"],[\"1. **[LayoutLMv3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv3)** (Microsoft Resear...\"],[\"1. **[LiLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flilt)** (South China University of Te...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (The FAIR team of Meta AI...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (Microsoft Research & Unive...\"],[\"1. **[LXMERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flxmert)** (UNC Chapel Hill 에서) Hao ...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[MaskFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmaskformer)** (Meta and UIUC 에서...\"],[\"1. **[mBART-50](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (Facebook 에서) Yuqing Tan...\"],[\"1. **[Megatron-GPT2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmegatron_gpt2)** (NVIDIA 에서)...\"],[\"1. **[Mixtral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmixtral)** (from Mistral AI) by Th...\"],[\"1. **[MobileBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilebert)** (CMU\\u002fGoogle Brain...\"],[\"1. **[MobileViTV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilevitv2)** (Apple 에서 제공)은 ...\"],[\"1. **[MT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmt5)** (Google AI 에서) Linting Xue, Noa...\"],[\"1. **[Nezha](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnezha)** (Huawei Noah’s Ark Lab 에서) ...\"],[\"1. **[Nyströmformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnystromformer)** (the Univer...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (Google AI 에서) Matthias ...\"],[\"1. **[PatchTST](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpatchtst)** (IBM 에서 제공)은 Yuqi Nie...\"],[\"1. **[Persimmon](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpersimmon)** (ADEPT 에서 제공)은 Eric...\"],[\"1. **[Pix2Struct](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpix2struct)** (Google 에서 제공)은 K...\"],[\"1. **[ProphetNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fprophetnet)** (Microsoft Resear...\"],[\"1. **[RAG](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frag)** (Facebook 에서) Patrick Lewis, Et...\"],[\"1. **[RemBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frembert)** (Google Research 에서) Hy...\"],[\"1. **[RoCBert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froc_bert)** (WeChatAI 에서) HuiSu, W...\"],[\"1. **[SeamlessM4Tv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fseamless_m4t_v2)** (from Met...\"],[\"1. **[SEW-D](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsew_d)** (ASAPP 에서) Felix Wu, Kwangy...\"],[\"1. **[SpeechToTextTransformer2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text_2)...\"],[\"1. **[Swin Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswin)** (Microsoft 에서) Ze...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (Google AI 에서) Colin Raffel and N...\"],[\"1. **[TAPEX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftapex)** (Microsoft Research 에서) Qia...\"],[\"1. **[Transformer-XL](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftransfo-xl)** (Google\\u002fCMU 에...\"],[\"1. **[UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ful2)** (Google Research 에서) Yi Tay, Mo...\"],[\"1. **[UniSpeechSat](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funispeech-sat)** (Microsoft R...\"],[\"1. **[VideoMAE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvideomae)** (Multimedia Computing...\"],[\"1. **[Vision Transformer (ViT)](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit)** (Google AI...\"],[\"1. **[VitDet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvitdet)** (Meta AI 에서 제공)은 Yanghao ...\"],[\"1. **[VITS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvits)** (Kakao Enterprise 에서 제공)은 Jae...\"],[\"1. **[Wav2Vec2Phoneme](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2_phoneme)** (Faceb...\"],[\"1. **[X-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxclip)** (Microsoft Research 에서) Bo...\"],[\"1. **[XLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm)** (Facebook 에서) Guillaume Lample ...\"],[\"1. **[XLM-V](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-v)** (Meta AI 에서) Davis Liang, H...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (Facebook A...\"],[\"1. 새로운 모델을 올리고 싶나요? 우리가 **상세한 가이드와 템플릿** 으로 새로운 모델을 올리도록 도와드릴게요. 가이드와 템플릿은 이 저장소의 [`templates`](.\\u002fte...\"],[\"각 모델이 Flax, PyTorch, TensorFlow으로 구현되었는지 또는 🤗 Tokenizers 라이브러리가 지원하는 토크나이저를 사용하는지 확인하려면, [이 표](https...\"],[\"| 섹션 | 설명 |\\n|-|-|\\n| [도큐먼트](https:\\u002f\\u002fhuggingface.co\\u002ftransformers\\u002f) | 전체 API 도큐먼트와 튜토리얼 |\\n| [과제 요약](htt...\"],[\"| [마이그레이션](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmigration) | `pytorch-transformers`나 `pytorch-pr...\"],[\"## 인용\\n\\n🤗 Transformers 라이브러리를 인용하고 싶다면, 이 [논문](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)을...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Check more detailed information for [oneccl_bind_pt](https:\\u002f\\u002fgithub.com\\u002fintel\\u002ftorch-ccl).\\n\\n### Intel...\"],[\"```\\npip install oneccl_bind_pt=={pytorch_version} -f https:\\u002f\\u002fdeveloper.intel.com\\u002fipex-whl-stable-cpu...\"],[\"```\\n\\n#### Intel® Extension for PyTorch installation\\n\\nIntel Extension for PyTorch (IPEX) provides per...\"],[\"```\\nThe following command enables training with a total of four processes on two Xeons (node0 and no...\"],[\"```\\n\\n## Usage with Kubernetes\\n\\nThe same distributed training job from the previous section can be de...\"],[\"```\\nFROM intel\\u002fai-workflows:torch-2.0.1-huggingface-multinode-py3.9\\n\\nWORKDIR \\u002fworkspace\\n\\n# Download ...\"],[\"```\\nThe image needs to be built and copied to the cluster's nodes or pushed to a container registry ...\"],[\"The snippet below is an example of a yaml file for a PyTorchJob with 4 workers running the\\n[question...\"],[\"- \\\"3e-5\\\"\\n                - --num_train_epochs\\n                - \\\"2\\\"\\n                - --max_seq_leng...\"],[\"- name: pvc-volume\\n                mountPath: \\u002ftmp\\u002fpvc-mount\\n              - mountPath: \\u002fdev\\u002fshm\\n   ...\"],[\"```\\nTo run this example, update the yaml based on your training script and the nodes in your cluster...\"],[\"```\\nNAME                                                     READY   STATUS                  RESTART...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper states the following:\\n\\n*Visual language data such as plots, charts, and in...\"],[\"## Usage\\n\\nCurrently 6 checkpoints are available for MatCha:\\n\\n- `google\\u002fmatcha`: the base MatCha mode...\"],[\"inputs = processor(images=image, text=\\\"Is the sum of all 4 places greater than Laos?\\\", return_tensor...\"],[\"```\\n\\n## Fine-tuning\\n\\nTo fine-tune MatCha, refer to the pix2struct [fine-tuning notebook](https:\\u002f\\u002fgit...\"],[\"!--Copyright 2022 The HuggingFace Team and The OpenBMB Team. All rights reserved.\\n\\nLicensed under th...\"],[\"## CpmAntConfig\\n\\n[[autodoc]] CpmAntConfig\\n    - all\\n\\n## CpmAntTokenizer\\n\\n[[autodoc]] CpmAntTokenizer...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The recent breakthroughs in natural language process...\"],[\"processor = AutoImageProcessor.from_pretrained('facebook\\u002fdinov2-base')\\nmodel = AutoModel.from_pretra...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[IDEFICS](..\\u002fmodel_doc\\u002fidefics) is an open-access vision and language model based on [Flamingo](http...\"],[\"Before you begin, make sure you have all the necessary libraries installed. \\n\\n```bash\\npip install -q...\"],[\"```\\n\\n\\u003cTip\\u003e\\nTo run the following examples with a non-quantized version of the model checkpoint you wi...\"],[\"```\\n\\nSetting `device_map` to `\\\"auto\\\"` will automatically determine how to load and store the model w...\"],[\"```\\n\\nNow that you have the model loaded in one of the suggested ways, let's move on to exploring tas...\"],[\"\\u003e\\u003e\\u003e inputs = processor(prompt, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\\u003e\\u003e\\u003e bad_words_ids = processor.tokeniz...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIt is a good idea to include the `bad_words_ids` in the call to `generate` to avoid erro...\"],[\"\\u003e\\u003e\\u003e inputs = processor(prompt, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\\u003e\\u003e\\u003e bad_words_ids = processor.tokeniz...\"],[\"```\\n\\n## Few-shot prompting\\n\\nWhile IDEFICS demonstrates great zero-shot results, your task may requir...\"],[\"Photo by [Juan Mayobre](https:\\u002f\\u002funsplash.com\\u002f@jmayobres).\\n  \\n```py\\n\\u003e\\u003e\\u003e prompt = [\\\"User:\\\",\\n...       ...\"],[\"\\u003e\\u003e\\u003e generated_ids = model.generate(**inputs, max_new_tokens=30, bad_words_ids=bad_words_ids)\\n\\u003e\\u003e\\u003e gen...\"],[\"```\\n\\nNotice that just from a single example (i.e., 1-shot) the model has learned how to perform the ...\"],[\"\\u003e\\u003e\\u003e inputs = processor(prompt, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\\u003e\\u003e\\u003e bad_words_ids = processor.tokeniz...\"],[\"```\\n\\n## Image classification\\n\\nIDEFICS is capable of classifying images into different categories wit...\"],[\"\\u003e\\u003e\\u003e generated_ids = model.generate(**inputs, max_new_tokens=6, bad_words_ids=bad_words_ids)\\n\\u003e\\u003e\\u003e gene...\"],[\"\\u003e\\u003e\\u003e inputs = processor(prompt, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\\u003e\\u003e\\u003e bad_words_ids = processor.tokeniz...\"],[\"```\\n\\nLooks like IDEFICS noticed the pumpkin on the doorstep and went with a spooky Halloween story a...\"],[\"## Running inference in batch mode\\n\\nAll of the earlier sections illustrated IDEFICS for a single exa...\"],[\"\\u003e\\u003e\\u003e inputs = processor(prompts, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\\u003e\\u003e\\u003e bad_words_ids = processor.tokeni...\"],[\"```\\n\\n## IDEFICS instruct for conversational use\\n\\nFor conversational use cases, you can find fine-tun...\"],[\"...         \\\"\\\\nUser:\\\",\\n...         \\\"https:\\u002f\\u002fstatic.wikia.nocookie.net\\u002fasterix\\u002fimages\\u002f2\\u002f25\\u002fR22b.gif\\u002fr...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003cb\\u003eEnglish\\u003c\\u002fb\\u003e |\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"\\u003ch3 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003eState-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\\u003c\\u002fp\\u003e\\n\\u003c\\u002fh...\"],[\"## Online demos\\n\\nYou can test most of our models directly on their pages from the [model hub](https:...\"],[\"In Natural Language Processing:\\n- [Masked word completion with BERT](https:\\u002f\\u002fhuggingface.co\\u002fbert-bas...\"],[\"- [Natural Language Inference with RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+dog+w...\"],[\"- [Question answering with...\"],[\"- [Translation with T5](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin...\"],[\"In Computer Vision:\\n- [Image classification with ViT](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-patch16...\"],[\"In Multimodal tasks:\\n- [Table Question Answering with TAPAS](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002ftapas-bas...\"],[\"## Quick tour\\n\\nTo immediately use a model on a given input (text, image, audio, ...), we provide the...\"],[\"```\\n\\nThe second line of code downloads and caches the pretrained model used by the pipeline, while t...\"],[\"# Allocate a pipeline for object detection\\n\\u003e\\u003e\\u003e object_detector = pipeline('object-detection')\\n\\u003e\\u003e\\u003e ob...\"],[\"```\\n\\nHere, we get a list of objects detected in the image, with a box surrounding the object and a c...\"],[\"```\\n\\nThe tokenizer is responsible for all the preprocessing the pretrained model expects and can be ...\"],[\"1. Easily customize a model or an example to your needs:\\n    - We provide examples for each architec...\"],[\"First, create a virtual environment with the version of Python you're going to use and activate it.\\n...\"],[\"```\\n\\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait fo...\"],[\"```\\n\\nFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with co...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BARTpho](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbartpho)** (from VinAI Research) r...\"],[\"1. **[CLAP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclap)** (from LAION-AI) released with...\"],[\"1. **[CodeGen](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcodegen)** (from Salesforce) relea...\"],[\"1. **[ConvBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconvbert)** (from YituTech) relea...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (from Tsinghua University) rele...\"],[\"1. **[Data2Vec](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdata2vec)** (from Facebook) relea...\"],[\"1. **[Deformable DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeformable_detr)** (from S...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (from Facebook) released with...\"],[\"1. **[DINOv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdinov2)** (from Meta AI) released w...\"],[\"1. **[DiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdit)** (from Microsoft Research) relea...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (from S...\"],[\"1. **[ERNIE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie)** (from Baidu) released with ...\"],[\"1. **[ESM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fesm)** (from Meta AI) are transformer ...\"],[\"1. **[FlauBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflaubert)** (from CNRS) released ...\"],[\"1. **[Funnel Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffunnel)** (from CMU\\u002fGoo...\"],[\"1. **[GPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopenai-gpt)** (from OpenAI) released w...\"],[\"1. **[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt2)** (from OpenAI) released with ...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (from BigCode) r...\"],[\"1. **[GroupViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgroupvit)** (from UCSD, NVIDIA) r...\"],[\"1. **[IDEFICS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fidefics)** (from HuggingFace) rele...\"],[\"1. **[Jukebox](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fjukebox)** (from OpenAI) released ...\"],[\"1. **[LayoutLMv3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv3)** (from Microsoft R...\"],[\"1. **[LiLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flilt)** (from South China University ...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (from The FAIR team of Me...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (from Microsoft Research & ...\"],[\"1. **[LXMERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flxmert)** (from UNC Chapel Hill) re...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[MaskFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmaskformer)** (from Meta and UI...\"],[\"1. **[MEGA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmega)** (from Meta\\u002fUSC\\u002fCMU\\u002fSJTU) rele...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MMS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmms)** (from Facebook) released with t...\"],[\"1. **[MobileViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilevit)** (from Apple) releas...\"],[\"1. **[MT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmt5)** (from Google AI) released with ...\"],[\"1. **[Nezha](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnezha)** (from Huawei Noah’s Ark Lab...\"],[\"1. **[Nyströmformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnystromformer)** (from the U...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (from Google AI) release...\"],[\"1. **[Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus)** (from Google) released ...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft) released with ...\"],[\"1. **[PLBart](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fplbart)** (from UCLA NLP) released ...\"],[\"1. **[PVT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpvt)** (from Nanjing University, The U...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (from Google Research...\"],[\"1. **[RoBERTa-PreLayerNorm](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta-prelayernorm)...\"],[\"1. **[SeamlessM4T](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fseamless_m4t)** (from Meta AI)...\"],[\"1. **[SEW](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsew)** (from ASAPP) released with the ...\"],[\"1. **[SpeechToTextTransformer2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text_2)...\"],[\"1. **[Swin Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswin)** (from Microsoft) ...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (from Google AI) released with th...\"],[\"1. **[TAPEX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftapex)** (from Microsoft Research) r...\"],[\"1. **[TrOCR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftrocr)** (from Microsoft), released ...\"],[\"1. **[UMT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fumt5)** (from Google Research) releas...\"],[\"1. **[UPerNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fupernet)** (from Peking University...\"],[\"1. **[VipLlava](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvipllava)** (from University of W...\"],[\"1. **[ViT Hybrid](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_hybrid)** (from Google AI) ...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (from Faceb...\"],[\"To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated to...\"],[\"## Citation\\n\\nWe now have a [paper](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f) you can cit...\"],[\"CodeParrot 🦜\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flvwerra\\u002frepo-images\\u002fra...\"],[\"```\\n\\nAdditionally, sure you have git-lfs installed. You can find instructions for how to install it ...\"],[\"- exact deduplication using each file's hash after having removed whistespaces.\\n- near deduplication...\"],[\"```\\nDuring preprocessing the dataset is downloaded and stored locally as well as caches of the compu...\"],[\"```\\nThis will initialize a new model with the architecture and configuration of `gpt2-large` and use...\"],[\"```\\n\\nRecall that you can see the full set of possible options with descriptions (for all scripts) by...\"],[\"```\\nIn addition we evaluate the model on OpenAI's _HumanEval_ benchmark. You can run the evaluation ...\"],[\"```\\n\\nThe results as well as reference values are shown in the following table:\\n\\n| Model | pass@1 | p...\"],[\"## Training with Megatron\\n[Megatron](https:\\u002f\\u002fgithub.com\\u002fNVIDIA\\u002fMegatron-LM) is a framework developed...\"],[\"```\\n\\nYou also need to add the vocabulary file and merges table of the tokenizer that you trained on ...\"],[\"```\\nThis outputs two files `codeparrot_content_document.idx` and `codeparrot_content_document.bin` w...\"],[\"### Training\\nYou can configure the model architecture and training parameters as shown below, or put...\"],[\"--lr-warmup-iters 2000\\n--weight-decay .1\\n--adam-beta2 .999\\n--fp16\\n--log-interval 10\\n--save-interval ...\"],[\"```\\nThe training takes almost 12 hours in this setting.\\n\\n### Convert model to `transformers`\\nAfter t...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Check more detailed information for [Auto Mixed Precision](https:\\u002f\\u002fintel.github.io\\u002fintel-extension-f...\"],[\"```\\npip install intel_extension_for_pytorch==\\u003cversion_name\\u003e -f https:\\u002f\\u002fdeveloper.intel.com\\u002fipex-whl-...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nTo print summary statistics for the GPU utilization and the training run with the [`Trainer`] w...\"],[\"```\\n\\nWe see that the kernels alone take up 1.3GB of GPU memory. Now let's see how much space the mod...\"],[\"```\\n\\n```bash\\nTue Jan 11 08:58:05 2022\\n+-------------------------------------------------------------...\"],[\"+-----------------------------------------------------------------------------+\\n| Processes:        ...\"],[\"```\\n\\nWe get the same number as before and you can also see that we are using a V100 GPU with 16GB of...\"],[\"```\\n\\nWe see that already a relatively small batch size almost fills up our GPU's entire memory. Howe...\"],[\"1. model weights\\n2. optimizer states\\n3. gradients\\n4. forward activations saved for gradient computat...\"],[\"**Functionality-specific memory**\\n\\nThen, your software could have special memory needs. For example,...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[ALBERT](..\\u002fmodel_doc\\u002falbert), [BART](..\\u002fmodel_doc\\u002fbart), [BERT](..\\u002fmodel_doc\\u002fbert), [BigBird](..\\u002fmo...\"],[\"[FNet](..\\u002fmodel_doc\\u002ffnet), [Funnel Transformer](..\\u002fmodel_doc\\u002ffunnel), [GPT-Sw3](..\\u002fmodel_doc\\u002fgpt-sw3...\"],[\"[Megatron-BERT](..\\u002fmodel_doc\\u002fmegatron-bert), [Mistral](..\\u002fmodel_doc\\u002fmistral), [Mixtral](..\\u002fmodel_doc...\"],[\"[RoBERTa-PreLayerNorm](..\\u002fmodel_doc\\u002froberta-prelayernorm), [RoCBert](..\\u002fmodel_doc\\u002froc_bert), [RoForm...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nThen take a look at an example:\\n\\n```py\\n\\u003e\\u003e\\u003e imdb[\\\"test\\\"][0]\\n{\\n    \\\"label\\\": 0,\\n    \\\"text\\\": \\\"I lov...\"],[\"```\\n\\nCreate a preprocessing function to tokenize `text` and truncate sequences to be no longer than ...\"],[\"```\\n\\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule....\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n[`Trainer`] applies dynamic padding by default when you pass `tokenizer` to it. In this ...\"],[\"```\\n\\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.pr...\"],[\"```\\n\\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\\n\\n```...\"],[\"```\\n\\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nTokenize the text and return TensorFlow tensors:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers import ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n *The design choices in the Transformer attention mec...\"],[\"## MegaConfig\\n\\n[[autodoc]] MegaConfig\\n\\n## MegaModel\\n\\n[[autodoc]] MegaModel\\n    - forward\\n\\n## MegaFor...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Overview\\n\\nThe XLM-ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequ...\"],[\"The Authors' code can be found [here](https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fProphetNet).\\n\\n## Resources\\n\\n- [Ca...\"],[\"!--Copyright 2022 NVIDIA and The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache L...\"],[\"The abstract from the paper is the following:\\n\\n*Grouping and recognition are important components of...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"## TFGroupViTVisionModel\\n\\n[[autodoc]] TFGroupViTVisionModel\\n    - call\\n\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nThis will create a `imagenette2` dir with two subdirectories `train` and `val` each with multip...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## PreTrainedModel\\n\\n[[autodoc]] PreTrainedModel\\n    - push_to_hub\\n    - all\\n\\n\\u003ca id='from_pretrained-...\"],[\"```\\n\\nMoreover, you can directly place the model on different devices if it doesn't fully fit in RAM ...\"],[\"```\\n\\nYou can inspect how the model was split across devices by looking at its `hf_device_map` attrib...\"],[\"```\\n\\nYou can also write your own device map following the same format (a dictionary layer name to de...\"],[\"```\\n\\nDue to Pytorch design, this functionality is only available for floating dtypes.\\n\\n\\n## ModuleUti...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The \\\"Roaring 20s\\\" of visual recognition began with t...\"],[\"\\u003csmall\\u003e ConvNeXT architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2201.03545\\\"\\u003eoriginal pa...\"],[\"## ConvNextFeatureExtractor\\n\\n[[autodoc]] ConvNextFeatureExtractor\\n\\n## ConvNextImageProcessor\\n\\n[[auto...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"In this example we will use the vision model from [CLIP](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=clip)\\n...\"],[\"```\\nhuggingface-cli repo create clip-roberta-base\\n```\\nNext we clone the model repository to add the ...\"],[\"```\\n\\nIf the checkpoints are in PyTorch then one could pass `text_from_pt=True` and `vision_from_pt=T...\"],[\"```\\n\\n### Prepare dataset files and split the dataset.\\n\\n```python\\nimport json\\nimport collections\\n\\nima...\"],[\"```\\n\\n\\u003e Note: The data loading and processing part of this script can still be improved for maximum p...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Vision-and-Language Pre-training (VLP) has improved ...\"],[\"## Usage tips\\n\\n- The quickest way to get started with ViLT is by checking the [example notebooks](ht...\"],[\"## ViltForMaskedLM\\n\\n[[autodoc]] ViltForMaskedLM\\n    - forward\\n\\n## ViltForQuestionAnswering\\n\\n[[autodo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Zero-shot image classification pipeline\\n\\nThe simplest way to try out inference with a model ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\nLet's take a different image to switch things up.\\n\\n```py\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e import r...\"],[\"```\\n\\nPass the inputs through the model, and post-process the results:\\n\\n```py\\n\\u003e\\u003e\\u003e import torch\\n\\n\\u003e\\u003e\\u003e w...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The recently-proposed Perceiver model obtains good r...\"],[\"Internally, [`PerceiverModel`] will create the latents, which is a tensor of shape `(batch_size, num...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fperceiver_ar...\"],[\"## Perceiver specific outputs\\n\\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverModelOutput\\n...\"],[\"## PerceiverProjectionDecoder\\n\\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverProjectionDe...\"],[\"## PerceiverForMaskedLM\\n\\n[[autodoc]] PerceiverForMaskedLM\\n    - forward\\n\\n## PerceiverForSequenceClas...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Development of the model was led by [Shinya Otani](https:\\u002f\\u002fgithub.com\\u002fSO0529), [Takayoshi Makabe](ht...\"],[\"\\u003e\\u003e\\u003e print(gen_text)\\n人とAIが協調するためには、AIと人が共存し、AIを正しく理解する必要があります。...\"],[\"```\\n\\n## Resources\\n\\n- [Causal language modeling task guide](..\\u002ftasks\\u002flanguage_modeling)\\n\\n## GPTNeoXJa...\"],[\"# Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pled...\"],[\"## Enforcement\\n\\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\\nreported ...\"],[\"**Consequence**: A permanent ban from any sort of public interaction within the\\ncommunity.\\n\\n## Attri...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"For custom datasets in `jsonlines` format please see: https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002floading_d...\"],[\"```\\n\\nOnly T5 models `t5-small`, `t5-base`, `t5-large`, `t5-3b` and `t5-11b` must use an additional a...\"],[\"```\\n\\nThe task of summarization supports custom CSV and JSONLINES formats.\\n\\n#### Custom CSV Files\\n\\nIf...\"],[\"```\\n\\nand you wanted to select only `text` and `summary`, then you'd pass these additional arguments:...\"],[\"```\\n\\n## With Accelerate\\n\\nBased on the script [`run_summarization_no_trainer.py`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"```\\n\\nand reply to the questions asked. Then\\n\\n```bash\\naccelerate test\\n```\\n\\nthat will check everything...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Many real-world applications require the prediction ...\"],[\"- Check out the Informer blog-post in HuggingFace blog: [Multivariate Probabilistic Time Series Fore...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nHaving downloaded COCO dataset manually you should be able to load with the `ydshieh\\u002fcoc_datase...\"],[\"```\\n\\n### Create a model from a vision encoder model and a text encoder model\\nWe can either load a CL...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"However, if the preferred batch size fits into memory, there's no reason to apply memory-optimizing ...\"],[\"Note: when using mixed precision with a small model and a large batch size, there will be some memor...\"],[\"[Tensor Core Requirements](https:\\u002f\\u002fdocs.nvidia.com\\u002fdeeplearning\\u002fperformance\\u002fdl-performance-matrix-mu...\"],[\"```\\n\\nIn the above example, your effective batch size becomes 4. \\n\\nAlternatively, use 🤗 Accelerate to...\"],[\"**Gradient checkpointing** offers a compromise between these two approaches and saves strategically ...\"],[\"```\\n\\nAlternatively, use 🤗 Accelerate - find the 🤗 Accelerate example [further in this guide](#using-...\"],[\"```\\n\\nIf you prefer to use 🤗 Accelerate, find the 🤗 Accelerate example [further in this guide](#using...\"],[\"```\\nimport torch\\ntorch.backends.cuda.matmul.allow_tf32 = True\\ntorch.backends.cudnn.allow_tf32 = True...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\ntf32 can't be accessed directly via `tensor.to(dtype=torch.tf32)` because it is an inter...\"],[\"[`Trainer`] integrates a variety of optimizers that can be used out of box: `adamw_hf`, `adamw_torch...\"],[\"```\\n\\nCombined with other approaches (gradient accumulation, gradient checkpointing, and mixed precis...\"],[\"```\\n\\nHowever, we can also use a third-party implementation of the 8-bit optimizer for demonstration ...\"],[\"optimizer_kwargs = {\\n    \\\"betas\\\": (training_args.adam_beta1, training_args.adam_beta2),\\n    \\\"eps\\\": t...\"],[\"```\\n\\nFinally, pass the custom optimizer as an argument to the `Trainer`:\\n\\n```py\\ntrainer = Trainer(mo...\"],[\"```\\n\\nCombined with other approaches (gradient accumulation, gradient checkpointing, and mixed precis...\"],[\"DeepSpeed is an open-source deep learning optimization library that is integrated with 🤗 Transformer...\"],[\"```\\n\\n`torch.compile` uses Python's frame evaluation API to automatically create a graph from existin...\"],[\"**Training & inference backends**:\\n* `dynamo.optimize(\\\"inductor\\\")` - Uses TorchInductor backend with...\"],[\"**Inference-only backend**s:\\n* `dynamo.optimize(\\\"ofi\\\")` -  Uses Torchscript optimize_for_inference. ...\"],[\"```\\n\\nThe full example training loop with 🤗 Accelerate is only a handful of lines of code long:\\n\\n```p...\"],[\"```\\n\\nFirst we wrap the dataset in a [`DataLoader`](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fdata.html#torch.u...\"],[\"At times, additional efforts may be required to pre-build some components. For instance, if you're u...\"],[\"![MoE Transformer 2x block](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve...\"],[\"And for Pytorch DeepSpeed has built one as well: [DeepSpeed-MoE: Advancing Mixture-of-Experts Infere...\"],[\"```\\n\\nOnce converted, train the model as usual.\\n\\n\\u003cTip warning={true}\\u003e\\n\\nThe PyTorch-native `scaled_dot...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## Motivation\\nWithout processing, english-\\u003e romanian mbart-large-en-ro gets BLEU score 26.8 on the W...\"],[\"```\\n\\n(2) define a function for post processing.\\n It removes diacritics and does other things I don't...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"In particular all \\\"Please explain\\\" questions or objectively very user-specific feature requests belo...\"],[\"```\\n    \\\"huggingface\\\" \\\"transformers\\\" your query\\n    ```\\n\\n    The first two quoted words tell Google ...\"],[\"```\\n\\n    And now we can use it to do the searching on your favorite search engine:\\n\\n    1. first for...\"],[\"```\\n\\n   then you'd search for `\\\"ValueError\\\" \\\"cannot be found\\\"`\\n\\n   As you search you will notice tha...\"],[\"```\\n   ```\\n   git clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\n   cd transformers\\n   pip instal...\"],[\"```\\n\\n   which would result in the following entry, which can be opened if desired, but otherwise tak...\"],[\"7. If you forked off some of this project's code or example applications, please, do not ask us to g...\"],[\"We currently don't have a triage service and we trust your capacity to identify the right domain and...\"],[\"For example, the very first comment is the most important one. If while the thread unfolds you reali...\"],[\"13. If you are replying to a last comment, it's totally fine to make your reply with just your comme...\"],[\"```\\n    \\u003e How big is your gpu cluster?\\n\\n    Our cluster is made of 256 gpus.\\n    ```\\n\\n    If you are...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*With the success of language pretraining, it is high...\"],[\"## Usage tips\\n\\n- Since Funnel Transformer uses pooling, the sequence length of the hidden states cha...\"],[\"## FunnelConfig\\n\\n[[autodoc]] FunnelConfig\\n\\n## FunnelTokenizer\\n\\n[[autodoc]] FunnelTokenizer\\n    - bui...\"],[\"[[autodoc]] TFFunnelBaseModel\\n    - call\\n\\n## TFFunnelModel\\n\\n[[autodoc]] TFFunnelModel\\n    - call\\n\\n##...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nor for an editable install:\\n\\n```bash\\npip install -e .[quality]\\n```\\n\\n\\n## Tests\\n\\nAll the jobs tha...\"],[\"```\\n\\nJust in case anything slipped through the cracks, the full test suite is also run daily.\\n\\n## Do...\"],[\"```\\n\\nThis can take a lot of time, so to run the same thing on only the files you modified in the cur...\"],[\"```\\n\\nAdditional checks concern PRs that add new models, mainly that:\\n\\n- All models added are in an A...\"],[\"```\\n\\nNote that instead of applying this to a whole class, you can apply it to the relevant methods t...\"],[\"```\\n\\nNote that there shouldn't be any spaces around the arrow (unless that space is part of the patt...\"],[\"```\\n\\nIn this case, the code is copied from `BertForSequenceClassification` by replacing:\\n- `Bert` by...\"],[\"Movement Pruning: Adaptive Sparsity by Fine-Tuning\\n\\nAuthor: @VictorSanh\\n\\n*Magnitude pruning is a wid...\"],[\"| Fine-pruning+Distillation\\u003cbr\\u003e(Teacher=BERT-base fine-tuned) | BERT base\\u003cbr\\u003efine-tuned | Remaining\\u003c...\"],[\"For more information, we invite you to check out [our paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2005.07683).\\nYou ...\"],[\"While movement pruning does not directly optimize for memory footprint (but rather the number of non...\"],[\"As examples, we release two English PruneBERT checkpoints (models fine-pruned from a pre-trained `BE...\"],[\"Note that we built our experiments on top of a stabilized version of the library (commit https:\\u002f\\u002fgit...\"],[\"```\\n\\n### Fine-pruning with other methods\\n\\nWe can also explore other fine-pruning methods by changing...\"],[\"```\\n\\nL0 regularization\\n```bash\\npython examples\\u002fmovement-pruning\\u002fmasked_run_squad.py \\\\\\n    --output_d...\"],[\"```\\n\\n### After fine-pruning\\n\\n**Counting parameters**\\n\\nRegularization based pruning methods (soft mov...\"],[\"```\\n@article{sanh2020movement,\\n    title={Movement Pruning: Adaptive Sparsity by Fine-Tuning},\\n    a...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e\\n    \\u003ciframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https:\\u002f\\u002fwww.youtube.com\\u002fembed\\u002fH39Z_72...\"],[\"### Encoder[[cv-encoder]]\\n\\nThe [Vision Transformer (ViT)](model_doc\\u002fvit) opened the door to computer...\"],[\"### Decoder[[cv-decoder]]\\n\\nDecoder-only vision models are rare because most vision models rely on an...\"],[\"### Encoder[[nlp-encoder]]\\n\\n[BERT](model_doc\\u002fbert) is an encoder-only Transformer that randomly mask...\"],[\"### Decoder[[nlp-decoder]]\\n\\n[GPT-2](model_doc\\u002fgpt2) is a decoder-only Transformer that predicts the ...\"],[\"### Encoder-decoder[[nlp-encoder-decoder]]\\n\\n[BART](model_doc\\u002fbart) keeps the original Transformer ar...\"],[\"### Encoder[[audio-encoder]]\\n\\n[Wav2Vec2](model_doc\\u002fwav2vec2) uses a Transformer encoder to learn spe...\"],[\"## Multimodal\\n\\n\\u003ciframe style=\\\"border: 1px solid rgba(0, 0, 0, 0.1);\\\" width=\\\"1000\\\" height=\\\"450\\\" src=\\\"...\"],[\"[CLIP](model_doc\\u002fclip) takes a different approach and makes a pair prediction of (`image`, `text`) ....\"],[\"## Reinforcement learning\\n\\n\\u003ciframe style=\\\"border: 1px solid rgba(0, 0, 0, 0.1);\\\" width=\\\"1000\\\" height...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"```\\n\\nIf you want to try out the brand new features, you might be interested in installing the librar...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nYou can load a PEFT adapter with either an `AutoModelFor` class or the base model class ...\"],[\"```\\n\\n## Add a new adapter\\n\\nYou can use [`~peft.PeftModel.add_adapter`] to add a new adapter to a mod...\"],[\"```\\n\\n## Enable and disable adapters\\n\\nOnce you've added an adapter to a model, you can enable or disa...\"],[\"```\\n\\nTo disable the adapter module:\\n\\n```py\\nmodel.disable_adapters()\\noutput = model.generate(**inputs...\"],[\"```\\n\\n## Add additional trainable layers to a PEFT adapter\\n\\nYou can also fine-tune additional trainab...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [jegormeister](https:\\u002f\\u002fhuggingface.co\\u002fjegormeister). The original code...\"],[\"!--Copyright 2023 Mistral AI and The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apac...\"],[\"Both `Mistral-7B-v0.1` and `Mistral-7B-Instruct-v0.1` are released under the Apache 2.0 license.\\n\\n##...\"],[\"```\\n\\nRaw weights for `Mistral-7B-v0.1` and `Mistral-7B-Instruct-v0.1` can be downloaded from:\\n\\n| Mod...\"],[\"```\\n\\n## Combining Mistral and Flash Attention 2\\n\\nFirst, make sure to install the latest version of F...\"],[\"```\\n\\n### Expected speedups\\n\\nBelow is a expected speedup diagram that compares pure inference time be...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Previous approaches preprocessed the audio to extract useful features from it. It is now more common...\"],[\"```\\n\\n### Automatic speech recognition\\n\\nAutomatic speech recognition (ASR) transcribes speech into te...\"],[\"```\\n\\n## Computer vision\\n\\nOne of the first and earliest successful computer vision tasks was recogniz...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n\\u003e\\u003e\\u003e classifier = pipeline(task=\\\"image-classification\\\")\\n...\"],[\"```\\n\\n### Object detection\\n\\nUnlike image classification, object detection identifies multiple objects...\"],[\"```\\n\\n### Image segmentation\\n\\nImage segmentation is a pixel-level task that assigns every pixel in an...\"],[\"```\\n\\n### Depth estimation\\n\\nDepth estimation predicts the distance of each pixel in an image from the...\"],[\"```\\n\\n## Natural language processing\\n\\nNLP tasks are among the most common types of tasks because text...\"],[\"```\\n\\n### Token classification\\n\\nIn any NLP task, text is preprocessed by separating the sequence of t...\"],[\"\\u003e\\u003e\\u003e classifier = pipeline(task=\\\"ner\\\")\\n\\u003e\\u003e\\u003e preds = classifier(\\\"Hugging Face is a French company based...\"],[\"{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}\\n{'entity': '...\"],[\"```\\n\\n### Question answering\\n\\nQuestion answering is another token-level task that returns an answer t...\"],[\"```\\n\\n### Summarization\\n\\nSummarization creates a shorter version of a text from a longer one while tr...\"],[\"```\\n\\n### Translation\\n\\nTranslation converts a sequence of text in one language to another. It is impo...\"],[\"```\\n\\n* masked: the model's objective is to predict a masked token in a sequence with full access to ...\"],[\"```\\n\\n## Multimodal\\n\\nMultimodal tasks require a model to process multiple data modalities (text, imag...\"],[\"```\\n\\nHopefully, this page has given you some more background information about all the types of task...\"],[\"!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\\n\\nLicensed under ...\"],[\"The abstract from the paper is the following:\\n\\n*Recent work in language modeling demonstrates that t...\"],[\"## Usage tips\\n\\nWe have provided pretrained [GPT2-345M](https:\\u002f\\u002fngc.nvidia.com\\u002fcatalog\\u002fmodels\\u002fnvidia:...\"],[\"```\\n\\nOnce you have obtained the checkpoint from NVIDIA GPU Cloud (NGC), you have to convert it to a ...\"],[\"Summarization (Seq2Seq model) training examples\\n\\nThe following example showcases how to finetune a s...\"],[\"```\\n\\nThis should finish in 37min, with validation loss and ROUGE2 score of 1.7785 and 17.01 respecti...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The `generate()` method can be used to generate text using GPTSAN-Japanese model.\\n\\n```python\\n\\u003e\\u003e\\u003e fro...\"],[\"```\\n\\n## GPTSAN Features\\n\\nGPTSAN has some unique features. It has a model structure of Prefix-LM. It ...\"],[\"\\u003e\\u003e\\u003e x_token = tokenizer(\\\"\\\", prefix_text=\\\"ｱｲｳｴ\\\")\\ninput_ids:      | SOT | ｱ | ｲ | ｳ | ｴ | SEG |\\ntoken_...\"],[\"## GPTSanJapaneseModel\\n\\n[[autodoc]] GPTSanJapaneseModel\\n\\n## GPTSanJapaneseForConditionalGeneration\\n\\n...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Multimodal pre-training with text, layout, and image...\"],[\"## Usage: MarkupLMProcessor\\n\\nThe easiest way to prepare data for the model is to use [`MarkupLMProce...\"],[\"```\\n\\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import MarkupLMProcessor\\n\\n\\u003e\\u003e\\u003e processor = MarkupLMProcessor.from_pre...\"],[\"```\\n\\n**Use case 2: web page classification (training, inference) + token classification (inference),...\"],[\"```\\n\\n**Use case 3: token classification (training), parse_html=False**\\n\\nFor token classification tas...\"],[\"```\\n\\n**Use case 4: web page question answering (inference), parse_html=True**\\n\\nFor question answerin...\"],[\"```\\n\\n**Use case 5: web page question answering (inference), parse_html=False**\\n\\nFor question answeri...\"],[\"```\\n\\n## Resources\\n\\n- [Demo notebooks](https:\\u002f\\u002fgithub.com\\u002fNielsRogge\\u002fTransformers-Tutorials\\u002ftree\\u002fmast...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n- The implementation is the same as [Roberta](roberta) except instead of using _Add a...\"],[\"[[autodoc]] RobertaPreLayerNormForMultipleChoice\\n    - forward\\n\\n## RobertaPreLayerNormForTokenClassi...\"],[\"[[autodoc]] FlaxRobertaPreLayerNormModel\\n    - __call__\\n\\n## FlaxRobertaPreLayerNormForCausalLM\\n\\n[[au...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### Summary\\nIn Phi-1 and Phi-1.5 papers, the authors showed how important the quality of the data is...\"],[\"The abstract from the Phi-1.5 paper is the following:\\n\\n*We continue the investigation into the power...\"],[\"### Example :\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import PhiForCausalLM, AutoTokenizer\\n\\n\\u003e\\u003e\\u003e # define th...\"],[\"```\\n\\n\\n## Combining Phi and Flash Attention 2\\n\\nFirst, make sure to install the latest version of Flas...\"],[\"```\\n\\n### Expected speedups\\nBelow is an expected speedup diagram that compares pure inference time be...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n{\\\"sentence1\\\": \\\"COVID-19 vaccine updates: How is the rollout proceeding?\\\", \\\"label\\\": \\\"news\\\"}\\n{\\\"sen...\"],[\"```\\npython run_text_classification.py \\\\\\n--model_name_or_path distilbert-base-cased \\\\\\n--train_file tr...\"],[\"```\\n\\n## run_glue.py\\n\\nThis script handles training on the GLUE dataset for various text classificatio...\"],[\"```\\npython run_glue.py \\\\\\n--model_name_or_path distilbert-base-cased \\\\\\n--task_name mnli \\\\\\n--do_train ...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*State-of-the-art computer vision systems are trained...\"],[\"To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-ove...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help ...\"],[\"**Image retrieval**\\n\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1bLVwVKpAndpEDHqjzxVPr_9...\"],[\"## CLIPConfig\\n\\n[[autodoc]] CLIPConfig\\n    - from_text_vision_configs\\n\\n## CLIPTextConfig\\n\\n[[autodoc]]...\"],[\"## TFCLIPModel\\n\\n[[autodoc]] TFCLIPModel\\n    - call\\n    - get_text_features\\n    - get_image_features\\n...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"For something slightly more challenging, you can also take a look at the [Good Second Issue](https:\\u002f...\"],[\"To get the OS and software versions automatically, run the following command:\\n\\n```bash\\ntransformers-...\"],[\"```\\n\\nYou can also run the same command from the root of the repository:\\n\\n```bash\\npython src\\u002ftransfor...\"],[\"```\\n\\n### Do you want a new feature?\\n\\nIf there is a new feature you'd like to see in 🤗 Transformers, ...\"],[\"## Do you want to add documentation?\\n\\nWe're always looking for improvements to the documentation tha...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n   ```bash\\n   git checkout -b a-descr...\"],[\"```\\n\\n   🤗 Transformers also uses `ruff` and a few custom scripts to check for coding mistakes. Quali...\"],[\"```\\n\\n   If you've already opened a pull request, you'll need to force push with the `--force` flag. ...\"],[\"### Pull request checklist\\n\\n☐ The pull request title should summarize your contribution.\\u003cbr\\u003e\\n☐ If yo...\"],[\"☐ All public methods must have informative docstrings (see\\n[`modeling_bert.py`](https:\\u002f\\u002fgithub.com\\u002fh...\"],[\"```\\n\\nSimilarly, for the `examples` directory, specify a *path to a subfolder or test file* to run th...\"],[\"```\\n\\nLike the slow tests, there are other environment variables available which not enabled by defau...\"],[\"```\\n\\nOne way to run the `make` command on Windows is with MSYS2:\\n\\n1. [Download MSYS2](https:\\u002f\\u002fwww.ms...\"],[\"!--Copyright 2023 The Intel Labs Team Authors, The Microsoft Research Team Authors and HuggingFace I...\"],[\"This paper has been accepted to the [AAAI'23](https:\\u002f\\u002faaai.org\\u002fConferences\\u002fAAAI-23\\u002f) conference. \\n\\nT...\"],[\"\\u003csmall\\u003e BridgeTower architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2206.08657\\\"\\u003eoriginal...\"],[\"\\u003e\\u003e\\u003e processor = BridgeTowerProcessor.from_pretrained(\\\"BridgeTower\\u002fbridgetower-large-itm-mlm-itc\\\")\\n\\u003e\\u003e...\"],[\"```\\n\\nThe following example shows how to run image-text retrieval using [`BridgeTowerProcessor`] and ...\"],[\"```\\n\\nThe following example shows how to run masked language modeling using [`BridgeTowerProcessor`] ...\"],[\"```\\n\\nTips:\\n\\n- This implementation of BridgeTower uses [`RobertaTokenizer`] to generate text embeddin...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2021 NVIDIA Corporation. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"```\\n\\nIn the container:\\n```\\ncd transformers\\u002fexamples\\u002fresearch_projects\\u002fquantization-qdqbert\\u002f\\n```\\n\\n## ...\"],[\"```\\n\\nUse `--recalibrate-weights` to calibrate the weight ranges according to the quantizer axis. Use...\"],[\"```\\npython3 ..\\u002f..\\u002fpytorch\\u002fquestion-answering\\u002frun_qa.py \\\\\\n  --model_name_or_path bert-base-uncased \\\\\\n...\"],[\"```\\n\\n### Evaluate the INT8 PTQ ONNX model inference with TensorRT\\n\\n```\\npython3 evaluate-hf-trt-qa.py...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Contrastive learning has shown remarkable success in...\"],[\"[[autodoc]] ClapFeatureExtractor\\n\\n## ClapProcessor\\n\\n[[autodoc]] ClapProcessor\\n\\n## ClapModel\\n\\n[[autod...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### XLM with language embeddings\\n\\nThe following XLM models use language embeddings to specify the la...\"],[\"```\\n\\nThe `lang2id` attribute of the tokenizer displays this model's languages and their ids:\\n\\n```py\\n...\"],[\"```\\n\\nThe [run_generation.py](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002fpytorch\\u002f...\"],[\"- `facebook\\u002fm2m100_418M` (Translation)\\n- `facebook\\u002fm2m100_1.2B` (Translation)\\n\\nIn this example, load...\"],[\"```\\n\\nTokenize the text:\\n\\n```py\\n\\u003e\\u003e\\u003e encoded_zh = tokenizer(chinese_text, return_tensors=\\\"pt\\\")\\n```\\n\\nM2...\"],[\"```\\n\\n## MBart\\n\\nThe following MBart models can be used for multilingual translation:\\n\\n- `facebook\\u002fmba...\"],[\"```\\n\\nTokenize the text:\\n\\n```py\\n\\u003e\\u003e\\u003e encoded_en = tokenizer(en_text, return_tensors=\\\"pt\\\")\\n```\\n\\nMBart f...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\n## Generate text\\n\\nA language model trained for [causal language modeling](tasks\\u002flanguage_model...\"],[\"Properly setting up the token selection step and the stopping condition is essential to make your mo...\"],[\"```\\n\\nYou'll notice two flags in the `from_pretrained` call:\\n\\n - `device_map` ensures the model is mo...\"],[\"```\\n\\nFinally, you don't need to do it one sequence at a time! You can batch your inputs, which will ...\"],[\"```\\n\\n### Generated output is too short\\u002flong\\n\\nIf not specified in the [`~generation.GenerationConfig`...\"],[\"```\\n\\n### Incorrect generation mode\\n\\nBy default, and unless specified in the [`~generation.Generation...\"],[\"```\\n\\n### Wrong padding side\\n\\nLLMs are [decoder-only](https:\\u002f\\u002fhuggingface.co\\u002flearn\\u002fnlp-course\\u002fchapter...\"],[\"```\\n\\n### Wrong prompt\\n\\nSome models and tasks expect a certain input prompt format to work properly. ...\"],[\"\\u003e\\u003e\\u003e set_seed(0)\\n\\u003e\\u003e\\u003e messages = [\\n...     {\\n...         \\\"role\\\": \\\"system\\\",\\n...         \\\"content\\\": \\\"You...\"],[\"```\\n\\n## Further resources\\n\\nWhile the autoregressive generation process is relatively straightforward...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## DataCollatorForTokenClassification\\n\\n[[autodoc]] data.data_collator.DataCollatorForTokenClassifica...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We release Code Llama, a family of large language mo...\"],[\"The `Llama2` family models, on which Code Llama is based, were trained using `bfloat16`, but the ori...\"],[\"Here is a sample usage:\\n\\n```bash\\npython src\\u002ftransformers\\u002fmodels\\u002fllama\\u002fconvert_llama_weights_to_hf.py...\"],[\"```\\n\\nNote that executing the script requires enough CPU RAM to host the whole model in float16 preci...\"],[\"```\\n\\nIf you only want the infilled part:\\n```python\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\u003e\\u003e\\u003e import ...\"],[\"```\\n\\nUnder the hood, the tokenizer [automatically splits by `\\u003cFILL_ME\\u003e`](https:\\u002f\\u002fhuggingface.co\\u002fdocs...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nIf you get a terrible BLEU score, make sure that you didn't forget to use the `--source_prefix`...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Illustrating feature maps of the first stage looks like below.\\n\\u003cdiv style=\\\"text-align: center\\\"\\u003e\\n\\u003cimg...\"],[\"```\\n`feature_maps` object now has three feature maps, each can be accessed like below. Say we would ...\"],[\"```\\n\\n`timm` models are also supported in transformers through `TimmBackbone` and `TimmBackboneConfig...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [Arthur Zucker](https:\\u002f\\u002fhuggingface.co\\u002fArthurZ), [Younes Belkada](http...\"],[\"\\u003cPipelineTag pipeline=\\\"text-generation\\\" \\u002f\\u003e\\n\\n- A notebook on [fine-tuning OPT with PEFT, bitsandbytes...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\" \\u002f\\u003e\\n\\n- [Text classification task guide](sequence_classifi...\"],[\"```\\n\\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more ab...\"],[\"```\\n\\n### Expected speedups\\n\\nBelow is an expected speedup diagram that compares pure inference time b...\"],[\"## TFOPTModel\\n\\n[[autodoc]] TFOPTModel\\n    - call\\n\\n## TFOPTForCausalLM\\n\\n[[autodoc]] TFOPTForCausalLM\\n...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Existing work in translation demonstrated the potent...\"],[\"**Supervised Training**\\n\\n```python\\nfrom transformers import M2M100Config, M2M100ForConditionalGenera...\"],[\"```\\n\\n**Generation**\\n\\nM2M100 uses the `eos_token_id` as the `decoder_start_token_id` for generation w...\"],[\"```\\n\\n## Resources\\n\\n- [Translation task guide](..\\u002ftasks\\u002ftranslation)\\n- [Summarization task guide](..\\u002f...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nFLAN-T5 includes the same improvements as T5 version 1.1 (see [here](https:\\u002f\\u002fhuggingface.co\\u002fdoc...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Overview\\n\\nThe RoBERTa model was proposed in [RoBERTa: A Robustly Optimized BERT Pretraining Appro...\"],[\"## Usage tips\\n\\n- This implementation is the same as [`BertModel`] with a tiny embeddings tweak as we...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- A blog on [Getting Started with Sentiment Analysis ...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- [`RobertaForTokenClassification`] is supported by ...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- A blog on [How to train a new language model from scratch usi...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- A blog on [Accelerated Inference with Optimum and Tr...\"],[\"**Multiple choice**\\n- [`RobertaForMultipleChoice`] is supported by this [example script](https:\\u002f\\u002fgit...\"],[\"[[autodoc]] RobertaForSequenceClassification\\n    - forward\\n\\n## RobertaForMultipleChoice\\n\\n[[autodoc]]...\"],[\"[[autodoc]] FlaxRobertaForCausalLM\\n    - __call__\\n\\n## FlaxRobertaForMaskedLM\\n\\n[[autodoc]] FlaxRobert...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"Please discuss on the [forum](https:\\u002f\\u002fdiscuss.huggingface.co\\u002f) or in an [issue](https:\\u002f\\u002fgithub.com\\u002fh...\"],[\"```\\nThen cd in the example folder of your choice and run\\n```bash\\npip install -r requirements.txt...\"],[\"```\\n\\nTo browse the examples corresponding to released versions of 🤗 Transformers, click on the line ...\"],[\"\\u003cdetails\\u003e\\n  \\u003csummary\\u003eExamples for older versions of 🤗 Transformers\\u003c\\u002fsummary\\u003e\\n\\t\\u003cul\\u003e\\n\\t    \\u003cli\\u003e\\u003ca href=...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv4.13.0\\u002fexamples\\\"\\u003ev4.13.0\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003c...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv4.4.2\\u002fexamples\\\"\\u003ev4.4.2\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003cli...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv3.1.0\\u002fexamples\\\"\\u003ev3.1.0\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003cli...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv2.4.0\\u002fexamples\\\"\\u003ev2.4.0\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003cli...\"],[\"Alternatively, you can switch your cloned 🤗 Transformers to a specific version (for instance with v3...\"],[\"```\\nand run the example command as usual afterward.\\n\\n## Running the Examples on Remote Hardware with...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large multimodal models trained on natural documents...\"],[\"## IdeficsImageProcessor\\n\\n[[autodoc]] IdeficsImageProcessor\\n    - preprocess\\n\\n## IdeficsProcessor\\n\\n[...\"],[\"!---\\nCopyright 2022 The Microsoft Inc. and The HuggingFace Inc. Team. All rights reserved.\\n\\nLicensed...\"],[\"### What Questions Can be Answered\\n\\nBenefiting from the powerfulness of generative models, TAPEX can...\"],[\"```bash\\nexport EXP_NAME=wikisql_tapex_base\\n\\npython run_wikisql_with_tapex.py \\\\\\n  --do_train \\\\\\n  --do...\"],[\"```\\n\\n#### TAPEX-Large on WikiSQL\\n\\nHere is how to run the script on the WikiSQL with `tapex-large`:\\n\\u003e...\"],[\"```\\n\\n#### TAPEX-Base on WikiTableQuestions\\n\\nHere is how to run the script on the WikiTableQuestions ...\"],[\"```\\n\\n#### TAPEX-Large on WikiTableQuestions\\n\\nHere is how to run the script on the WikiTableQuestions...\"],[\"```\\n\\n### How to Evaluate TAPEX Fine-tuned Models on TableQA\\n\\nWe provide fine-tuned model weights to ...\"],[\"```\\n\\n## Table Fact Verification Tasks\\n\\n### What is Table Fact Verification\\n\\n![Example](https:\\u002f\\u002ftable...\"],[\"```\\n\\n#### TAPEX-Large on TabFact\\n\\nHere is how to run the script on the TabFact:\\n\\u003e The default hyper-...\"],[\"```\\n\\n### How to Evaluate TAPEX Fine-tuned Models on TableFV\\n\\nWe provide fine-tuned model weights to ...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"🤗 Transformers aporta miles de modelos preentrenados Para realizar tareas en diferentes modalidades ...\"],[\"🤗 Transformers está respaldado por las tres bibliotecas de deep learning más populares — [Jax](https...\"],[\"En procesamiento del lenguaje natural:\\n- [Terminación de palabras enmascaradas con BERT](https:\\u002f\\u002fhug...\"],[\"- [Inferencia del lenguaje natural con RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+d...\"],[\"- [Responder a preguntas con...\"],[\"- [Traducción con T5](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"En visión de ordenador:\\n- [Clasificación de imágenes con ViT](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base...\"],[\"## Si está buscando soporte personalizado del equipo de Hugging Face\\n\\n\\u003ca target=\\\"_blank\\\" href=\\\"https...\"],[\"```\\n\\nLa segunda línea de código descarga y almacena en caché el modelo previamente entrenado que usa...\"],[\"# Allocate a pipeline for object detection\\n\\u003e\\u003e\\u003e object_detector = pipeline('object_detection')\\n\\u003e\\u003e\\u003e ob...\"],[\"```\\n\\nAquí obtenemos una lista de objetos detectados en la imagen, con un cuadro que rodea el objeto ...\"],[\"```\\n\\nY aquí está el código equivalente para TensorFlow:\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoT...\"],[\"```\\n\\nEl tokenizador es responsable de todo el preprocesamiento que espera el modelo preentrenado y s...\"],[\"1. Menores costes de cómputo, menor huella de carbono:\\n    - Los investigadores pueden compartir mod...\"],[\"## ¿Por qué no debería usar transformers?\\n\\n- Esta biblioteca no es una caja de herramientas modular ...\"],[\"Primero, crea un entorno virtual con la versión de Python que vas a usar y actívalo.\\n\\nLuego, deberás...\"],[\"```\\n\\nSi deseas jugar con los ejemplos o necesitas la última versión del código y no puedes esperar a...\"],[\"```\\n\\nSigue las páginas de instalación de Flax, PyTorch o TensorFlow para ver cómo instalarlos con co...\"],[\"1. **[GPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopenai-gpt)** (from OpenAI) released w...\"],[\"1. **[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt2)** (from OpenAI) released with ...\"],[\"1. **[IDEFICS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fidefics)** (from HuggingFace) rele...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (from The FAIR team of Me...\"],[\"1. **[MEGA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmega)** (from Facebook) released with...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MobileViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilevit)** (from Apple) releas...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (from Google AI) release...\"],[\"1. **[Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus)** (from Google) released ...\"],[\"1. **[PLBart](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fplbart)** (from UCLA NLP) released ...\"],[\"1. **[RoBERTa-PreLayerNorm](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta-prelayernorm)...\"],[\"1. **[TAPEX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftapex)** (from Microsoft Research) r...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (from Faceb...\"],[\"Para comprobar si cada modelo tiene una implementación en Flax, PyTorch o TensorFlow, o tiene un tok...\"],[\"## Aprender más\\n\\n| Sección | Descripción |\\n|-|-|\\n| [Documentación](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftrans...\"],[\"## Citación\\n\\nAhora nosotros tenemos un [papel](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Development of the model was led by Sid Black, Stella Biderman and Eric Hallahan, and the model was ...\"],[\"```\\n\\nGPT-NeoX-20B also has a different tokenizer from the one used in GPT-J-6B and GPT-Neo. The new ...\"],[\"```\\n\\n## Using Flash Attention 2\\n\\nFlash Attention 2 is an faster, optimized version of the model.\\n\\n##...\"],[\"```\\n\\n\\n### Expected speedups\\n\\nBelow is an expected speedup diagram that compares pure inference time ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nWhile the main concepts discussed in this guide are likely applicable across frameworks, here...\"],[\"TP is almost always used within a single node. That is TP size \\u003c= GPUs per node.\\n\\n**Case 3: Largest ...\"],[\"### DataParallel vs DistributedDataParallel\\n\\nTo understand the key differences in inter-GPU communic...\"],[\"This is not an exhaustive list of differences between DP and DDP, however, other nuances are out of ...\"],[\"```\\nrm -r \\u002ftmp\\u002ftest-clm; CUDA_VISIBLE_DEVICES=0,1 \\\\\\npython examples\\u002fpytorch\\u002flanguage-modeling\\u002frun_cl...\"],[\"```\\n\\n**DDP w\\u002fo NVlink**\\n\\n```\\nrm -r \\u002ftmp\\u002ftest-clm; NCCL_P2P_DISABLE=1 CUDA_VISIBLE_DEVICES=0,1 \\\\\\ntorc...\"],[\"```\\n\\nHere are the same benchmarking results gathered in a table for convenience:\\n\\n| Type   | NVlink ...\"],[\"```\\n\\nIf we have 3 GPUs, ZeRO-DP splits the model onto 3 GPUs like so:\\n\\n```\\nGPU0:\\nLa | Lb | Lc\\n---|--...\"],[\"```\\n\\nThe inputs are passed without modifications as if they would be processed by the original model...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nWhile reading the literature on this topic you may encounter the following synonyms: Sharded...\"],[\"```\\n===================  ===================\\n|  0 | 1 | 2 | 3  |  |  4 | 5 | 6 | 7  |\\n==============...\"],[\"```\\n\\nIn this example, when data moves from layer 0 to 3, it's no different from regular forward pass...\"],[\"The following illustration from the [GPipe paper](https:\\u002f\\u002fai.googleblog.com\\u002f2019\\u002f03\\u002fintroducing-gpip...\"],[\"Note that this is the same concept as gradient accumulation steps. PyTorch uses `chunks`, while Deep...\"],[\"Pipeline API solutions have been implemented in:\\n- PyTorch\\n- DeepSpeed\\n- Megatron-LM\\n\\nThese come wit...\"],[\"We have not experimented with Varuna and SageMaker but their papers report that they have overcome t...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumen...\"],[\"If we split the weight matrix `A` column-wise across `N` GPUs and perform matrix multiplications `XA...\"],[\"This section is based on the original much more [detailed TP overview](https:\\u002f\\u002fgithub.com\\u002fhuggingfac...\"],[\"## Data Parallelism + Pipeline Parallelism\\n\\nThe following diagram from the DeepSpeed [pipeline tutor...\"],[\"To get an even more efficient training a 3D parallelism is used where PP is combined with TP and DP....\"],[\"When ZeRO-DP is combined with PP (and optionally TP) it typically enables only ZeRO stage 1 (optimiz...\"],[\"## FlexFlow\\n\\n[FlexFlow](https:\\u002f\\u002fgithub.com\\u002fflexflow\\u002fFlexFlow) also solves the parallelization proble...\"],[\"The significance of this framework is that it takes resources like (1) GPU\\u002fTPU\\u002fCPU vs. (2) RAM\\u002fDRAM ...\"],[\"### Number of GPUs\\n\\nFor example, if you have 4 GPUs and you only want to use the first 2:\\n\\n\\u003chfoption...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Accelerate\\\"\\u003e\\n\\nUse `--num_processes` to select how many GPUs to use.\\n\\n...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nAs with any environment variable, they can be exported instead of being a...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### Memory usage and data loading\\n\\nOne thing to note is that all data is loaded into memory in this ...\"],[\"```\\npython run_qa.py \\\\\\n--model_name_or_path distilbert-base-cased \\\\\\n--output_dir output \\\\\\n--dataset_...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In this paper, we design and train a Generative Imag...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [patrickvonplaten](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten). The origi...\"],[\"\\u003e\\u003e\\u003e # train...\\n\\u003e\\u003e\\u003e loss = bert2bert(input_ids=input_ids, decoder_input_ids=labels, labels=labels).lo...\"],[\"```\\n\\nPretrained [`EncoderDecoderModel`] are also directly available in the model hub, e.g.:\\n\\n```pyth...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-trained Language Models (PLMs) have proven to be...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The recently-developed DETR approach applies the tra...\"],[\"## Resources\\n\\n- [Object detection task guide](..\\u002ftasks\\u002fobject_detection)\\n\\n## ConditionalDetrConfig\\n\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Convolutional Neural Networks (ConvNets) are commonl...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nWe now have a tokenizer trained on the files we defined. We can either continue using it in tha...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\n**Note:** This script only works with models that have a fast tokenizer (backed by the [🤗 Token...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```bash\\npython run_audio_classification.py \\\\\\n    --model_name_or_path facebook\\u002fwav2vec2-base \\\\\\n    -...\"],[\"```\\n\\nOn a single V100 GPU (16GB), this script should run in ~14 minutes and yield accuracy of **98.2...\"],[\"```\\n\\nOn 4 V100 GPUs (16GB), this script should run in ~1 hour and yield accuracy of **79.45%**.\\n\\n👀 S...\"],[\"```\\n\\n### Examples\\n\\nThe following table shows a couple of demonstration fine-tuning runs.\\nIt has been...\"],[\"| Dataset | Pretrained Model | # transformer layers | Accuracy on eval | GPU setup | Training time |...\"],[\"| Keyword Spotting | [asapp\\u002fsew-mid-100k](https:\\u002f\\u002fhuggingface.co\\u002fasapp\\u002fsew-mid-100k) | 24 | 0.9757 |...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"*Recent advancements in automatic speech translation have dramatically expanded language coverage, i...\"],[\"model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. Th...\"],[\"one’s voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attenti...\"],[\"naturalness, and expressivity. To ensure that our models can be used safely and responsibly, we impl...\"],[\"a pivotal look at the technical foundation needed to turn the Universal Speech Translator from a sci...\"],[\"## Usage\\n\\nIn the following example, we'll load an Arabic audio sample and an English text sample and...\"],[\"```\\n\\nYou can seamlessly use this model on text or on audio, to generated either translated text or t...\"],[\"```\\n\\nWith basically the same code, I've translated English text and Arabic speech to Russian speech ...\"],[\"```\\n\\nOr you can replace the text-to-text generation snippet with the model dedicated to the T2TT tas...\"],[\"```\\n\\nFeel free to try out [`SeamlessM4Tv2ForSpeechToText`] and [`SeamlessM4Tv2ForTextToSpeech`] as w...\"],[\"### Difference with SeamlessM4T-v1\\n\\nThe architecture of this new version differs from the first in a...\"],[\"This model was contributed by [ylacombe](https:\\u002f\\u002fhuggingface.co\\u002fylacombe). The original code can be ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [patrickvonplaten](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten).\\n\\n## Usage...\"],[\"\\u003cPipelineTag pipeline=\\\"automatic-speech-recognition\\\"\\u002f\\u003e\\n\\n- A blog post on [boosting Wav2Vec2 with n-g...\"],[\"## Wav2Vec2Config\\n\\n[[autodoc]] Wav2Vec2Config\\n\\n## Wav2Vec2CTCTokenizer\\n\\n[[autodoc]] Wav2Vec2CTCToken...\"],[\"\\u003e\\u003e\\u003e # import model, feature extractor, tokenizer\\n\\u003e\\u003e\\u003e model = AutoModelForCTC.from_pretrained(\\\"patric...\"],[\"...     transcription = processor.batch_decode(logits.cpu().numpy(), pool).text\\n...     batch[\\\"trans...\"],[\"```\\n\\n## Wav2Vec2 specific outputs\\n\\n[[autodoc]] models.wav2vec2_with_lm.processing_wav2vec2_with_lm.W...\"],[\"## Wav2Vec2ForPreTraining\\n\\n[[autodoc]] Wav2Vec2ForPreTraining\\n    - forward\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n## TFWav2Ve...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"where \\\\\\\\(\\\\log p_\\\\theta (x_i|x_{\\u003ci})\\\\\\\\) is the log-likelihood of the ith token conditioned on the pre...\"],[\"Instead, the sequence is typically broken into subsequences equal to the model's maximum input size....\"],[\"## Example: Calculating perplexity with GPT-2 in 🤗 Transformers\\n\\nLet's demonstrate this process with...\"],[\"```\\n\\nWe'll load in the WikiText-2 dataset and evaluate the perplexity using a few different sliding-...\"],[\"```\\n\\nWith 🤗 Transformers, we can simply pass the `input_ids` as the `labels` to our model, and the a...\"],[\"```\\n\\nRunning this with the stride length equal to the max input length is equivalent to the suboptim...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"| Task | Example model | Example dataset | 🤗 Datasets | Colab\\n|---|---|---|:---:|:---:|\\n| [**`causal...\"],[\"## Intro: JAX and Flax\\n\\n[JAX](https:\\u002f\\u002fgithub.com\\u002fgoogle\\u002fjax) is a numerical computation library that...\"],[\"Each example README contains more details on the specific model and training\\nprocedure.\\n\\n\\n## Running...\"],[\"To specify a given repository name, use the `--hub_model_id` argument. You will need to specify the ...\"],[\"# 🔥 Model cards now live inside each huggingface.co model repo 🔥\\n\\n\\nFor consistency, ease of use and ...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Dictionary\\n\\nHugging Face: गले लगाओ चेहरा\\ntoken: शब्द (और मूल अंग्रेजी को कोष्ठक में चिह्नित करें）\\nto...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"🤗 Transformers 100 से अधिक भाषाओं में पाठ वर्गीकरण, सूचना निष्कर्षण, प्रश्न उत्तर, सारांशीकरण, अनुवा...\"],[\"🤗 Transformers तीन सबसे लोकप्रिय गहन शिक्षण पुस्तकालयों का समर्थन करता है： [Jax](https:\\u002f\\u002fjax.readthe...\"],[\"यहाँ कुछ उदाहरण हैं：\\n- [शब्द को भरने के लिए मास्क के रूप में BERT का प्रयोग करें](https:\\u002f\\u002fhuggingfac...\"],[\"- [बार्ट के साथ पाठ सारांश](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fbart-large-cnn?text=The+tower+is+324+met...\"],[\"- [डिस्टिलबर्ट के साथ...\"],[\"प्रश्नोत्तर](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+...\"],[\"is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portug...\"],[\"28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazo...\"],[\"2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwou...\"],[\"regenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+fores...\"],[\"af+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C00...\"],[\"000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%2...\"],[\"etres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belong...\"],[\"y+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+...\"],[\"0%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amoun...\"],[\"or+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+dep...\"],[\"s+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+h...\"],[\"s+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse...\"],[\"odiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees...\"],[\"al+trees+divided+into+16%2C000+species)...\"],[\"- [अनुवाद के लिए T5 का प्रयोग करें](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+li...\"],[\"**[Write With Transformer](https:\\u002f\\u002ftransformer.huggingface.co)**，हगिंग फेस टीम द्वारा बनाया गया, यह ...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n# भावना विश्लेषण पाइपलाइन का उपयोग करना\\n\\u003e\\u003e\\u003e classif...\"],[\"```\\n\\nकोड की दूसरी पंक्ति पाइपलाइन द्वारा उपयोग किए गए पूर्व-प्रशिक्षित मॉडल को डाउनलोड और कैश करती ह...\"],[\"```\\n\\nउत्तर देने के अलावा, पूर्व-प्रशिक्षित मॉडल संगत आत्मविश्वास स्कोर भी देता है, जहां उत्तर टोकनयु...\"],[\"```\\n\\nटोकननाइज़र सभी पूर्व-प्रशिक्षित मॉडलों के लिए प्रीप्रोसेसिंग प्रदान करता है और इसे सीधे एक स्ट्...\"],[\"## ट्रांसफार्मर का उपयोग क्यों करें?\\n\\n1. उपयोग में आसानी के लिए उन्नत मॉडल:\\n    - एनएलयू और एनएलजी प...\"],[\"1. आसानी से अनन्य मॉडल को अनुकूलित करें और अपनी आवश्यकताओं के लिए मामलों का उपयोग करें:\\n    - हम मूल...\"],[\"## मुझे ट्रांसफॉर्मर का उपयोग कब नहीं करना चाहिए?\\n\\n- यह लाइब्रेरी मॉड्यूलर न्यूरल नेटवर्क टूलबॉक्स न...\"],[\"## स्थापित करना\\n\\n### पिप का उपयोग करना\\n\\nइस रिपॉजिटरी का परीक्षण Python 3.8+, Flax 0.4.1+, PyTorch 1....\"],[\"देखें start-locally या [Flax स्थापना पृष्ठ](https:\\u002f\\u002fgithub.com\\u002fgoogle\\u002fflax#quick-install).\\n\\nजब इनमें...\"],[\"```\\n\\nयदि आप उपयोग के मामलों को आज़माना चाहते हैं या आधिकारिक रिलीज़ से पहले नवीनतम इन-डेवलपमेंट कोड ...\"],[\"```\\n\\nकोंडा के माध्यम से Flax, PyTorch, या TensorFlow में से किसी एक को स्थापित करने के लिए, निर्देशो...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (Google Research and the ...\"],[\"1. **[Audio Spectrogram Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002faudio-spectr...\"],[\"1. **[BARThez](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbarthez)** (से École polytechnique...\"],[\"1. **[BERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbert)** (गूगल से) साथ वाला पेपर [बीईआ...\"],[\"1. **[BigBird-Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbigbird_pegasus)** (गूगल र...\"],[\"1. **[BioGpt](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbiogpt)** (from Microsoft Research ...\"],[\"1. **[BlenderbotSmall](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot-small)** (फेसबु...\"],[\"1. **[BORT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbort)** (एलेक्सा से) कागज के साथ [बीई...\"],[\"1. **[ByT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbyt5)** (Google अनुसंधान से) साथ में ...\"],[\"1. **[CANINE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcanine)** (Google रिसर्च से) साथ मे...\"],[\"1. **[CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclip)** (OpenAI से) साथ वाला पेपर [लर...\"],[\"1. **[CodeGen](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcodegen)** (सेल्सफोर्स से) साथ में...\"],[\"1. **[Conditional DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconditional_detr)** (माइक...\"],[\"1. **[ConvNeXTV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconvnextv2)** (from Facebook AI...\"],[\"1. **[CTRL](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fctrl)** (सेल्सफोर्स से) साथ में पेपर ...\"],[\"1. **[DeBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta)** (Microsoft से) साथ में ...\"],[\"1. **[Deformable DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeformable_detr)** (सेंसटा...\"],[\"1. **[DETA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeta)** (from The University of Texas...\"],[\"1. **[DiNAT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdinat)** (from SHI Labs) released wi...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (हगिंगफेस से), सा...\"],[\"1. **[Donut](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdonut)** (NAVER से) साथ में कागज [OC...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (from S...\"],[\"1. **[EnCodec](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fencodec)** (Meta AI से) Alexandre ...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (Baidu से) Xuan Ouyang, ...\"],[\"1. **[ESM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fesm)** (मेटा AI से) ट्रांसफॉर्मर प्रोट...\"],[\"ज़िटनिक, जेरी मा और रॉब फर्गस। **ESM-1v** को पेपर के साथ जारी किया गया था [भाषा मॉडल प्रोटीन फ़ंक्शन...\"],[\"प्रोटीन अनुक्रम सटीक संरचना भविष्यवाणी को सक्षम करते हैं](https:\\u002f\\u002fdoi.org\\u002f10.1101\\u002f2022.07.20.500902)...\"],[\"1. **[FLAN-UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-ul2)** (from Google AI) rele...\"],[\"1. **[FLAVA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflava)** (FLAVA: A फाउंडेशनल लैंग्वे...\"],[\"1. **[Funnel Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffunnel)** (सीएमयू\\u002fगूगल ...\"],[\"1. **[GLPN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fglpn)** (KAIST से) साथ वाला पेपर [वर्...\"],[\"1. **[GPT NeoX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_neox)** (EleutherAI से) पेपर ...\"],[\"1. **[GPT-J](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgptj)** (EleutherAI से) साथ वाला पेप...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (BigCode से) Lou...\"],[\"1. **[Graphormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgraphormer)** (from Microsoft) ...\"],[\"1. **[Hubert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fhubert)** (फेसबुक से) साथ में पेपर ...\"],[\"1. **[ImageGPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fimagegpt)** (from OpenAI) release...\"],[\"1. **[KOSMOS-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fkosmos-2)** (from Microsoft Resea...\"],[\"1. **[LayoutLMv3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv3)** (माइक्रोसॉफ्ट रिस...\"],[\"1. **[LeViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flevit)** (मेटा AI से) साथ वाला पेपर ...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (The FAIR team of Meta AI...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (Microsoft Research & Unive...\"],[\"1. **[LXMERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flxmert)** (UNC चैपल हिल से) साथ में...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[Mask2Former](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmask2former)** (FAIR and UIUC ...\"],[\"1. **[mBART](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (फेसबुक से) साथ में पेपर [न...\"],[\"1. **[Megatron-BERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmegatron-bert)** (NVIDIA से)...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MMS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmms)** (Facebook से) Vineel Pratap, An...\"],[\"1. **[MobileNetV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v2)** (from Google I...\"],[\"1. **[MPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmpt)** (MosaiML से) the MosaicML NLP T...\"],[\"1. **[MusicGen](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmusicgen)** (from Meta) released ...\"],[\"1. **[NLLB](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnllb)** (फ्रॉम मेटा) साथ में पेपर [नो...\"],[\"1. **[Nyströmformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnystromformer)** (विस्कॉन्सि...\"],[\"1. **[OPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmaster\\u002fmodel_doc\\u002fopt)** (from Meta AI) released ...\"],[\"1. **[PatchTSMixer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpatchtsmixer)** ( IBM Researc...\"],[\"1. **[Perceiver IO](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fperceiver)** (दीपमाइंड से) सा...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft) released with ...\"],[\"1. **[Pix2Struct](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpix2struct)** (Google से) Kento...\"],[\"1. **[Pop2Piano](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpop2piano)** released with the p...\"],[\"1. **[QDQBert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fqdqbert)** (NVIDIA से) साथ वाला पे...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (from Google Research...\"],[\"1. **[ResNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fresnet)** (माइक्रोसॉफ्ट रिसर्च से) ...\"],[\"1. **[RoCBert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froc_bert)** (from WeChatAI) releas...\"],[\"1. **[SeamlessM4Tv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fseamless_m4t_v2)** (from Met...\"],[\"1. **[SEW](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsew)** (ASAPP से) साथ देने वाला पेपर [...\"],[\"1. **[SpeechToTextTransformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text)** ...\"],[\"1. **[SqueezeBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsqueezebert)** (बर्कले से) काग...\"],[\"1. **[Swin Transformer V2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswinv2)** (Microsoft स...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (来自 Google AI)कॉलिन रैफेल और नोम ...\"],[\"1. **[TAPAS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftapas)** (Google AI से) साथ में कागज...\"],[\"1. **[Trajectory Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftrajectory_transfor...\"],[\"1. **[TVP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftvp)** (from Intel) released with the ...\"],[\"1. **[UniSpeech](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funispeech)** (माइक्रोसॉफ्ट रिसर्...\"],[\"1. **[UnivNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funivnet)** (from Kakao Corporation...\"],[\"1. **[VideoMAE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvideomae)** (मल्टीमीडिया कम्प्यूट...\"],[\"1. **[Vision Transformer (ViT)](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit)** (गूगल एआई ...\"],[\"1. **[ViT Hybrid](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_hybrid)** (from Google AI) ...\"],[\"1. **[ViTMatte](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvitmatte)** (HUST-VL से) Jingfeng...\"],[\"1. **[ViViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvivit)** (from Google Research) rele...\"],[\"1. **[Wav2Vec2Phoneme](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2_phoneme)** (Faceb...\"],[\"1. **[Whisper](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwhisper)** (OpenAI से) साथ में काग...\"],[\"1. **[XGLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxglm)** (From Facebook AI) released w...\"],[\"1. **[XLM-RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-roberta)** (फेसबुक एआई से)...\"],[\"1. **[XLNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlnet)** (Google\\u002fCMU से) साथ वाला पे...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (फेसबुक एआई...\"],[\"1. एक नए मॉडल में योगदान देना चाहते हैं? नए मॉडल जोड़ने में आपका मार्गदर्शन करने के लिए हमारे पास एक...\"],[\"यह जांचने के लिए कि क्या किसी मॉडल में पहले से ही Flax, PyTorch या TensorFlow का कार्यान्वयन है, या ...\"],[\"## अधिक समझें\\n\\n|अध्याय | विवरण |\\n|-|-|\\n| [दस्तावेज़ीकरण](https:\\u002f\\u002fhuggingface.co\\u002ftransformers\\u002f) | पूर...\"],[\"## उद्धरण\\n\\nहमने आधिकारिक तौर पर इस लाइब्रेरी का [पेपर](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-d...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Language models have become a key step to achieve st...\"],[\"## FlaubertConfig\\n\\n[[autodoc]] FlaubertConfig\\n\\n## FlaubertTokenizer\\n\\n[[autodoc]] FlaubertTokenizer\\n\\n...\"],[\"[[autodoc]] TFFlaubertForMultipleChoice\\n    - call\\n\\n## TFFlaubertForTokenClassification\\n\\n[[autodoc]]...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"DeepSpeed ZeRO-3 can be used for inference as well, since it allows huge models to be loaded on mult...\"],[\"```\\n\\nor via `transformers`' `extras`:\\n\\n```bash\\npip install transformers[deepspeed]\\n```\\n\\nor find more...\"],[\"```\\n\\nSo if you get `8, 6`, then use `TORCH_CUDA_ARCH_LIST=\\\"8.6\\\"`. If you have multiple different car...\"],[\"```\\n\\nIf the output is:\\n\\n```bash\\n_CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, tot...\"],[\"```\\nor use the launcher provided by `deepspeed`:\\n\\n```bash\\ndeepspeed --num_gpus=2 your_program.py \\u003cno...\"],[\"```\\n\\nNote that in the DeepSpeed documentation you are likely to see `--deepspeed --deepspeed_config ...\"],[\"```\\n\\nThis is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly to use jus...\"],[\"```\\n\\nwhich enables optimizer offload and some other important features. You may experiment with the ...\"],[\"```\\n\\n  In this example, we tell DeepSpeed to use GPU 1 (second gpu).\\n\\n\\n\\n\\u003ca id='deepspeed-multi-node'...\"],[\"```\\nhostname1 slots=8\\nhostname2 slots=8\\n```\\nand then you can launch it as:\\n\\n```bash\\ndeepspeed --num_...\"],[\"```\\n\\nUnlike the `torch.distributed.run` launcher, `deepspeed` will automatically launch this command...\"],[\"```\\n\\nAll is left is to schedule it to run:\\n```bash\\nsbatch launch.slurm\\n```\\n\\n`srun` will take care of...\"],[\"```\\n\\nNote: `...` stands for the normal arguments that you'd pass to the functions.\\n\\nIf you want to u...\"],[\"\\\"zero_optimization\\\": {\\n        \\\"stage\\\": 3,\\n        \\\"offload_optimizer\\\": {\\n            \\\"device\\\": \\\"cpu...\"],[\"```\\n\\nIf the training script is in a normal file and not in the notebook cells, you can launch `deeps...\"],[\"```\\n\\nSome more examples are to be found in the [main repo](https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fDeepSpeed) a...\"],[\"```\\n\\nWhen you execute the program, DeepSpeed will log the configuration it received from the [`Train...\"],[\"```\\n\\n\\u003ca id='deepspeed-config-shared'\\u003e\\u003c\\u002fa\\u003e\\n\\n### Shared Configuration\\n\\n\\n\\u003cTip warning={true}\\u003e\\n\\nThis sec...\"],[\"\\u003ca id='deepspeed-zero'\\u003e\\u003c\\u002fa\\u003e\\n\\n### ZeRO\\n\\n[Zero Redundancy Optimizer (ZeRO)](https:\\u002f\\u002fwww.deepspeed.ai\\u002ft...\"],[\"```\\n\\n**Performance tuning:**\\n\\n- enabling `offload_optimizer` should reduce GPU RAM usage (it require...\"],[\"```\\n\\nThis is a stage 2 optimization for CPU offloading that parallelizes gradient copying to CPU mem...\"],[\"```\\n\\nIf you are getting OOMs, because your model or activations don't fit into the GPU memory and yo...\"],[\"The following configuration values depend on the model's hidden size:\\n\\n- `reduce_bucket_size`: `hidd...\"],[\"You can leave `sub_group_size` to its default value of *1e9* when not using NVMe offload. You may wa...\"],[\"```\\n\\nThis will essentially disable ZeRO without you needing to change anything else.\\n\\n\\n#### ZeRO-1 C...\"],[\"```\\n\\n\\n\\n\\u003ca id='deepspeed-nvme'\\u003e\\u003c\\u002fa\\u003e\\n\\n### NVMe Support\\n\\nZeRO-Infinity allows for training incredibly l...\"],[\"```\\n\\nYou can choose to offload both optimizer states and params to NVMe, or just one of them or none...\"],[\"It's possible to adjust ZeRO-3 configuration to make it perform closer to ZeRO-2:\\n\\n- set `stage3_par...\"],[\"\\\"zero_optimization\\\": {\\n        \\\"stage\\\": 2,\\n        \\\"offload_optimizer\\\": {\\n            \\\"device\\\": \\\"cpu...\"],[\"```\\n\\nHere is a full ZeRO-2 all-enabled manually set configuration file. It is here mainly for you to...\"],[\"```\\n\\n\\u003ca id='deepspeed-zero3-example'\\u003e\\u003c\\u002fa\\u003e\\n\\n#### ZeRO-3 Example\\n\\nHere is a full ZeRO-3 auto-configura...\"],[\"\\\"gradient_accumulation_steps\\\": \\\"auto\\\",\\n    \\\"gradient_clipping\\\": \\\"auto\\\",\\n    \\\"steps_per_print\\\": 2000,...\"],[\"```\\n\\nHere is a full ZeRO-3 all-enabled manually set configuration file. It is here mainly for you to...\"],[\"```\\n\\n#### How to Choose Which ZeRO Stage and Offloads To Use For Best Performance\\n\\nSo now you know t...\"],[\"8. Definitely use mixed half-precision over fp32 - so bf16 on Ampere and higher GPUs and fp16 on old...\"],[\"These notes were written primarily for the training mode, but they should mostly apply for inference...\"],[\"Therefore you have two ways to take advantage of this very beneficial feature:\\n\\n1. If you want to us...\"],[\"\\u003ca id='deepspeed-optimizer'\\u003e\\u003c\\u002fa\\u003e\\n\\n#### Optimizer\\n\\n\\nDeepSpeed's main optimizers are Adam, AdamW, OneB...\"],[\"```\\n\\nNote that the command line arguments will set the values in the configuration file. This is so ...\"],[\"```\\nto the top level configuration.\\n\\n\\n\\n\\u003ca id='deepspeed-scheduler'\\u003e\\u003c\\u002fa\\u003e\\n\\n#### Scheduler\\n\\nDeepSpeed s...\"],[\"```\\n\\nSince *\\\"auto\\\"* is used the [`Trainer`] arguments will set the correct values in the configurati...\"],[\"```\\n\\nand `total_num_steps`, `warmup_max_lr`, `warmup_num_steps` and `total_num_steps` will be set at...\"],[\"```\\n\\nIf you're using the Ampere-architecture based GPU, pytorch version 1.7 and higher will automati...\"],[\"```\\n\\nand the [`Trainer`] will automatically enable or disable it based on the value of\\n`args.fp16_ba...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nAs of `deepspeed==0.6.0` the bf16 support is new and experimental.\\n\\nIf you use [gradient...\"],[\"```\\nThe valid values as of this writing are \\\"fp16\\\", \\\"bfp16\\\", \\\"fp32\\\".\\n\\nnote: stage zero 3 had a bug w...\"],[\"```\\n\\nand the [`Trainer`] will automatically set `train_micro_batch_size_per_gpu` to the value of\\n`ar...\"],[\"```\\n\\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSp...\"],[\"```\\n\\n**FP32 Weights:**\\n\\nWhile the fp16 weights are fine for resuming training, if you finished finet...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nNote, that once `load_state_dict_from_zero_checkpoint` was run, the `model` will no long...\"],[\"```\\n\\n**Offline FP32 Weights Recovery:**\\n\\nDeepSpeed creates a special conversion script `zero_to_fp32...\"],[\"```\\n\\nIn this example there is just one DeepSpeed checkpoint sub-folder *global_step1*. Therefore to ...\"],[\"```\\n\\nAs you can see this gives you a randomly initialized model.\\n\\nIf you want to use a pretrained mo...\"],[\"```\\n\\nIf you're using the official example scripts and your command line arguments include `--deepspe...\"],[\"```\\n\\nstress on `tensor([1.])`, or if you get an error where it says the parameter is of size `1`, in...\"],[\"```\\n\\nSince for inference there is no need for additional large memory used by the optimizer states a...\"],[\"Let's estimate how much memory is needed to finetune \\\"bigscience\\u002fT0_3B\\\" on a single GPU:\\n\\n```bash\\n$ ...\"],[\"```\\n\\nSo you can fit it on a single 80GB GPU and no CPU offload, or a tiny 8GB GPU but then need ~60G...\"],[\"If you have enough GPU memory make sure to disable the CPU\\u002fNVMe offload as it'll make everything fas...\"],[\"```\\n\\nSo here you'd want 2x 32GB GPUs or higher without offloading to CPU.\\n\\nFor full information plea...\"],[\"```\\n\\n4. If possible include a link to a Google Colab notebook that we can reproduce the problem with...\"],[\"### Troubleshooting\\n\\n#### the `deepspeed` process gets killed at startup without a traceback\\n\\nIf the...\"],[\"```\\n\\nand you see in your log that Deepspeed reports `OVERFLOW!` as follows:\\n\\n```\\n0%|                ...\"],[\"```\\n\\nthat means that the Deepspeed loss scaler can't figure out a scaling co-efficient that overcome...\"],[\"If you're using Deepspeed ZeRO-1 or ZeRO-2 you don't need to use `HfDeepSpeedConfig` at all.\\n\\nFor ex...\"],[\"```\\n\\nor for non-pretrained model:\\n\\n```python\\nfrom transformers.integrations import HfDeepSpeedConfig...\"],[\"```\\n\\nPlease note that if you're not using the [`Trainer`] integration, you're completely on your own...\"],[\"```python\\n#!\\u002fusr\\u002fbin\\u002fenv python\\n\\n# This script demonstrates how to use Deepspeed ZeRO in an inferenc...\"],[\"os.environ[\\\"TOKENIZERS_PARALLELISM\\\"] = \\\"false\\\"  # To avoid warnings about parallelism in tokenizers\\n...\"],[\"# keeping the same format as json for consistency, except it uses lower case for true\\u002ffalse\\n# fmt: o...\"],[\"# Deepspeed ZeRO can process unrelated inputs on each GPU. So for 2 gpus you process 2 inputs at onc...\"],[\"```\\n\\nLet's save it as `t0.py` and run it:\\n```\\n$ deepspeed --num_gpus 2 t0.py\\nrank0:\\n   in=Is this re...\"],[\"```\\nRUN_SLOW=1 pytest tests\\u002fdeepspeed\\n```\\n\\n\\n\\n\\n## Main DeepSpeed Resources\\n\\n- [Project's github](http...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n### Model checkpoints\\n\\n|     Model Name      | Language |           Description           |\\n|:-...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Compression plays an important role on the efficient...\"],[\"## Resources\\n\\nDemo notebooks for Swin2SR can be found [here](https:\\u002f\\u002fgithub.com\\u002fNielsRogge\\u002fTransform...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\npython run_mlm.py \\\\\\n--model_name_or_path distilbert-base-cased \\\\\\n--output_dir output \\\\\\n--dataset...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Natural Language Processing (NLP) has recently achie...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"## MobileBertForTokenClassification\\n\\n[[autodoc]] MobileBertForTokenClassification\\n    - forward\\n\\n## ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The recent \\\"Text-to-Text Transfer Transformer\\\" (T5) ...\"],[\"- [google\\u002fmt5-xxl](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fmt5-xxl).\\n\\nThis model was contributed by [patrickvo...\"],[\"[[autodoc]] TFMT5Model\\n\\n## TFMT5ForConditionalGeneration\\n\\n[[autodoc]] TFMT5ForConditionalGeneration\\n...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recent studies have demonstrated the efficiency of g...\"],[\"* Causal language modeling (CLM) which is the traditional autoregressive training (so this model cou...\"],[\"[[autodoc]] XLMModel\\n    - forward\\n\\n## XLMWithLMHeadModel\\n\\n[[autodoc]] XLMWithLMHeadModel\\n    - forw...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\nNew behaviour\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import NllbTokenizer\\n\\n\\u003e\\u003e\\u003e tokenizer = NllbTokeniz...\"],[\"```\\n\\nFor more details, feel free to check the linked [PR](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformer...\"],[\"The abstract of the paper is the following:\\n\\n*Driven by the goal of eradicating language barriers on...\"],[\"Note that we're using the BCP-47 code for French `fra_Latn`. See [here](https:\\u002f\\u002fgithub.com\\u002ffacebookr...\"],[\"```\\n\\n### Generating from any other language than English\\n\\nEnglish (`eng_Latn`) is set as the default...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [zphang](https:\\u002f\\u002fhuggingface.co\\u002fzphang) with contributions from [Black...\"],[\"```\\n\\nNote that executing the script requires enough CPU RAM to host the whole model in float16 preci...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fgith...\"],[\"🚀 Deploy\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002flxe\\u002fsimple-llama-finetuner\\u002fblob\\u002fmas...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*This paper shows that masked autoencoders (MAE) are ...\"],[\"\\u003csmall\\u003e MAE architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2111.06377\\\"\\u003eoriginal paper.\\u003c...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"ESM-2 outperforms all tested single-sequence protein language models across a range of structure pre...\"],[\"The abstract from \\n\\\"Biological structure and function emerge from scaling unsupervised learning to 2...\"],[\"The original code can be found [here](https:\\u002f\\u002fgithub.com\\u002ffacebookresearch\\u002fesm) and was\\nwas developed...\"],[\"## EsmForMaskedLM\\n\\n[[autodoc]] EsmForMaskedLM\\n    - forward\\n\\n## EsmForSequenceClassification\\n\\n[[auto...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We design a family of image classification architect...\"],[\"- Compared to ViT, LeViT models use an additional distillation head to effectively learn from a teac...\"],[\"contrast with the original ViT model, which used external data like the JFT-300M dataset\\u002fImagenet-21...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- Similar to other models in the library, [`TimeSeriesTransformerModel`] is the raw Transformer with...\"],[\"Examples are \\\"day of the month\\\", \\\"month of the year\\\", etc. as scalar values (and then stacked togeth...\"],[\"- At inference time, we give the final value of the `past_values` as input to the decoder. Next, we ...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## Note on custom data\\n\\nIn case you'd like to use the script with custom data, there are 2 things re...\"],[\"# step 2: create DatasetDict\\ndataset = DatasetDict({\\n    \\\"train\\\": train_dataset,\\n    \\\"validation\\\": v...\"],[\"```\\n\\nAn example of such a dataset can be seen at [nielsr\\u002fade20k-demo](https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\nYou can easily upload this by clicking on \\\"Add file\\\" in the \\\"Files and versions\\\" tab of your re...\"],[\"```\\n\\nThe resulting model can be seen here: https:\\u002f\\u002fhuggingface.co\\u002fnielsr\\u002fsegformer-finetuned-sidewal...\"],[\"The script leverages [🤗 `Accelerate`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002faccelerate), which allows to wr...\"],[\"```\\n\\nand reply to the questions asked regarding the environment on which you'd like to train. Then\\n\\n...\"],[\"```\\n\\nand perform inference as follows:\\n\\n```python\\nfrom PIL import Image\\nimport requests\\nimport torch...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nIf you run into any issues running this model, please reinstall the last version that supported...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers have a potential of learning longer-ter...\"],[\"## Usage tips\\n\\n- Transformer-XL uses relative sinusoidal positional embeddings. Padding can be done ...\"],[\"## TransfoXLTokenizer\\n\\n[[autodoc]] TransfoXLTokenizer\\n    - save_vocabulary\\n\\n## TransfoXL specific o...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] pytorch_utils.find_pruneable_heads_and_indices\\n\\n[[autodoc]] pytorch_utils.prune_layer\\n\\n[...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The example scripts are not expected to work out-of-the-box on every problem, and you may need to ad...\"],[\"```\\n\\nFor older versions of the example scripts, click on the toggle below:...\"],[\"\\u003cdetails\\u003e\\n  \\u003csummary\\u003eExamples for older versions of 🤗 Transformers\\u003c\\u002fsummary\\u003e\\n\\t\\u003cul\\u003e\\n\\t\\t\\u003cli\\u003e\\u003ca href=\\\"ht...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv3.3.1\\u002fexamples\\\"\\u003ev3.3.1\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003cli...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv2.6.0\\u002fexamples\\\"\\u003ev2.6.0\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003cli...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv1.0.0\\u002fexamples\\\"\\u003ev1.0.0\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\u003c\\u002ful...\"],[\"Then switch your current clone of 🤗 Transformers to a specific version, like v3.5.1 for example:\\n\\n``...\"],[\"```\\n\\nAfter you've setup the correct library version, navigate to the example folder of your choice a...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nThe example script downloads and preprocesses a dataset from the 🤗 [Datasets](https:\\u002f...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Distributed training and mixed precision\\n\\nThe [Trainer](https:\\u002f\\u002fhu...\"],[\"```\\n\\nTensorFlow scripts utilize a [`MirroredStrategy`](https:\\u002f\\u002fwww.tensorflow.org\\u002fguide\\u002fdistributed_...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nTensor Processing Units (TPUs) are specifically designed to accelerate performance. T...\"],[\"```\\n\\nTest your setup to make sure it is configured correctly:\\n\\n```bash\\naccelerate test\\n```\\n\\nNow you ...\"],[\"```\\n\\n## Test a script\\n\\nIt is often a good idea to run your script on a smaller number of dataset exa...\"],[\"```\\n\\n## Resume training from checkpoint\\n\\nAnother helpful option to enable is resuming training from ...\"],[\"```\\n\\n## Share your model\\n\\nAll scripts can upload your final model to the [Model Hub](https:\\u002f\\u002fhugging...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Get a closer look at [DistilBERT](model_doc\\u002fdistilbert) by accessing [`DistilBertConfig`] to inspect...\"],[\"```\\n\\n[`DistilBertConfig`] displays all the default attributes used to build a base [`DistilBertModel...\"],[\"```\\n\\nTo reuse the configuration file, load it with [`~PretrainedConfig.from_pretrained`]:\\n\\n```py\\n\\u003e\\u003e\\u003e...\"],[\"```\\n\\nThis creates a model with random values instead of pretrained weights. You won't be able to use...\"],[\"```\\n\\nWhen you load pretrained weights, the default model configuration is automatically loaded if th...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nFor example, [`TFDistilBertForSequenceClassification`] is a base DistilBERT model wit...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Tokenizer\\n\\nThe last base class you need before using a model for t...\"],[\"```\\n\\nCreate a fast tokenizer with the [`DistilBertTokenizerFast`] class:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformer...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIf you aren't looking for any customization, just use the `from_pretrained` method to lo...\"],[\"```\\n\\n## Feature Extractor\\n\\nA feature extractor processes audio inputs. It inherits from the base [`~...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIf you aren't looking for any customization, just use the `from_pretrained` method to lo...\"],[\"```\\n\\nCombine the feature extractor and tokenizer in [`Wav2Vec2Processor`]:\\n\\n```py\\n\\u003e\\u003e\\u003e from transform...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [stas](https:\\u002f\\u002fhuggingface.co\\u002fstas). The original code can be found\\n[h...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the...\"],[\"New in v2:\\n\\n- **Vocabulary** In v2 the tokenizer is changed to use a new vocabulary of size 128K bui...\"],[\"## DebertaV2Config\\n\\n[[autodoc]] DebertaV2Config\\n\\n## DebertaV2Tokenizer\\n\\n[[autodoc]] DebertaV2Tokeniz...\"],[\"## TFDebertaV2PreTrainedModel\\n\\n[[autodoc]] TFDebertaV2PreTrainedModel\\n    - call\\n\\n## TFDebertaV2ForM...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Multimodal pre-training with text, layout, and image...\"],[\"```\\n\\nNote that LayoutXLM has its own tokenizer, based on\\n[`LayoutXLMTokenizer`]\\u002f[`LayoutXLMTokenizer...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We present in this paper a new architecture, named C...\"],[\"## Usage tips\\n\\n- CvT models are regular Vision Transformers, but trained with convolutions. They out...\"],[\"If you're interested in submitting a resource to be included here, please feel free to open a Pull R...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\nthis should make a directory called `xsum\\u002f` with files like `test.source`.\\nTo use your own data,...\"],[\"```\\ntrain.source\\ntrain.target\\nval.source\\nval.target\\ntest.source\\ntest.target...\"],[\"```\\nThe `.source` files are the input, the `.target` files are the desired output.\\n\\n### Potential is...\"],[\"Summarization Tips:\\n- (summ) 1 epoch at batch size 1 for bart-large takes 24 hours and requires 13GB...\"],[\"### Fine-tuning using Seq2SeqTrainer\\nTo use `Seq2SeqTrainer` for fine-tuning you should use the `fin...\"],[\"```\\n\\nFor multi-gpu training use `torch.distributed.launch`, e.g. with 2 gpus:\\n```bash\\ntorchrun --npr...\"],[\"```\\n\\n## Evaluation Commands\\n\\nTo create summaries for each article in dataset, we use `run_eval.py`, ...\"],[\"```\\n\\n### Multi-GPU Evaluation\\nhere is a command to run xsum evaluation on 8 GPUS. It is more than li...\"],[\"```\\n\\n   `--info` is an additional argument available for the same purpose of tracking the conditions...\"],[\"```\\n    --search \\\"num_beams=5:10 length_penalty=0.8:1.0:1.2 early_stopping=true:false\\\"\\n   ```\\n   whi...\"],[\"```\\n\\nIf you pass `--info \\\"some experiment-specific info\\\"` it will get printed before the results tab...\"],[\"```\\nsplits `wmt_en_ro\\u002ftrain` into 11,197 uneven lengthed batches and can finish 1 epoch in 8 minutes...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*Driven by the goal of eradicating language barriers on...\"],[\"## Implementation differences with SwitchTransformers\\n\\nThe biggest difference is the way the tokens ...\"],[\"\\u003e\\u003e\\u003e translated_tokens = model.generate(\\n...     **inputs, forced_bos_token_id=tokenizer.lang_code_to...\"],[\"```\\n\\n### Generating from any other language than English\\n\\nEnglish (`eng_Latn`) is set as the default...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by repl...\"],[\"## FNetConfig\\n\\n[[autodoc]] FNetConfig\\n\\n## FNetTokenizer\\n\\n[[autodoc]] FNetTokenizer\\n    - build_input...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We tackle the task of conditional music generation. ...\"],[\"```\\n\\n## Generation\\n\\nMusicGen is compatible with two generation modes: greedy and sampling. In practi...\"],[\"```\\n\\nThe audio outputs are a three-dimensional Torch tensor of shape `(batch_size, num_channels, seq...\"],[\"```\\n\\nThe `guidance_scale` is used in classifier free guidance (CFG), setting the weighting between t...\"],[\"```\\n\\nFor batched audio-prompted generation, the generated `audio_values` can be post-processed to re...\"],[\"```\\n\\n### Generation Configuration\\n\\nThe default parameters that control the generation process, such ...\"],[\"```\\n\\nNote that any arguments passed to the generate method will **supersede** those in the generatio...\"],[\"```\\n\\nSince the text encoder and audio encoder\\u002fdecoder models are frozen during training, the MusicGe...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] generation.GenerationConfig\\n\\t- from_pretrained\\n\\t- from_model_config\\n\\t- save_pretrained\\n\\n...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## AutoTokenizer\\n\\nNearly every NLP task begins with a tokenizer. A tokenizer converts your input int...\"],[\"```\\n\\nThen tokenize your input as shown below:\\n\\n```py\\n\\u003e\\u003e\\u003e sequence = \\\"In a hole in the ground there l...\"],[\"```\\n\\n## AutoProcessor\\n\\nMultimodal tasks require a processor that combines two types of preprocessing...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nFor PyTorch models, the `from_pretrained()` method uses `torch.load()` wh...\"],[\"```\\n\\nEasily reuse the same checkpoint to load an architecture for a different task:\\n\\n```py\\n\\u003e\\u003e\\u003e from ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[ALBERT](..\\u002fmodel_doc\\u002falbert), [BART](..\\u002fmodel_doc\\u002fbart), [BERT](..\\u002fmodel_doc\\u002fbert), [BigBird](..\\u002fmo...\"],[\"[Megatron-BERT](..\\u002fmodel_doc\\u002fmegatron-bert), [MobileBERT](..\\u002fmodel_doc\\u002fmobilebert), [MPNet](..\\u002fmodel...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"```\\n\\nWe encourage you to log in to your Hugging Face account so you can upload and share your model ...\"],[\"```\\n\\nThen take a look at an example:\\n\\n```py\\n\\u003e\\u003e\\u003e eli5[\\\"train\\\"][0]\\n{'answers': {'a_id': ['c3d1aib', 'c...\"],[\"```\\n\\nWhile this may look like a lot, you're only really interested in the `text` field. What's cool ...\"],[\"```\\n\\nYou'll notice from the example above, the `text` field is actually nested inside `answers`. Thi...\"],[\"```py\\n\\u003e\\u003e\\u003e eli5 = eli5.flatten()\\n\\u003e\\u003e\\u003e eli5[\\\"train\\\"][0]\\n{'answers.a_id': ['c3d1aib', 'c3d4lya'],\\n 'answ...\"],[\"```\\n\\nEach subfield is now a separate column as indicated by the `answers` prefix, and the `text` fie...\"],[\"```\\n\\nThis dataset contains the token sequences, but some of these are longer than the maximum input ...\"],[\"```\\n\\nNow create a batch of examples using [`DataCollatorForLanguageModeling`]. It's more efficient t...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\u003cTip\\u003e\\n\\nIf you aren't familiar with finetuning a model with Keras, take a look at the ...\"],[\"```\\n\\nConfigure the model for training with [`compile`](https:\\u002f\\u002fkeras.io\\u002fapi\\u002fmodels\\u002fmodel_training_ap...\"],[\"```\\n\\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use...\"],[\"```\\n\\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ...\"],[\"```\\n\\nPass your inputs to the model and return the `logits` of the masked token:\\n\\n```py\\n\\u003e\\u003e\\u003e from tran...\"],[\"```\\n\\nPass your inputs to the model and return the `logits` of the masked token:\\n\\n```py\\n\\u003e\\u003e\\u003e from tran...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Existing pre-trained models are generally geared tow...\"],[\"## Usage tips\\n\\n- UL2 is an encoder-decoder model pre-trained on a mixture of denoising functions as ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- [self-hosted (push)](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002f.github\\u002fworkflows\\u002fself-...\"],[\"```\\n\\n   The results can be observed [here](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002factions).\\n\\n\\n\\n...\"],[\"```\\n\\nHere:\\n\\n- `tests\\u002ftest_optimization.py` - the file with tests\\n- `OptimizationTest` - the name of ...\"],[\"```\\n\\n### Run `accelerate` tests\\n\\nSometimes you need to run `accelerate` tests on your models. For th...\"],[\"```\\n\\nJust run the following line to automatically test every docstring example in the desired file: ...\"],[\"```\\n\\nor `pytest.ini`\\u002f``tox.ini`` files:\\n\\n```ini\\n[pytest]\\nlooponfailroots = transformers tests\\n```\\n\\nT...\"],[\"```\\n\\n### Clearing state\\n\\nCI builds and when isolation is important (against speed), cache should be ...\"],[\"```\\n\\nImportant: the presence of `pytest-random-order` will automatically randomize tests, no configu...\"],[\"```\\n\\nTo disable the shuffling for all tests:\\n\\n```bash\\npytest --random-order-bucket=none\\n```\\n\\nBy defa...\"],[\"```\\n\\n```bash\\npytest --instafail\\n```\\n\\n### To GPU or not to GPU\\n\\nOn a GPU-enabled setup, to test in CP...\"],[\"```\\n\\nThis is handy when you want to run different tasks on different GPUs.\\n\\nSome tests must be run o...\"],[\"```\\n\\nThese decorators can be stacked. For example, if a test is slow and requires at least one GPU u...\"],[\"```\\nAlternative backends may also require the replacement of device-specific functions. For example ...\"],[\"```\\nThis format also allows for specification of any additional imports required. To use this file t...\"],[\"```\\n\\nTo send test results to JUnit format output:\\n\\n```bash\\npy.test tests --junitxml=result.xml\\n```\\n\\n...\"],[\"```\\n\\nNow, by default this test will be run 3 times, each time with the last 3 arguments of `test_flo...\"],[\"```\\n\\nThe module [parameterized](https:\\u002f\\u002fpypi.org\\u002fproject\\u002fparameterized\\u002f) which is already in the dev...\"],[\"```\\n\\nas in the previous example.\\n\\n\\n\\n### Files and directories\\n\\nIn tests often we need to know where ...\"],[\"```\\n\\nIf you don't need to manipulate paths via `pathlib` or you just need a path as a string, you ca...\"],[\"```\\n\\n`tmp_dir` will contain the path to the created temporary dir. It will be automatically removed ...\"],[\"```\\n\\n### Skipping tests\\n\\nThis is useful when a bug is found and a new test is written, yet the bug i...\"],[\"```\\n\\nor the whole module:\\n\\n```python\\nimport pytest\\n\\nif not pytest.config.getoption(\\\"--custom-flag\\\"):...\"],[\"```\\n\\nOnce a test is marked as `@slow`, to run such tests set `RUN_SLOW=1` env var, e.g.:\\n\\n```bash\\nRU...\"],[\"```\\n\\nAs explained at the beginning of this document, slow tests get to run on a scheduled basis, rat...\"],[\"```\\n\\nHere is a an example of a [script](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fscript...\"],[\"```\\n\\nAnd, of course, most of the time, `stderr` will come as a part of an exception, so try\\u002fexcept h...\"],[\"```\\n\\nHere is a full test example:\\n\\n```python\\nfrom transformers.testing_utils import CaptureStdout\\n\\nm...\"],[\"```\\n\\n### Testing with environment variables\\n\\nIf you want to test the impact of environment variables...\"],[\"```\\n\\nDepending on whether the test file was under the `tests` test suite or `examples` it'll correct...\"],[\"```\\n\\n## Working with github actions workflows\\n\\nTo trigger a self-push workflow CI job, you must:\\n\\n1....\"],[\"That way experiments on CI functionality itself won't interfere with the normal workflow.\\n\\nNow how c...\"],[\"```\\n\\nFor simple commands you could also do:\\n\\n```bash\\ncmd_that_may_fail || true\\n```\\n\\nOf course, once ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce dense vision transformers, an architect...\"],[\"## Usage tips\\n\\nDPT is compatible with the [`AutoBackbone`] class. This allows to use the DPT framewo...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help ...\"],[\"Awesome projects built with Transformers\\n\\nThis page lists awesome projects built on top of Transform...\"],[\"Keywords: inpainting, SD, Stable Diffusion\\n\\n## [flair](https:\\u002f\\u002fgithub.com\\u002fflairNLP\\u002fflair)\\n\\nFLAIR is ...\"],[\"Keywords: LLMs, Large Language Models, Data Retrieval, Indices, Knowledge Augmentation \\n\\n## [ParlAI]...\"],[\"Keywords: Stable-Diffusion, WebUI, CLI\\n\\n## [PaddleNLP](https:\\u002f\\u002fgithub.com\\u002fPaddlePaddle\\u002fPaddleNLP)\\n\\n[...\"],[\"## [imagen-pytorch](https:\\u002f\\u002fgithub.com\\u002flucidrains\\u002fimagen-pytorch)\\n\\nAn open-source Implementation of ...\"],[\"Keywords: Conversational, ASR, TTS, LLMs, NLP\\n\\n## [Runhouse](https:\\u002f\\u002fgithub.com\\u002frun-house\\u002frunhouse)\\n...\"],[\"Keywords: LLM, Agents, HF Hub\\n\\n## [transformers.js](https:\\u002f\\u002fxenova.github.io\\u002ftransformers.js\\u002f)\\n\\n[tra...\"],[\"Keywords: NLP, Framework\\n\\n## [speechbrain](https:\\u002f\\u002fgithub.com\\u002fspeechbrain\\u002fspeechbrain)\\n\\nSpeechBrain ...\"],[\"Keywords: Haiku, Model parallelism, LLM, TPU\\n\\n## [deepchem](https:\\u002f\\u002fgithub.com\\u002fdeepchem\\u002fdeepchem)\\n\\nD...\"],[\"Keywords: Stable-Diffusion, Blender\\n\\n## [seldon-core](https:\\u002f\\u002fgithub.com\\u002fSeldonIO\\u002fseldon-core)\\n\\nSeld...\"],[\"Stable-Dreamfusion is a pytorch implementation of the text-to-3D model Dreamfusion, powered by the S...\"],[\"Keywords: LLM, Evaluation, Few-shot\\n\\n## [gpt-neox](https:\\u002f\\u002fgithub.com\\u002fEleutherAI\\u002fgpt-neox)\\n\\nThis rep...\"],[\"Keywords: Training, Inference, Sequence Processing, Sequence Generation\\n\\n## [LaTeX-OCR](https:\\u002f\\u002fgith...\"],[\"Keywords: Federated Learning, Analytics, Collaborative ML, Decentralized\\n\\n## [gpt-code-clippy](https...\"],[\"Keywords: LLM, WebUI\\n\\n## [libra](https:\\u002f\\u002fgithub.com\\u002fPalashio\\u002flibra)\\n\\nAn ergonomic machine learning [...\"],[\"Keywords: Deployment, BERT, XLNet\\n\\n## [towhee](https:\\u002f\\u002fgithub.com\\u002ftowhee-io\\u002ftowhee)\\n\\nTowhee makes it...\"],[\"Keywords: Training, Generation\\n\\n## [diffgram](https:\\u002f\\u002fgithub.com\\u002fdiffgram\\u002fdiffgram)\\n\\nDiffgram aims t...\"],[\"Keywords: Knowledge Extraction, Knowledge Graphs\\n\\n## [Nebuly](https:\\u002f\\u002fgithub.com\\u002fnebuly-ai\\u002fnebuly)\\n\\n...\"],[\"Keywords: Differential privacy\\n\\n## [LAVIS](https:\\u002f\\u002fgithub.com\\u002fsalesforce\\u002fLAVIS)\\n\\n[LAVIS](https:\\u002f\\u002fgit...\"],[\"Keywords: Rust, BERT, Inference\\n\\n## [EasyNLP](https:\\u002f\\u002fgithub.com\\u002falibaba\\u002fEasyNLP)\\n\\n[EasyNLP](https:\\u002f...\"],[\"Keywords: Semi-structured documents, Unstructured documents, LLM, Document Question Answering\\n\\n## [C...\"],[\"Keywords: Model deployment, CLoud, Mobile, Edge\\n\\n## [underthesea](https:\\u002f\\u002fgithub.com\\u002fundertheseanlp\\u002f...\"],[\"Keywords: Model interpretation, Visualization\\n\\n## [mlrun](https:\\u002f\\u002fgithub.com\\u002fmlrun\\u002fmlrun)\\n\\nMLRun is ...\"],[\"Keywords: Thai, NLP, NLTK\\n\\n## [FlagAI](https:\\u002f\\u002fgithub.com\\u002fFlagAI-Open\\u002fFlagAI)\\n\\n[FlagAI](https:\\u002f\\u002fgith...\"],[\"Keywords: Active Learning, Research, Labeling\\n\\n## [cleanlab](https:\\u002f\\u002fgithub.com\\u002fcleanlab\\u002fcleanlab)\\n\\n...\"],[\"Keywords: PEFT, fine-tuning, LLaMA-2, ChatGLM, Qwen...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003cb\\u003eEnglish\\u003c\\u002fb\\u003e |\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"\\u003ch3 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003eAprendizado de máquina de última geração para JAX, PyTorch e TensorFlow\\u003c\\u002f...\"],[\"A biblioteca 🤗 Transformers oferece APIs para baixar e usar rapidamente esses modelos pré-treinados ...\"],[\"- [Completar palavra mascarada com BERT](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+...\"],[\"- [Sumarização com BART](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fbart-large-cnn?text=The+tower+is+324+metres...\"],[\"- [Resposta a perguntas com...\"],[\"- [Tradução com T5](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"Em Visão Computacional:\\n- [Classificação de Imagens com ViT](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-...\"],[\"Em Tarefas Multimodais:\\n- [Respostas de Perguntas em Tabelas com TAPAS](https:\\u002f\\u002fhuggingface.co\\u002fgoogl...\"],[\"## Se você está procurando suporte personalizado da equipe Hugging Face\\n\\n\\u003ca target=\\\"_blank\\\" href=\\\"ht...\"],[\"```\\n\\nA segunda linha de código baixa e armazena em cache o modelo pré-treinado usado pelo pipeline, ...\"],[\"```\\n\\n\\nAqui obtemos uma lista de objetos detectados na imagem, com uma caixa envolvendo o objeto e um...\"],[\"```\\n\\nE aqui está o código equivalente para TensorFlow:\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoT...\"],[\"```\\n\\nO tokenizador é responsável por todo o pré-processamento que o modelo pré-treinado espera, e po...\"],[\"1. Menores custos de computação, menor pegada de carbono:\\n    - Pesquisadores podem compartilhar mod...\"],[\"## Por que não devo usar transformers?\\n\\n- Esta biblioteca não é uma caixa de ferramentas modular par...\"],[\"### Com pip\\n\\nEste repositório é testado no Python 3.8+, Flax 0.4.1+, PyTorch 1.10+ e TensorFlow 2.6+...\"],[\"```\\nSe você deseja experimentar com os exemplos ou precisa da versão mais recente do código e não po...\"],[\"```\\n\\nSiga as páginas de instalação do Flax, PyTorch ou TensorFlow para ver como instalá-los com cond...\"],[\"1. **[FNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffnet)** (from Google Research) releas...\"],[\"1. **[GLPN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fglpn)** (from KAIST) released with th...\"],[\"1. **[GPT NeoX Japanese](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_neox_japanese)** (fr...\"],[\"1. **[Jukebox](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fjukebox)** (from OpenAI) released ...\"],[\"1. **[LayoutXLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutxlm)** (from Microsoft Res...\"],[\"1. **[LLaMA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama)** (from The FAIR team of Meta...\"],[\"1. **[Megatron-GPT2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmegatron_gpt2)** (from NVIDI...\"],[\"1. **[Persimmon](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpersimmon)** (from ADEPT) releas...\"],[\"1. **[PoolFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpoolformer)** (from Sea AI Labs...\"],[\"1. **[QDQBert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fqdqbert)** (from NVIDIA) released ...\"],[\"1. **[RegNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fregnet)** (from META Platforms) rel...\"],[\"1. **[SegFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsegformer)** (from NVIDIA) relea...\"],[\"1. **[SpeechT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeecht5)** (from Microsoft Resea...\"],[\"1. **[SqueezeBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsqueezebert)** (from Berkeley)...\"],[\"1. **[Swin2SR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswin2sr)** (from University of Wür...\"],[\"1. **[Table Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftable-transformer)** (fr...\"],[\"1. **[Trajectory Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftrajectory_transfor...\"],[\"1. **[UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ful2)** (from Google Research) released...\"],[\"1. **[Vision Transformer (ViT)](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit)** (from Goog...\"],[\"1. **[ViTMAE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_mae)** (from Meta AI) released ...\"],[\"1. **[ViViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvivit)** (from Google Research) rele...\"],[\"1. **[WavLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwavlm)** (from Microsoft Research) r...\"],[\"1. **[XGLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxglm)** (From Facebook AI) released w...\"],[\"1. **[XLM-RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-roberta)** (from Facebook ...\"],[\"1. **[XLS-R](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxls_r)** (from Facebook AI) released...\"],[\"Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh....\"],[\"1. Quer contribuir com um novo modelo? Adicionamos um **guia detalhado e modelos de exemplo** para o...\"],[\"## Saiba mais\\n\\n| Seção | Descrição |\\n|-|-|\\n| [Documentação](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers...\"],[\"## Citação\\n\\nAgora temos um [artigo](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f) que você p...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We propose Masked Siamese Networks (MSN), a self-sup...\"],[\"## Usage tips\\n\\n- MSN (masked siamese networks) is a method for self-supervised pre-training of Visio...\"],[\"## ViTMSNConfig\\n\\n[[autodoc]] ViTMSNConfig\\n\\n## ViTMSNModel\\n\\n[[autodoc]] ViTMSNModel\\n    - forward\\n\\n##...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Although convolutional neural networks (CNNs) have a...\"],[\"- PVTv1 on ImageNet-1K\\n\\n| **Model variant**  |**Size** |**Acc@1**|**Params (M)**|\\n|-----------------...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c!--This tip is automatically generated by `make fix-copies`, do not fill manually!--\\u003e...\"],[\"[BART](..\\u002fmodel_doc\\u002fbart), [BERT](..\\u002fmodel_doc\\u002fbert), [Bert Generation](..\\u002fmodel_doc\\u002fbert-generation...\"],[\"[GPTBigCode](..\\u002fmodel_doc\\u002fgpt_bigcode), [GPT Neo](..\\u002fmodel_doc\\u002fgpt_neo), [GPT NeoX](..\\u002fmodel_doc\\u002fgpt...\"],[\"[PLBart](..\\u002fmodel_doc\\u002fplbart), [ProphetNet](..\\u002fmodel_doc\\u002fprophetnet), [QDQBert](..\\u002fmodel_doc\\u002fqdqbert...\"],[\"```\\n\\nWe encourage you to log in to your Hugging Face account so you can upload and share your model ...\"],[\"```\\n\\nWhile this may look like a lot, you're only really interested in the `text` field. What's cool ...\"],[\"```\\n\\nYou'll notice from the example above, the `text` field is actually nested inside `answers`. Thi...\"],[\"```\\n\\nThis dataset contains the token sequences, but some of these are longer than the maximum input ...\"],[\"```\\n\\nNow create a batch of examples using [`DataCollatorForLanguageModeling`]. It's more efficient t...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\u003cTip\\u003e\\n\\nIf you aren't familiar with finetuning a model with Keras, take a look at the ...\"],[\"```\\n\\nConfigure the model for training with [`compile`](https:\\u002f\\u002fkeras.io\\u002fapi\\u002fmodels\\u002fmodel_training_ap...\"],[\"```\\n\\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use...\"],[\"```\\n\\nUse the [`~transformers.generation_utils.GenerationMixin.generate`] method to generate text.\\nFo...\"],[\"```\\n\\nUse the [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] method to create the s...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Semi-supervised learning through pseudo-labeling has...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"The example script uses the 🤗 Datasets library. You can easily customize them to your needs if you n...\"],[\"```\\nhuggingface-cli repo create english-roberta-base-dummy\\n```\\n\\nNext we clone the model repository t...\"],[\"```\\n\\n### Train model\\n\\nNext we can run the example script to pretrain the model.\\nCompared to the defa...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformer-based models are widely used in natural ...\"],[\"## Usage tips\\n\\n- The YOSO attention algorithm is implemented through custom CUDA kernels, functions ...\"],[\"## YosoForMaskedLM\\n\\n[[autodoc]] YosoForMaskedLM\\n    - forward\\n\\n## YosoForSequenceClassification\\n\\n[[a...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n1. CLVP is an integral part of the Tortoise TTS model.\\n2. CLVP can be used to compare...\"],[\"\\u003e\\u003e\\u003e # Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using ...\"],[\"```\\n\\n\\n## ClvpConfig\\n\\n[[autodoc]] ClvpConfig\\n    - from_sub_model_configs\\n\\n## ClvpEncoderConfig\\n\\n[[au...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The tremendous success of CLIP (Radford et al., 2021...\"],[\"\\u003e\\u003e\\u003e url = \\\"https:\\u002f\\u002fclip-cn-beijing.oss-cn-beijing.aliyuncs.com\\u002fpokemon.jpeg\\\"\\n\\u003e\\u003e\\u003e image = Image.open(...\"],[\"```\\n\\nCurrently, following scales of pretrained Chinese-CLIP models are available on 🤗 Hub:\\n\\n- [OFA-S...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pretrained multilingual large language models have t...\"],[\"## Usage tips \\n\\n- UMT5 was only pre-trained on [mC4](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmc4) excluding ...\"],[\"```\\n\\n\\u003cTip\\u003e \\n\\nRefer to [T5's documentation page](t5) for more tips, code examples and notebooks.\\n\\u003c\\u002fTi...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"ProphetNet is an encoder-decoder model and can predict n-future tokens for \\\"ngram\\\" language modeling...\"],[\"## ProphetNetConfig\\n\\n[[autodoc]] ProphetNetConfig\\n\\n## ProphetNetTokenizer\\n\\n[[autodoc]] ProphetNetTok...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"According to the abstract,\\n\\n- Pegasus' pretraining task is intentionally similar to summarization: i...\"],[\"All the [checkpoints](https:\\u002f\\u002fhuggingface.co\\u002fmodels?search=pegasus) are fine-tuned for summarization...\"],[\"## Usage Example\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import PegasusForConditionalGeneration, PegasusTok...\"],[\"```\\n\\n## Resources\\n\\n- [Script](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002fresearc...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [anton-l](https:\\u002f\\u002fhuggingface.co\\u002fanton-l).\\n\\n## Usage tips\\n\\n- SEW-D is ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While the general idea of self-supervised learning i...\"],[\"## Usage tips\\n\\n- Data2VecAudio, Data2VecText, and Data2VecVision have all been trained using the sam...\"],[\"**Data2VecAudio documentation resources**\\n- [Audio classification task guide](..\\u002ftasks\\u002faudio_classif...\"],[\"## Data2VecTextModel\\n\\n[[autodoc]] Data2VecTextModel\\n    - forward\\n\\n## Data2VecTextForCausalLM\\n\\n[[aut...\"],[\"## TFData2VecVisionForImageClassification\\n\\n[[autodoc]] TFData2VecVisionForImageClassification\\n    - ...\"],[\"Token classification\\n\\n## PyTorch version, no Trainer\\n\\nFine-tuning (m)LUKE for token classification t...\"],[\"```\\n\\nand reply to the questions asked. Then\\n\\n```bash\\naccelerate test\\n```\\n\\nthat will check everything...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*Building open-domain chatbots is a challenging area fo...\"],[\"## BlenderbotSmallTokenizerFast\\n\\n[[autodoc]] BlenderbotSmallTokenizerFast\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c!--This tip is automatically generated by `make fix-copies`, do not fill manually!--\\u003e\\n\\n[BART](..\\u002fmo...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nThe preprocessing function you want to create needs to:\\n\\n1. Prefix the input with a prompt so T...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers import DataCollatorForSeq2Seq\\n\\n\\u003e\\u003e\\u003e data_collator = DataC...\"],[\"```\\n\\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule....\"],[\"```\\n\\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`Seq2SeqTr...\"],[\"```\\n\\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t...\"],[\"```\\n\\nConfigure the model for training with [`compile`](https:\\u002f\\u002fkeras.io\\u002fapi\\u002fmodels\\u002fmodel_training_ap...\"],[\"```\\n\\nThen bundle your callbacks together:\\n\\n```py\\n\\u003e\\u003e\\u003e callbacks = [metric_callback, push_to_hub_callb...\"],[\"```\\n\\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nTokenize the text and return the `input_ids` as TensorFlow tensors:\\n\\n```py\\n\\u003e\\u003e\\u003e from t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n| **Input**                                                                                    ...\"],[\"```\\n| **Input**                                                                                     ...\"],[\"```\\n| **Input**                                                                                     ...\"],[\"```\\n\\nTo use BigCode or OpenAssistant, start by logging in to have access to the Inference API:\\n\\n```p...\"],[\"```\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransfo...\"],[\"```\\n\\nHere, the model could interpret in two ways:\\n- Have the `text-to-image` generate a capybara swi...\"],[\"```\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransfo...\"],[\"#### Tools\\n\\nTools are very simple: they're a single function, with a name, and a description. We the...\"],[\"### A curated set of tools\\n\\nWe identify a set of tools that can empower such agents. Here is an upda...\"],[\"```\\n\\n### Custom tools\\n\\nWhile we identify a curated set of tools, we strongly believe that the main v...\"],[\"```\\n\\nreturns the following code\\n\\n```python\\nfrom transformers import load_tool\\n\\nimage_generator = loa...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The paper aims at creating a single unified foundation model which can work across vision, language\\n...\"],[\"## FlavaFeatureExtractor\\n\\n[[autodoc]] FlavaFeatureExtractor\\n\\n## FlavaImageProcessor\\n\\n[[autodoc]] Fla...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large-scale language models show promising text gene...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Causal languag...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nT5 Version 1.1 includes the following improvements compared to the original T5 model:\\n\\n- GEGLU ...\"],[\"- [google\\u002ft5-v1_1-xl](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002ft5-v1_1-xl)\\n\\n- [google\\u002ft5-v1_1-xxl](https:\\u002f\\u002fhugg...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe encoded versions have different lengths:\\n\\n```python\\n\\u003e\\u003e\\u003e len(encoded_sequence_a), len(encode...\"],[\"```\\n\\n### autoencoding models\\n\\nSee [encoder models](#encoder-models) and [masked language modeling](#...\"],[\"### convolution\\n\\nA type of layer in a neural network where the input matrix is multiplied element-wi...\"],[\"### deep learning (DL)\\n\\nMachine learning algorithms which uses neural networks with several layers.\\n...\"],[\"For an input of size `[batch_size, sequence_length]`, the memory required to store the intermediate ...\"],[\"### head\\n\\nThe model head refers to the last layer of a neural network that accepts the raw hidden st...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import BertTokenizer\\n\\n\\u003e\\u003e\\u003e tokenizer = BertTokenizer.from_pretrained(...\"],[\"```\\n\\nThe tokenizer takes care of splitting the sequence into tokens available in the tokenizer vocab...\"],[\"```\\n\\nbecause this is the way a [`BertModel`] is going to expect its inputs.\\n\\n## L\\n\\n### labels\\n\\nThe l...\"],[\"- For sequence classification models, ([`BertForSequenceClassification`]), the model expects a tenso...\"],[\"- For object detection models, ([`DetrForObjectDetection`]), the model expects a list of dictionarie...\"],[\"Each model's labels may be different, so be sure to always check the documentation of each model for...\"],[\"For more details, see [Pipelines for inference](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fpipeline_tu...\"],[\"### preprocessing\\n\\nThe task of preparing raw data into a format that can be easily consumed by machi...\"],[\"### self-attention\\n\\nEach element of the input finds out which other elements of the input they shoul...\"],[\"Another name for the foundational [ZeRO](#zero-redundancy-optimizer--zero-) concept as used by vario...\"],[\"```python\\n\\u003e\\u003e\\u003e # [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]...\"],[\"```\\n\\nWe can use our tokenizer to automatically generate such a sentence by passing the two sequences...\"],[\"```\\n\\nThe first sequence, the \\\"context\\\" used for the question, has all its tokens represented by a `0...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Try out the inference widget here: https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-patch16-224\\n\\nContent:\\n- [P...\"],[\"```\\n\\n👀 See the results here: [nateraw\\u002fvit-base-beans](https:\\u002f\\u002fhuggingface.co\\u002fnateraw\\u002fvit-base-beans)...\"],[\"```\\n\\nInternally, the script will use the [`ImageFolder`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fv2.0.0...\"],[\"# example 4: providing several splits\\ndataset = load_dataset(\\\"imagefolder\\\", data_files={\\\"train\\\": [\\\"p...\"],[\"```\\n\\n`ImageFolder` will create a `label` column, and the label name is based on the directory name.\\n...\"],[\"```\\n\\n## PyTorch version, no Trainer\\n\\nBased on the script [`run_image_classification_no_trainer.py`](...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nHere's a code snippet you can use to listen to the resulting audio in a notebook: \\n\\n```python\\n\\u003e...\"],[\"```\\n\\nor alternatively for AMD GPUs:\\n\\n```bash\\n!rocm-smi\\n```\\n\\n\\u003c\\u002fTip\\u003e\\n\\nWe encourage you to log in to yo...\"],[\"```\\n\\n### Text cleanup for SpeechT5 tokenization \\n\\nStart by cleaning up the text data. You'll need th...\"],[\"```\\n\\nThe dataset examples contain `raw_text` and `normalized_text` features. When deciding which fea...\"],[\"```\\n\\nNow you have two sets of characters: one with the vocabulary from the dataset and one with the ...\"],[\"```\\n\\nNow that you have dealt with special characters in the text, it's time to shift focus to the au...\"],[\"```\\n\\nLet's check how many speakers remain: \\n\\n```py\\n\\u003e\\u003e\\u003e len(set(dataset[\\\"speaker_id\\\"]))\\n42\\n```\\n\\nLet's...\"],[\"```\\n\\nYou are left with just under 10,000 examples from approximately 40 unique speakers, which shoul...\"],[\"\\u003e\\u003e\\u003e def create_speaker_embedding(waveform):\\n...     with torch.no_grad():\\n...         speaker_embedd...\"],[\"```\\n\\nIt's important to note that the `speechbrain\\u002fspkrec-xvect-voxceleb` model was trained on Englis...\"],[\"```\\n\\nSpeaker embeddings should be a 512-element vector:\\n\\n```py\\n\\u003e\\u003e\\u003e processed_example[\\\"speaker_embedd...\"],[\"```\\n\\nNext, create a basic train\\u002ftest split: \\n\\n```py\\n\\u003e\\u003e\\u003e dataset = dataset.train_test_split(test_size...\"],[\"```\\n\\n### Data collator\\n\\nIn order to combine multiple examples into a batch, you need to define a cus...\"],[\"...         # not used during fine-tuning\\n...         del batch[\\\"decoder_attention_mask\\\"]\\n\\n...      ...\"],[\"```\\n\\nIn SpeechT5, the input to the decoder part of the model is reduced by a factor 2. In other word...\"],[\"```\\n\\nThe `use_cache=True` option is incompatible with gradient checkpointing. Disable it for trainin...\"],[\"```\\n\\nAnd with that, you're ready to start training! Training will take several hours. Depending on y...\"],[\"```\\n\\nNow you can pass the text and speaker embeddings to the pipeline, and it will take care of the ...\"],[\"```\\n\\nCreate a spectrogram with your model: \\n\\n```py\\n\\u003e\\u003e\\u003e spectrogram = model.generate_speech(inputs[\\\"i...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-supervised learning (SSL) is a long-standing go...\"],[\"## Resources\\n\\n- [Audio classification task guide](..\\u002ftasks\\u002faudio_classification)\\n- [Automatic speech...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"For question answering, TAPAS has 2 heads on top: a cell selection head and an aggregation head, for...\"],[\"The abstract from the paper is the following:\\n\\n*Answering natural language questions over tables is ...\"],[\"In addition, the authors have further pre-trained TAPAS to recognize **table entailment**, by creati...\"],[\"- TAPAS is a model that uses relative position embeddings by default (restarting the position embedd...\"],[\"- TAPAS is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It ...\"],[\"## Usage: fine-tuning\\n\\nHere we explain how you can fine-tune [`TapasForQuestionAnswering`] on your o...\"],[\"To summarize:\\n\\n| **Task**                            | **Example dataset** | **Description**        ...\"],[\"\\u003e\\u003e\\u003e # or, the base sized model with WikiSQL configuration\\n\\u003e\\u003e\\u003e config = TapasConfig(\\\"google-base-fine...\"],[\"```\\n\\nOf course, you don't necessarily have to follow one of these three ways in which TAPAS was fine...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nInitializing a model with a pre-trained base and randomly initialized classification ...\"],[\"```\\n\\nOf course, you don't necessarily have to follow one of these three ways in which TAPAS was fine...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nWhat you can also do is start from an already fine-tuned checkpoint. ...\"],[\"- `id`: optional, id of the table-question pair, for bookkeeping purposes.\\n- `annotator`: optional, ...\"],[\"**STEP 3: Convert your data into tensors using TapasTokenizer**\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\nThird, give...\"],[\"[`TapasTokenizer`] creates the `labels`, `numeric_values` and `numeric_values_scale` based on the `a...\"],[\"\\u003e\\u003e\\u003e model_name = \\\"google\\u002ftapas-base\\\"\\n\\u003e\\u003e\\u003e tokenizer = TapasTokenizer.from_pretrained(model_name)\\n\\n\\u003e\\u003e\\u003e...\"],[\"```\\n\\nNote that [`TapasTokenizer`] expects the data of the table to be **text-only**. You can use `.a...\"],[\"...     def __len__(self):\\n...         return len(self.data)\\n\\n\\n\\u003e\\u003e\\u003e data = pd.read_csv(tsv_path, sep=...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nThird, given that you've prepared your data in this TSV\\u002fCSV format (and corresponding...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import TapasTokenizer\\n\\u003e\\u003e\\u003e import pandas as pd\\n\\n\\u003e\\u003e\\u003e model_name = \\\"google\\u002f...\"],[\"```\\n\\nNote that [`TapasTokenizer`] expects the data of the table to be **text-only**. You can use `.a...\"],[\"\\u003e\\u003e\\u003e class TableDataset:\\n...     def __init__(self, data, tokenizer):\\n...         self.data = data\\n.....\"],[\"...     def __len__(self):\\n...         return len(self.data)\\n\\n\\n\\u003e\\u003e\\u003e data = pd.read_csv(tsv_path, sep=...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nNote that here, we encode each table-question pair independently. Thi...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import TapasConfig, TapasForQuestionAnswering, AdamW\\n\\n\\u003e\\u003e\\u003e # this is the ...\"],[\"...         # zero the parameter gradients\\n...         optimizer.zero_grad()\\n\\n...         # forward ...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nYou can then fine-tune [`TFTapasForQuestionAnswering`] as follows (shown here for the...\"],[\"...         # forward + backward + optimize\\n...         with tf.GradientTape() as tape:\\n...         ...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Usage: inference\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\nHere we explain how you ...\"],[\"\\u003e\\u003e\\u003e data = {\\\"Actors\\\": [\\\"Brad Pitt\\\", \\\"Leonardo Di Caprio\\\", \\\"George Clooney\\\"], \\\"Number of movies\\\": [\\\"8...\"],[\"\\u003e\\u003e\\u003e display(table)\\n\\u003e\\u003e\\u003e print(\\\"\\\")\\n\\u003e\\u003e\\u003e for query, answer, predicted_agg in zip(queries, answers, aggre...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nHere we explain how you can use [`TFTapasForQuestionAnswering`] for inference (i.e. m...\"],[\"\\u003e\\u003e\\u003e data = {\\\"Actors\\\": [\\\"Brad Pitt\\\", \\\"Leonardo Di Caprio\\\", \\\"George Clooney\\\"], \\\"Number of movies\\\": [\\\"8...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nIn case of a conversational set-up, then each table-question pair mus...\"],[\"\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n## TFTapasModel\\n[[autodoc]] TFTapasModel\\n    - call\\n    \\n## TFTapasForMaskedLM\\n[[autodoc...\"],[\"## Translating the Transformers documentation into your language\\n\\nAs part of our mission to democrat...\"],[\"```\\n\\nHere, `LANG-ID` should be one of the ISO 639-1 or ISO 639-2 language codes -- see [here](https:...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[ALBERT](..\\u002fmodel_doc\\u002falbert), [BERT](..\\u002fmodel_doc\\u002fbert), [BigBird](..\\u002fmodel_doc\\u002fbig_bird), [CamemBE...\"],[\"[MRA](..\\u002fmodel_doc\\u002fmra), [Nezha](..\\u002fmodel_doc\\u002fnezha), [Nyströmformer](..\\u002fmodel_doc\\u002fnystromformer), [...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nWhile it looks like there are a lot of fields here, it is actually pretty straightforward:\\n\\n- `...\"],[\"```\\n\\nThe preprocessing function you want to create needs to:\\n\\n1. Make four copies of the `sent1` fie...\"],[\"```\\n\\n🤗 Transformers doesn't have a data collator for multiple choice, so you'll need to adapt the [`...\"],[\"...         batch = self.tokenizer.pad(\\n...             flattened_features,\\n...             padding=...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n```py\\n\\u003e\\u003e\\u003e from dataclasses import dataclass\\n\\u003e\\u003e\\u003e from transformers.tokenization_utils_...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Evaluate\\n\\nIncluding a metric during training is often helpful for ...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\u003cTip\\u003e\\n\\nIf you aren't familiar with finetuning a model with Keras, take a look at the ...\"],[\"```\\n\\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.pr...\"],[\"```\\n\\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\\n\\n```...\"],[\"```\\n\\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use...\"],[\"```\\n\\nPass your inputs and labels to the model and return the `logits`:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers ...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### Fine-tuning BERT on SQuAD1.0\\n\\nThe [`run_qa.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob...\"],[\"This example code fine-tunes BERT on the SQuAD1.0 dataset. It runs in 24 min (with BERT-base) or 68 ...\"],[\"```\\n\\nTraining with the previously defined hyper-parameters yields the following results:\\n\\n```bash\\nf1...\"],[\"```\\n\\n#### Command for SQuAD2.0:\\n\\n```bash\\nexport SQUAD_DIR=\\u002fpath\\u002fto\\u002fSQUAD\\n\\npython run_qa_beam_search....\"],[\"```\\n\\n### Fine-tuning T5 on SQuAD2.0\\n\\nThe [`run_seq2seq_qa.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransf...\"],[\"```\\n\\n## Accelerate-based scripts\\n\\nBased on the scripts `run_qa_no_trainer.py` and `run_qa_beam_searc...\"],[\"```\\n\\nThis command is the same and will work for:\\n\\n- a CPU-only setup\\n- a setup with one GPU\\n- a dist...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large-scale pretrained language models have achieved...\"],[\"## RoCBertModel\\n\\n[[autodoc]] RoCBertModel\\n    - forward\\n\\n## RoCBertForPreTraining\\n\\n[[autodoc]] RoCBe...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transfer of pre-trained representations improves sam...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Local attention\\n\\n[Longformer](#longformer) uses local attention: often, the local context (e.g., ...\"],[\"Using those attention matrices with less parameters then allows the model to have inputs having a bi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Types of Segmentation\\n\\nSemantic segmentation assigns a label or class to every single pixel ...\"],[\"```\\n\\nThe segmentation pipeline output includes a mask for every predicted class. \\n```bash\\n[{'score':...\"],[\"```\\n\\nTaking a look at the mask for the car class, we can see every car is classified with the same m...\"],[\"```\\nChecking out one of the car masks below.\\n\\n```python\\nresults[2][\\\"mask\\\"]\\n```\\n\\u003cdiv class=\\\"flex just...\"],[\"```\\nAs you can see below, we have more classes. We will later illustrate to see that every pixel is ...\"],[\"```\\n\\nLet's have a side by side comparison for all types of segmentation.\\n\\n\\u003cdiv class=\\\"flex justify-c...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\n\\n### Load SceneParse150 dataset\\n\\nStart by loading a smaller...\"],[\"```\\n\\nSplit the dataset's `train` split into a train and test set with the [`~datasets.Dataset.train_...\"],[\"```\\n\\n- `image`: a PIL image of the scene.\\n- `annotation`: a PIL image of the segmentation map, which...\"],[\"```\\n\\n#### Custom dataset\\n\\nYou could also create and use your own dataset if you prefer to train with...\"],[\"# step 2: create DatasetDict\\n     dataset = DatasetDict({\\n          \\\"train\\\": train_dataset,\\n        ...\"],[\"```\\n\\n2. an id2label dictionary mapping the class integers to their class names\\n\\n     ```py\\n     impo...\"],[\"```\\n\\nNow create two preprocessing functions to prepare the images and annotations for the model. The...\"],[\"```\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003ctf\\u003e\\nIt is common to apply some data augmentation...\"],[\"```\\n\\nNext, create two preprocessing functions to prepare batches of images and annotations for the m...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n### Evaluate\\n\\nIncluding a metric during training is often helpful for...\"],[\"```\\n\\nThen create a function to [`~evaluate.EvaluationModule.compute`] the metrics. Your predictions ...\"],[\"```\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003ctf\\u003e\\n\\n```py\\n\\u003e\\u003e\\u003e def compute_metrics(eval_pred):\\n...\"],[\"```\\n\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nYour `compute_metrics` function is ready to go now, and you'll retur...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t...\"],[\"```\\n\\nThen, load SegFormer with [`TFAutoModelForSemanticSegmentation`] along with the label mappings,...\"],[\"```\\n\\nTo compute the accuracy from the predictions and push your model to the 🤗 Hub, use [Keras callb...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nPass your input to the model and return the `logits`:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers import TFAut...\"],[\"```\\n\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nTo visualize the results, load the [dataset color palette](https:\\u002f\\u002fg...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Scene text recognition (STR) has been an active rese...\"],[\"MGP-STR is trained on two synthetic datasets [MJSynth]((http:\\u002f\\u002fwww.robots.ox.ac.uk\\u002f~vgg\\u002fdata\\u002ftext\\u002f))...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\n\\u003e\\u003e\\u003e import request...\"],[\"```\\n\\n## MgpstrConfig\\n\\n[[autodoc]] MgpstrConfig\\n\\n## MgpstrTokenizer\\n\\n[[autodoc]] MgpstrTokenizer\\n    ...\"],[\"Zero-shot classifier distillation\\n\\nAuthor: @joeddav \\n\\nThis script provides a way to improve the spee...\"],[\"```\\n\\n`\\u003cunlabeled_data.txt\\u003e` should be a text file with a single unlabeled example per line. `\\u003cclass_...\"],[\"Any of the arguments in the 🤗 Trainer's\\n[`TrainingArguments`](https:\\u002f\\u002fhuggingface.co\\u002ftransformers\\u002fma...\"],[\"\\u003e\\u003e\\u003e zero_shot_classifier = pipeline(\\\"zero-shot-classification\\\", model=\\\"roberta-large-mnli\\\")\\n\\u003e\\u003e\\u003e zero...\"],[\"```\\n\\nUnfortunately, inference is slow since each of our 4 class names must be fed through the large ...\"],[\"```\\n\\nand even used trivially with a `TextClassificationPipeline`:\\n\\n```python\\n\\u003e\\u003e\\u003e distilled_classifie...\"],[\"```\\n\\n```python\\n%%time\\nfor _ in range(1000):\\n    distilled_classifier([sequence] * 16)\\n# runs in 10.3...\"],[\"!--Copyright 2023 Mistral AI and The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apac...\"],[\"This model was contributed by [Younes Belkada](https:\\u002f\\u002fhuggingface.co\\u002fybelkada) and [Arthur Zucker](...\"],[\"These ready-to-use checkpoints can be downloaded and used via the HuggingFace Hub:\\n\\n```python\\n\\u003e\\u003e\\u003e fr...\"],[\"```\\n\\nTo use the raw checkpoints with HuggingFace you can use the `convert_mixtral_weights_to_hf.py` ...\"],[\"```\\n\\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more ab...\"],[\"```\\n\\n### Expected speedups\\n\\nBelow is a expected speedup diagram that compares pure inference time be...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### Structure of the prompt\\n\\nLet's take a closer look at how the prompt is structured to understand ...\"],[\"```\\n\\nTask: \\\"Identify the oldest person in the `document` and create an image showcasing the result a...\"],[\"```\\n\\nWe can see that the tool name is short and precise. The description includes two parts, the fir...\"],[\"```\\n\\n````\\n\\nThe pattern the model is prompted to repeat has three parts: The task statement, the agen...\"],[\"```\\n\\nHuman: I tried this code, it worked but didn't give me a good result. The question is in French...\"],[\"```\\nwhich the agent completes. Contrary to the `run` command, the `chat` command then appends the co...\"],[\"```\\n\\n```text\\n'This is a tool that creates an image according to a prompt, which is a text descriptio...\"],[\"```\\nreturns\\n```text\\n==Explanation from the agent== \\nI will use the following tools `image_generator`...\"],[\"```\\n\\nwhich is definitely closer to what we had in mind! However, we want to have both the house and ...\"],[\"```\\n\\nTherefore it is important that the examples of the custom `chat` prompt template also make use ...\"],[\"```\\ntemplate = \\\"\\\"\\\" [...] \\\"\\\"\\\"\\n\\nagent = HfAgent(url_endpoint=your_endpoint, chat_prompt_template=templ...\"],[\"```\\n\\nUpon adding custom tools to an agent, the tools' descriptions and names are automatically\\ninclu...\"],[\"```\\n\\nThe set of curated tools already has an `image_transformer` tool which is hereby replaced with ...\"],[\"```\\n\\n```text\\n==Explanation from the agent==\\nI will use the following tool: `image_transformer` to tr...\"],[\"```\\n\\nFor the task `text-classification`, this returns `'facebook\\u002fbart-large-mnli'`, for `translation...\"],[\"```\\n\\nWe now have our tool handy. Save it in a file and import it from your main script. Let's name t...\"],[\"```\\n\\nand generates the following audio.\\n\\n| **Audio**                                                ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nBeware when replacing tools with others! This will also adjust the agent's prompt. This ...\"],[\"```\\n\\nThe model adequately leverages the tool:\\n```text\\n==Explanation from the agent==\\nI will use the ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nHereby, _inference_ is defined by a single forward pass, and _training_ is defined by a singl...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n```py\\n\\u003e\\u003e\\u003e from transformers import TensorFlowBenchmark, TensorFlowBenchmarkArguments\\n...\"],[\"```\\n\\nAn instantiated benchmark object can then simply be run by calling `benchmark.run()`.\\n\\n```py\\n\\u003e\\u003e...\"],[\"====================      INFERENCE - MEMORY - RESULT       ====================\\n-------------------...\"],[\"====================        ENVIRONMENT INFORMATION         ====================\\n\\n- transformers_ver...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n```bash\\npython examples\\u002ftensorflow\\u002fbenchmarking\\u002frun_benchmark_tf.py --help...\"],[\"```\\n\\nAn instantiated benchmark object can then simply be run by calling `benchmark.run()`.\\n\\n```py\\n\\u003e\\u003e...\"],[\"====================      INFERENCE - MEMORY - RESULT       ====================\\n-------------------...\"],[\"====================        ENVIRONMENT INFORMATION         ====================\\n\\n- transformers_ver...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nBy default, the _time_ and the _required memory_ for _inference_ are ...\"],[\"\\u003e\\u003e\\u003e benchmark = PyTorchBenchmark(args, configs=[config_base, config_384_hid, config_6_lay])\\n\\u003e\\u003e\\u003e benc...\"],[\"====================      INFERENCE - MEMORY - RESULT       ====================\\n-------------------...\"],[\"====================        ENVIRONMENT INFORMATION         ====================\\n\\n- transformers_ver...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n```py\\n\\u003e\\u003e\\u003e from transformers import TensorFlowBenchmark, TensorFlowBenchmarkArguments,...\"],[\"\\u003e\\u003e\\u003e benchmark = TensorFlowBenchmark(args, configs=[config_base, config_384_hid, config_6_lay])\\n\\u003e\\u003e\\u003e b...\"],[\"====================      INFERENCE - MEMORY - RESULT       ====================\\n-------------------...\"],[\"====================        ENVIRONMENT INFORMATION         ====================\\n\\n- transformers_ver...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nAgain, _inference time_ and _required memory_ for _inference_ are mea...\"],[\"With the new _benchmark_ tools, it is easier than ever to share your benchmark results with the comm...\"],[\"!--Copyright 2023 IBM and HuggingFace Inc. team. All rights reserved.\\n\\nLicensed under the Apache Lic...\"],[\"The abstract from the paper is the following:\\n\\n*TSMixer is a lightweight neural architecture exclusi...\"],[\"## Sample usage \\n```python\\n\\nfrom transformers import PatchTSMixerConfig, PatchTSMixerForPrediction\\nf...\"],[\"```\\n\\n## Usage tips\\n\\nThe model can also be used for time series classification and time series regres...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Overview\\n\\nOpenAI GPT-2 model was proposed in [Language Models are Unsupervised Multitask Learners...\"],[\"## Usage tips\\n\\n- GPT-2 is a model with absolute position embeddings so it's usually advised to pad t...\"],[\"- [`GPT2LMHeadModel`] is supported by this [causal language modeling example script](https:\\u002f\\u002fgithub....\"],[\"## GPT2Config\\n\\n[[autodoc]] GPT2Config\\n\\n## GPT2Tokenizer\\n\\n[[autodoc]] GPT2Tokenizer\\n    - save_vocabu...\"],[\"## TFGPT2LMHeadModel\\n\\n[[autodoc]] TFGPT2LMHeadModel\\n    - call\\n\\n## TFGPT2DoubleHeadsModel\\n\\n[[autodoc...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## RetriBertConfig\\n\\n[[autodoc]] RetriBertConfig\\n\\n## RetriBertTokenizer\\n\\n[[autodoc]] RetriBertTokeniz...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nActivate the virtual environment. On Linux and MacOs:\\n\\n```bash\\nsource .env\\u002fbin\\u002factivate\\n```\\nAct...\"],[\"```\\n\\n## Install from source\\n\\nInstall 🤗 Transformers from source with the following command:\\n\\n```bash...\"],[\"```\\n\\nYour Python environment will find the `main` version of 🤗 Transformers on the next run.\\n\\n## Ins...\"],[\"```\\n\\nThis script should run without hanging or waiting to timeout because it won't attempt to downlo...\"],[\"```\\n\\n    3. Now when you're offline, reload your files with [`PreTrainedModel.from_pretrained`] from...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nSee the [How to download files from the Hub](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002fhow-to-down...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003e\\u003e\\u003e prompt = (\\n...     \\\"In a shocking finding, scientists discovered a herd of unicorns living in a ...\"],[\"```\\n\\n## Combining GPT-Neo and Flash Attention 2\\n\\nFirst, make sure to install the latest version of F...\"],[\"```\\n\\n### Expected speedups\\n\\nBelow is an expected speedup diagram that compares pure inference time b...\"],[\"## GPTNeoForTokenClassification\\n\\n[[autodoc]] GPTNeoForTokenClassification\\n    - forward\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003cjax\\u003e...\"],[\"DeeBERT: Early Exiting for *BERT\\n\\nThis is the code base for the paper [DeeBERT: Dynamic Early Exitin...\"],[\"```\\n@inproceedings{xin-etal-2020-deebert,\\n    title = \\\"{D}ee{BERT}: Dynamic Early Exiting for Accele...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"```\\n\\nYou'll use 🤗 Datasets to load a dataset from the Hugging Face Hub, 🤗 Transformers to train your...\"],[\"```\\n\\nYou'll see that this dataset already comes with a training set containing 1000 images and a tes...\"],[\"```\\n\\nThe examples in the dataset have the following fields:\\n- `image_id`: the example image id\\n- `im...\"],[\"\\u003e\\u003e\\u003e for i in range(len(annotations[\\\"id\\\"])):\\n...     box = annotations[\\\"bbox\\\"][i]\\n...     class_idx =...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fi.imgur.com\\u002fTdaqPJO.png\\\" alt=\\\"CPPE-5 Im...\"],[\"```\\n\\n## Preprocess the data\\n\\nTo finetune a model, you must preprocess the data you plan to use to ma...\"],[\"```\\n\\nBefore passing the images to the `image_processor`, apply two preprocessing transformations to ...\"],[\"```\\n\\nThe `image_processor` expects the annotations to be in the following format: `{'image_id': int,...\"],[\"```\\n\\nNow you can combine the image and annotation transformations to use on a batch of examples:\\n\\n``...\"],[\"```\\n\\nApply this preprocessing function to the entire dataset using 🤗 Datasets [`~datasets.Dataset.wi...\"],[\"[[ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],\\n          [ 1.3081,  1.3081,  1.3081,...\"],[\"[[ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],\\n          [ 1.4200,  1.4200,  1.4200,...\"],[\"```\\n\\nYou have successfully augmented the individual images and prepared their annotations. However, ...\"],[\"```\\n\\n## Training the DETR model\\nYou have done most of the heavy lifting in the previous sections, so...\"],[\"```\\n\\nIn the [`TrainingArguments`] use `output_dir` to specify where to save your model, then configu...\"],[\"```\\n\\n## Evaluate\\nObject detection models are commonly evaluated with a set of \\u003ca href=\\\"https:\\u002f\\u002fcocod...\"],[\"...     return annotations\\n\\n\\n\\u003e\\u003e\\u003e # Save images and annotations into the files torchvision.datasets.C...\"],[\"...     with open(path_anno, \\\"w\\\") as file:\\n...         json.dump(output_json, file, ensure_ascii=Fal...\"],[\"```\\n\\nNext, prepare an instance of a `CocoDetection` class that can be used with `cocoevaluator`.\\n\\n``...\"],[\"```\\n\\nFinally, load the metrics and run the evaluation.\\n\\n```py\\n\\u003e\\u003e\\u003e import evaluate\\n\\u003e\\u003e\\u003e from tqdm impo...\"],[\"...         module.add(prediction=results, reference=labels)\\n...         del batch\\n\\n\\u003e\\u003e\\u003e results = mo...\"],[\"```\\nThese results can be further improved by adjusting the hyperparameters in [`~transformers.Traini...\"],[\"```\\n\\nYou can also manually replicate the results of the pipeline if you'd like:\\n\\n```py\\n\\u003e\\u003e\\u003e image_pro...\"],[\"```\\n\\nLet's plot the result:\\n```py\\n\\u003e\\u003e\\u003e draw = ImageDraw.Draw(image)\\n\\n\\u003e\\u003e\\u003e for score, label, box in zip...\"],[\"Fine-Tuning week of XLSR-Wav2Vec2 on 60 languages 🌍\\n\\nWelcome to the fine-tuning week! The goal of th...\"],[\"**Please keep in mind:**\\nThe spirit of the fine-tuning week is to provide state-of-the-art speech re...\"],[\"## Table of Contents\\n\\n- [Organization of the fine tuning week](#organization-of-the-fine-tuning-week...\"],[\"## Organization of the fine tuning week\\n\\nThe week officially starts on 22.03.2021 and ends on 29.03....\"],[\"Two possible setups can be used to fine-tune Wav2Vec2. The easiest setup is to simply use [google co...\"],[\"**2.**: Next, head over to the official [Fine-Tune XLSR-Wav2Vec2 with 🤗 Transformes](https:\\u002f\\u002fcolab.r...\"],[\"**5.**: It is time to start running the google colab! Make sure that you have selected \\\"GPU\\\" as your...\"],[\"When running the google colab make sure that you uncomment the cell corresponding to mounting your g...\"],[\"```\\n\\nand the line:\\n\\n```python\\n  output_dir=\\\"\\u002fcontent\\u002fgdrive\\u002fMyDrive\\u002fwav2vec2-large-xlsr-turkish-demo...\"],[\"```\\n\\nfurther below (which should already be uncommented).\\n\\nHaving finished the training you should f...\"],[\"### Local machine\\n\\nWe have provided `run_common_voice.py` script to run fine-tuning on local machine...\"],[\"```\\n$ git clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers.git\\n```\\n\\nSecond, head over to the `examp...\"],[\"```\\n\\n\\t**Note**: Installing the latest version of `torchaudio` will also upgrade `torch` to it's late...\"],[\"```\\n\\n\\t**To lanuch fine-tuninig on multiple GPUs:**\\n\\t\\n\\t```bash\\n\\tpython -m torch.distributed.launch \\\\\\n...\"],[\"```\\n\\n\\tThe above command will launch the training on 4 GPUs. Use the `--nproc_per_node` option to spe...\"],[\"```\\n\\nThen and add the following files that fully define a XLSR-Wav2Vec2 checkpoint into the reposito...\"],[\"```\\n\\nThe next **very important** step is to create the model card. For people to use your fine-tuned...\"],[\"\\u003c======================Copy **raw** version from here=========================\\n---\\nlanguage: {lang_i...\"],[\"type: common_voice\\n      args: {lang_id} #TODO: replace {lang_id} in your language code here. Make s...\"],[\"# Wav2Vec2-Large-XLSR-53-{language} #TODO: replace language with your {language}, *e.g.* French\\n\\nFin...\"],[\"processor = Wav2Vec2Processor.from_pretrained(\\\"{model_id}\\\") #TODO: replace {model_id} with your mode...\"],[\"```\\n\\n\\n## Evaluation\\n\\nThe model can be evaluated as follows on the {language} test data of Common Voi...\"],[\"# Preprocessing the datasets.\\n# We need to read the aduio files as arrays\\ndef speech_file_to_array_f...\"],[\"```\\n\\n**Test Result**: XX.XX %  # TODO: write output of print here. IMPORTANT: Please remember to als...\"],[\"In this section, we will quickly go over what data is allowed to be used as training \\ndata, what kin...\"],[\"## Tips and tricks\\n\\nThis section summarizes a couple of tips and tricks across various topics. It wi...\"],[\"If you are interested in learning more about the model though, here are a couple of resources that a...\"],[\"- What data was used to XLSR-Wav2Vec2? The checkpoint we will use for further fine-tuning was pretra...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Read more about it [in the release blogpost](https:\\u002f\\u002fwww.mosaicml.com\\u002fblog\\u002fmpt-7b)\\n\\n## Usage tips\\n\\n-...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Open-vocabulary object detection has benefited great...\"],[\"## Usage example\\n\\nOWLv2 is, just like its predecessor [OWL-ViT](owlvit), a zero-shot text-conditione...\"],[\"\\u003e\\u003e\\u003e url = \\\"http:\\u002f\\u002fimages.cocodataset.org\\u002fval2017\\u002f000000039769.jpg\\\"\\n\\u003e\\u003e\\u003e image = Image.open(requests.g...\"],[\"```\\n\\n## Resources\\n\\n- A demo notebook on using OWLv2 for zero- and one-shot (image-guided) object det...\"],[\"Flax\\u002fJAX community week 🤗\\n\\nWelcome to the Flax\\u002fJAX community week! The goal of this week is to make ...\"],[\"Don't forget to sign up [here](https:\\u002f\\u002fforms.gle\\u002ftVGPhjKXyEsSgUcs8)! \\n\\n## Table of Contents\\n\\n- [Orga...\"],[\"## Organization\\n\\nParticipants can propose ideas for an interesting NLP and\\u002for CV project. Teams of 3...\"],[\"## Important dates\\n\\n- **23.06.** Official announcement of the community week. Make sure to sign-up i...\"],[\"For issues with Flax\\u002fJAX, Transformers, Datasets or for questions that are specific to your project ...\"],[\"## Projects\\n\\nDuring the first week after the community week announcement, **23.06. - 30.06.**, teams...\"],[\"### How to form a team around a project\\n\\nYou can check out all existing projects ideas on the forum ...\"],[\"Once created, the team can start refining their project:\\n\\n- What is the goal of the project? *E.g.*,...\"],[\"## Tips on how to organize the project\\n\\nThis section gives you some tips on how to most efficiently ...\"],[\"By \\\"has defined\\\" we don't meant that the corresponding code already has to be written and ready \\nto ...\"],[\"As a conclusion, being honest about one's expected involvement is crucial so that \\nthe workload can ...\"],[\"### Other tips\\n\\nHere is a collection of some more tips:\\n\\n- We strongly recommend to work as publicly...\"],[\"```\\n- Ask for help. If you are stuck, use the public Slack channel or the [forum](https:\\u002f\\u002fdiscuss.hu...\"],[\"```\\n\\nYou can activate your venv by running\\n\\n```bash\\nsource ~\\u002f\\u003cyour-venv-name\\u003e\\u002fbin\\u002factivate\\n```\\n\\nWe s...\"],[\"```\\n\\n4. Set up a flax environment by running the following command in a virtual environment:\\n\\n   ```...\"],[\"```\\n$ cd ~\\u002f\\n$ git clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets.git\\n$ cd datasets\\n$ pip install -e \\\"...\"],[\"```\\nlibtpu.so already in used by another process. Not attempting to load libtpu.so in this process.\\n...\"],[\"```\\n\\nNext you should install JAX's TPU version on TPU by running the following command: \\n\\n```\\n$ pip ...\"],[\"```\\n\\n**Note**: Running this command might actually throw an error, such as:\\n```\\n Building wheel for ...\"],[\"```\\nJax should have been installed correctly nevertheless.\\n\\nTo verify that JAX was correctly install...\"],[\"```\\n$ cd ~\\u002f\\n$ git clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets.git\\n$ cd datasets\\n$ pip install -e \\\"...\"],[\"```\\n\\n## Quickstart flax and jax\\n\\n[JAX](https:\\u002f\\u002fjax.readthedocs.io\\u002fen\\u002flatest\\u002findex.html) is Autograd ...\"],[\"- [BART](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fmain\\u002fsrc\\u002ftransformers\\u002fmodels\\u002fbart\\u002fmodeling...\"],[\"- [T5](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fmain\\u002fsrc\\u002ftransformers\\u002fmodels\\u002ft5\\u002fmodeling_fla...\"],[\"You can find all available training scripts for JAX\\u002fFlax under the \\nofficial [flax example folder](h...\"],[\"### **Flax design philosophy in Transformers**\\n\\nThis section will explain how Flax models are implem...\"],[\"In PyTorch, the weights matrices would be stored as `torch.nn.Linear` objects alongside the model's ...\"],[\"```\\n\\nInstantiating an object `model_pytorch` of the class `ModelPyTorch` would actually allocate mem...\"],[\"```\\n\\nIn a more abstract way, this can be represented as passing the word embeddings to the model fun...\"],[\"```\\n\\nAt first glance the linear layer class `flax.linen.Dense` looks very similar to PyTorch's `torc...\"],[\"```\\n\\nVisually, the forward pass would now be represented as passing all tensors required for the com...\"],[\"The `FlaxPreTrainedModel` is an abstract class that holds a Flax module, handles weights initializat...\"],[\"First, write a Flax module that will declare the layers and computation.\\n\\n```python\\nimport flax.line...\"],[\"```\\n\\nNow let's define the `FlaxPreTrainedModel` model class.\\n\\n```python\\nfrom transformers.modeling_f...\"],[\"```\\n\\nNow the `FlaxMLPModel` will have a similar interface as PyTorch or Tensorflow models and allows...\"],[\"Another significant difference between Flax and PyTorch models is that, we can pass the `labels` dir...\"],[\"model = FlaxRobertaModel.from_pretrained(\\\"julien-c\\u002fdummy-unknown\\\")\\n\\n@jax.jit\\ndef run_model(input_ids...\"],[\"```\\n\\nWe use `jax.jit` to compile the function to get maximum performance. Note that in the above exa...\"],[\"```\\n\\nAs explained above we don't compute the loss inside the model, but rather in the task-specific ...\"],[\"```\\n\\nFinally, let's run our training loop.\\n\\n```python\\n# train loop\\nfor i in range(10):\\n   params, op...\"],[\"```\\n\\nNote that, as JAX is backed by the [XLA](https:\\u002f\\u002fwww.tensorflow.org\\u002fxla) compiler any JAX\\u002fFlax ...\"],[\"Speaker        | Topic                           | Time                  |  Video |\\n|-------------|-...\"],[\"### Thursday, July 1st\\n- [Watch the talks on YouTube](https:\\u002f\\u002fwww.youtube.com\\u002fwatch?v=__eG63ZP_5g)\\n-...\"],[\"Speaker        | Topic                           | Time                  | Video |\\n|-------------|--...\"],[\"| Rohan Anil, Google Brain | Scalable Second Order Optimization for Deep Learning      | 7.00pm-7.30...\"],[\"### Friday, July 2nd\\n- [Watch the talks on YouTube](https:\\u002f\\u002fwww.youtube.com\\u002fwatch?v=ZCMOPkcTu3s)\\n- [...\"],[\"Speaker        | Topic                           | Time                  |  Video |\\n|-------------|-...\"],[\"| Siddhartha Kamalakara, Joanna Yoo & João G M Araújo, Cohere | Training large scale language models...\"],[\"### Talks & Speakers\\n\\n#### Skye Wanderman-Milne, JAX developer, Google Brain\\n- Talk: Intro to JAX on...\"],[\"#### Pablo Castro, Staff Research Software Developer; Google Research, Brain Team\\n- Talk: Using Jax ...\"],[\"#### Sabrina J. Mielke, PhD student at The Johns Hopkins University & Part-time research intern at H...\"],[\"#### Mostafa Dehghani, Research Scientist, Google Brain\\n- Talk: Long Range Arena: Benchmarking Effic...\"],[\"#### Rohan Anil, Senior Staff Software Engineer, Google Research, Brain Team\\n- Talk: Scalable Second...\"],[\"#### Ben Wang, Independent AI Researcher, EleutherAI\\n- Talk: Multihost Training in Mesh Transformer ...\"],[\"#### Siddhartha Kamalakara, Joanna Yoo, João G M Araújo, MLE at Cohere\\n- Talk: Training large scale ...\"],[\"Now let's explain in more detail how a project can be created on the hub. Having an officially defin...\"],[\"Great, now we have a project directory with integrated git version control and a public model page, ...\"],[\"```\\n\\nNext we can clone the repo:\\n\\n```bash\\n$ git clone https:\\u002f\\u002fhuggingface.co\\u002fflax-community\\u002froberta-...\"],[\"```\\n\\nCool! The file is now displayed on the model page under the [files tab](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"```\\n\\nThis creates and saves our tokenizer directly in the cloned repository.\\nFinally, we can start t...\"],[\"```\\n\\nSince the dataset is tiny this command should actually run in less than 5 minutes. Note that we...\"],[\"bytes_output = serialization.to_bytes(params)\\n\\nrepo = Repository(\\\"flax-model\\\", clone_from=\\\"flax-comm...\"],[\"```\\n\\n**Note**: Make sure to have `huggingface_hub \\u003e= 0.0.13` to make this command work.\\n\\nFor more in...\"],[\"```\\nYou don't have sufficient permission to view this page\\n```\\n- this is expected! \\n\\nGreat, now you ...\"],[\"```\\n\\t\\nThis should ssh you into the TPU VM!\\nNow you can follow the steps of the section [How to insta...\"],[\"**NLP**\\n* **Conversational:** To have the best conversations!. [Example](https:\\u002f\\u002fhuggingface.co\\u002fmicr...\"],[\"**Speech**\\n* **Audio to Audio:** For tasks such as audio source separation or speech enhancement. \\n*...\"],[\"```\\npip install huggingface_hub\\n```\\n\\nHere is an example downloading (and caching!) a specific file d...\"],[\"```\\n\\n\\nWe'll provide more examples on Streamlit demos next week. Stay tuned!\\n\\n### Using a Gradio demo...\"],[\"### Jury\\n\\n* [Niki Parmar](https:\\u002f\\u002fresearch.google\\u002fpeople\\u002fNikiParmar\\u002f): Staff Research Scientist at G...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The cost of vision-and-language pre-training has bec...\"],[\"## Usage tips\\n\\n- BLIP-2 can be used for conditional text generation given an image and an optional t...\"],[\"[[autodoc]] Blip2Model\\n    - forward\\n    - get_text_features\\n    - get_image_features\\n    - get_qfor...\"],[\"Text Summarization with Pretrained Encoders\\n\\nThis folder contains part of the code necessary to repr...\"],[\"```\\n\\nAnd move all the stories to the same folder. We will refer as `$DATA_PATH` the path to where yo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*This paper presents a new vision Transformer, called...\"],[\"\\u003csmall\\u003e Swin Transformer architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2102.03334\\\"\\u003eori...\"],[\"If you're interested in submitting a resource to be included here, please feel free to open a Pull R...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] onnx.config.OnnxConfigWithPast\\n\\n### OnnxSeq2SeqConfigWithPast\\n\\n[[autodoc]] onnx.config.O...\"],[\"!--Copyright 2023 The Intel Team Authors and HuggingFace Inc. team. All rights reserved.\\n\\nLicensed u...\"],[\"The abstract from the paper is the following:\\n\\n*In this paper, we study the problem of temporal vide...\"],[\"This research addresses temporal video grounding (TVG), which is the process of pinpointing the star...\"],[\"The goal of this model is to incorporate trainable prompts into both visual inputs and textual featu...\"],[\"def pyav_decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\\n    '''\\n    ...\"],[\"def decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\\n    '''\\n    Decod...\"],[\"decoder_kwargs = dict(\\n    container=av.open(file, metadata_errors=\\\"ignore\\\"),\\n    sampling_rate=1,\\n ...\"],[\"```\\n\\nTips:\\n\\n- This implementation of TVP uses [`BertTokenizer`] to generate text embeddings and Resn...\"],[\"# Adversarial evaluation of model performances\\n\\nHere is an example on evaluating a model using adver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original checkpoints can be found [here](https:\\u002f\\u002fgithub.com\\u002fgoogle-research\\u002ft5x\\u002fblob\\u002fmain\\u002fdocs\\u002fm...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nRefer to [T5's documentation page](t5) for API reference, tips, code examples and notebo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"FlashAttention-2 is currently supported for the following architectures:\\n* [Bark](https:\\u002f\\u002fhuggingfac...\"],[\"* [Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral#transformers.MistralModel)\\n* ...\"],[\"You can request to add FlashAttention-2 support for another model by opening a GitHub Issue or Pull ...\"],[\"```\\n\\nWe strongly suggest referring to the detailed [installation instructions](https:\\u002f\\u002fgithub.com\\u002fDa...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nFlashAttention-2 can only be used when the model's dtype is `fp16` or `bf16`. Make sure ...\"],[\"```\\n\\n### Expected speedups\\n\\nYou can benefit from considerable speedups for inference, especially for...\"],[\"For sequences with padding tokens (generating with padding tokens), you need to unpad\\u002fpad the input ...\"],[\"For now, Transformers supports SDPA inference and training for the following architectures:\\n* [Bart]...\"],[\"input_text = \\\"Hello my dog is cute and\\\"\\ninputs = tokenizer(input_text, return_tensors=\\\"pt\\\").to(\\\"cuda...\"],[\"```\\n\\nIf you see a bug with the traceback below, try using the nightly version of PyTorch which may h...\"],[\"```\\n\\n## BetterTransformer\\n\\n\\u003cTip warning={true}\\u003e\\n\\nSome BetterTransformer features are being upstreame...\"],[\"```\\n\\nYou can return the original Transformers model with the [`~PreTrainedModel.reverse_bettertransf...\"],[\"```\\n\\n### 8-bit\\n\\n\\u003cTip\\u003e\\n\\nIf you're curious and interested in learning more about the concepts underlyi...\"],[\"```\\n\\nTo load a model in 4-bit for inference with multiple GPUs, you can control how much GPU RAM you...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nFeel free to try running a 11 billion parameter [T5 model](https:\\u002f\\u002fcolab.research.google...\"],[\"ORT is supported by 🤗 Optimum which can be used in 🤗 Transformers. You'll need to use an [`~optimum....\"],[\"```\\n\\nNow you're free to use the model for inference:\\n\\n```py\\nfrom optimum.pipelines import pipeline\\nf...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Introduction\\n\\nSplitting a text into smaller chunks is a task that is harder than it looks, and th...\"],[\"```\\n[\\\"Don't\\\", \\\"you\\\", \\\"love\\\", \\\"🤗\\\", \\\"Transformers?\\\", \\\"We\\\", \\\"sure\\\", \\\"do.\\\"]\\n```\\n\\nThis is a sensible firs...\"],[\"```\\n\\nAs can be seen space and punctuation tokenization, as well as rule-based tokenization, is used ...\"],[\"## Subword tokenization\\n\\n\\u003cYoutube id=\\\"zHvTiHr506c\\\"\\u002f\\u003e\\n\\nSubword tokenization algorithms rely on the pr...\"],[\"```\\n\\nBecause we are considering the uncased model, the sentence was lowercased first. We can see tha...\"],[\"```\\n\\nWe'll get back to the meaning of those `\\\"▁\\\"` when we look at [SentencePiece](#sentencepiece). A...\"],[\"```\\n(\\\"hug\\\", 10), (\\\"pug\\\", 5), (\\\"pun\\\", 12), (\\\"bun\\\", 4), (\\\"hugs\\\", 5)\\n```\\n\\nConsequently, the base vocabu...\"],[\"```\\n\\nBPE then identifies the next most common symbol pair. It's `\\\"u\\\"` followed by `\\\"n\\\"`, which occur...\"],[\"```\\n\\nAssuming, that the Byte-Pair Encoding training would stop at this point, the learned merge rule...\"],[\"\\u003ca id='wordpiece'\\u003e\\u003c\\u002fa\\u003e\\n\\n### WordPiece\\n\\nWordPiece is the subword tokenization algorithm used for [BER...\"],[\"\\u003ca id='unigram'\\u003e\\u003c\\u002fa\\u003e\\n\\n### Unigram\\n\\nUnigram is a subword tokenization algorithm introduced in [Subwor...\"],[\"```\\n[\\\"b\\\", \\\"g\\\", \\\"h\\\", \\\"n\\\", \\\"p\\\", \\\"s\\\", \\\"u\\\", \\\"ug\\\", \\\"un\\\", \\\"hug\\\"],...\"],[\"```\\n\\n`\\\"hugs\\\"` could be tokenized both as `[\\\"hug\\\", \\\"s\\\"]`, `[\\\"h\\\", \\\"ug\\\", \\\"s\\\"]` or `[\\\"h\\\", \\\"u\\\", \\\"g\\\", \\\"s\\\"]...\"],[\"The [`XLNetTokenizer`] uses SentencePiece for example, which is also why in the example earlier the\\n...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-trained language models like BERT and its varian...\"],[\"## ConvBertConfig\\n\\n[[autodoc]] ConvBertConfig\\n\\n## ConvBertTokenizer\\n\\n[[autodoc]] ConvBertTokenizer\\n ...\"],[\"## TFConvBertForMultipleChoice\\n\\n[[autodoc]] TFConvBertForMultipleChoice\\n    - call\\n\\n## TFConvBertFor...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"After such a [`VisionEncoderDecoderModel`] has been trained\\u002ffine-tuned, it can be saved\\u002floaded just ...\"],[\"```\\n\\n## Initialising `VisionEncoderDecoderModel` from a pretrained encoder and a pretrained decoder....\"],[\"```\\n\\n## Loading an existing `VisionEncoderDecoderModel` checkpoint and perform inference.\\n\\nTo load f...\"],[\"```\\n\\n## Loading a PyTorch checkpoint into `TFVisionEncoderDecoderModel`.\\n\\n[`TFVisionEncoderDecoderMo...\"],[\"```\\n\\n## Training\\n\\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other e...\"],[\"```\\n\\nThis model was contributed by [nielsr](https:\\u002f\\u002fgithub.com\\u002fnielsrogge). This model's TensorFlow ...\"],[\"Distil*\\n\\nAuthor: @VictorSanh\\n\\nThis folder contains the original code used to train Distil* as well a...\"],[\"**October 23, 2019 - Update** We release **DistilRoBERTa**: 95% of `RoBERTa-base`'s performance on G...\"],[\"## What is Distil*\\n\\nDistil* is a class of compressed models that started with DistilBERT. DistilBERT...\"],[\"For more information on DistilBERT, please refer to our [NeurIPS workshop paper](https:\\u002f\\u002farxiv.org\\u002fa...\"],[\"| Model                     | Macro-score                    | CoLA | MNLI | MRPC | QNLI | QQP  | RT...\"],[\"| DistilRoBERTa\\u003csup\\u003e1\\u003c\\u002fsup\\u003e |  **79.0**\\u002f**82.3**\\u003csup\\u003e2\\u003c\\u002fsup\\u003e | 59.3 | 84.0 | 86.6 | 90.8 | 89.4 | 67...\"],[\"\\u003csup\\u003e1\\u003c\\u002fsup\\u003e We did not use the MNLI checkpoint for fine-tuning but directly perform transfer learni...\"],[\"- `distilbert-base-uncased`: DistilBERT English language model pretrained on the same data used to p...\"],[\"- `distilbert-base-german-cased`: DistilBERT German language model pretrained on 1\\u002f2 of the data use...\"],[\"- `distilbert-base-multilingual-cased`: DistilmBERT multilingual model pretrained with the supervisi...\"],[\"Using DistilBERT is very similar to using BERT. DistilBERT share the same tokenizer as BERT's `bert-...\"],[\"```\\n\\nSimilarly, using the other Distil* models simply consists in calling the base classes with a di...\"],[\"```\\n\\nOur implementation of masked language modeling loss follows [XLM](https:\\u002f\\u002fgithub.com\\u002ffacebookre...\"],[\"```\\n\\nBy default, this will launch a training on a single GPU (even if more are available on the clus...\"],[\"```\\n\\n**Tips:** Starting distilled training with good initialization of the model weights is crucial ...\"],[\"How to propose a Flax\\u002fJAX + Transformers project \\n\\nGreat that you've opened this document! \\nWhile we...\"],[\"## How to submit a project proposal\\n\\nFirst, you should make sure that you are [logged in](https:\\u002f\\u002fhu...\"],[\"1. *A clear description of the project*\\n2. *In which language should the project be conducted?* Engl...\"],[\"4. *What data should be used?* It is important to state at least what kind of data you would like to...\"],[\"Feel free to copy-paste the following format for your project proposal and fill out the respective s...\"],[\"```\\n# \\u003cFILL ME: Name of project\\u003e\\n\\n\\u003cFILL ME: A clear description of the project\\u003e\\n\\n## 2. Language\\n\\nThe...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n- We have released a series of models [here](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=mvp...\"],[\"\\u003e\\u003e\\u003e generated_ids = model_with_prompt.generate(**inputs)\\n\\u003e\\u003e\\u003e tokenizer.batch_decode(generated_ids, s...\"],[\"```\\n\\nFor data-to-text generation, it is an example to use MVP and multi-task pre-trained variants.\\n`...\"],[\"```\\n\\nFor lightweight tuning, *i.e.*, fixing the model and only tuning prompts, you can load MVP with...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Question ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers-based models, such as BERT, have been o...\"],[\"The original code can be found [here](https:\\u002f\\u002fgithub.com\\u002fgoogle-research\\u002fbigbird).\\n\\n## Usage tips\\n\\n-...\"],[\"[[autodoc]] BigBirdPegasusForConditionalGeneration\\n    - forward\\n\\n## BigBirdPegasusForSequenceClassi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nTake a look at the example again:\\n\\n```py\\n\\u003e\\u003e\\u003e minds[\\\"train\\\"][0]\\n{'audio': {'array': array([-0.00...\"],[\"```\\n\\nThe MInDS-14 dataset has a sampling rate of 8000kHz (you can find this information in its [data...\"],[\"```\\n\\nAs you can see in the `transcription` above, the text contains a mix of upper and lowercase cha...\"],[\"```\\n\\n🤗 Transformers doesn't have a data collator for ASR, so you'll need to adapt the [`DataCollator...\"],[\"```\\n\\nNow instantiate your `DataCollatorForCTCWithPadding`:\\n\\n```py\\n\\u003e\\u003e\\u003e data_collator = DataCollatorCT...\"],[\"```\\n\\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"\\u003e\\u003e\\u003e trainer = Trainer(\\n...     model=model,\\n...     args=training_args,\\n...     train_dataset=encode...\"],[\"```\\n\\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nThe transcription is decent, but it could be better! Try finetuning your model on more e...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [junnyu](https:\\u002f\\u002fhuggingface.co\\u002fjunnyu). The original code can be foun...\"],[\"## RoFormerForSequenceClassification\\n\\n[[autodoc]] RoFormerForSequenceClassification\\n    - forward\\n\\n#...\"],[\"## FlaxRoFormerModel\\n\\n[[autodoc]] FlaxRoFormerModel\\n    - __call__\\n\\n## FlaxRoFormerForMaskedLM\\n\\n[[au...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*This paper presents XLSR which learns cross-lingual ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Program synthesis strives to generate a computer pro...\"],[\"## Checkpoint Naming\\n\\n* CodeGen model [checkpoints](https:\\u002f\\u002fhuggingface.co\\u002fmodels?other=codegen) are...\"],[\"```\\n\\n## Resources\\n\\n- [Causal language modeling task guide](..\\u002ftasks\\u002flanguage_modeling)\\n\\n## CodeGenCo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In the remainder of this guide, you will learn what's needed to add a new TensorFlow model architect...\"],[\"- Don't reinvent the wheel! More often than not, there are at least two reference implementations yo...\"],[\"\\u003cTip\\u003e\\n\\nBefore starting the work on a TensorFlow model architecture, double-check that there is no on...\"],[\"```\\n\\n3. Set up a development environment, for instance by running the following command:\\n\\n```bash\\npy...\"],[\"```\\n\\n8. Once you are satisfied, go to the webpage of your fork on GitHub. Click on “Pull request”. M...\"],[\"### 4. Model implementation\\n\\nNow it's time to finally start coding. Our suggested starting point is ...\"],[\"Sadly, there is no prescription to convert a PyTorch model into TensorFlow. You can, however, follow...\"],[\"Double-check the documentation!\\n- PyTorch's `nn.Parameter` variables typically need to be initialize...\"],[\"`TFBrandNewBertModel` will simply be a wrapper around this layer.\\n- Keras models need to be built in...\"],[\"In addition to the model file itself, you will also need to add the pointers to the model classes an...\"],[\"When you're happy with your implementation, run the following checklist to confirm that your model a...\"],[\"```\\n\\nThe most likely outcome is that you'll see a bunch of errors. Don't worry, this is expected! De...\"],[\"```\\n\\nand we will merge your PR! Congratulations on the milestone 🎉\\n\\n**7. (Optional) Build demos and ...\"],[\"First of all, let's talk about why understanding these mismatches matters. Many community members wi...\"],[\"In some cases, in discussion with the 🤗 Transformers team, we might find that fixing the mismatch is...\"],[\"Image Captioning (vision-encoder-text-decoder model) training example\\n\\nThe following example showcas...\"],[\"```\\n\\n### Create a model from a vision encoder model and a text decoder model\\nNext, we create a [Flax...\"],[\"```\\n\\n### Train the model\\nFinally, we can run the example script to train the model:\\n\\n```bash\\npython3...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fsimmim_archi...\"],[\"```\\n\\nHere, we train for 100 epochs with a learning rate of 2e-5. Note that the SimMIM authors used a...\"],[\"```\\n\\nNext, we can run the script by providing the path to this custom configuration (replace `path_t...\"],[\"```\\n\\n## MAE\\n\\nThe `run_mae.py` script can be used to pre-train a Vision Transformer as a masked autoe...\"],[\"```bash\\npython run_mae.py \\\\\\n    --dataset_name cifar10 \\\\\\n    --output_dir .\\u002fvit-mae-demo \\\\\\n    --rem...\"],[\"```\\n\\nHere we set:\\n- `mask_ratio` to 0.75 (to mask 75% of the patches for each image)\\n- `norm_pix_los...\"],[\"```\\n\\nNote that you can put images in dummy subfolders, whose names will be ignored by default (as la...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [sijunhe](https:\\u002f\\u002fhuggingface.co\\u002fsijunhe). The original code can be fo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In deep learning, models typically reuse the same pa...\"],[\"## Resources\\n\\n- [Translation task guide](..\\u002ftasks\\u002ftranslation)\\n- [Summarization task guide](..\\u002ftasks...\"],[\"Simple VQGAN CLIP\\n\\nAuthor: @ErwannMillon \\n\\nThis is a very simple VQGAN-CLIP implementation that was ...\"],[\"```\\nfrom VQGAN_CLIP import VQGAN_CLIP\\nvqgan_clip = VQGAN_CLIP()\\nvqgan_clip.generate(\\\"a picture of a ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recent work has shown that either (1) increasing the...\"],[\"## Usage tips\\n\\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`...\"],[\"```python\\n\\u003e\\u003e\\u003e import evaluate\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\u003e\\u003e\\u003e from transformers import Aut...\"],[\"```\\n\\n\\n## Resources\\n\\n- [Translation task guide](..\\u002ftasks\\u002ftranslation)\\n- [Summarization task guide](.....\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [novice03](https:\\u002f\\u002fhuggingface.co\\u002fnovice03).\\nThe original code can be ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note tha...\"],[\"```\\n\\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two f...\"],[\"```\\n\\nThe main advantage of doing this for big models is that during step 2 of the workflow shown abo...\"],[\"```\\n\\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrain...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [matthijs](https:\\u002f\\u002fhuggingface.co\\u002fMatthijs). The original code and wei...\"],[\"Unsupported features:\\n\\n- The [`MobileNetV1Model`] outputs a globally pooled version of the last hidd...\"],[\"## MobileNetV1Config\\n\\n[[autodoc]] MobileNetV1Config\\n\\n## MobileNetV1FeatureExtractor\\n\\n[[autodoc]] Mob...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## BloomModel\\n\\n[[autodoc]] BloomModel\\n    - forward\\n\\n## BloomForCausalLM\\n\\n[...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Language model pre-training has been shown to captur...\"],[\"## RealmRetriever\\n\\n[[autodoc]] RealmRetriever\\n\\n## RealmEmbedder\\n\\n[[autodoc]] RealmEmbedder\\n    - for...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"\\u003ch3 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003eJAX, PyTorch మరియు TensorFlow కోసం అత్యాధునిక యంత్ర అభ్యాసం\\u003c\\u002fp\\u003e\\n\\u003c\\u002fh3\\u003e\\n\\n\\u003ch...\"],[\"సహజ భాషా ప్రాసెసింగ్‌లో:\\n- [BERT తో మాస్క్‌డ్ వర్డ్ కంప్లీషన్](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-unca...\"],[\"- [RoBERTa తో సహజ భాషా అనుమితి](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+dog+was+Lost.+Nob...\"],[\"- [DistilBERT తో ప్రశ్న...\"],[\"సమాధానం](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+used...\"],[\"lso+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese...\"],[\"rtuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3...\"],[\"mazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29...\"],[\"nwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+th...\"],[\"orest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+sq...\"],[\"2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2...\"],[\"s+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+...\"],[\"longing+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+...\"],[\"+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+i...\"],[\"mounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departm...\"],[\"+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+...\"],[\"er+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tra...\"],[\"erse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+div...\"],[\"rees+divided+into+16%2C000+species)...\"],[\"- [T5 తో అనువాదం](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"కంప్యూటర్ దృష్టిలో:\\n- [VIT తో చిత్ర వర్గీకరణ](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-patch16-224)\\n- ...\"],[\"మల్టీమోడల్ టాస్క్‌లలో:\\n- [TAPAS తో టేబుల్ ప్రశ్న సమాధానాలు](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002ftapas-base...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n# Allocate a pipeline for sentiment-analysis\\n\\u003e\\u003e\\u003e cl...\"],[\"```\\n\\nరెండవ లైన్ కోడ్ డౌన్‌లోడ్ మరియు పైప్‌లైన్ ఉపయోగించే ప్రీట్రైన్డ్ మోడల్‌ను కాష్ చేస్తుంది, మూడవద...\"],[\"```\\n\\nఇక్కడ మనం ఆబ్జెక్ట్ చుట్టూ ఉన్న బాక్స్ మరియు కాన్ఫిడెన్స్ స్కోర్‌తో చిత్రంలో గుర్తించబడిన వస్తు...\"],[\"```\\n\\nప్రిట్రైన్డ్ మోడల్ ఆశించే అన్ని ప్రీప్రాసెసింగ్‌లకు టోకెనైజర్ బాధ్యత వహిస్తుంది మరియు నేరుగా ఒక...\"],[\"## నేను ట్రాన్స్‌ఫార్మర్‌లను ఎందుకు ఉపయోగించకూడదు?\\n\\n- ఈ లైబ్రరీ న్యూరల్ నెట్‌ల కోసం బిల్డింగ్ బ్లాక్...\"],[\"```\\n\\nమీరు ఉదాహరణలతో ప్లే చేయాలనుకుంటే లేదా కోడ్ యొక్క బ్లీడింగ్ ఎడ్జ్ అవసరం మరియు కొత్త విడుదల కోసం ...\"],[\"```\\n\\nFlax, PyTorch లేదా TensorFlow యొక్క ఇన్‌స్టాలేషన్ పేజీలను కొండాతో ఎలా ఇన్‌స్టాల్ చేయాలో చూడటాని...\"],[\"1. **[GIT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgit)** (from Microsoft Research) relea...\"],[\"1. **[GPT NeoX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_neox)** (from EleutherAI) rel...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (from Google AI) release...\"],[\"1. **[Perceiver IO](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fperceiver)** (from Deepmind) ...\"],[\"1. **[SeamlessM4T](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fmodel_doc\\u002fseamless_m4t)** (from Met...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (from Faceb...\"],[\"ప్రతి మోడల్ ఫ్లాక్స్, పైటార్చ్ లేదా టెన్సర్‌ఫ్లోలో అమలు చేయబడిందా లేదా 🤗 Tokenizers లైబ్రరీ ద్వారా అ...\"],[\"## అనులేఖనం\\n\\n🤗 ట్రాన్స్‌ఫార్మర్స్ లైబ్రరీ కోసం మీరు ఉదహరించగల [పేపర్](https:\\u002f\\u002fwww.aclweb.org\\u002fantholo...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Most widely-used pre-trained language models operate...\"],[\"## Usage example\\n\\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\\n\\n```python\\n\\u003e...\"],[\"```\\n\\nFor batched inference and training it is however recommended to make use of the tokenizer:\\n\\n```...\"],[\"```\\n\\nSimilar to [T5](t5), ByT5 was trained on the span-mask denoising task. However, \\nsince the mode...\"],[\"\\u003e\\u003e\\u003e input_ids = torch.tensor([input_ids[:8] + [258] + input_ids[14:21] + [257] + input_ids[28:]])\\n\\u003e\\u003e...\"],[\"\\u003e\\u003e\\u003e # ^- Note how 258 descends to 257, 256, 255\\n\\n\\u003e\\u003e\\u003e # Now we need to split on the sentinel tokens, ...\"],[\"```\\n\\n\\n## ByT5Tokenizer\\n\\n[[autodoc]] ByT5Tokenizer\\n\\nSee [`ByT5Tokenizer`] for all details....\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage example\\n\\n```python\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transformers import AutoModel, AutoTokenizer\\n\\n...\"],[\"```\\n\\n## Usage tips\\n\\n- Following mBART, BARTpho uses the \\\"large\\\" architecture of BART with an additio...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Dictionary\\n\\nAPI: API (不翻譯）\\nadd: 加入\\ncheckpoint: 檢查點\\ncode: 程式碼\\ncommunity: 社群\\nconfidence: 信賴度\\ndataset: ...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"🤗 Transformers 提供了數以千計的預訓練模型，支援 100 多種語言的文本分類、資訊擷取、問答、摘要、翻譯、文本生成。它的宗旨是讓最先進的 NLP 技術人人易用。\\n\\n🤗 Transform...\"],[\"這裡是一些範例：\\n- [用 BERT 做遮蓋填詞](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+F...\"],[\"- [用 RoBERTa 做自然語言推論](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+a...\"],[\"做問答](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+...\"],[\"- [用 T5 做翻譯](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"**[Write With Transformer](https:\\u002f\\u002ftransformer.huggingface.co)**，由 Hugging Face 團隊所打造，是一個文本生成的官方 dem...\"],[\"```\\n\\n第二行程式碼下載並快取 pipeline 使用的預訓練模型，而第三行程式碼則在給定的文本上進行了評估。這裡的答案“正面” (positive) 具有 99.97% 的信賴度。\\n\\n許多的 NL...\"],[\"```\\n這裡是對應的 TensorFlow 程式碼：\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, TFAutoModel\\n\\n\\u003e\\u003e\\u003e to...\"],[\"```\\n\\nTokenizer 為所有的預訓練模型提供了預處理，並可以直接轉換單一字串（比如上面的例子）或串列 (list)。它會輸出一個的字典 (dict) 讓你可以在下游程式碼裡使用或直接藉由 `*...\"],[\"1. 對於模型生命週期的每一個部分都面面俱到：\\n    - 訓練先進的模型，只需 3 行程式碼\\n    - 模型可以在不同深度學習框架之間任意轉換\\n    - 為訓練、評估和生產選擇最適合的框架，並完...\"],[\"這個 Repository 已在 Python 3.8+、Flax 0.4.1+、PyTorch 1.10+ 和 TensorFlow 2.6+ 下經過測試。\\n\\n你可以在[虛擬環境](https:\\u002f\\u002f...\"],[\"```\\n\\n如果你想要試試範例或者想在正式發布前使用最新開發中的程式碼，你必須[從原始碼安裝](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002finstallation...\"],[\"```\\n\\n要藉由 conda 安裝 Flax、PyTorch 或 TensorFlow 其中之一，請參閱它們各自安裝頁面的說明。\\n\\n## 模型架構\\n\\n**🤗 Transformers 支援的[所有的模...\"],[\"1. **[DINOv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdinov2)** (from Meta AI) released w...\"],[\"1. **[DiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdit)** (from Microsoft Research) relea...\"],[\"1. **[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt2)** (from OpenAI) released with ...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (from BigCode) r...\"],[\"1. **[MaskFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmaskformer)** (from Meta and UI...\"],[\"1. **[MobileViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilevit)** (from Apple) releas...\"],[\"1. **[Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus)** (from Google) released ...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (from Google Research...\"],[\"1. **[RoBERTa-PreLayerNorm](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta-prelayernorm)...\"],[\"1. **[SpeechToTextTransformer2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text_2)...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (from Google AI) released with th...\"],[\"1. **[TrOCR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftrocr)** (from Microsoft) released w...\"],[\"1. **[UPerNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fupernet)** (from Peking University...\"],[\"1. **[XLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm)** (from Facebook) released togeth...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (from Faceb...\"],[\"要檢查某個模型是否已有 Flax、PyTorch 或 TensorFlow 的實作，或其是否在🤗 Tokenizers 函式庫中有對應的 tokenizer，敬請參閱[此表](https:\\u002f\\u002fhugg...\"],[\"## 引用\\n\\n我們已將此函式庫的[論文](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)正式發表。如果你使用了 🤗 Transformers...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-training techniques have been verified successfu...\"],[\"```python\\ndef normalize_bbox(bbox, width, height):\\n    return [\\n        int(1000 * (bbox[0] \\u002f width)...\"],[\"```\\n\\nHere, `width` and `height` correspond to the width and height of the original document in which...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help ...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\" \\u002f\\u003e\\n\\n- A notebook on how to [ fine-tune LayoutLM for tok...\"],[\"## TFLayoutLMModel\\n\\n[[autodoc]] TFLayoutLMModel\\n\\n## TFLayoutLMForMaskedLM\\n\\n[[autodoc]] TFLayoutLMFor...\"],[\"Wav2Vec2 Contrastive Loss PreTraining examples\\n\\nThe following example showcases how to pretrain a wa...\"],[\"```\\n\\nTo ensure that all tensorboard traces will be uploaded correctly, we need to \\ntrack them. You c...\"],[\"```\\n\\n### Create the model configuration\\n\\nLet's first create the model configuration and store it in ...\"],[\"```\\n\\n### Create a feature extractor configuration\\n\\nBefore we can start the training, we need to defi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-training video transformers on extra large-scale...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original code can be found [here](https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fDialoGPT).\\n\\n## Usage tips\\n\\n- Dial...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Humans read and write hundreds of billions of messag...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [s-JoL](https:\\u002f\\u002fhuggingface.co\\u002fs-JoL).\\nThe original code was released ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### Agent\\n\\n[[autodoc]] Agent\\n    - chat\\n    - run\\n    - prepare_for_new_chat\\n\\n## Tools\\n\\n### load_too...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This guide demonstrates how you can distill a [fine-tuned ViT model](https:\\u002f\\u002fhuggingface.co\\u002fmerve\\u002fvi...\"],[\"```\\n\\nIn this example, we are using the `merve\\u002fbeans-vit-224` model as teacher model. It's an image c...\"],[\"```\\n\\nEssentially, we want the student model (a randomly initialized MobileNet) to mimic the teacher ...\"],[\"with torch.no_grad():\\n          teacher_output = self.teacher(**inputs)\\n\\n        # Compute soft targ...\"],[\"```\\n\\nWe will now login to Hugging Face Hub so we can push our model to the Hugging Face Hub through ...\"],[\"```\\n\\nWe can use `compute_metrics` function to evaluate our model on the test set. This function will...\"],[\"```\\n\\nWe can evaluate the model on the test set.\\n\\n```python\\ntrainer.evaluate(processed_datasets[\\\"test...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## Connectionist Temporal Classification\\n\\nThe script [`run_speech_recognition_ctc.py`](https:\\u002f\\u002fgithu...\"],[\"```bash\\nOMP_NUM_THREADS=1 python run_speech_recognition_ctc ......\"],[\"```\\n\\nIf the environment variable is not set, the training script might freeze, *i.e.* see: https:\\u002f\\u002fg...\"],[\"```\\n\\nOn a single V100 GPU, this script should run in *ca.* 1 hour 20 minutes and yield a CTC loss of...\"],[\"```\\n\\nOn 8 V100 GPUs, this script should run in *ca.* 18 minutes and yield a CTC loss of **0.39** and...\"],[\"```bash\\n**torchrun \\\\\\n\\t--nproc_per_node 4 run_speech_recognition_ctc_streaming.py \\\\\\n\\t--dataset_name=\\\"...\"],[\"```\\n\\nOn 4 V100 GPUs, this script should run in *ca.* 3h 31min and yield a CTC loss of **0.35** and w...\"],[\"| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval...\"],[\"| [TIMIT](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftimit_asr)| -  | [wav2vec2-base](https:\\u002f\\u002fhuggingface.co\\u002ffa...\"],[\"| [TIMIT](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftimit_asr)| -  | [ntu-spml\\u002fdistilhubert](https:\\u002f\\u002fhuggingfa...\"],[\"#### Librispeech CTC\\n\\n- [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr)...\"],[\"| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval...\"],[\"| [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr)| `\\\"clean\\\"` - `\\\"train.100\\\"` |  [micr...\"],[\"| [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr)| `\\\"clean\\\"` - `\\\"train.100\\\"` |  [face...\"],[\"| [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr)| `\\\"clean\\\"` - `\\\"train.100\\\"` |  [asap...\"],[\"#### Common Voice CTC\\n\\n- [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcommon_voice)...\"],[\"| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval...\"],[\"| [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmozilla-foundation\\u002fcommon_voice_3_0)| `\\\"it\\\"`  | [fa...\"],[\"| [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcommon_voice)| `\\\"tr\\\"`  | [facebook\\u002fwav2vec2-large-x...\"],[\"| [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcommon_voice)| `\\\"tr\\\"`  | [facebook\\u002fwav2vec2-large-x...\"],[\"| [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcommon_voice)| `\\\"tr\\\"`  | [facebook\\u002fwav2vec2-xls-r-1...\"],[\"#### Multilingual Librispeech CTC\\n\\n- [Multilingual Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmult...\"],[\"| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval...\"],[\"| [Multilingual Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmultilingual_librispeech)| `\\\"german\\\"`  ...\"],[\"## Connectionist Temporal Classification With Adapters\\n\\nThe script [`run_speech_recognition_ctc_adap...\"],[\"#### Common Voice CTC Adapter\\n\\nAs in the examples [above](#examples-ctc), we fine-tune on Common Voi...\"],[\"```\\nhuggingface-cli login\\n```\\n\\nNow, let's run an example and upload it to the Hub under `wav2vec2-co...\"],[\"```\\n\\nThis should take less than 10 minutes on most GPUs and you should very quickly get word error r...\"],[\"```sh\\npython run_speech_recognition_ctc.py \\\\\\n\\t--dataset_name=\\\"common_voice\\\" \\\\\\n\\t--model_name_or_path=...\"],[\"```\\n\\nNow you should have both `adapter.tur.safetensors` and `adapter.swe.safetensors` in the model r...\"],[\"```\\nrespectively.\\n\\n## Sequence to Sequence\\n\\nThe script [`run_speech_recognition_seq2seq.py`](https:\\u002f...\"],[\"#### Single GPU Whisper Training\\nThe following example shows how to fine-tune the [Whisper small](ht...\"],[\"```\\nOn a single V100, training should take approximately 8 hours, with a final cross-entropy loss of...\"],[\"#### Multi GPU Whisper Training\\nThe following example shows how to fine-tune the [Whisper small](htt...\"],[\"```\\nOn two V100s, training should take approximately 4 hours, with a final cross-entropy loss of **1...\"],[\"```bash\\nhuggingface-cli repo create wav2vec2-2-bart-base\\ngit clone https:\\u002f\\u002fhuggingface.co\\u002f\\u003cyour-user...\"],[\"```\\n\\nNext, run the following script **inside** the just cloned repo:\\n\\n```python\\nfrom transformers im...\"],[\"```\\n\\nNote that we have added a randomly initialized _adapter layer_ to `wav2vec2-base` with the argu...\"],[\"In the script [`run_speech_recognition_seq2seq`], we load the warm-started model, \\nfeature extractor...\"],[\"```\\n\\nIf the environment variable is not set, the training script might freeze, *i.e.* see: https:\\u002f\\u002fg...\"],[\"```bash\\npython run_speech_recognition_seq2seq.py \\\\\\n\\t--dataset_name=\\\"librispeech_asr\\\" \\\\\\n\\t--model_name...\"],[\"```\\n\\nOn a single V100 GPU, this script should run in *ca.* 5 hours and yield a \\ncross-entropy loss o...\"],[\"```\\n\\nOn 8 V100 GPUs, this script should run in *ca.* 45 minutes and yield a cross-entropy loss of **...\"],[\"| Dataset                                                        | Dataset Config            | Pretr...\"],[\"|----------------------------------------------------------------|---------------------------|------...\"],[\"-|--------------------------------------------------------------------------------------------------...\"],[\"-----------------------------------------------------------------|-------------------------|--------...\"],[\"----------------------------|------------|---------------|------------------------------------------...\"],[\"-------------------------------------|--------------------------------------------------------------...\"],[\"-----------------------------------------------------------------|...\"],[\"| [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr) | `\\\"clean\\\"` - `\\\"train.100\\\"` | [face...\"],[\"| [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr) | `\\\"clean\\\"` - `\\\"train.100\\\"` | [face...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"MobileViTV2 is the second version of MobileViT, constructed by replacing the multi-headed self-atten...\"],[\"## Usage tips\\n\\n- MobileViTV2 is more like a CNN than a Transformer model. It does not work on sequen...\"],[\"LXMERT DEMO\\n\\n1. make a virtualenv: ``virtualenv venv`` and activate ``source venv\\u002fbin\\u002factivate``\\n2. ...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\n### Train tokenizer\\n\\nIn the first step, we train a tokenizer to efficiently process the text in...\"],[\"```\\n\\nGreat, we have set up our model repository. During training, we will automatically\\npush the tra...\"],[\"```\\n\\nTraining should converge at a loss and accuracy \\nof 1.78 and 0.64 respectively after 18 epochs ...\"],[\"```\\n\\n### Train tokenizer\\n\\nIn the first step, we train a tokenizer to efficiently process the text in...\"],[\"```\\n\\n### Create configuration\\n\\nNext, we create the model's configuration file. This is as simple \\nas...\"],[\"```\\n\\nTraining should converge at a loss and perplexity \\nof 3.24 and 25.72 respectively after 20 epoc...\"],[\"```\\n\\n### Train tokenizer\\n\\nIn the first step, we train a tokenizer to efficiently process the text in...\"],[\"# Train tokenizer\\ntokenizer.train_from_iterator(\\n    iterator=batch_iterator(input_sentence_size=inp...\"],[\"```\\n\\n### Create configuration\\n\\nNext, we create the model's configuration file. This is as simple \\nas...\"],[\"```\\n\\nTraining should converge at a loss and accuracy \\nof 2.36 and 57.0 respectively after 3 epochs o...\"],[\"```\\n\\n### Train tokenizer\\nIn the first step, we train a tokenizer to efficiently process the text inp...\"],[\"```\\n\\nGreat, we have set up our model repository. During training, we will automatically\\npush the tra...\"],[\"```\\n\\nTraining should converge at a loss and accuracy \\nof 1.36 and 0.77 respectively after 3 epochs o...\"],[\"### Script to run MLM with PyTorch\\u002fXLA on TPUv3-8\\n\\nFor comparison one can run the same pre-training ...\"],[\"```\\n\\n, set the following environment variables:\\n\\n```bash\\nexport XRT_TPU_CONFIG=\\\"localservice;0;local...\"],[\"```\\n\\n### Script to compare pre-training with PyTorch on 8 GPU V100's\\n\\nFor comparison you can run the...\"],[\"```\\n\\n, and can start training as follows:\\n\\n```bash\\npython3 -m torch.distributed.launch --nproc_per_n...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large pre-trained language models have been shown to...\"],[\"## RagConfig\\n\\n[[autodoc]] RagConfig\\n\\n## RagTokenizer\\n\\n[[autodoc]] RagTokenizer\\n\\n## Rag specific outp...\"],[\"Robust Speech Challenge 🤗\\n\\nWelcome to the robust speech recognition challenge 🎙️ !\\n\\nThe goal of this...\"],[\"## TLDR\\n\\nParticipants are encouraged to leverage pre-trained speech recognition checkpoints,\\nprefera...\"],[\"For training, it is recommended to use the [official training script](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"During the event, the speech recognition system will be evaluated on both the Common Voice `\\\"test\\\"` ...\"],[\"## Important dates\\n\\n![timeline](https:\\u002f\\u002fgithub.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fraw\\u002fmaster\\u002fRob...\"],[\"```\\n\\nincludes more or less the same data as\\n\\n```python\\nload_dataset(\\\"mozilla-foundation\\u002fcommon_voice...\"],[\"```\\n\\nHowever, we strongly encourage participants to make use of Common Voice's other splits, *e.g.* ...\"],[\"Next, let's talk about preprocessing. Audio data and transcriptions have to be brought into the corr...\"],[\"It is allowed (and recommended) to normalize the data to only have lower-case characters. It is also...\"],[\"Since those choices are not always obvious when in doubt feel free to ask on Discord or even better ...\"],[\"```\\n\\nYou can activate your venv by running\\n\\n```bash\\nsource ~\\u002f\\u003cyour-venv-name\\u003e\\u002fbin\\u002factivate\\n```\\n\\nTo b...\"],[\"```\\n\\n4. Set up a PyTorch environment by running the following command your virtual environment:\\n\\n   ...\"],[\"```\\n$ cd ~\\u002f\\n$ git clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets.git\\n$ cd datasets\\n$ pip install -e \\\"...\"],[\"```\\n\\n## How to finetune an acoustic model\\n\\nIn this section, we show you how to fine-tune a pre-train...\"],[\"The blog can also be opened and directly fine-tuned in a google colab notebook.\\nIn this section, we ...\"],[\"```\\n\\nto login. It is recommended to login with your access token that can be found under your huggin...\"],[\"```\\n\\n3. **Add your training script and `run`-command to the repository**\\n\\nWe encourage participants ...\"],[\"```\\n\\nAlright, finally we can define the training script. We'll simply use some \\ndummy hyper-paramete...\"],[\"```\\n\\n4. **Start training**\\n\\nNow all that is left to do is to start training the model by executing t...\"],[\"```\\n\\n, clone it locally (assuming the `\\u003cusername\\u003e` is `hf-test`)\\n\\n```bash\\ngit clone hf-test\\u002fxls-r-30...\"],[\"```\\n\\n, and, define the following hyperparameters for training\\n\\n```bash\\necho '''python run_speech_rec...\"],[\"```\\n\\nThe training takes *ca.* 7 hours and yields a reasonable test word \\nerror rate of 27% as can be...\"],[\"### Setting up an AI notebook\\n1. Go to the `Public Cloud` page and select `Project Management` -\\u003e `U...\"],[\"For more quick tutorials about OVHcloud AI products, check out the showcase https:\\u002f\\u002fvimeo.com\\u002fshowca...\"],[\"## Evaluation\\n\\nFinally, we have arrived at the most fun part of the challenge - sitting back and\\nwat...\"],[\"```\\n\\nNext, we should adapt `eval.py` so that it fits our evaluation data. Here it is \\nimportant to k...\"],[\"- 1. The following input arguments should not be changed and keep their original functionality\\u002fmeani...\"],[\"- a. Somehow giving the model access to the target transcriptions to improve performance. The model ...\"],[\"Uff, that was a lot of text describing how to make sure your `eval.py` script \\nis in the correct for...\"],[\"```\\n\\nTo log each of the model's predictions with the target transcriptions, you can just \\nadd the `-...\"],[\"```\\n- \\\"sv\\\"\\n- \\\"robust-speech-event\\\"\\n```\\n\\nunder `tags:` as done [here](https:\\u002f\\u002fhuggingface.co\\u002fhf-test\\u002f...\"],[\"```\\n\\nThe dataset `WER_REAL_AUDIO_TEST` is hidden and will only be published \\nat the end of the robus...\"],[\"The following table summarizes what platform to use for which problem.\\n\\n- Problem\\u002fquestion\\u002fbug with ...\"],[\"We are very excited to be hosting 2 days of talks from Kensho-Technologies, Mozilla's Common Voice, ...\"],[\"### Friday, January 21th\\n\\n Speaker        | Topic                           | Time                  ...\"],[\"#### Raymond Grossman, Jeremy Lopez, Machine Learning Engineer, Kensho Technologies\\n- Talk: PyCTCDec...\"],[\"#### Changhan Wang, Main author of XLS-R and Research Engineer, Meta AI Research\\n- Talk: XLS-R: Larg...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [AI Sweden](https:\\u002f\\u002fhuggingface.co\\u002fAI-Sweden).\\n\\n## Usage example\\n\\n```p...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token cla...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"Testing mixed int8 quantization\\n\\n![HFxbitsandbytes.png](https:\\u002f\\u002fcdn-uploads.huggingface.co\\u002fproductio...\"],[\"```\\nFor the latest pytorch instructions please see [this](https:\\u002f\\u002fpytorch.org\\u002fget-started\\u002flocally\\u002f)\\n...\"],[\"```\\nls -l $CONDA_PREFIX\\u002flib\\u002flibcudart.so\\n```\\nor \\n```\\nls -l $LD_LIBRARY_PATH\\n```\\nCheck if `libcudart....\"],[\"```\\n\\nOn each path (`$path`) separated by `:`.\\nIf not, simply run\\n```bash\\nls -l $LD_LIBRARY_PATH\\u002flibc...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Driven by improved architectures and better represen...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pipelined NLP systems have largely been superseded b...\"],[\"## Usage tips\\n\\n- CANINE uses no less than 3 Transformer encoders internally: 2 \\\"shallow\\\" encoders (w...\"],[\"\\u003e\\u003e\\u003e model = CanineModel.from_pretrained(\\\"google\\u002fcanine-c\\\")  # model pre-trained with autoregressive ...\"],[\"```\\n\\nFor batched inference and training, it is however recommended to make use of the tokenizer (to ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Image segmentation groups pixels with different sema...\"],[\"## Usage tips\\n\\n- Mask2Former uses the same preprocessing and postprocessing steps as [MaskFormer](ma...\"],[\"## Mask2FormerModel\\n\\n[[autodoc]] Mask2FormerModel\\n    - forward\\n\\n## Mask2FormerForUniversalSegmentat...\"],[\"Plug and Play Language Models: a Simple Approach to Controlled Text Generation\\n\\nAuthors: [Sumanth Da...\"],[\"```\\n\\n### Tuning hyperparameters for bag-of-words control\\n\\n1. Increase `--stepsize` to intensify topi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### GPT-2\\u002fGPT and causal language modeling\\n\\nThe following example fine-tunes GPT-2 on WikiText-2. We...\"],[\"```\\n\\nThis takes about half an hour to train on a single K80 GPU and about one minute for the evaluat...\"],[\"```\\n\\n### RoBERTa\\u002fBERT\\u002fDistilBERT and masked language modeling\\n\\nThe following example fine-tunes RoBE...\"],[\"```\\n\\nIf your dataset is organized with one sample per line, you can use the `--line_by_line` flag (o...\"],[\"```\\n\\n**Note:** On TPU, you should use the flag `--pad_to_max_length` in conjunction with the `--line...\"],[\"```\\n\\nIf your dataset is organized with one sample per line, you can use the `--line_by_line` flag (o...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Reinforcement learning (RL) is typically concerned w...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nFor example:\\n\\n```bash\\ndoc-builder preview transformers docs\\u002fsource\\u002fen\\u002f\\n```\\n\\nThe docs will be vi...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\nFo...\"],[\"### Adding a new model\\n\\nWhen adding a new model:\\n\\n- Create a file `xxx.md` or under `.\\u002fsource\\u002fmodel_...\"],[\"```\\n## XXXConfig\\n\\n[[autodoc]] XXXConfig\\n```\\n\\nThis will include every public method of the configurat...\"],[\"```\\n## XXXTokenizer\\n\\n[[autodoc]] XXXTokenizer\\n    - all\\n    - __call__\\n```\\n\\n### Writing source docum...\"],[\"```\\n\\nIf the description is too long to fit in one line, another indentation is necessary before writ...\"],[\"```\\n```\\n# first line of code\\n# second line\\n# etc\\n```\\n````\\n\\nWe follow the [doctest](https:\\u002f\\u002fdocs.pyth...\"],[\"```\\n\\n#### Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that ...\"],[\"```\\n    Example:\\n\\n    ```python\\n    \\u003e\\u003e\\u003e from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\n ...\"],[\"```\\n```\\n\\nThe docstring should give a minimal, clear example of how the respective model \\nis to be us...\"],[\"```\\n\\n### For Markdown files\\n\\nYou can test locally a given file with this command (here testing the q...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Those processors inherit from the following base class that implements the saving and loading functi...\"],[\"Those processors are:\\n\\n- [`~data.processors.utils.MrpcProcessor`]\\n- [`~data.processors.utils.MnliPro...\"],[\"- [`~data.processors.utils.XnliProcessor`]\\n\\nPlease note that since the gold labels are available on ...\"],[\"[[autodoc]] data.processors.squad.squad_convert_examples_to_features\\n\\n\\nThese processors as well as t...\"],[\"```\\n\\nUsing *tensorflow_datasets* is as easy as using a data file:\\n\\n```python\\n# tensorflow_datasets o...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"`BrosForTokenClassification` has a simple linear layer on top of BrosModel. It predicts the label of...\"],[\"The abstract from the paper is the following:\\n\\n*Key information extraction (KIE) from document image...\"],[\"```python\\ndef expand_and_normalize_bbox(bboxes, doc_width, doc_height):\\n    # here, bboxes are numpy...\"],[\"```\\n\\n- [`~transformers.BrosForTokenClassification.forward`, `~transformers.BrosSpadeEEForTokenClassi...\"],[\"```\\n\\n## Resources\\n\\n- Demo scripts can be found [here](https:\\u002f\\u002fgithub.com\\u002fclovaai\\u002fbros).\\n\\n## BrosConf...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While large pretrained Transformer models have prove...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage example\\n\\n```py\\nimport torch\\nfrom transformers import AutoTokenizer, RwkvConfig, RwkvModel\\n\\n...\"],[\"```\\n\\nIf you want to make sure the model stops generating when `'\\\\n\\\\n'` is detected, we recommend usi...\"],[\"```\\n\\n## RwkvConfig\\n\\n[[autodoc]] RwkvConfig\\n\\n## RwkvModel\\n\\n[[autodoc]] RwkvModel\\n    - forward\\n\\n## Rw...\"],[\"In comparison, the RWKV attention is given by\\n\\n$$O_{i} = \\\\sigma(R_{i}) \\\\frac{\\\\sum_{j=1}^{i} e^{W_{i-...\"],[\"so \\\\\\\\(\\\\hat{N}_{i}\\\\\\\\) (called `numerator_state` in the code) satistfies\\n\\n$$\\\\hat{N}_{0} = 0 \\\\hbox{  an...\"],[\"with \\\\\\\\(M\\\\\\\\) the maximum of all \\\\\\\\(x_{j}\\\\\\\\). So here on top of saving the numerator state (\\\\\\\\(\\\\hat{N...\"],[\"and\\n\\n$$D_{i} = e^{u + K_{i} - q} + e^{M_{i}} \\\\tilde{D}_{i} \\\\hbox{  where  } q = \\\\max(u + K_{i}, M_{i...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Universal Image Segmentation is not a new concept. P...\"],[\"This model was contributed by [Jitesh Jain](https:\\u002f\\u002fhuggingface.co\\u002fpraeclarumjj3). The original code...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\n## With Accelerate\\n\\nBased on the script [run_swag_no_trainer.py](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"```\\n\\nand reply to the questions asked. Then\\n\\n```bash\\naccelerate test\\n```\\n\\nthat will check everything...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"...\"],[\"```\\n\\nNotice how the entire chat is condensed into a single string. If we use `tokenize=True`, which ...\"],[\"```\\n\\nNote that this time, the tokenizer has added the control tokens [INST] and [\\u002fINST] to indicate ...\"],[\"```\\n\\nNow that our input is formatted correctly for Zephyr, we can use the model to generate a respon...\"],[\"```\\n\\n```text\\nConversation id: 76d886a0-74bd-454e-9804-0467041a63dc\\nsystem: You are a friendly chatbo...\"],[\"```\\n\\nAnd here's what it looks like **with** a generation prompt:\\n\\n```python\\ntokenizer.apply_chat_tem...\"],[\"```\\n\\nNote that this time, we've added the tokens that indicate the start of a bot response. This ens...\"],[\"```\\nAnd we get:\\n```text\\n\\u003c|user|\\u003e\\nWhich is bigger, the moon or the sun?\\u003c\\u002fs\\u003e\\n\\u003c|assistant|\\u003e\\nThe sun.\\u003c\\u002fs...\"],[\"```\\n\\nIf you've never seen one of these before, this is a [Jinja template](https:\\u002f\\u002fjinja.palletsproje...\"],[\"```\\n{% for message in messages %}\\n    {% if message['role'] == 'user' %}\\n        {{ bos_token + '[IN...\"],[\"```\\n{% for message in messages %}\\n    {% if message['role'] == 'user' %}\\n        {{ bos_token + '[IN...\"],[\"```\\n\\nThe method [`~PreTrainedTokenizer.apply_chat_template`] which uses your chat template is called...\"],[\"If you're training a model from scratch, or fine-tuning a base language model for chat, on the other...\"],[\"```\\n{% for message in messages %}\\n    {{'\\u003c|im_start|\\u003e' + message['role'] + '\\\\n' + message['content']...\"],[\"```\\n\\nThe \\\"user\\\", \\\"system\\\" and \\\"assistant\\\" roles are the standard for chat, and we recommend using th...\"],[\"If you're unfamiliar with Jinja, we generally find that the easiest way to write a chat template is ...\"],[\"```\\n{% for message in messages %}\\n{{ message['content'] }}\\n{% endfor %}\\n```\\n\\nNote that whatever's in...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"🤗Transformersは、テキスト、視覚、音声などの異なるモダリティに対してタスクを実行するために、事前に学習させた数千のモデルを提供します。\\n\\nこれらのモデルは次のような場合に適用できます:\\n\\n...\"],[\"🤗Transformersは[Jax](https:\\u002f\\u002fjax.readthedocs.io\\u002fen\\u002flatest\\u002f)、[PyTorch](https:\\u002f\\u002fpytorch.org\\u002f)、[TensorFl...\"],[\"自然言語処理にて:\\n- [BERTによるマスクドワード補完](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+%5BMASK%5D...\"],[\"- [RoBERTaによる自然言語推論](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+an...\"],[\"-...\"],[\"[DistilBERTによる質問応答](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+i...\"],[\"h+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%2...\"],[\"orest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2...\"],[\"B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoner...\"],[\"Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadlea...\"],[\"broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C0...\"],[\"es+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilome...\"],[\"e+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory...\"],[\"erritory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60...\"],[\"+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+mino...\"],[\"ith+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States...\"],[\".+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents...\"],[\"presents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+bio...\"],[\"most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individua...\"],[\"ndividual+trees+divided+into+16%2C000+species)...\"],[\"- [T5による翻訳](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"コンピュータビジョンにて:\\n- [ViTによる画像分類](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-patch16-224)\\n- [DETRによる物体検出](htt...\"],[\"## Hugging Faceチームによるカスタム・サポートをご希望の場合\\n\\n\\u003ca target=\\\"_blank\\\" href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fsupport\\\"\\u003e\\n   ...\"],[\"```\\n\\n2行目のコードでは、pipelineで使用される事前学習済みモデルをダウンロードしてキャッシュし、3行目では与えられたテキストに対してそのモデルを評価します。ここでは、答えは99.97%の信...\"],[\"```\\n\\nここでは、画像から検出されたオブジェクトのリストが得られ、オブジェクトを囲むボックスと信頼度スコアが表示されます。左側が元画像、右側が予測結果を表示したものです:\\n\\n\\u003ch3 align=\\\"c...\"],[\"```\\n\\nそしてこちらはTensorFlowと同等のコードとなります:\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, TFAutoMode...\"],[\"```\\n\\nトークナイザは学習済みモデルが期待するすべての前処理を担当し、単一の文字列 (上記の例のように) またはリストに対して直接呼び出すことができます。これは下流のコードで使用できる辞書を出力しま...\"],[\"## なぜtransformersを使う必要があるのでしょうか？\\n\\n1. 使いやすい最新モデル:\\n    - 自然言語理解・生成、コンピュータビジョン、オーディオの各タスクで高いパフォーマンスを発揮し...\"],[\"1. モデルやサンプルをニーズに合わせて簡単にカスタマイズ可能:\\n    - 原著者が発表した結果を再現するために、各アーキテクチャの例を提供しています。\\n    - モデル内部は可能な限り一貫して公...\"],[\"## インストール\\n\\n### pipにて\\n\\nこのリポジトリは、Python 3.8+, Flax 0.4.1+, PyTorch 1.10+, TensorFlow 2.6+ でテストされています。\\n...\"],[\"```\\n\\nもしサンプルを試したい、またはコードの最先端が必要で、新しいリリースを待てない場合は、[ライブラリをソースからインストール](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftran...\"],[\"```\\n\\nFlax、PyTorch、TensorFlowをcondaでインストールする方法は、それぞれのインストールページに従ってください。\\n\\n\\u003e **_注意:_**  Windowsでは、キャッシュ...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (Google Research and the ...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BARTpho](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbartpho)** (VinAI Research から) Ngu...\"],[\"1. **[BERTweet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbertweet)** (VinAI Research から) D...\"],[\"1. **[BioGpt](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbiogpt)** (Microsoft Research AI4Sc...\"],[\"1. **[BlenderbotSmall](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot-small)** (Faceb...\"],[\"1. **[BORT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbort)** (Alexa から) Adrian de Wynter a...\"],[\"1. **[CamemBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcamembert)** (Inria\\u002fFacebook\\u002fSor...\"],[\"1. **[CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclip)** (OpenAI から) Alec Radford, Jon...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (MetaAI から) Baptis...\"],[\"1. **[ConvNeXT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconvnext)** (Facebook AI から) Zhua...\"],[\"1. **[CPM-Ant](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpmant)** (OpenBMB から) [OpenBMB](h...\"],[\"1. **[DeBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta)** (Microsoft から) Pengchen...\"],[\"1. **[DeiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeit)** (Facebook から) Hugo Touvron, M...\"],[\"1. **[DialoGPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdialogpt)** (Microsoft Research か...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (HuggingFace から),...\"],[\"1. **[Donut](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdonut)** (NAVER から), Geewook Kim, Te...\"],[\"1. **[EfficientNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientnet)** (from Google ...\"],[\"1. **[ERNIE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie)** (Baidu から) Yu Sun, Shuohuan...\"],[\"1. **[ESM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fesm)** (Meta AI から) はトランスフォーマープロテイン言語モ...\"],[\"1. **[FLAN-T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-t5)** (Google AI から) Hyung Wo...\"],[\"1. **[FlauBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflaubert)** (CNRS から) Hang Le, Lo...\"],[\"1. **[Funnel Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffunnel)** (CMU\\u002fGoogle B...\"],[\"1. **[GPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopenai-gpt)** (OpenAI から) Alec Radford...\"],[\"1. **[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt2)** (OpenAI から) Alec Radford*, J...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (BigCode から) Lou...\"],[\"1. **[Graphormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgraphormer)** (Microsoft から) Ch...\"],[\"1. **[I-BERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fibert)** (Berkeley から) Sehoon Kim, ...\"],[\"1. **[InstructBLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002finstructblip)** (Salesforce か...\"],[\"1. **[LayoutLMv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv2)** (Microsoft Resear...\"],[\"1. **[LeViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flevit)** (Meta AI から) Ben Graham, Al...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (The FAIR team of Meta AI...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (Microsoft Research & Unive...\"],[\"1. **[LXMERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flxmert)** (UNC Chapel Hill から) Hao ...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[MaskFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmaskformer)** (Meta and UIUC から...\"],[\"1. **[MEGA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmega)** (Facebook から) Xuezhe Ma, Chun...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MMS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmms)** (Facebook から) Vineel Pratap, An...\"],[\"1. **[MobileNetV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v2)** (Google Inc. か...\"],[\"1. **[MPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmpt)** (MosaiML から) the MosaicML NLP T...\"],[\"1. **[MVP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmvp)** (RUC AI Box から) Tianyi Tang, Ju...\"],[\"1. **[NLLB-MOE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnllb-moe)** (Meta から) the NLLB te...\"],[\"1. **[OpenLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopen-llama)** (from [s-JoL](http...\"],[\"1. **[PatchTSMixer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpatchtsmixer)** ( IBM Researc...\"],[\"1. **[Perceiver IO](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fperceiver)** (Deepmind から) An...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft) released with ...\"],[\"1. **[PLBart](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fplbart)** (UCLA NLP から) Wasi Uddin ...\"],[\"1. **[PVT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpvt)** (Nanjing University, The Univer...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (Google Research から) ...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (Facebook から), Yinhan L...\"],[\"1. **[RWKV](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frwkv)** (Bo Peng から) Bo Peng. から公開された...\"],[\"1. **[Segment Anything](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsam)** (Meta AI から) Alexa...\"],[\"1. **[SpeechT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeecht5)** (Microsoft Research か...\"],[\"1. **[SqueezeBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsqueezebert)** (Berkeley から) F...\"],[\"1. **[Swin2SR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswin2sr)** (University of Würzburg...\"],[\"1. **[T5v1.1](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5v1.1)** (Google AI から) Colin Raff...\"],[\"1. **[Time Series Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftime_series_transf...\"],[\"1. **[TrOCR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftrocr)** (Microsoft から), Minghao Li,...\"],[\"1. **[UMT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fumt5)** (Google Research から) Hyung Wo...\"],[\"1. **[UnivNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funivnet)** (from Kakao Corporation...\"],[\"1. **[ViLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvilt)** (NAVER AI Lab\\u002fKakao Enterpris...\"],[\"1. **[VisualBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvisual_bert)** (UCLA NLP から) Li...\"],[\"1. **[ViTMatte](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvitmatte)** (HUST-VL から) Jingfeng...\"],[\"1. **[Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2)** (Facebook AI から) Alex...\"],[\"1. **[WavLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwavlm)** (Microsoft Research から) San...\"],[\"1. **[X-MOD](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxmod)** (Meta AI から) Jonas Pfeiffer,...\"],[\"1. **[XLM-ProphetNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-prophetnet)** (Microsof...\"],[\"1. **[XLM-V](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-v)** (Meta AI から) Davis Liang, H...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (Facebook A...\"],[\"1. 新しいモデルを投稿したいですか？新しいモデルを追加するためのガイドとして、**詳細なガイドとテンプレート**が追加されました。これらはリポジトリの[`templates`](.\\u002ftemplate...\"],[\"各モデルがFlax、PyTorch、TensorFlowで実装されているか、🤗Tokenizersライブラリに支えられた関連トークナイザを持っているかは、[この表](https:\\u002f\\u002fhuggingfa...\"],[\"## さらに詳しく\\n\\n| セクション | 概要 |\\n|-|-|\\n| [ドキュメント](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002f) | 完全なAPIドキュメント...\"],[\"## 引用\\n\\n🤗 トランスフォーマーライブラリに引用できる[論文](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)が出来ました:\\n```bi...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It should be noted that each of the first three modules can support conditional speaker embeddings t...\"],[\"```\\n\\n#### Using CPU offload\\n\\nAs mentioned above, Bark is made up of 4 sub-models, which are called u...\"],[\"```\\n\\nNote that 🤗 Optimum must be installed before using this feature. [Here's how to install it.](ht...\"],[\"```\\n\\n##### Performance comparison\\n\\n\\nThe following diagram shows the latency for the native attention...\"],[\"```\\n\\nFind out more on inference optimization techniques [here](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransform...\"],[\"```\\n\\nBark can generate highly realistic, **multilingual** speech as well as other audio - including ...\"],[\"```\\n\\n## BarkConfig\\n\\n[[autodoc]] BarkConfig\\n    - all\\n\\n## BarkProcessor\\n\\n[[autodoc]] BarkProcessor\\n  ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*BERT adopts masked language modeling (MLM) for pre-t...\"],[\"## MPNetConfig\\n\\n[[autodoc]] MPNetConfig\\n\\n## MPNetTokenizer\\n\\n[[autodoc]] MPNetTokenizer\\n    - build_i...\"],[\"[[autodoc]] TFMPNetForMultipleChoice\\n    - call\\n\\n## TFMPNetForTokenClassification\\n\\n[[autodoc]] TFMPN...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Vision-Language Pre-training (VLP) has advanced the ...\"],[\"## BlipConfig\\n\\n[[autodoc]] BlipConfig\\n    - from_text_vision_configs\\n\\n## BlipTextConfig\\n\\n[[autodoc]]...\"],[\"## TFBlipVisionModel\\n\\n[[autodoc]] TFBlipVisionModel\\n    - call\\n\\n## TFBlipForConditionalGeneration\\n\\n[...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recent studies have shown that multilingual pretrain...\"],[\"```\\n\\nNote that mLUKE has its own tokenizer, [`MLukeTokenizer`]. You can initialize it as follows:\\n\\n`...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*Building open-domain chatbots is a challenging area fo...\"],[\"An example:\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import BlenderbotTokenizer, BlenderbotForConditionalGen...\"],[\"```\\n\\n## Implementation Notes\\n\\n- Blenderbot uses a standard [seq2seq model transformer](https:\\u002f\\u002farxiv...\"],[\"## BlenderbotForCausalLM\\n\\n[[autodoc]] BlenderbotForCausalLM\\n    - forward\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n## TFBlenderb...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"A Hugging Face team member will be available to help you along the way so you'll never be alone. 🤗 ❤...\"],[\"With this in mind, let's go a bit deeper into the general library design.\\n\\n### Overview of models\\n\\nT...\"],[\"```python\\nmodel = BrandNewBertModel.from_pretrained(\\\"brandy\\u002fbrand_new_bert\\\")\\nmodel.config  # model h...\"],[\"```\\n\\nSimilar to the model, the configuration inherits basic serialization and deserialization functi...\"],[\"### Overview of tokenizers\\n\\nNot quite ready yet :-( This section will be added soon!\\n\\n## Step-by-ste...\"],[\"In the following, we try to give you a general recipe that we found most useful when porting a model...\"],[\"-  What type of model is *brand_new_bert*? BERT-like encoder-only model? GPT2-like decoder-only mode...\"],[\"```\\n\\n3. Set up a development environment, for instance by running the following command:\\n\\n```bash\\npy...\"],[\"```\\n\\nNow you have set up a development environment to port *brand_new_bert* to 🤗 Transformers.\\n\\n### ...\"],[\"It is very important that before you start the porting process, you can **efficiently** debug code i...\"],[\"```python\\nmodel = BrandNewBertModel.load_pretrained_checkpoint(\\\"\\u002fpath\\u002fto\\u002fcheckpoint\\u002f\\\")\\ninput_ids = [...\"],[\"```\\n\\nNext, regarding the debugging strategy, there are generally a few from which to choose from:\\n\\n-...\"],[\"However, if the original code-base is very complex or only allows intermediate components to be run ...\"],[\"```\\n[[\\n [-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024],\\n [-0.4417, -0.5920,  0.3450,  ...\"],[\"```\\n\\nWe expect that every model added to 🤗 Transformers passes a couple of integration tests, meanin...\"],[\"- Find the best way of debugging intermediate results. Is the original repository written in PyTorch...\"],[\"you have to input a string, then try to find out where in the forward call the string input is chang...\"],[\"The following section gives you more specific details\\u002ftips on how you can do this for *brand_new_ber...\"],[\"```\\n\\nIn the special case that you are adding a model whose architecture exactly matches the model ar...\"],[\"```\\n\\n4. Push the changes to your account using:\\n\\n```bash\\ngit push -u origin a-descriptive-name-for-m...\"],[\"```\\n\\nIn general, all questions you might have regarding the model or your implementation should be a...\"],[\"**Note** that at this point, you don't have to be very sure that your code is fully correct or clean...\"],[\"```\\n\\nThe above command will create a model according to the default parameters as defined in `BrandN...\"],[\"```\\n\\nYou can have some more custom schemes if you need a special initialization for some modules. Fo...\"],[\"```\\n\\nThe `_is_hf_initialized` flag is internally used to make sure we only initialize a submodule on...\"],[\"In the following, we'll quickly explain how PyTorch models store layer weights and define layer name...\"],[\"```\\n\\nNow we can create an instance of this model definition which will fill all weights: `dense`, `i...\"],[\"```\\ntensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\\n         -0.207...\"],[\"```\\n\\nIn the conversion script, you should fill those randomly initialized weights with the exact wei...\"],[\"```\\n\\nIf either the shape or the name doesn't match, you probably assigned the wrong checkpoint weigh...\"],[\"```\\n\\n**7. Implement the forward pass**\\n\\nHaving managed to correctly load the pretrained weights into...\"],[\"```\\n\\nIt is very likely that the 🤗 Transformers implementation and the original model implementation ...\"],[\"The best way to fix the problem is usually to look at the forward pass of the original implementatio...\"],[\"```\\n\\nHaving fixed all common tests, it is now crucial to ensure that all the nice work you have done...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIn case you are using Windows, you should replace `RUN_SLOW=1` with `SET RUN_SLOW=1`\\n\\n\\u003c\\u002f...\"],[\"```\\n\\nYou might have to take a deeper look again into the original repository to find the correct tok...\"],[\"```\\n\\nWhen both `input_ids` yield the same values, as a final step a tokenizer test file should also ...\"],[\"Next, make sure that the docstring added to `src\\u002ftransformers\\u002fmodels\\u002fbrand_new_bert\\u002fmodeling_brand_n...\"],[\"```\\n\\nand verify that your coding style passes the quality check:\\n\\n```bash\\nmake quality\\n```\\n\\nThere ar...\"],[\"```\\n\\nIt is worth spending some time to create fitting model cards for each checkpoint. The model car...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Self-training\\n\\nThis is an implementation of the self-training algorithm (without task augmentation) ...\"],[\"```\\nThis will install PyTorch as a backend.\\n\\n## Self-training\\n### Running self-training with a base ...\"],[\"```python\\nimport os\\nfrom selftraining import selftrain\\n\\ndata_dir = '\\u002fpath\\u002fto\\u002fyour\\u002fdata\\u002fdir'\\nparamete...\"],[\"```\\n\\n**Note**: We checkpoint periodically during self-training. In case of preemptions, just re-run ...\"],[\"```\\n\\n2. Run your script with the following command:\\n\\n```sh\\npython -m torch.distributed.launch --nnod...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n2. Pass your input to the [`pipeline`]. In the case of speech recognition, this is an audio inp...\"],[\"```\\n\\nNow this result looks more accurate! For a deep-dive comparison on Wav2Vec2 vs Whisper, refer t...\"],[\"```\\n\\nLet's check out 3 important ones:\\n\\n### Device\\n\\nIf you use `device=n`, the pipeline automaticall...\"],[\"```\\n\\nThis runs the pipeline on the 4 provided audio files, but it will pass them in batches of 2\\nto ...\"],[\"```\\n\\nAs you can see, the model inferred the text and also outputted **when** the various sentences w...\"],[\"```\\n\\nThe iterator `data()` yields each result, and the pipeline automatically\\nrecognizes the input i...\"],[\"```\\n\\n\\n## Using pipelines for a webserver\\n\\n\\u003cTip\\u003e\\nCreating an inference engine is a complex topic whic...\"],[\"```\\n\\n## Text pipeline\\n\\nUsing a [`pipeline`] for NLP tasks is practically identical.\\n\\n```py\\n\\u003e\\u003e\\u003e from ...\"],[\"```\\n\\n## Multimodal pipeline\\n\\nThe [`pipeline`] supports more than one modality. For example, a visual...\"],[\"```\\n\\n\\u003c\\u002fTip\\u003e\\n\\n## Using `pipeline` on large models with 🤗 `accelerate`:\\n\\nYou can easily run `pipeline`...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"From the abstract of the XLM-V paper:\\n\\n*Large multilingual language models typically rely on a singl...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Combining simple architectures with large-scale pre-...\"],[\"## Usage tips\\n\\nOWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses [CLIP](c...\"],[\"\\u003e\\u003e\\u003e url = \\\"http:\\u002f\\u002fimages.cocodataset.org\\u002fval2017\\u002f000000039769.jpg\\\"\\n\\u003e\\u003e\\u003e image = Image.open(requests.g...\"],[\"```\\n\\n## Resources\\n\\nA demo notebook on using OWL-ViT for zero- and one-shot (image-guided) object det...\"],[\"Security Policy\\n\\n## Reporting a Vulnerability\\n\\n🤗 We have our bug bounty program set up with HackerOn...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"*The MobileNetV2 architecture is based on an inverted residual structure where the input and output ...\"],[\"- One can use [`MobileNetV2ImageProcessor`] to prepare images for the model.\\n\\n- The available image ...\"],[\"- The DeepLabV3+ segmentation head does not use the final convolution layer from the backbone, but t...\"],[\"## MobileNetV2ForImageClassification\\n\\n[[autodoc]] MobileNetV2ForImageClassification\\n    - forward\\n\\n#...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"According to the abstract\\n\\n*Code summarization and generation empower conversion between programming...\"],[\"However, for fine-tuning, in some cases no language token is provided in cases where a single langua...\"],[\"```\\n\\n### Generation\\n\\n  While generating the target text set the `decoder_start_token_id` to the targ...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Causal la...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce a self-supervised vision representation...\"],[\"- BEiT models are regular Vision Transformers, but pre-trained in a self-supervised way rather than ...\"],[\"- The available checkpoints are either (1) pre-trained on [ImageNet-22k](http:\\u002f\\u002fwww.image-net.org\\u002f) ...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransformers...\"],[\"[[autodoc]] BeitFeatureExtractor\\n    - __call__\\n    - post_process_semantic_segmentation\\n\\n## BeitIma...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This version of the model is for tasks where the state is a vector.\\n\\nThis model was contributed by [...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*This paper presents XLS-R, a large-scale model for c...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage example\\n\\n```python\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transformers import AutoModel, AutoTokenizer\\n\\n...\"],[\"```\\n\\n\\u003cTip\\u003e \\n\\nPhoBERT implementation is the same as BERT, except for tokenization. Refer to [EART doc...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Text recognition is a long-standing research problem...\"],[\"## Usage tips\\n\\n- The quickest way to get started with TrOCR is by checking the [tutorial\\n  notebooks...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- A blog post on [Accelerating Document AI](https:\\u002f\\u002fh...\"],[\"- [Casual language modeling](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002ftasks\\u002flanguage_modeling) task ...\"],[\"\\u003e\\u003e\\u003e pixel_values = processor(image, return_tensors=\\\"pt\\\").pixel_values\\n\\u003e\\u003e\\u003e generated_ids = model.gene...\"],[\"```\\n\\nSee the [model hub](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=trocr) to look for TrOCR checkpoints.\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n`compile()` comes with multiple modes for compiling, which essentially differ in compilation ti...\"],[\"```\\n\\n#### Object Detection with DETR\\n\\n```python \\nfrom transformers import AutoImageProcessor, AutoMo...\"],[\"```\\n\\nBelow you can find the list of the models we benchmarked.\\n\\n**Image Classification** \\n- [google\\u002f...\"],[\"Below you can find visualization of inference durations with and without `torch.compile()` and perce...\"],[\"Below you can find inference durations in milliseconds for each model with and without `compile()`. ...\"],[\"### A100 (batch size: 4)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecomp...\"],[\"### A100 (batch size: 16)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecom...\"],[\"### V100 (batch size: 1)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecomp...\"],[\"### V100 (batch size: 4)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecomp...\"],[\"### V100 (batch size: 16)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecom...\"],[\"### T4 (batch size: 1)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecompil...\"],[\"### T4 (batch size: 4)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecompil...\"],[\"### T4 (batch size: 16)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecompi...\"],[\"### A100\\n\\n| **Task\\u002fModel** | **Batch Size** | **torch 2.0 - no compile** | **torch 2.0 -\\u003cbr\\u003e compile...\"],[\"### V100\\n\\n| **Task\\u002fModel** | **Batch Size** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecom...\"],[\"### T4\\n\\n| **Task\\u002fModel** | **Batch Size** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecompi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nWith the `mps` device set, you can:\\n\\n* train larger networks or batch sizes locally\\n* reduce...\"],[\"```\\n\\n[`TrainingArguments`] uses the `mps` device by default if it's available which means you don't ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransformers...\"],[\"- Step-by-step PDF transcription\\n\\n```py\\n\\u003e\\u003e\\u003e from huggingface_hub import hf_hub_download\\n\\u003e\\u003e\\u003e import r...\"],[\"\\u003e\\u003e\\u003e sequence = processor.batch_decode(outputs, skip_special_tokens=True)[0]\\n\\u003e\\u003e\\u003e sequence = processor...\"],[\"```\\n\\nSee the [model hub](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=nougat) to look for Nougat checkpoints...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-supervised approaches for speech representation...\"],[\"## HubertConfig\\n\\n[[autodoc]] HubertConfig\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## HubertModel\\n\\n[[autodoc]] Hube...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It extends [NAT](nat) by adding a Dilated Neighborhood Attention pattern to capture global context,\\n...\"],[\"\\u003cimg\\nsrc=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdilated-neig...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[`SeamlessM4TModel`] can perform all the above tasks, but each task also has its own dedicated sub-m...\"],[\"## Usage\\n\\nFirst, load the processor and a checkpoint of the model:\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers ...\"],[\"```\\n\\nYou can seamlessly use this model on text or on audio, to generated either translated text or t...\"],[\"```\\n\\nWith basically the same code, I've translated English text and Arabic speech to Russian speech ...\"],[\"```\\n\\nFeel free to try out [`SeamlessM4TForSpeechToText`] and [`SeamlessM4TForTextToSpeech`] as well....\"],[\"This model was contributed by [ylacombe](https:\\u002f\\u002fhuggingface.co\\u002fylacombe). The original code can be ...\"],[\"[[autodoc]] SeamlessM4TProcessor\\n    - __call__\\n\\n## SeamlessM4TCodeHifiGan\\n\\n[[autodoc]] SeamlessM4TC...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"3. Check the [Migration](migration) guide if you use an older version of 🤗 Transformers since some i...\"],[\"```\\nValueError: Connection error, and we cannot find the requested files in the cached path.\\nPlease ...\"],[\"```\\n\\nHere are some potential solutions you can try to lessen memory use:\\n\\n- Reduce the [`per_device_...\"],[\"```\\n\\n- Save the model with [`~TFPretrainedModel.save_pretrained`] and load it again with [`~TFPreTra...\"],[\"```\\n\\n## Incorrect output when padding tokens aren't masked\\n\\nIn some cases, the output `hidden_state`...\"],[\"```\\n\\nMost of the time, you should provide an `attention_mask` to your model to ignore the padding to...\"],[\"```\\n\\n🤗 Transformers doesn't automatically create an `attention_mask` to mask a padding token if it i...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n\\u003e Visually-situated language is ubiquitous -- sources...\"],[\"If you want to use the model to perform conditional text captioning, make sure to use the processor ...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"## The Big Table of Tasks\\n\\nHere is the list of all our examples:\\n\\n| Task | Example datasets |\\n|---|-...\"],[\"Patience-based Early Exit\\n\\nPatience-based Early Exit (PABEE) is a plug-and-play inference method for...\"],[\"```\\n\\n## Inference\\n\\nYou can inference with different patience settings by:\\n```bash\\nexport GLUE_DIR=\\u002fp...\"],[\"```\\nwhere `patience` can be a list of patience settings, separated by a comma. It will help determin...\"],[\"| Model         | \\\\#Param | Speed\\\\-up | MNLI  | SST\\\\-2 | STS\\\\-B |\\n|---------------|---------|-------...\"],[\"Long Form Question Answering\\n\\nAuthor: @yjernite\\n\\nThis folder contains the code for the Long Form Que...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recently, neural networks purely based on attention ...\"],[\"- Compared to ViT, DeiT models use a so-called distillation token to effectively learn from a teache...\"],[\"contrast with the original ViT model, which used external data like the JFT-300M dataset\\u002fImagenet-21...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"## DeiTForImageClassification\\n\\n[[autodoc]] DeiTForImageClassification\\n    - forward\\n\\n## DeiTForImage...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Light-weight convolutional neural networks (CNNs) ar...\"],[\"## Usage tips\\n\\n- MobileViT is more like a CNN than a Transformer model. It does not work on sequence...\"],[\"model_ckpt = \\\"apple\\u002fmobilevit-xx-small\\\"\\nmodel = TFMobileViTForImageClassification.from_pretrained(mo...\"],[\"```\\n\\n  The resulting model will be just **about an MB** making it a good fit for mobile applications...\"],[\"## MobileViTForImageClassification\\n\\n[[autodoc]] MobileViTForImageClassification\\n    - forward\\n\\n## Mo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[ALBERT](..\\u002fmodel_doc\\u002falbert), [BART](..\\u002fmodel_doc\\u002fbart), [BERT](..\\u002fmodel_doc\\u002fbert), [BigBird](..\\u002fmo...\"],[\"Neo](..\\u002fmodel_doc\\u002fgpt_neo), [GPT NeoX](..\\u002fmodel_doc\\u002fgpt_neox), [GPT-J](..\\u002fmodel_doc\\u002fgptj), [I-BERT](...\"],[\"[Nyströmformer](..\\u002fmodel_doc\\u002fnystromformer), [OPT](..\\u002fmodel_doc\\u002fopt), [QDQBert](..\\u002fmodel_doc\\u002fqdqbert...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nThen take a look at an example:\\n\\n```py\\n\\u003e\\u003e\\u003e squad[\\\"train\\\"][0]\\n{'answers': {'answer_start': [515]...\"],[\"```\\n\\nThere are a few preprocessing steps particular to question answering tasks you should be aware ...\"],[\"...         # Find the start and end of the context\\n...         idx = 0\\n...         while sequence_i...\"],[\"```\\n\\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.Dataset...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\u003cTip\\u003e\\n\\nIf you aren't familiar with finetuning a model with Keras, take a look at the ...\"],[\"```\\n\\nConfigure the model for training with [`compile`](https:\\u002f\\u002fkeras.io\\u002fapi\\u002fmodels\\u002fmodel_training_ap...\"],[\"```\\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use ...\"],[\"```\\n\\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ...\"],[\"```\\n\\nDecode the predicted tokens to get the answer:\\n\\n```py\\n\\u003e\\u003e\\u003e predict_answer_tokens = inputs.input_...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Expanding the language coverage of speech technology...\"],[\"Tips:\\n\\n- All ASR models accept a float array corresponding to the raw waveform of the speech signal....\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nYou can safely ignore a warning such as:\\n\\n```text\\nSome weights of Wav2Vec2ForCTC were no...\"],[\"```\\n\\n#### Inference\\n\\nNext, let's look at how we can run MMS in inference and change adapter layers a...\"],[\"```\\n\\nNow we process the audio data, pass the processed audio data to the model and transcribe the mo...\"],[\"```\\n\\nIn the same way the language can be switched out for all other supported languages. Please have...\"],[\"```\\n\\nThe resulting waveform can be saved as a `.wav` file:\\n\\n```python\\nimport scipy\\n\\nscipy.io.wavfile...\"],[\"```\\n\\n**Tips:**\\n\\n* The MMS-TTS checkpoints are trained on lower-cased, un-punctuated text. By default...\"],[\"```\\n\\n### Language Identification (LID)\\n\\nDifferent LID models are available based on the number of la...\"],[\"```\\n\\nNext, we load the model and processor\\n\\n```py\\nfrom transformers import Wav2Vec2ForSequenceClassi...\"],[\"```\\n\\nTo see all the supported languages of a checkpoint, you can print out the language ids as follo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## BetterTransformer\\n\\nBetterTransformer accelerates inference with its fastpath (native PyTorch spec...\"],[\"```\\n\\n## TorchScript\\n\\nTorchScript is an intermediate PyTorch model representation that can be run in ...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nFor PyTorch \\u003e= 1.14.0, JIT-mode could benefit any model for prediction an...\"],[\"```\\n\\n## 🤗 Optimum\\n\\n\\u003cTip\\u003e\\n\\nLearn more details about using ORT with 🤗 Optimum in the [Optimum Inferenc...\"],[\"!--Copyright 2023 The HuggingFace and Baidu Team. All rights reserved.\\n\\nLicensed under the Apache Li...\"],[\"The abstract from the paper is the following:\\n\\n*Recent studies have demonstrated that pre-trained cr...\"],[\"## ErnieMConfig\\n\\n[[autodoc]] ErnieMConfig\\n\\n\\n## ErnieMTokenizer\\n\\n[[autodoc]] ErnieMTokenizer\\n    - bu...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Contrastive language-image pretraining has shown gre...\"],[\"\\u003csmall\\u003e X-CLIP architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.02816\\\"\\u003eoriginal pape...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nHaving downloaded COCO dataset manually you should be able to load with the `ydshieh\\u002fcoc_datase...\"],[\"```\\n\\nThis loads both the text and vision encoders using pre-trained weights, the projection layers a...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nTo convert lots of models you can pass your list of Tatoeba model names to `resolver.convert_mo...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[`PreTrainedTokenizer`] and [`PreTrainedTokenizerFast`] thus implement the main\\nmethods for using al...\"],[\"## PreTrainedTokenizerFast\\n\\nThe [`PreTrainedTokenizerFast`] depend on the [tokenizers](https:\\u002f\\u002fhuggi...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```py\\nfrom starlette.applications import Starlette\\nfrom starlette.responses import JSONResponse\\nfrom...\"],[\"```\\n\\nNow you can start it with:\\n```bash\\nuvicorn server:app\\n```\\n\\nAnd you can query it:\\n```bash\\ncurl -...\"],[\"```\\n\\nAgain, the proposed code is optimized for readability, not for being the best code.\\nFirst of al...\"],[\"### Blocking the main thread\\n\\nCurrently PyTorch is not async aware, and computation will block the m...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [valhalla](https:\\u002f\\u002fhuggingface.co\\u002fvalhalla). The original code can be ...\"],[\"```python\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transformers import Speech2TextProcessor, Speech2TextForConditio...\"],[\"```\\n\\n- Multilingual speech translation\\n\\n  For multilingual speech translation models, `eos_token_id`...\"],[\"```\\n\\nSee the [model hub](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=speech_to_text) to look for Speech2Tex...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While originally designed for natural language proce...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recently, significant progress has been made applyin...\"],[\"## Resources\\n\\n\\u003cPipelineTag pipeline=\\\"object-detection\\\"\\u002f\\u003e\\n\\n- A demo notebook for the Table Transforme...\"],[\"# Sequence to Sequence Training and Evaluation\\n\\nThis directory contains examples for finetuning and ...\"],[\"```\\nthis should make a directory called `wmt_en_ro\\u002f` with 6 files.\\n\\n#### WMT English-German\\n\\n```bash...\"],[\"```\\nThe `.source` files are the input, the `.target` files are the desired output.\\n\\n### Potential is...\"],[\"Summarization Tips:\\n- (summ) 1 epoch at batch size 1 for bart-large takes 24 hours and requires 13GB...\"],[\"To see all the possible command line options, run:\\n\\n```bash\\n.\\u002ffinetune.py --help...\"],[\"```\\n\\n### Finetuning Training Params\\n\\nTo override the pretrained model's training params, you can pas...\"],[\"```\\nThis should take \\u003c 6h\\u002fepoch on a 16GB v100 and achieve test BLEU above 26\\nTo get results in line...\"],[\"```\\n### Finetuning Outputs\\nAs you train, `output_dir` will be filled with files, that look kind of l...\"],[\"```\\n\\n### Converting pytorch-lightning checkpoints\\npytorch lightning ``-do_predict`` often fails, aft...\"],[\"```\\nuses 12,723 batches of length 48 and takes slightly more time 9.5 minutes.\\n\\nThe feature is still...\"],[\"![DBART](https:\\u002f\\u002fhuggingface.co\\u002ffront\\u002fthumbnails\\u002fdistilbart_large.png)\\n\\n+ For the CNN\\u002fDailyMail data...\"],[\"### Evaluation\\n\\nuse [run_distributed_eval](.\\u002frun_distributed_eval.py), with the following convenient...\"],[\"```\\nOn a 1 GPU system, here are four commands (that assume `xsum`, `cnn_dm` are downloaded, cmd-F fo...\"],[\"```\\n\\n### Distillation\\n+ For all of the following commands, you can get roughly equivalent result and...\"],[\"```\\nor for `pegasus-xsum`\\n```bash\\npython make_student.py google\\u002fpegasus-xsum --save_path dpx_xsum_16...\"],[\"```\\n\\n+ Note: The command that produced `sshleifer\\u002fdistilbart-cnn-12-6` is at [train_distilbart_cnn.s...\"],[\"```\\n\\u003c!--- runtime: 6H on NVIDIA RTX 24GB GPU --\\u003e\\n+ Tip: You can get the same simple distillation log...\"],[\"```\\n\\n\\n\\nTo combine datasets, as in Section 6.2, try something like:\\n```bash\\ncurl -S https:\\u002f\\u002fcdn-datas...\"],[\"```\\n\\n+ Expected ROUGE-2 between 21.3 and 21.6, run time ~13H.\\n+ direct KD + Pegasus is VERY slow and...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nThe `Fuyu` models were trained using `bfloat16`, but the original inference us...\"],[\"- To convert the model, you need to clone the original repository using `git clone https:\\u002f\\u002fgithub.co...\"],[\"```\\n\\nFor the chat model:\\n```bash\\nwget https:\\u002f\\u002faxtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-o...\"],[\"```\\n\\nThis model was contributed by [Molbap](https:\\u002f\\u002fhuggingface.co\\u002fMolbap).\\nThe original code can be...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cbr\\u003e\\n\\nFeel free to check out the [API reference](.\\u002fmain_classes\\u002ftrainer) for these other [`Trainer`]...\"],[\"```\\n\\nThis guide provides an overview of the [`Trainer`] class.\\n\\n## Basic usage\\n\\n[`Trainer`] includes...\"],[\"```\\n\\nPass `training_args` to the [`Trainer`] along with a model, dataset, something to preprocess th...\"],[\"```\\n\\nYou can save your checkpoints (the optimizer state is not saved by default) to the Hub by setti...\"],[\"* [`~Trainer.get_train_dataloader`] creates a training DataLoader\\n* [`~Trainer.get_eval_dataloader`]...\"],[\"```\\n\\n### Callbacks\\n\\nAnother option for customizing the [`Trainer`] is to use [callbacks](callbacks)....\"],[\"```\\n\\n## Logging\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [logging](.\\u002fmain_classes\\u002flogging) API reference for more infor...\"],[\"For example, to set your main code and modules to use the same log level according to each node:\\n\\n``...\"],[\"```\\n\\nUse different combinations of `log_level` and `log_level_replica` to configure what gets logged...\"],[\"```\\n\\nNEFTune is disabled after training to restore the original embedding layer to avoid any unexpec...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"FSDP\\\"\\u003e\\n\\n```yml\\ncompute_environment: LOCAL_MACHINE\\ndistributed_type: F...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"DeepSpeed with Accelerate plugin\\\"\\u003e\\n\\n```yml\\ncompute_environment: LOCAL...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\nThe [`accelerate_launch`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002faccelerate\\u002fpack...\"],[\"```\\n\\nYou could also specify the parameters from the `config_file.yaml` file directly in the command ...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"The following examples show how to fine-tune a `\\\"base\\\"`-sized Wav2Vec2 model as well as a `\\\"large\\\"`-...\"],[\"```bash\\naccelerate launch run_wav2vec2_pretraining_no_trainer.py \\\\\\n\\t--dataset_name=\\\"librispeech_asr\\\"...\"],[\"```\\n\\nThe results of this run can be seen [here](https:\\u002f\\u002fwandb.ai\\u002fpatrickvonplaten\\u002fwav2vec2-pretraine...\"],[\"```bash\\naccelerate launch run_wav2vec2_pretraining_no_trainer.py \\\\\\n\\t--dataset_name=librispeech_asr \\\\...\"],[\"```\\n\\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 4 days. \\nIn case you have more than 8...\"],[\"```bash\\naccelerate launch run_wav2vec2_pretraining_no_trainer.py \\\\ \\n\\t--dataset_name=librispeech_asr ...\"],[\"```\\n\\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 7 days. \\nIn case you have more than 8...\"],[\"p align=\\\"center\\\"\\u003e \\u003cimg src=\\\"http:\\u002f\\u002fsayef.tech:8082\\u002fuploads\\u002fFSNER-LOGO-2.png\\\" alt=\\\"FSNER LOGO\\\"\\u003e \\u003c\\u002fp\\u003e\\n...\"],[\"You can use the FSNER model in 3 ways:\\n\\n1. Install directly from PyPI: `pip install fsner` and impor...\"],[\"# each list in supports are the examples of one entity type\\n# wrap entities around with [E] and [\\u002fE]...\"],[\"!--⚠️ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar t...\"],[\"| Notebook     |      Description      |      Author      |      |\\n|:----------|:-------------|:----...\"],[\"| [Train T5 on TPU](https:\\u002f\\u002fgithub.com\\u002fpatil-suraj\\u002fexploring-T5\\u002fblob\\u002fmaster\\u002fT5_on_TPU.ipynb)  | How ...\"],[\"| [Fine-tune DialoGPT on New Datasets and Languages](https:\\u002f\\u002fgithub.com\\u002fncoop57\\u002fi-am-a-nerd\\u002fblob\\u002fmas...\"],[\"| [Fine-tune BART for Summarization](https:\\u002f\\u002fgithub.com\\u002fohmeow\\u002fohmeow_website\\u002fblob\\u002fmaster\\u002fposts\\u002f2021...\"],[\"| [Optimize 🤗 Hugging Face models with Weights & Biases](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fwa...\"],[\"| [Fine-tune Longformer for QA](https:\\u002f\\u002fgithub.com\\u002fpatil-suraj\\u002fNotebooks\\u002fblob\\u002fmaster\\u002flongformer_qa_t...\"],[\"| [Fine-tune T5 for Sentiment Span Extraction](https:\\u002f\\u002fgithub.com\\u002fenzoampil\\u002ft5-intro\\u002fblob\\u002fmaster\\u002ft5_...\"],[\"|[Fine-tune BERT for Multi-label Classification](https:\\u002f\\u002fgithub.com\\u002fabhimishra91\\u002ftransformers-tutori...\"],[\"|[Speed up Fine-Tuning in Transformers with Dynamic Padding \\u002f Bucketing](https:\\u002f\\u002fgithub.com\\u002fELS-RD\\u002ft...\"],[\"|[Expand and Fine Tune Sci-BERT](https:\\u002f\\u002fgithub.com\\u002flordtt13\\u002fword-embeddings\\u002fblob\\u002fmaster\\u002fCOVID-19%20...\"],[\"|[Fine-tune Electra and interpret with Integrated Gradients](https:\\u002f\\u002fgithub.com\\u002felsanns\\u002fxai-nlp-note...\"],[\"|[Fine-tune a DistilBERT Model for Multi Label Classification task](https:\\u002f\\u002fgithub.com\\u002fDhavalTaunk08...\"],[\"|[Fine-tune Roberta for sentiment analysis](https:\\u002f\\u002fgithub.com\\u002fDhavalTaunk08\\u002fNLP_scripts\\u002fblob\\u002fmaster...\"],[\"|[Leverage BERT for Encoder-Decoder Summarization on CNN\\u002fDailymail](https:\\u002f\\u002fgithub.com\\u002fpatrickvonpla...\"],[\"|[Fine-tune TAPAS on Sequential Question Answering (SQA)](https:\\u002f\\u002fgithub.com\\u002fNielsRogge\\u002fTransformers...\"],[\"|[Fine-tuning mBART for translation](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fvasudevgupta7\\u002fhuggingf...\"],[\"|[Fine-Tune DistilGPT2 and Generate Text](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002ftripathiaakash\\u002fDi...\"],[\"|[Evaluate LED on Arxiv](https:\\u002f\\u002fgithub.com\\u002fpatrickvonplaten\\u002fnotebooks\\u002fblob\\u002fmaster\\u002fLED_on_Arxiv.ipyn...\"],[\"|[Wav2Vec2 CTC decoding with GPT2 adjustment](https:\\u002f\\u002fgithub.com\\u002fvoidful\\u002fhuggingface_notebook\\u002fblob\\u002fm...\"],[\"|[Evaluate Big Bird on Trivia QA](https:\\u002f\\u002fgithub.com\\u002fpatrickvonplaten\\u002fnotebooks\\u002fblob\\u002fmaster\\u002fEvaluati...\"],[\"| [Fine-tune the Vision Transformer on CIFAR-10 using PyTorch Lightning](https:\\u002f\\u002fgithub.com\\u002fNielsRog...\"],[\"| [Fine-tune the Vision Transformer on CIFAR-10 using the 🤗 Trainer](https:\\u002f\\u002fgithub.com\\u002fNielsRogge\\u002fT...\"],[\"| [Evaluate LUKE on TACRED, a relation extraction dataset](https:\\u002f\\u002fgithub.com\\u002fstudio-ousia\\u002fluke\\u002fblob...\"],[\"| [Evaluate BigBird-Pegasus on PubMed dataset](https:\\u002f\\u002fgithub.com\\u002fvasudevgupta7\\u002fbigbird\\u002fblob\\u002fmain\\u002fno...\"],[\"| [Detect objects in an image with DETR](https:\\u002f\\u002fgithub.com\\u002fNielsRogge\\u002fTransformers-Tutorials\\u002fblob\\u002fm...\"],[\"| [Finetune T5 for Named Entity Recognition](https:\\u002f\\u002fgithub.com\\u002fToluClassics\\u002fNotebooks\\u002fblob\\u002fmain\\u002fT5_...\"],[\"\\u003c!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ve...\"],[\"🤗 Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provide...\"],[\"- **MAIN CLASSES** details the most important classes like configuration, model, tokenizer, and pipe...\"],[\"|                                  Model                                   | PyTorch support | Tenso...\"],[\"|                 [BertJapanese](model_doc\\u002fbert-japanese)                  |       ✅        |       ...\"],[\"|                          [CLAP](model_doc\\u002fclap)                          |       ✅        |       ...\"],[\"|                    [DeBERTa-v2](model_doc\\u002fdeberta-v2)                    |       ✅        |       ...\"],[\"|                       [ELECTRA](model_doc\\u002felectra)                       |       ✅        |       ...\"],[\"|                          [GLPN](model_doc\\u002fglpn)                          |       ✅        |       ...\"],[\"|                      [KOSMOS-2](model_doc\\u002fkosmos-2)                      |       ✅        |       ...\"],[\"|                        [Marian](model_doc\\u002fmarian)                        |       ✅        |       ...\"],[\"|                     [MobileViT](model_doc\\u002fmobilevit)                     |       ✅        |       ...\"],[\"|                           [OPT](model_doc\\u002fopt)                           |       ✅        |       ...\"],[\"|                           [RAG](model_doc\\u002frag)                           |       ✅        |       ...\"],[\"|        [Speech Encoder decoder](model_doc\\u002fspeech-encoder-decoder)        |       ✅        |       ...\"],[\"|                  [Transformer-XL](model_doc\\u002ftransfo-xl)                  |       ✅        |       ...\"],[\"|                           [ViT](model_doc\\u002fvit)                           |       ✅        |       ...\"],[\"|                [XLM-ProphetNet](model_doc\\u002fxlm-prophetnet)                |       ✅        |       ...\"],[\"\\u003c!-- End table--\\u003e...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"By exposing a graph with standardized operators and data types, ONNX makes it easy to\\nswitch between...\"],[\"```\\n\\nTo check out all available arguments, refer to the [🤗 Optimum docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs...\"],[\"```\\n\\nThe example above illustrates exporting a checkpoint from 🤗 Hub. When exporting a local model, ...\"],[\"```\\n\\nThe process is identical for TensorFlow checkpoints on the Hub. For instance, here's how you wo...\"],[\"```\\n\\n### Exporting a model for an unsupported architecture\\n\\nIf you wish to contribute by adding supp...\"],[\"```\\n\\nThe required output names (like `[\\\"last_hidden_state\\\"]`) can be obtained by taking a look at th...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"To explain how tasks are solved, we'll walk through what goes on inside the model to output useful p...\"],[\"This model has four main components:\\n\\n1. A *feature encoder* takes the raw audio waveform, normalize...\"],[\"### Automatic speech recognition\\n\\nTo use the pretrained model for automatic speech recognition, add ...\"],[\"ViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an ...\"],[\"4. The output, specifically only the output with the `[CLS]` token, is passed to a multilayer percep...\"],[\"\\u003csmall\\u003eA basic convolution without padding or stride, taken from \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1603...\"],[\"The output from the convolution blocks is passed to a classification head which converts the outputs...\"],[\"3. DETR uses a *bipartite matching loss* during training to compare a fixed number of predictions wi...\"],[\"There are three main components to Mask2Former:\\n\\n1. A [Swin](model_doc\\u002fswin) backbone accepts an ima...\"],[\"### Depth estimation\\n\\n[GLPN](model_doc\\u002fglpn), *Global-Local Path Network*, is a Transformer for dept...\"],[\"## Natural language processing\\n\\nThe Transformer was initially designed for machine translation, and ...\"],[\"3. The input embeddings are passed through multiple encoder layers to output some final hidden state...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Text generation\\n\\n[GPT-2](model_doc\\u002fgpt2) is a decoder-only model pretrained on a large a...\"],[\"Ready to try your hand at text generation? Check out our complete [causal language modeling guide](t...\"],[\"2. The encoder's output is passed to the decoder, which must predict the masked tokens and any uncor...\"],[\"Ready to try your hand at translation? Check out our complete [translation guide](tasks\\u002fsummarizatio...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [Soonhwan-Kwon](https:\\u002f\\u002fgithub.com\\u002fSoonhwan-Kwon) and [stefan-it](http...\"],[\"[[autodoc]] XLMRobertaXLForSequenceClassification\\n    - forward\\n\\n## XLMRobertaXLForMultipleChoice\\n\\n[...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nTo export a model's checkpoint from the 🤗 Hub, for example, `bert-base-uncased`, run the follow...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We present a new method that views object detection ...\"],[\"## How DETR works\\n\\nHere's a TLDR explaining how [`~transformers.DetrForObjectDetection`] works:\\n\\nFir...\"],[\"Next, this is sent through the encoder, outputting `encoder_hidden_states` of the same shape (you ca...\"],[\"DETR can be naturally extended to perform panoptic segmentation (which unifies semantic segmentation...\"],[\"- DETR uses so-called **object queries** to detect objects in an image. The number of queries determ...\"],[\"_num_boxes_ variable in the _DetrLoss_ class of _modeling_detr.py_. When training on multiple nodes,...\"],[\"Alternatively, one can also define a custom `collate_fn` in order to batch images together, using\\n  ...\"],[\"There are three ways to instantiate a DETR model (depending on what you prefer):\\n\\nOption 1: Instanti...\"],[\"```\\n\\nOption 2: Instantiate DETR with randomly initialized weights for Transformer, but pre-trained w...\"],[\"```\\n\\nAs a summary, consider the following table:...\"],[\"| Task | Object detection | Instance segmentation | Panoptic segmentation |\\n|------|----------------...\"],[\"| **Postprocessing** (i.e. converting the output of the model to Pascal VOC format) | [`~transformer...\"],[\"In short, one should prepare the data either in COCO detection or COCO panoptic format, then use\\n[`~...\"],[\"## DetrConfig\\n\\n[[autodoc]] DetrConfig\\n\\n## DetrImageProcessor\\n\\n[[autodoc]] DetrImageProcessor\\n    - p...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In our example, we will take a couple of arguments of the ResNet class that we might want to tweak. ...\"],[\"```\\n\\nThe three important things to remember when writing you own configuration are the following:\\n- ...\"],[\"```\\n\\nYou can also use any other method of the [`PretrainedConfig`] class, like [`~PretrainedConfig.p...\"],[\"```\\n\\nFor the model that will classify images, we just change the forward method:\\n\\n```py\\nimport torch...\"],[\"```\\n\\nIn both cases, notice how we inherit from `PreTrainedModel` and call the superclass initializat...\"],[\"```\\n\\nNow let's see how to make sure that when we do [`~PreTrainedModel.save_pretrained`] or [`~PreTr...\"],[\"```\\n\\nNote that the first argument used when registering your custom config to [`AutoConfig`] needs t...\"],[\"```\\n\\nThen you have to tell the library you want to copy the code files of those objects when using t...\"],[\"```\\n\\n\\u003c\\u002fTip\\u003e\\n\\nNext, let's create the config and models as we did before:\\n\\n```py\\nresnet50d_config = Re...\"],[\"```\\n\\nOn top of the modeling weights and the configuration in json format, this also copied the model...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Open-domain question answering relies on efficient p...\"],[\"## DPRQuestionEncoderTokenizerFast\\n\\n[[autodoc]] DPRQuestionEncoderTokenizerFast\\n\\n## DPRReaderTokeniz...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nThen take a look at an example:...\"],[\"```py\\n\\u003e\\u003e\\u003e billsum[\\\"train\\\"][0]\\n{'summary': 'Existing law authorizes state agencies to enter into cont...\"],[\"'text': 'The people of the State of California do enact as follows:\\\\n\\\\n\\\\nSECTION 1.\\\\nSection 10295.3...\"],[\"but not limited to, the employee’s or dependent’s identification as transgender.\\\\n(2) For purposes o...\"],[\"operations that occur under any of the following conditions:\\\\n(A) Within the state.\\\\n(B) On real pro...\"],[\"contractor’s insurance provider, any request by an employee or applicant for employment benefits or ...\"],[\"as determined by the state agency, that endangers the public health, welfare, or safety, or the cont...\"],[\"authorize application of this section.\\\\n(4) The contractor is providing wholesale or bulk water, pow...\"],[\"the benefits, pays the actual costs incurred in obtaining the benefit.\\\\n(2) If a contractor is unabl...\"],[\"contractor falsely certifies that it is in compliance with this section, the contract with that cont...\"],[\"the application of any existing remedies otherwise available to the department or other contracting ...\"],[\"or circumstances, it is the intent of the state that the court or agency sever that clause, sentence...\"],[\"may be incurred by a local agency or school district will be incurred because this act creates a new...\"],[\"'title': 'An act to add Section 10295.35 to the Public Contract Code, relating to public contracts.'...\"],[\"```\\n\\nThere are two fields that you'll want to use:\\n\\n- `text`: the text of the bill which'll be the i...\"],[\"```\\n\\nNow create a batch of examples using [`DataCollatorForSeq2Seq`]. It's more efficient to *dynami...\"],[\"```\\n\\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule....\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`Seq2SeqTr...\"],[\"```\\n\\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t...\"],[\"```\\n\\nConfigure the model for training with [`compile`](https:\\u002f\\u002fkeras.io\\u002fapi\\u002fmodels\\u002fmodel_training_ap...\"],[\"```\\n\\nThen bundle your callbacks together:\\n\\n```py\\n\\u003e\\u003e\\u003e callbacks = [metric_callback, push_to_hub_callb...\"],[\"```\\n\\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ...\"],[\"```\\n\\nDecode the generated token ids back into text:\\n\\n```py\\n\\u003e\\u003e\\u003e tokenizer.decode(outputs[0], skip_spe...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"* [Methods and tools for efficient training on a single GPU](perf_train_gpu_one): start here to lear...\"],[\"When making contributions that A is better than B, please try to include a reproducible benchmark an...\"],[\"!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\\n\\nLicensed under ...\"],[\"The abstract from the paper is the following:\\n\\n*Recent work in language modeling demonstrates that t...\"],[\"## Usage tips\\n\\nWe have provided pretrained [BERT-345M](https:\\u002f\\u002fngc.nvidia.com\\u002fcatalog\\u002fmodels\\u002fnvidia:...\"],[\"```\\n\\nBERT-345M-cased:\\n\\n```bash\\nwget --content-disposition https:\\u002f\\u002fapi.ngc.nvidia.com\\u002fv2\\u002fmodels\\u002fnvidi...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token cla...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"For custom datasets in `jsonlines` format please see: https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002floading_d...\"],[\"```\\n\\nMBart and some T5 models require special handling.\\n\\nT5 models `t5-small`, `t5-base`, `t5-large`...\"],[\"```\\n\\nIf you get a terrible BLEU score, make sure that you didn't forget to use the `--source_prefix`...\"],[\"```\\n\\nAnd here is how you would use the translation finetuning on your own files, after adjusting the...\"],[\"```\\nHere the languages are Romanian (`ro`) and English (`en`).\\n\\nIf you want to use a pre-processed d...\"],[\"```\\n\\nthen\\n\\n```bash\\npython run_translation_no_trainer.py \\\\\\n    --model_name_or_path Helsinki-NLP\\u002fopus...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Try out the inference widget here: https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-patch16-224\\n\\n## TensorFlow...\"],[\"```\\n\\n👀 See the results here: [amyeroberts\\u002fvit-base-beans](https:\\u002f\\u002fhuggingface.co\\u002famyeroberts\\u002fvit-bas...\"],[\"```\\n\\nInternally, the script will use the [`ImageFolder`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fv2.0.0...\"],[\"```\\n\\n`ImageFolder` will create a `label` column, and the label name is based on the directory name.\\n...\"],[\"ere is how to convert a GPT2 model generated outside of `transformers`\\n\\n* [Megatron-LM](https:\\u002f\\u002fgith...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"According to the abstract, MBART is a sequence-to-sequence denoising auto-encoder pretrained on larg...\"],[\"\\u003e\\u003e\\u003e inputs = tokenizer(example_english_phrase, text_target=expected_translation_romanian, return_ten...\"],[\"```\\n\\n- Generation\\n\\n  While generating the target text set the `decoder_start_token_id` to the target...\"],[\"```\\n\\n## Overview of MBart-50\\n\\nMBart-50 was introduced in the [Multilingual Translation with Extensib...\"],[\"-  Supervised training\\n\\n```python\\nfrom transformers import MBartForConditionalGeneration, MBart50Tok...\"],[\"```\\n\\n- Generation\\n\\n  To generate using the mBART-50 multilingual translation models, `eos_token_id` ...\"],[\"# translate Arabic to English\\ntokenizer.src_lang = \\\"ar_AR\\\"\\nencoded_ar = tokenizer(article_ar, return...\"],[\"```\\n\\n## Documentation resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification...\"],[\"## TFMBartModel\\n\\n[[autodoc]] TFMBartModel\\n    - call\\n\\n## TFMBartForConditionalGeneration\\n\\n[[autodoc]...\"],[\"# MM-IMDb\\n\\nBased on the script [`run_mmimdb.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fma...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"XTREME-S covers speech recognition with Fleurs, Multilingual LibriSpeech (MLS) and VoxPopuli, speech...\"],[\"## Fine-tuning for the XTREME-S tasks\\n\\nBased on the [`run_xtreme_s.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingfa...\"],[\"```\\n\\nwhere `TASK_NAME` can be one of: `mls, voxpopuli, covost2, fleurs-asr, fleurs-lang_id, minds14`...\"],[\"### Speech Recognition with MLS\\n\\nThe following command shows how to fine-tune the [XLS-R](https:\\u002f\\u002fhu...\"],[\"```\\n\\nOn 8 V100 GPUs, this script should run in ~19 hours and yield a cross-entropy loss of **0.6215*...\"],[\"```bash\\npython -m torch.distributed.launch \\\\\\n    --nproc_per_node=2 \\\\\\n    run_xtreme_s.py \\\\\\n    --ta...\"],[\"```\\n\\nOn 2 A100 GPUs, this script should run in ~5 hours and yield a cross-entropy loss of **0.4119**...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Example of using a model with MeCab and WordPiece tokenization:\\n\\n```python\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from...\"],[\"```\\n\\nExample of using a model with Character tokenization:\\n\\n```python\\n\\u003e\\u003e\\u003e bertjapanese = AutoModel.f...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransformers...\"],[\"```\\nfrom transformers import LiltModel\\n\\nmodel = LiltModel.from_pretrained(\\\"path_to_your_files\\\")\\nmode...\"],[\"```\\n\\n- When preparing data for the model, make sure to use the token vocabulary that corresponds to ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Before you can fine-tune a pretrained model, download a dataset and prepare it for training. The pre...\"],[\"```\\n\\nAs you now know, you need a tokenizer to process the text and include a padding and truncation ...\"],[\"```\\n\\n\\u003ca id='trainer'\\u003e\\u003c\\u002fa\\u003e\\n\\n## Train\\n\\nAt this point, you should follow the section corresponding to t...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nYou will see a warning about some of the pretrained weights not being used and some weig...\"],[\"```\\n\\nCall [`~evaluate.compute`] on `metric` to calculate the accuracy of your predictions. Before pa...\"],[\"```\\n\\nThen fine-tune your model by calling [`~transformers.Trainer.train`]:\\n\\n```py\\n\\u003e\\u003e\\u003e trainer.train(...\"],[\"```\\n\\nFinally, load, [`compile`](https:\\u002f\\u002fkeras.io\\u002fapi\\u002fmodels\\u002fmodel_training_apis\\u002f#compile-method), an...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nYou don't have to pass a loss argument to your models when you `compile()` them! Hugging...\"],[\"```\\n\\nRemember that Hugging Face datasets are stored on disk by default, so this will not inflate you...\"],[\"```\\n\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\u003ca id='pytorch_native'\\u003e\\u003c\\u002fa\\u003e\\n\\n## Train in native PyTorch\\n\\n\\u003cframeworkc...\"],[\"```\\n\\n### DataLoader\\n\\nCreate a `DataLoader` for your training and test datasets so you can iterate ov...\"],[\"```\\n\\nLastly, specify `device` to use a GPU if you have access to one. Otherwise, training on a CPU m...\"],[\"```\\n\\n### Evaluate\\n\\nJust like how you added an evaluation function to [`Trainer`], you need to do the...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"According to the abstract,\\n\\n- Bart uses a standard seq2seq\\u002fmachine translation architecture with a b...\"],[\"## Implementation Notes\\n\\n- Bart doesn't use `token_type_ids` for sequence classification. Use [`Bart...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help ...\"],[\"- A blog post on [Distributed Training: Train BART\\u002fT5 for Summarization using 🤗 Transformers and Ama...\"],[\"- An example of how to train [`BartForConditionalGeneration`] with a Hugging Face `datasets` object ...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`BartForConditionalGeneration`] is supported by this [exampl...\"],[\"\\u003cPipelineTag pipeline=\\\"translation\\\"\\u002f\\u003e\\n\\n- A notebook on how to [finetune mBART using Seq2SeqTrainer f...\"],[\"## BartTokenizerFast\\n\\n[[autodoc]] BartTokenizerFast\\n    - all\\n\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## BartMode...\"],[\"## FlaxBartForQuestionAnswering\\n\\n[[autodoc]] FlaxBartForQuestionAnswering\\n    - __call__\\n    - encod...\"],[\"Intro\\n\\nAuthors: @patrickvonplaten and @lhoestq\\n\\nAimed at tackling the knowledge-intensive NLP tasks ...\"],[\"```\\nWe publish two `base` models which can serve as a starting point for finetuning on downstream ta...\"],[\"```\\nYou will then be able to pass `path\\u002fto\\u002fcheckpoint` as `model_name_or_path` to the `finetune_rag....\"],[\"```\\n\\nUsing Ray can lead to retrieval speedups on multi-GPU settings since multiple processes load th...\"],[\"```\\nDoes He Love You\\tDoes He Love You\\tRed Sandy Spika dress of Reba McEntire\\tGreatest Hits Volume Tw...\"],[\"```\\n   ```bash\\n   # EXPLANATION\\n    python examples\\u002fresearch_projects\\u002frag\\u002feval_rag.py \\\\\\n        --mo...\"],[\"```\\n- `ans` - where a single line contains a single expected answer, e.g.:\\n```\\nXiu Li Dai\\n```\\n\\nPredi...\"],[\"```\\n\\nThe created outputs in `path\\u002fto\\u002fmy_knowledge_dataset` can then be used to finetune RAG as follo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr).\\nThe original code can be foun...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nDeepSpeed compiles CUDA C++ code and it can be a potential source of errors when building PyTor...\"],[\"```\\n\\n`PATH` lists the locations of the executables and `LD_LIBRARY_PATH` lists where to look for sha...\"],[\"```\\n\\n## Multi-GPU Network Issues Debug\\n\\nWhen training or inferencing with `DistributedDataParallel` ...\"],[\"```\\n\\nThis will dump a lot of NCCL-related debug information, which you can then search online if you...\"],[\"```\\nDetected inf\\u002fnan during batch_number=0\\nLast 21 forward frames:\\nabs min  abs max  metadata\\n      ...\"],[\"0.00e+00 8.76e+03 input[0]\\n0.00e+00 9.74e+03 output\\n                  encoder.block.2.layer.1.DenseR...\"],[\"```\\n\\nThe example output has been trimmed in the middle for brevity.\\n\\nThe second column shows the val...\"],[\"```\\nDetected inf\\u002fnan during batch_number=0\\nLast 21 forward frames:\\nabs min  abs max  metadata\\n[...]\\n...\"],[\"```\\n\\nThe last frame reports for `Dropout.forward` function with the first entry for the only input a...\"],[\"def forward(self, hidden_states):\\n        hidden_gelu = self.gelu_act(self.wi_0(hidden_states))\\n    ...\"],[\"```\\n\\nNow it's easy to see the `dropout` call, and all the previous calls as well.\\n\\nSince the detecti...\"],[\"```\\n\\nSince the automatic detector only reports on inputs and outputs of full frames, once you know w...\"],[\"```\\n\\nAnd now full batches 1 and 3 will be traced using the same format as the underflow\\u002foverflow det...\"],[\"# Information Gain Filtration(IGF)\\n\\nAuthors @Tuko @mraunak\\n\\nThis folder contains the code how to imp...\"],[\"![IGF performance](result_igf.png)\\n\\nFigure 1: Comparing IGF to Standard Fine-tuning:\\nIGF with consta...\"],[\"## How to use this project?\\n\\nTo fine-tune a transformer model with IGF on a language modeling task, ...\"],[\"```python\\npython run_clm_igf.py\\\\\\n--model_name_or_path \\\"gpt2\\\" \\\\\\n--data_file=\\\"data\\u002ftokenized_stories_t...\"],[\"```\\n\\n## Citation\\n\\nIf you find the resource useful, please cite the following paper\\n\\n```\\n@inproceedin...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Overview\\n\\nThe T5 model was presented in [Exploring the Limits of Transfer Learning with a Unified...\"],[\"## Usage tips\\n\\n- T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised ...\"],[\"- [t5-11b](https:\\u002f\\u002fhuggingface.co\\u002ft5-11b).\\n\\nBased on the original T5 model, Google has released some...\"],[\"## Training\\n\\nT5 is an encoder-decoder model and converts all NLP problems into a text-to-text format...\"],[\"\\u003e\\u003e\\u003e input_ids = tokenizer(\\\"The \\u003cextra_id_0\\u003e walks in \\u003cextra_id_1\\u003e park\\\", return_tensors=\\\"pt\\\").input_...\"],[\"```\\n\\nIf you're interested in pre-training T5 on a new corpus, check out the [run_t5_mlm_flax.py](htt...\"],[\"```\\n\\nAs you can see, only 2 inputs are required for the model in order to compute a loss: `input_ids...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import T5Tokenizer, T5ForConditionalGeneration\\n\\u003e\\u003e\\u003e import torch\\n\\n\\u003e\\u003e\\u003e...\"],[\"\\u003e\\u003e\\u003e # replace padding token id's of the labels by -100 so it's ignored by the loss\\n\\u003e\\u003e\\u003e labels[labels...\"],[\"```\\n\\nAdditional training tips:\\n\\n- T5 models need a slightly higher learning rate than the default on...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import T5Tokenizer, T5ForConditionalGeneration\\n\\n\\u003e\\u003e\\u003e tokenizer = T5To...\"],[\"```\\n\\nNote that T5 uses the `pad_token_id` as the `decoder_start_token_id`, so when doing generation ...\"],[\"```\\n\\nBecause T5 has been trained with the span-mask denoising objective,\\nit can be used to predict t...\"],[\"```\\n\\n## Performance\\n\\nIf you'd like a faster training and inference performance, install [NVIDIA APEX...\"],[\"\\u003cPipelineTag pipeline=\\\"text-generation\\\"\\u002f\\u003e\\n\\n- A notebook for [Finetuning CodeT5 for generating docstr...\"],[\"- A notebook to [Finetune T5-base-dutch to perform Dutch abstractive summarization on a TPU](https:\\u002f...\"],[\"- [`FlaxT5ForConditionalGeneration`] is supported by this [example script](https:\\u002f\\u002fgithub.com\\u002fhuggin...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`FlaxT5ForConditionalGeneration`] is supported by this [exam...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- A notebook on how to [finetune T5 for question answe...\"],[\"## T5ForSequenceClassification\\n\\n[[autodoc]] T5ForSequenceClassification\\n    - forward\\n\\n## T5ForQuest...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"The following example shows how to fine-tune the [Whisper small](https:\\u002f\\u002fhuggingface.co\\u002fopenai\\u002fwhisp...\"],[\"```\\n\\nOn a TPU v4-8, training should take approximately 25 minutes, with a final cross-entropy loss o...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Here's an example:\\n\\n```python\\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\\n\\ntokenizer = G...\"],[\"```\\n\\nThe `generation_output` object is a [`~generation.GreedySearchDecoderOnlyOutput`], as we can\\nse...\"],[\"```\\n\\nwill return the tuple `(generation_output.sequences, generation_output.scores)` for instance.\\n\\n...\"],[\"[[autodoc]] generation.TFBeamSearchDecoderOnlyOutput\\n\\n[[autodoc]] generation.TFBeamSampleEncoderDeco...\"],[\"[[autodoc]] ForcedEOSTokenLogitsProcessor\\n    - __call__\\n\\n[[autodoc]] ForceTokensLogitsProcessor\\n   ...\"],[\"[[autodoc]] TopKLogitsWarper\\n    - __call__\\n\\n[[autodoc]] TopPLogitsWarper\\n    - __call__\\n\\n[[autodoc]...\"],[\"[[autodoc]] TFTemperatureLogitsWarper\\n    - __call__\\n\\n[[autodoc]] TFTopKLogitsWarper\\n    - __call__\\n...\"],[\"A [`StoppingCriteria`] can be used to change when to stop generation (other than EOS token). Please ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*With the capability of modeling bidirectional contex...\"],[\"## Usage tips\\n\\n- The specific attention pattern can be controlled at training and test time using th...\"],[\"## XLNetTokenizerFast\\n\\n[[autodoc]] XLNetTokenizerFast\\n\\n## XLNet specific outputs\\n\\n[[autodoc]] models...\"],[\"\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## XLNetModel\\n\\n[[autodoc]] XLNetModel\\n    - forward\\n\\n## XLNetLMHeadModel\\n\\n[...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nLayoutLMv2 solves the document question-answering task by a...\"],[\"```\\n\\n```bash\\npip install 'git+https:\\u002f\\u002fgithub.com\\u002ffacebookresearch\\u002fdetectron2.git'\\npip install torchv...\"],[\"```\\n\\nAs you can see, the dataset is split into train and test sets already. Take a look at a random ...\"],[\"```\\n\\nNote that the LayoutLMv2 checkpoint that we use in this guide has been trained with `max_positi...\"],[\"```\\n\\nFinally, the data exploration won't be complete if we don't peek at an image example.\\n\\n```py\\n\\u003e\\u003e...\"],[\"```\\n\\nTo apply this preprocessing to the entire dataset in a fast way, use [`~datasets.Dataset.map`]....\"],[\"```\\n\\nOn top of the preprocessing mentioned above, we also need to add the labels for the model. For ...\"],[\"```\\n\\nTo illustrate how this function finds the position of the answer, let's use it on an example:...\"],[\"```py\\n\\u003e\\u003e\\u003e example = dataset_with_ocr[\\\"train\\\"][1]\\n\\u003e\\u003e\\u003e words = [word.lower() for word in example[\\\"word...\"],[\"Words: ['wie', 'baw', 'brown', '&', 'williamson', 'tobacco', 'corporation', 'research', '&', 'develo...\"],[\"'and', 'sell.', 'novel', 'is', 'defined', 'as:', 'of', 'a', 'new', 'kind,', 'or', 'different', 'from...\"],[\"'first', 'task', 'of', 'the', 'product', 'innovation', 'group', 'was', 'to', 'assemble,', 'review', ...\"],[\"'combination', 'of', 'the', 'above,', 'filters,', 'packaging', 'and', 'brand', 'extensions.', 'appea...\"],[\"'unburned', 'section', 'for', 'future', 'smoking.', '«short', 'cigarette,', 'tobacco', 'section', '3...\"],[\"'papers;', 'seasonal', 'promotions,', 'e.g.', 'pastel', 'colored', 'cigarettes', 'for', 'easter', 'o...\"],[\"Answer:  T.F. Riehl\\nstart_index 17\\nend_index 18...\"],[\"```\\n\\nOnce examples are encoded, however, they will look like this:\\n\\n```py\\n\\u003e\\u003e\\u003e encoding = tokenizer(e...\"],[\"```\\n\\nWe'll need to find the position of the answer in the encoded input.\\n* `token_type_ids` tells us...\"],[\"...         if match:\\n...             # if match is found, use `token_type_ids` to find where words ...\"],[\"...         else:\\n...             start_positions.append(cls_index)\\n...             end_positions.ap...\"],[\"```\\n\\nNow that we have this preprocessing function, we can encode the entire dataset:\\n\\n```py\\n\\u003e\\u003e\\u003e enco...\"],[\"```\\n\\n## Evaluation\\n\\nEvaluation for document question answering requires a significant amount of post...\"],[\"```\\n\\nIn the [`TrainingArguments`] use `output_dir` to specify where to save your model, and configur...\"],[\"```\\n\\nTo add the final model to 🤗 Hub, create a model card and call `push_to_hub`:\\n\\n```py\\n\\u003e\\u003e\\u003e trainer...\"],[\"```\\n\\nYou can also manually replicate the results of the pipeline if you'd like:\\n1. Take an image and...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We extract an optimal subset of architectural parame...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- Splitting the embedding matrix into two smaller matrices.\\n- Using repeating layers split among gro...\"],[\"## Usage tips\\n\\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad ...\"],[\"- [`TFAlbertForSequenceClassification`] is supported by this [example script](https:\\u002f\\u002fgithub.com\\u002fhug...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`AlbertForMaskedLM`] is supported by this [example script](h...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`AlbertForQuestionAnswering`] is supported by this ...\"],[\"**Multiple choice**\\n\\n- [`AlbertForMultipleChoice`] is supported by this [example script](https:\\u002f\\u002fgit...\"],[\"## AlbertForPreTraining\\n\\n[[autodoc]] AlbertForPreTraining\\n    - forward\\n\\n## AlbertForMaskedLM\\n\\n[[aut...\"],[\"## FlaxAlbertModel\\n\\n[[autodoc]] FlaxAlbertModel\\n    - __call__\\n\\n## FlaxAlbertForPreTraining\\n\\n[[autod...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[ALBERT](..\\u002fmodel_doc\\u002falbert), [BERT](..\\u002fmodel_doc\\u002fbert), [BigBird](..\\u002fmodel_doc\\u002fbig_bird), [BioGpt]...\"],[\"[OpenAI GPT-2](..\\u002fmodel_doc\\u002fgpt2), [GPTBigCode](..\\u002fmodel_doc\\u002fgpt_bigcode), [GPT Neo](..\\u002fmodel_doc\\u002fgp...\"],[\"[QDQBert](..\\u002fmodel_doc\\u002fqdqbert), [RemBERT](..\\u002fmodel_doc\\u002frembert), [RoBERTa](..\\u002fmodel_doc\\u002froberta), [...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nEach number in `ner_tags` represents an entity. Convert the numbers to their label names to fin...\"],[\"```\\n\\nAs you saw in the example `tokens` field above, it looks like the input has already been tokeni...\"],[\"```\\n\\nHowever, this adds some special tokens `[CLS]` and `[SEP]` and the subword tokenization creates...\"],[\"...     labels = []\\n...     for i, label in enumerate(examples[f\\\"ner_tags\\\"]):\\n...         word_ids =...\"],[\"```\\n\\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.Dataset...\"],[\"```\\n\\nGet the NER labels first, and then create a function that passes your true predictions and true...\"],[\"```\\n\\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your...\"],[\"```\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\u003cTip\\u003e\\n\\nIf you aren't familiar with finetuning a model with the [`Traine...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\u003cTip\\u003e\\n\\nIf you aren't familiar with finetuning a model with Keras, take a look at the ...\"],[\"```\\n\\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.pr...\"],[\"```\\n\\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\\n\\n```...\"],[\"```\\n\\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ...\"],[\"```\\n\\nPass your inputs to the model and return the `logits`:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers import Auto...\"],[\"```\\n\\nGet the class with the highest probability, and use the model's `id2label` mapping to convert i...\"],[\"## Saved Pseudo-Labels\\nThese are the generations of various large models on various large **training...\"],[\"### Available Pseudo-labels\\n| Dataset | Model                       | Link                          ...\"],[\"| CNN\\u002fDM  | `sshleifer\\u002fpegasus-cnn-ft-v2` | [download](https:\\u002f\\u002fcdn-datasets.huggingface.co\\u002fpseudo\\u002fcn...\"],[\"(EN_RO = WMT 2016 English-Romanian).\\n\\nExample Download Command:\\n```bash\\ncurl -S https:\\u002f\\u002fcdn-datasets...\"],[\"```\\n### Generating New Pseudolabels\\nHere is the command I used to generate the pseudolabels in the s...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"If you would like to list benchmark results on your favorite models of the [model hub](https:\\u002f\\u002fhuggi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In the past decade, convolutional neural networks (C...\"],[\"## Usage tips\\n\\n- When fine-tuning the Audio Spectrogram Transformer (AST) on your own dataset, it's ...\"],[\"If you're interested in submitting a resource to be included here, please feel free to open a Pull R...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## How to enable Hyperparameter search in example\\n\\nDefine the hyperparameter search space, diff...\"],[\"```\\n\\nOptuna provides multi-objective HPO. You can pass `direction` in `hyperparameter_search` and de...\"],[\"```\\n\\nFor wandb, see wandb [object_parameter](https:\\u002f\\u002fdocs.wandb.ai\\u002fguides\\u002fsweeps\\u002fconfiguration), it'...\"],[\"```\\n\\nCreate a [`Trainer`] with your `model_init` function, training arguments, training and test dat...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Image Transformer has recently achieved significant ...\"],[\"```\\n\\nThis will load the model pre-trained on masked image modeling. Note that this won't include the...\"],[\"```\\n\\nThis particular checkpoint was fine-tuned on [RVL-CDIP](https:\\u002f\\u002fwww.cs.cmu.edu\\u002f~aharley\\u002frvl-cdi...\"],[\"urrently the following model proposals are available:\\n\\n- \\u003cs\\u003e[BigBird (Google)](.\\u002fADD_BIG_BIRD.md)\\u003c\\u002fs...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We present SegFormer, a simple, efficient yet powerf...\"],[\"- SegFormer consists of a hierarchical Transformer encoder, and a lightweight all-MLP decoder head.\\n...\"],[\"- One can also check out [this interactive demo on Hugging Face Spaces](https:\\u002f\\u002fhuggingface.co\\u002fspace...\"],[\"used by [`SegformerForSemanticSegmentation`]). However, other datasets use the 0 index as\\n  backgrou...\"],[\"| **Model variant** | **Depths**    | **Hidden sizes**    | **Decoder hidden size** | **Params (M)**...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"## SegformerConfig\\n\\n[[autodoc]] SegformerConfig\\n\\n## SegformerFeatureExtractor\\n\\n[[autodoc]] Segformer...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Motivated by the success of T5 (Text-To-Text Transfe...\"],[\"## SpeechT5FeatureExtractor\\n\\n[[autodoc]] SpeechT5FeatureExtractor\\n    - __call__\\n\\n## SpeechT5Process...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nNow, let's load an image.\\n\\n```python\\nfrom PIL import Image\\nimport requests\\n\\nurl = \\\"https:\\u002f\\u002fhugg...\"],[\"```\\n\\n`pipeline` abstracts away the preprocessing and postprocessing steps that we have to do ourselv...\"],[\"```\\n\\nWe need to squeeze the output and get rid of axis 0, clip the values, then convert it to be num...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Entity representations are useful in natural languag...\"],[\"## Usage tips\\n\\n- This implementation is the same as [`RobertaModel`] with the addition of entity emb...\"],[\"- There are three head models for the former use case:\\n\\n  - [`LukeForEntityClassification`], for tas...\"],[\"\\u003e\\u003e\\u003e text = \\\"Beyoncé lives in Los Angeles.\\\"\\n\\u003e\\u003e\\u003e entity_spans = [(0, 7)]  # character-based entity spa...\"],[\"\\u003e\\u003e\\u003e model = LukeForEntityPairClassification.from_pretrained(\\\"studio-ousia\\u002fluke-large-finetuned-tacre...\"],[\"```\\n\\n## Resources\\n\\n- [A demo notebook on how to fine-tune [`LukeForEntityPairClassification`] for re...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[BEiT](..\\u002fmodel_doc\\u002fbeit), [BiT](..\\u002fmodel_doc\\u002fbit), [ConvNeXT](..\\u002fmodel_doc\\u002fconvnext), [ConvNeXTV2](...\"],[\"[PVT](..\\u002fmodel_doc\\u002fpvt), [RegNet](..\\u002fmodel_doc\\u002fregnet), [ResNet](..\\u002fmodel_doc\\u002fresnet), [SegFormer](....\"],[\"```\\n\\nWe encourage you to log in to your Hugging Face account to upload and share your model with the...\"],[\"```\\n\\nNow you can convert the label id to a label name:\\n\\n```py\\n\\u003e\\u003e\\u003e id2label[str(79)]\\n'prime_rib'\\n```\\n...\"],[\"```\\n\\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.Dataset...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003ctf\\u003e\\n\\nTo avoid overfitting and to make the model ...\"],[\"```\\n\\nNext, create functions to apply appropriate transformations to a batch of images, instead of on...\"],[\"```\\n\\nAs a final preprocessing step, create a batch of examples using `DefaultDataCollator`. Unlike o...\"],[\"```\\n\\nYour `compute_metrics` function is ready to go now, and you'll return to it when you set up you...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"\\u003e\\u003e\\u003e trainer = Trainer(\\n...     model=model,\\n...     args=training_args,\\n...     data_collator=data_c...\"],[\"```\\n\\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t...\"],[\"```\\n\\nThen, load ViT with [`TFAutoModelForImageClassification`] along with the label mappings:\\n\\n```py...\"],[\"```\\n\\nTo compute the accuracy from the predictions and push your model to the 🤗 Hub, use [Keras callb...\"],[\"```\\n\\nFinally, you are ready to train your model! Call `fit()` with your training and validation data...\"],[\"```\\n\\nCongratulations! You have fine-tuned your model and shared it on the 🤗 Hub. You can now use it ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nPass your inputs to the model and return the logits:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers import AutoMo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n- The model usually performs well without requiring any finetuning.\\n- The architectur...\"],[\"```\\nThe script will automatically determine all necessary parameters from the OpenAI checkpoint. A `...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help ...\"],[\"```\\n\\n## WhisperConfig\\n\\n[[autodoc]] WhisperConfig\\n\\n## WhisperTokenizer\\n\\n[[autodoc]] WhisperTokenizer\\n...\"],[\"## TFWhisperForConditionalGeneration\\n\\n[[autodoc]] TFWhisperForConditionalGeneration\\n    - call\\n\\n\\u003c\\u002ftf...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import GPTJForCausalLM\\n\\u003e\\u003e\\u003e import torch\\n\\n\\u003e\\u003e\\u003e device = \\\"cuda\\\"\\n\\u003e\\u003e\\u003e mod...\"],[\"```\\n\\n- The model should fit on 16GB GPU for inference. For training\\u002ffine-tuning it would take much m...\"],[\"```\\n\\n...or in float16 precision:\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import GPTJForCausalLM, AutoTokeni...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help ...\"],[\"- Description of [GPT-J](https:\\u002f\\u002fhuggingface.co\\u002fEleutherAI\\u002fgpt-j-6B).\\n- A blog on how to [Deploy GPT...\"],[\"- [`GPTJForCausalLM`] is supported by this [causal language modeling example script](https:\\u002f\\u002fgithub....\"],[\"**Documentation resources**\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe `outputs` object is a [`~modeling_outputs.SequenceClassifierOutput`], as we can see in the\\n...\"],[\"```\\n\\nwill return the tuple `(outputs.loss, outputs.logits)` for instance.\\n\\nWhen considering our `out...\"],[\"## CausalLMOutputWithCrossAttentions\\n\\n[[autodoc]] modeling_outputs.CausalLMOutputWithCrossAttentions...\"],[\"[[autodoc]] modeling_outputs.SemanticSegmenterOutput\\n\\n## ImageClassifierOutput\\n\\n[[autodoc]] modeling...\"],[\"## TFBaseModelOutputWithPast\\n\\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPast\\n\\n## TFBaseMo...\"],[\"## TFSeq2SeqSequenceClassifierOutput\\n\\n[[autodoc]] modeling_tf_outputs.TFSeq2SeqSequenceClassifierOut...\"],[\"[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqModelOutput\\n\\n## FlaxCausalLMOutputWithCrossAttentions\\n\\n...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- A blog post on how to [Accelerate Large Model Train...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\" \\u002f\\u003e\\n\\n- [`DebertaForTokenClassification`] is supported by...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`DebertaForMaskedLM`] is supported by this [example script](...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`DebertaForQuestionAnswering`] is supported by this...\"],[\"## DebertaPreTrainedModel\\n\\n[[autodoc]] DebertaPreTrainedModel\\n\\n## DebertaForMaskedLM\\n\\n[[autodoc]] De...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce a state-of-the-art real-time, high-fide...\"],[\"\\u003e\\u003e\\u003e model = EncodecModel.from_pretrained(\\\"facebook\\u002fencodec_24khz\\\")\\n\\u003e\\u003e\\u003e processor = AutoProcessor.fro...\"],[\"```\\n\\n## EncodecConfig\\n\\n[[autodoc]] EncodecConfig\\n\\n## EncodecFeatureExtractor\\n\\n[[autodoc]] EncodecFea...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"emb = jnp.zeros((50264, model.config.hidden_size))\\n# update the first 50257 weights using pre-traine...\"],[\"```\\n\\n\\n### Train Model\\n\\n```bash\\npython run_clm_mp.py \\\\\\n    --model_name_or_path gpt-neo-1.3B  \\\\\\n    -...\"],[\"VisualBERT Demo\\n\\nThis demo shows usage of VisualBERT VQA model and is adapted from LXMERT demo prese...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [gchhablani](https:\\u002f\\u002fhuggingface.co\\u002fgchhablani). The original code can...\"],[\"The [`BertTokenizer`] is used to encode the text. A custom detector\\u002fimage processor must be used\\nto ...\"],[\"\\u003e\\u003e\\u003e visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\\n\\u003e\\u003e\\u003e visual_attent...\"],[\"```\\n\\n## VisualBertConfig\\n\\n[[autodoc]] VisualBertConfig\\n\\n## VisualBertModel\\n\\n[[autodoc]] VisualBertMo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers have emerged as a powerful tool for a b...\"],[\"## NystromformerModel\\n\\n[[autodoc]] NystromformerModel\\n    - forward\\n\\n## NystromformerForMaskedLM\\n\\n[[...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce the Segment Anything (SA) project: a ne...\"],[\"Below is an example on how to run mask generation given an image and a 2D point:\\n\\n```python\\nimport t...\"],[\"```\\n\\nResources:\\n\\n- [Demo notebook](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fexamples\\u002fsegme...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransformers...\"],[\"```\\n\\nTo use another vision backbone, like [ConvNeXt](convnext), simply instantiate the model with th...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers have shown great potential in computer ...\"],[\"This model was contributed by [heytanay](https:\\u002f\\u002fhuggingface.co\\u002fheytanay). The original code can be ...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!---\\nCopyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\\n\\nLicense...\"],[\"```\\n\\nor directly on the hub under *Training metrics*.\\n\\nsample Metrics - [tfhub.dev](https:\\u002f\\u002ftensorbo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nWhen you run `accelerate config`, you'll be prompted with a series of options to configure your...\"],[\"### Wrapping policy\\n\\nFSDP is applied by wrapping each layer in the network. The wrapping is usually ...\"],[\"```\\n\\nHowever, when training ends, you want to save the full state dict because sharded state dict is...\"],[\"```\\n\\nThe [`xla_fsdp_settings`](https:\\u002f\\u002fgithub.com\\u002fpytorch\\u002fxla\\u002fblob\\u002f2e6e183e0724818f137c8135b34ef273d...\"],[\"```\\n\\n```bash\\naccelerate launch --fsdp=\\\"full shard\\\" --fsdp_config=\\\"path\\u002fto\\u002ffsdp_config\\u002f my-trainer-sc...\"],[\"Testing new Hugging Face Deep Learning Container.\\n\\nThis document explains the testing strategy for r...\"],[\"```\\nThese tests take around 10-15 minutes to finish. Preferably make a screenshot of the successfull...\"],[\"repository_info:\\n  training_repository: &TRAINING_REPOSITORY\\n    image_type: &TRAINING_IMAGE_TYPE tr...\"],[\"```\\n2. In the PR comment describe what test, we ran and with which package versions. Here you can co...\"],[\"```\\nThese tests take around 10-15 minutes to finish. Preferably make a screenshot of the successfull...\"],[\"repository_info:\\n  training_repository: &TRAINING_REPOSITORY\\n    image_type: &TRAINING_IMAGE_TYPE tr...\"],[\"```\\n2. In the PR comment describe what test we ran and with which framework versions. Here you can c...\"],[\"| ID                                  | Description                                                 ...\"],[\"| tensorflow-transfromers-test-single | Test bert finetuning using BERT from transformer lib+TF     ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [patrickvonplaten](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten)\\n\\nThe origi...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Converting custom checkpoints \\n\\n\\u003cTip\\u003e\\n\\nFalcon models were initially added to the Hugging Face Hub...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Depth estimation from a single image is an important...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-trained representations are becoming crucial for...\"],[\"[`AlignProcessor`] wraps [`EfficientNetImageProcessor`] and [`BertTokenizer`] into a single instance...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The library was designed with two strong goals in mind:\\n\\n1. Be as easy and fast to use as possible:\\n...\"],[\"2. Provide state-of-the-art models with performances as close as possible to the original models:\\n\\n ...\"],[\"## Main concepts\\n\\nThe library is built around three types of classes for each model:\\n\\n- **Model clas...\"],[\"All these classes can be instantiated from pretrained instances, saved locally, and shared on the Hu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Natural Language Processing\\n\\n\\u003cYoutube id=\\\"Yffk5aydLzg\\\"\\u002f\\u003e\\n\\nThe main tool for preprocessing te...\"],[\"```\\n\\nThe tokenizer returns a dictionary with three important items:\\n\\n* [input_ids](glossary#input-id...\"],[\"```\\n\\nAs you can see, the tokenizer added two special tokens - `CLS` and `SEP` (classifier and separa...\"],[\"```\\n\\n### Pad\\n\\nSentences aren't always the same length which can be an issue because tensors, the mod...\"],[\"```\\n\\nThe first and third sentences are now padded with `0`'s because they are shorter.\\n\\n### Truncati...\"],[\"Set the `truncation` parameter to `True` to truncate a sequence to the maximum length accepted by th...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [Padding and truncation](.\\u002fpad_truncation) concept guide to learn more dif...\"],[\"Set the `return_tensors` parameter to either `pt` for PyTorch, or `tf` for TensorFlow:\\n\\n\\u003cframeworkco...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n```py\\n\\u003e\\u003e\\u003e batch_sentences = [\\n...     \\\"But what about second breakfast?\\\",\\n...     \\\"Do...\"],[\"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)\\u003e,\\n 'attention_mask': \\u003ctf.Tensor: shape=...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Audio\\n\\nFor audio tasks, you'll need a [feature extractor](main_cla...\"],[\"```\\n\\nThis returns three items:\\n\\n* `array` is the speech signal loaded - and potentially resampled - ...\"],[\"```\\n\\nNext, load a feature extractor to normalize and pad the input. When padding textual data, a `0`...\"],[\"```\\n\\nCreate a function to preprocess the dataset so the audio samples are the same lengths. Specify ...\"],[\"```\\n\\n## Computer vision\\n\\nFor computer vision tasks, you'll need an [image processor](main_classes\\u002fim...\"],[\"```\\n\\nNext, take a look at the image with 🤗 Datasets [`Image`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fp...\"],[\"```\\n\\nFirst, let's add some image augmentation. You can use any library you prefer, but in this tutor...\"],[\"```\\n\\n2. The model accepts [`pixel_values`](model_doc\\u002fvisionencoderdecoder#transformers.VisionEncoder...\"],[\"```\\n\\nHere is what the image looks like after the transforms are applied. The image has been randomly...\"],[\"```\\n\\n## Multimodal\\n\\nFor tasks involving multimodal inputs, you'll need a [processor](main_classes\\u002fpr...\"],[\"```\\n\\nNow take a look at the `audio` and `text` columns:\\n\\n```py\\n\\u003e\\u003e\\u003e lj_speech[0][\\\"audio\\\"]\\n{'array': a...\"],[\"```\\n\\n1. Create a function to process the audio data contained in `array` to `input_values`, and toke...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This guide covers the prompt engineering best practices to help you craft better LLM prompts and sol...\"],[\"When using a pipeline to generate text with an LLM, it's important to know what type of LLM you are ...\"],[\"```\\n\\nTo run inference with an encoder-decoder, use the `text2text-generation` pipeline:\\n\\n```python\\n\\u003e...\"],[\"```\\n\\nNext, let's load the model with the appropriate pipeline (`\\\"text-generation\\\"`): \\n\\n```python\\n\\u003e\\u003e\\u003e...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nNote that Falcon models were trained using the `bfloat16` datatype, so we recommend you ...\"],[\"```\\n\\nAs a result, the output contains a classification label from the list we have provided in the i...\"],[\"```\\n\\nAs you can see, the model correctly identified two named entities from the given text.\\n\\n#### Tr...\"],[\"```\\n\\nHere we've added a `do_sample=True` and `top_k=10` to allow the model to be a bit more flexible...\"],[\"```\\n\\n#### Question answering\\n\\nFor question answering task we can structure the prompt into the follo...\"],[\"```\\n\\n#### Reasoning\\n\\nReasoning is one of the most difficult tasks for LLMs, and achieving good resul...\"],[\"```\\n\\nThis is a wrong answer, it should be 12. In this case, this can be due to the prompt being too ...\"],[\"In few-shot prompting, we provide examples in the prompt giving the model more context to improve th...\"],[\"```\\n\\nIn the above code snippet we used a single example to demonstrate the desired output to the mod...\"],[\"```\\n\\n## Prompting vs fine-tuning\\n\\nYou can achieve great results by optimizing your prompts, however,...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Several TensorFlow methods in 🤗 Transformers have been rewritten to be XLA-compatible, including tex...\"],[\"```\\n\\nThe above model accepts inputs having a dimension of `(10, )`. We can use the model for running...\"],[\"```\\n\\nAnd then you can run the following code:\\n\\n```py\\nimport tensorflow as tf\\nfrom transformers impor...\"],[\"```\\n\\nAs you can notice, enabling XLA on `generate()` is just a single line of code. The rest of the ...\"],[\"xla_generate = tf.function(model.generate, jit_compile=True)\\n\\n# Here, we call the tokenizer with pad...\"],[\"```\\n\\nThis way, you can ensure that the inputs to `xla_generate()` will always receive inputs with th...\"],[\"```\\nThe first call to `xla_generate()` is time-consuming because of tracing, but the successive call...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"| Task | Example datasets | Trainer support | 🤗 Accelerate | 🤗 Datasets | Colab\\n|---|---|:---:|:---:...\"],[\"| [**`question-answering`**](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002fpytorch\\u002f...\"],[\"| [**`text-generation`**](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002fpytorch\\u002ftex...\"],[\"| [**`speech-recognition`**](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002fpytorch\\u002f...\"],[\"| [**`image-pretraining`**](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002fpytorch\\u002fi...\"],[\"## Running quick tests\\n\\nMost examples are equipped with a mechanism to truncate the number of datase...\"],[\"```\\nexamples\\u002fpytorch\\u002ftoken-classification\\u002frun_ner.py \\\\\\n--max_train_samples 50 \\\\\\n--max_eval_samples 5...\"],[\"```\\n\\n## Resuming training\\n\\nYou can resume training from a previous checkpoint like this:\\n\\n1. Pass `-...\"],[\"A few notes on this integration:\\n\\n- you will need to be logged in to the Hugging Face website locall...\"],[\"```\\n\\nAs an example, here is how you would fine-tune the BERT large model (with whole word masking) o...\"],[\"```\\n\\nIf you have a GPU with mixed precision capabilities (architecture Pascal or more recent), you c...\"],[\"```\\n\\nAs an example, here is how you would fine-tune the BERT large model (with whole word masking) o...\"],[\"```\\n\\nthat will check everything is ready for training. Finally, you can launch training with\\n\\n```bas...\"],[\"```\\n\\nIf you are in Jupyter or Colab, you should login with:\\n\\n```python\\nimport wandb\\nwandb.login()\\n``...\"],[\"```\\n\\n`conda`:\\n\\n```bash\\nconda install -c conda-forge neptune\\n```\\n\\nNext, in your model training script...\"],[\"```\\n\\nNow, when you start the training with `trainer.train()`, your metadata will be logged in Neptun...\"],[\"```\\n\\n\\nTo enable logging to ClearML, include `\\\"clearml\\\"` in the `report_to` of your `TrainingArgument...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### Power and Cooling\\n\\nIf you bought an expensive high end GPU make sure you give it the correct pow...\"],[\"Next let's have a look at one of the most important aspects when having multiple GPUs: connectivity....\"],[\"```\\nnvidia-smi topo -m\\n```\\n\\nand it will tell you how the GPUs are inter-connected. On a machine with...\"],[\"```\\n\\nSo the first report `NV2` tells us the GPUs are interconnected with 2 NVLinks, and the second r...\"],[\"So the higher `X` you get in the report of `NVX` in the output of `nvidia-smi topo -m` the better. T...\"],[\"{'train_runtime': 101.9003, 'train_samples_per_second': 1.963, 'epoch': 0.69}\\n\\n# DDP w\\u002fo NVLink\\n\\nrm ...\"],[\"```\\n\\nHardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks (`NV2` in `nvidia-smi topo -m`)\\nSoftwa...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Modern approaches typically formulate semantic segme...\"],[\"## Usage tips\\n\\n-  MaskFormer's Transformer decoder is identical to the decoder of [DETR](detr). Duri...\"],[\"## MaskFormer specific outputs\\n\\n[[autodoc]] models.maskformer.modeling_maskformer.MaskFormerModelOut...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While the Transformer architecture has become the de...\"],[\"## ViTHybridConfig\\n\\n[[autodoc]] ViTHybridConfig\\n\\n## ViTHybridImageProcessor\\n\\n[[autodoc]] ViTHybridIm...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-supervised pre-training techniques have achieve...\"],[\"## Usage tips\\n\\n- In terms of data processing, LayoutLMv3 is identical to its predecessor [LayoutLMv2...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- [`LayoutLMv2ForSequenceClassification`] is supporte...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- [`LayoutLMv3ForTokenClassification`] is supported ...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`LayoutLMv2ForQuestionAnswering`] is supported by t...\"],[\"## LayoutLMv3ForTokenClassification\\n\\n[[autodoc]] LayoutLMv3ForTokenClassification\\n    - forward\\n\\n## ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large-scale autoregressive language models such as G...\"],[\"## XGLMConfig\\n\\n[[autodoc]] XGLMConfig\\n\\n## XGLMTokenizer\\n\\n[[autodoc]] XGLMTokenizer\\n    - build_input...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This is a machine translation model that supports many low-resource languages, and that is competiti...\"],[\"```\\n\\nGoogle has released the following variants:\\n\\n- [google\\u002fmadlad400-3b-mt](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\nfrom transformers import Pipeline\\n\\n\\nclass MyPipeline(Pipeline):\\n    def _sanitize_paramete...\"],[\"```\\n\\nThe structure of this breakdown is to support relatively seamless support for CPU\\u002fGPU, while su...\"],[\"A classic example would be a `top_k` argument in the post processing in classification tasks.\\n\\n```py...\"],[\"```\\n\\nIn order to achieve that, we'll update our `postprocess` method with a default parameter to `5`...\"],[\"```\\n\\nYou can specify a default model if you want, in which case it should come with a specific revis...\"],[\"```\\n\\n## Share your pipeline on the Hub\\n\\nTo share your custom pipeline on the Hub, you just have to s...\"],[\"```\\n\\nThe implementation is framework agnostic, and will work for PyTorch and TensorFlow models. If w...\"],[\"```\\n\\n## Add the pipeline to 🤗 Transformers\\n\\nIf you want to contribute your pipeline to 🤗 Transformer...\"],[\"Training a masked language model end-to-end from scratch on TPUs\\n\\nIn this example, we're going to de...\"],[\"## Setting up a TPU-VM\\n\\nSince this example focuses on using TPUs, the first step is to set up access...\"],[\"```\\n\\nThe script will automatically load the `train` split of the WikiText dataset and train a [Unigr...\"],[\"```\\n\\n**Notes**:\\n\\n* While running the above script, you need to specify the `split` accordingly. The ...\"],[\"```\\n\\nIf you had specified a `hub_model_id` while launching training, then your model will be pushed ...\"],[\"```\\n\\nYou can also try out inference using the [Inference Widget](https:\\u002f\\u002fhuggingface.co\\u002ftf-tpu\\u002frober...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Load the Pokémon BLIP captions dataset\\n\\nUse the 🤗 Dataset library to load a dataset that con...\"],[\"```\\n\\nLet's visualize a couple of samples from the training set. \\n\\n\\n```python\\nfrom textwrap import wr...\"],[\"```\\n\\nThe processor will internally pre-process the image (which includes resizing, and pixel scaling...\"],[\"```\\n\\n## Evaluate\\n\\nImage captioning models are typically evaluated with the [Rouge Score](https:\\u002f\\u002fhug...\"],[\"```\\n\\n## Train!\\n\\nNow, you are ready to start fine-tuning the model. You will use the 🤗 [`Trainer`] fo...\"],[\"```\\n\\n## Inference\\n\\nTake a sample image from `test_ds` to test the model.\\n\\n\\n```python\\nfrom PIL import...\"],[\"!--Copyright 2022 The HuggingFace Team and Microsoft. All rights reserved.\\n\\nLicensed under the MIT L...\"],[\"The abstract from the paper is the following:\\n\\n*The Transformer architecture has become a dominant c...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Can Transformer perform 2D object- and region-level ...\"],[\"If you're interested in submitting a resource to be included here, please feel free to open a Pull R...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Overview\\n\\nThe ELECTRA model was proposed in the paper [ELECTRA: Pre-training Text Encoders as Dis...\"],[\"This model was contributed by [lysandre](https:\\u002f\\u002fhuggingface.co\\u002flysandre). The original code can be ...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"## ElectraForTokenClassification\\n\\n[[autodoc]] ElectraForTokenClassification\\n    - forward\\n\\n## Electr...\"],[\"[[autodoc]] FlaxElectraForCausalLM\\n    - __call__\\n\\n## FlaxElectraForMaskedLM\\n\\n[[autodoc]] FlaxElectr...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In this work, we present a conceptually simple and e...\"],[\"The [`AltCLIPProcessor`] wraps a [`CLIPImageProcessor`] and a [`XLMRobertaTokenizer`] into a single ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nThis model is based on `CLIPModel`, use it like you would use the original [CLIP](clip)....\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While existing large vision-language multimodal mode...\"],[\"```\\n\\nFor multiple turns conversation:\\n\\n```bash\\nA chat between a curious human and an artificial inte...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nOnce the installation is done, you can use the CLI command `add-new-model` to generate your mod...\"],[\"```\\n--\\u003e\\n\\nOnce the command has finished, you should have a total of 7 new files spread across the rep...\"],[\"```\\n\\nFeel free to modify each file to mimic the behavior of your model. \\n\\n⚠ You should be careful ab...\"],[\"```\\n\\nThis will start a small questionnaire you have to fill.\\n\\n```\\nWhat identifier would you like to ...\"],[\"```\\nWill your new model use the same processing class as Xxx (XxxTokenizer\\u002fXxxFeatureExtractor\\u002fXxxIm...\"],[\"```\\npython -m pytest .\\u002ftests\\u002ftest_*\\u003cmodel_name\\u003e*.py\\n```\\n\\n⚠ You should be careful about the classes p...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg alt=\\\"\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fwarmu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce Kosmos-2, a Multimodal Large Language M...\"],[\"## Example\\n\\n```python\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e import requests\\n\\u003e\\u003e\\u003e from transformers import Aut...\"],[\"\\u003e\\u003e\\u003e caption, entities = processor.post_process_generation(generated_text)\\n\\u003e\\u003e\\u003e caption\\n'An image of a...\"],[\"```\\n\\nThis model was contributed by [Yih-Dar SHIEH](https:\\u002f\\u002fhuggingface.co\\u002fydshieh). The original cod...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- information extraction from scanned documents: the [FUNSD](https:\\u002f\\u002fguillaumejaume.github.io\\u002fFUNSD\\u002f...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-training of text and layout has proved effective...\"],[\"```\\npython -m pip install 'git+https:\\u002f\\u002fgithub.com\\u002ffacebookresearch\\u002fdetectron2.git'\\npython -m pip ins...\"],[\"```\\n(If you are developing for LayoutLMv2, note that passing the doctests also requires the installa...\"],[\"- The main difference between LayoutLMv1 and LayoutLMv2 is that the latter incorporates visual embed...\"],[\"in Detectron2 are pre-trained using the BGR format. The `bbox` input are the bounding boxes (i.e. 2D...\"],[\"```\\n\\nHere, `width` and `height` correspond to the width and height of the original document in which...\"],[\"```\\n\\nHowever, this model includes a brand new [`~transformers.LayoutLMv2Processor`] which can be use...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- A notebook on how to [finetune LayoutLMv2 for toke...\"],[\"```\\n\\nIn short, one can provide a document image (and possibly additional data) to [`LayoutLMv2Proces...\"],[\"processor = LayoutLMv2Processor.from_pretrained(\\\"microsoft\\u002flayoutlmv2-base-uncased\\\")\\n\\nimage = Image....\"],[\"```\\n\\n**Use case 2: document image classification (training, inference) + token classification (infer...\"],[\"```\\n\\n**Use case 3: token classification (training), apply_ocr=False**\\n\\nFor token classification task...\"],[\"```\\n\\n**Use case 4: visual question answering (inference), apply_ocr=True**\\n\\nFor visual question answ...\"],[\"```\\n\\n**Use case 5: visual question answering (inference), apply_ocr=False**\\n\\nFor visual question ans...\"],[\"```\\n\\n## LayoutLMv2Config\\n\\n[[autodoc]] LayoutLMv2Config\\n\\n## LayoutLMv2FeatureExtractor\\n\\n[[autodoc]] L...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\n**Note:** This script only works with models that have a fast tokenizer (backed by the 🤗 Tokeni...\"],[\"```\\n\\nthen\\n\\n```bash\\nexport TASK_NAME=ner\\n\\npython run_ner_no_trainer.py \\\\\\n  --model_name_or_path bert-...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper:\\n\\n\\n*Inductive transfer learning, enabled by self-supervised learning, have...\"],[\"## BarthezTokenizer\\n\\n[[autodoc]] BarthezTokenizer\\n\\n## BarthezTokenizerFast\\n\\n[[autodoc]] BarthezToken...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In recent years, a series of Transformer-based model...\"],[\"\\u003e\\u003e\\u003e # HerBERT can also be loaded using AutoTokenizer and AutoModel:\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transf...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nHerbert implementation is the same as `BERT` except for the tokenization method. Refer t...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [anton-l](https:\\u002f\\u002fhuggingface.co\\u002fanton-l).\\n\\n## Usage tips\\n\\n- SEW is a ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- accessing all the hidden-states of BERT\\u002fGPT\\u002fGPT-2,\\n- accessing all the attention weights for each ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformer-based models are unable to process long ...\"],[\"## Longformer Self Attention\\n\\nLongformer self attention employs self attention on both a \\\"local\\\" con...\"],[\"For more information, please refer to the official [paper](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2004.05150.pdf).\\n\\n\\n...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token cla...\"],[\"[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerBaseModelOutputWithPooling\\n\\n[[autod...\"],[\"## TFLongformerModel\\n\\n[[autodoc]] TFLongformerModel\\n    - call\\n\\n## TFLongformerForMaskedLM\\n\\n[[autodo...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\\n\\nLicense...\"],[\"```\\n\\nor directly on the hub under *Training metrics*.\\n\\nTraining with the previously defined hyper-pa...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In this work, we present the Textless Vision-Languag...\"],[\"## Usage tips\\n\\n- TVLT is a model that takes both `pixel_values` and `audio_values` as input. One can...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Overview\\n\\nThe DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: ...\"],[\"This model was contributed by [victorsanh](https:\\u002f\\u002fhuggingface.co\\u002fvictorsanh). This model jax versio...\"],[\"- A blog post on [Getting Started with Sentiment Analysis using Python](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002f...\"],[\"- [`TFDistilBertForSequenceClassification`] is supported by this [example script](https:\\u002f\\u002fgithub.com...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- [`DistilBertForTokenClassification`] is supported ...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`DistilBertForMaskedLM`] is supported by this [example scrip...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`DistilBertForQuestionAnswering`] is supported by t...\"],[\"**Multiple choice**\\n- [`DistilBertForMultipleChoice`] is supported by this [example script](https:\\u002f\\u002f...\"],[\"🚀 Deploy\\n\\n- A blog post on how to [deploy DistilBERT on Google Cloud](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fho...\"],[\"```\\n\\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more ab...\"],[\"```\\n\\n\\n## DistilBertConfig\\n\\n[[autodoc]] DistilBertConfig\\n\\n## DistilBertTokenizer\\n\\n[[autodoc]] DistilB...\"],[\"[[autodoc]] TFDistilBertForMultipleChoice\\n    - call\\n\\n## TFDistilBertForTokenClassification\\n\\n[[autod...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"The crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, e...\"],[\"At the time of writing this guide, LLMs consist of at least a couple billion parameters. Each parame...\"],[\"To give some examples of how much VRAM it roughly takes to load a model in bfloat16:\\n\\n-   **GPT3** r...\"],[\"🤗 Transformers does not support tensor parallelism out of the box as it requires the model architect...\"],[\"```\\n```python\\nfrom transformers import AutoModelForCausalLM\\n\\nmodel = AutoModelForCausalLM.from_pretr...\"],[\"```\\n\\nNow what if your GPU does not have 32 GB of VRAM? It has been found that model weights can be q...\"],[\"for every matrix multiplication. Dequantization and re-quantization is performed sequentially for al...\"],[\"```\\n\\nOverall, we saw that running OctoCoder in 8-bit precision reduced the required GPU VRAM from 32...\"],[\"with \\\\\\\\( s^a_{ij} \\\\\\\\) and \\\\\\\\( s^b_{ij} \\\\\\\\) being some softmax normalization statistics that need to ...\"],[\"```\\nFor demonstration purposes, we duplicate the system prompt by ten so that the input length is lo...\"],[\"```\\n\\nWe're getting the same output as before, however this time, the model repeats the answer multip...\"],[\"```\\n\\nFor more information on how to use Flash Attention, please have a look at [this doc page](https...\"],[\"![](\\u002fblog\\u002fassets\\u002f163_optimize_llm\\u002fself_attn_tokens.png)\\n\\nEach word token is given a probability mass...\"],[\"The authors of the [*Attention Is All You Need*](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1706.03762) paper introduced ...\"],[\"Sinusoidal and learned position embeddings used to be the predominant methods to encode sentence ord...\"],[\"```\\n\\n**Output**:\\n```\\nshape of input_ids torch.Size([1, 1])\\nlength of key-value cache 20\\nshape of inp...\"],[\"```\\n\\nAs one can see, when using the key-value cache the text input tokens are *not* increased in len...\"],[\"```\\n\\nIn this chat, the LLM runs auto-regressive decoding twice:\\n  1. The first time, the key-value c...\"],[\"```python\\n# Generation as usual\\nprompt = system_prompt + \\\"Question: Please write a function in Pytho...\"],[\"```\\n\\n**Output**:\\n```\\n is a modified version of the function that returns Mega bytes instead.\\n\\ndef by...\"],[\"```\\n\\nRoughly 8 billion float values! Storing 8 billion float values in `float16` precision requires ...\"],[\"In addition to memory savings, MQA also leads to improved computational efficiency as explained in t...\"],[\"Also, the checkpoint used in this notebook - `bigcode\\u002foctocoder` - makes use of MQA.\\n\\n#### 3.2.3 Gro...\"],[\"## Conclusion\\n\\nThe research community is constantly coming up with new, nifty ways to speed up infer...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformer-based models are unable to process long ...\"],[\"## Usage tips\\n\\n- [`LEDForConditionalGeneration`] is an extension of\\n  [`BartForConditionalGeneration...\"],[\"This model was contributed by [patrickvonplaten](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten).\\n\\n## Resou...\"],[\"[[autodoc]] models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput\\n\\n[[autodoc]] models.led.m...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper states the following:\\n\\n*Visual language such as charts and plots is ubiqui...\"],[\"## Usage example\\n\\nCurrently one checkpoint is available for DePlot:\\n\\n- `google\\u002fdeplot`: DePlot fine-...\"],[\"```\\n\\n## Fine-tuning\\n\\nTo fine-tune DePlot, refer to the pix2struct [fine-tuning notebook](https:\\u002f\\u002fgit...\"],[\"*NOTE**: This example is outdated and is not longer actively maintained. Please \\nfollow the new inst...\"],[\"```bash\\n#!\\u002fusr\\u002fbin\\u002fenv bash\\npython run_asr.py \\\\\\n--output_dir=\\\".\\u002fwav2vec2-base-timit-asr\\\" \\\\\\n--num_tra...\"],[\"```\\n\\nThe resulting model and inference examples can be found [here](https:\\u002f\\u002fhuggingface.co\\u002felgeish\\u002fw...\"],[\"```\\n\\nThe instance above is used as follows:\\n* creates a tokenizer with `do_lower_case=True` (ignores...\"],[\"```bash\\n#!\\u002fusr\\u002fbin\\u002fenv bash\\npython run_asr.py \\\\\\n--output_dir=\\\".\\u002fwav2vec2-large-xlsr-53-arabic-speech...\"],[\"```\\n\\nFirst, let's understand how this dataset represents Arabic text; it uses a format called\\n[Buckw...\"],[\"```\\n\\nThe instance above is used as follows:\\n* creates a tokenizer with Buckwalter vocabulary and `wo...\"],[\"```\\nPYTHONPATH=..\\u002f..\\u002f..\\u002fsrc deepspeed --num_gpus 2 \\\\\\nrun_asr.py \\\\\\n--output_dir=output_dir --num_trai...\"],[\"```\\n    \\\"zero_optimization\\\": {\\n        ...\\n        \\\"find_unused_parameters\\\": true,\\n        ...\\n    }...\"],[\"```\\nPYTHONPATH=..\\u002f..\\u002f..\\u002fsrc deepspeed --num_gpus 4 run_pretrain.py \\\\\\n--output_dir=\\\".\\u002fwav2vec2-base-l...\"],[\"```\\n\\n\\n### Forced Alignment\\n\\nCharacter level forced alignment for audio and text pairs with wav2vec2 ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Multilingual pre-trained models are known to suffer ...\"],[\"### Input language\\n\\nThere are two ways to specify the input language:\\n1. By setting a default langua...\"],[\"```\\n\\n2. By explicitly passing the index of the language adapter for each sample:\\n\\n```python\\nimport t...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token cla...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\nAll the methods of this logging module are documented below, the main ones are\\n[`logging.get_v...\"],[\"See reference of the `captureWarnings` method below.\\n\\n[[autodoc]] logging.captureWarnings\\n\\n## Base s...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- [`DefaultFlowCallback`] which handles the default behavior for logging, saving and evaluation.\\n- [...\"],[\"If a package is installed but you don't wish to use the accompanying integration, you can change `Tr...\"],[\"## TrainerCallback\\n\\n[[autodoc]] TrainerCallback\\n\\nHere is an example of how to register a custom call...\"],[\"```\\n\\nAnother way to register a callback is to call `trainer.add_callback()` as follows:\\n\\n```python\\nt...\"],[\"ow to add BigBird to 🤗 Transformers?\\n=====================================\\n\\nMentor: [Patrick](https:...\"],[\"A good first starting point to better understand the library is to read\\nthe [documentation of our ph...\"],[\"7.  [ ] Successfully ran forward pass in Transformers that gives\\n    identical output to original ch...\"],[\"-   What type of model is *BigBird*? BERT-like encoder-only\\n    model? GPT2-like decoder-only model?...\"],[\"#### Make sure you've understood the fundamental aspects of BigBird\\n\\nAlright, now you should be read...\"],[\"2.  Clone your `transformers` fork to your local disk, and add the base\\n    repository as a remote:\\n...\"],[\"```\\n\\n3.  Set up a development environment, for instance by running the\\n    following command:\\n\\n    `...\"],[\"```\\n\\nNow you have set up a development environment to port *BigBird*\\nto 🤗 Transformers.\\n\\n### Run a p...\"],[\"Jupyter notebooks have the advantage that they allow for cell-by-cell\\nexecution which can be helpful...\"],[\"However, if the original code-base is very complex or only allows\\nintermediate components to be run ...\"],[\"```\\n\\nWe expect that every model added to 🤗 Transformers passes a couple of\\nintegration tests, meanin...\"],[\"#### (Important) More details on how to create a debugging environment for BigBird \\n\\n- BigBird has m...\"],[\"Otherwise, let's start generating a new model with the amazing\\nCookiecutter!\\n\\n**Use the Cookiecutter...\"],[\"```\\n    git checkout -b add_big_bird\\n```\\n\\n2.  Commit the automatically generated code:\\n\\n```\\n    git ...\"],[\"```\\n\\n5.  Once you are satisfied, go to the webpage of your fork on GitHub.\\n    Click on \\\"Pull reques...\"],[\"Now you can finally start coding :). The generated code in\\n`src\\u002ftransformers\\u002fmodels\\u002fbig_bird\\u002fmodelin...\"],[\"```\\n\\nThe above command will create a model according to the default\\nparameters as defined in `BigBir...\"],[\"-   A good starting point to convert the original TF BigBird implementation to the PT Hugging Face i...\"],[\"```\\n\\nIf either the shape or the name doesn't match, you probably assigned\\nthe wrong checkpoint weigh...\"],[\"```\\n\\n**7. Implement the forward pass**\\n\\nHaving managed to correctly load the pretrained weights into...\"],[\"```\\n\\nIt is very likely that the 🤗 Transformers implementation and the\\noriginal model implementation ...\"],[\"The best way to fix the problem is usually to look at the forward pass\\nof the original implementatio...\"],[\"```\\n\\nHaving fixed all common tests, it is now crucial to ensure that all the\\nnice work you have done...\"],[\"```\\n\\n**Note**: In case you are using Windows, you should replace `RUN_SLOW=1` with\\n`SET RUN_SLOW=1`\\n...\"],[\"```\\n\\nTo ensure that the tokenizer works correctly, it is recommended to first\\ncreate a script in the...\"],[\"```\\n\\nWhen both `input_ids` yield the same values, as a final step a tokenizer\\ntest file should also ...\"],[\"Next, make sure that the docstring added to\\n`src\\u002ftransformers\\u002fmodels\\u002fbig_bird\\u002fmodeling_big_bird.py` ...\"],[\"```\\n\\nThere are a couple of other very strict design tests in 🤗 Transformers\\nthat might still be fail...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The BigCode project is an open-scientific collaborat...\"],[\"## Implementation details\\n\\nThe main differences compared to GPT2.\\n- Added support for Multi-Query At...\"],[\"```\\n\\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more ab...\"],[\"```\\n\\n### Expected speedups\\n\\nBelow is a expected speedup diagram that compares pure inference time be...\"],[\"Examples\\nIn this folder we showcase some examples to use code models for downstream tasks.\\n\\n## Compl...\"],[\"```\\n\\n## Code generation: text to python\\nIn this task we want to train a model to generate code from ...\"],[\"```\\n\\n## Code explanation: python to text\\nIn this task we want to train a model to explain python cod...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\nIn several question answering benchmarks, pretrained ...\"],[\"## Usage tips\\n\\n- Splinter was trained to predict answers spans conditioned on a special [QUESTION] t...\"],[\"## SplinterForQuestionAnswering\\n\\n[[autodoc]] SplinterForQuestionAnswering\\n    - forward\\n\\n## Splinter...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"As shown on the following figure, Jukebox is made of 3 `priors` which are decoder only models. They ...\"],[\"## Usage tips\\n\\n- This model only supports inference. This is for a few reasons, mostly because it re...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self atten...\"],[\"## Usage tips\\n\\n- One can use the [`AutoImageProcessor`] API to prepare images for the model.\\n- NAT c...\"],[\"## NatConfig\\n\\n[[autodoc]] NatConfig\\n\\n## NatModel\\n\\n[[autodoc]] NatModel\\n    - forward\\n\\n## NatForImage...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large multimodal models (LMM) have recently shown en...\"],[\"- For better results, we recommend users to prompt the model with the correct prompt format: \\n\\n```ba...\"],[\"```\\n\\nFor multiple turns conversation:\\n\\n```bash\\n\\\"USER: \\u003cimage\\u003e\\\\n\\u003cprompt1\\u003eASSISTANT: \\u003canswer1\\u003eUSER: \\u003cp...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*General-purpose language models that can solve vario...\"],[\"## Usage tips\\n\\nInstructBLIP uses the same architecture as [BLIP-2](blip2) with a tiny but important ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nYou will then be able to use the auto classes like you would usually do!\\n\\n\\u003cTip warning={true}\\u003e\\n...\"],[\"### AutoModelForPreTraining\\n\\n[[autodoc]] AutoModelForPreTraining\\n\\n### TFAutoModelForPreTraining\\n\\n[[a...\"],[\"### TFAutoModelForSeq2SeqLM\\n\\n[[autodoc]] TFAutoModelForSeq2SeqLM\\n\\n### FlaxAutoModelForSeq2SeqLM\\n\\n[[a...\"],[\"### AutoModelForTokenClassification\\n\\n[[autodoc]] AutoModelForTokenClassification\\n\\n### TFAutoModelFor...\"],[\"### AutoModelForVideoClassification\\n\\n[[autodoc]] AutoModelForVideoClassification\\n\\n### AutoModelForMa...\"],[\"### AutoModelForZeroShotObjectDetection\\n\\n[[autodoc]] AutoModelForZeroShotObjectDetection\\n\\n## Audio\\n\\n...\"],[\"### AutoModelForTableQuestionAnswering\\n\\n[[autodoc]] AutoModelForTableQuestionAnswering\\n\\n### TFAutoMo...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [camembert](https:\\u002f\\u002fhuggingface.co\\u002fcamembert). The original code can b...\"],[\"## CamembertForSequenceClassification\\n\\n[[autodoc]] CamembertForSequenceClassification\\n\\n## CamembertF...\"],[\"# Token classification\\n\\nBased on the scripts [`run_ner.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransform...\"],[\"```\\n\\nThe GermEval 2014 dataset contains some strange \\\"control character\\\" tokens like `'\\\\x96', '\\\\u200...\"],[\"```\\n\\n#### Run the Pytorch version\\n\\nTo start training, just run:\\n\\n```bash\\npython3 run_ner.py --data_d...\"],[\"```\\n\\nIt must be saved with a `.json` extension and can be used by running `python3 run_ner.py config...\"],[\"```\\n\\n#### Run the Tensorflow 2 version\\n\\nTo start training, just run:\\n\\n```bash\\npython3 run_tf_ner.py ...\"],[\"```\\n\\nOn the test dataset the following results could be achieved:\\n```bash\\n           precision    re...\"],[\"```\\n\\n### Emerging and Rare Entities task: WNUT’17 (English NER) dataset\\n\\nDescription of the WNUT’17 ...\"],[\"```\\n\\nLet's define some variables that we need for further pre-processing steps:\\n\\n```bash\\nexport MAX_...\"],[\"```\\n\\n#### Run the Pytorch version\\n\\nFine-tuning with the PyTorch version can be started using the `ru...\"],[\"```\\n\\nIf your GPU supports half-precision training, please set `fp16` to `true`.\\n\\nSave this JSON-base...\"],[\"```\\n\\nWNUT’17 is a very difficult task. Current state-of-the-art results on this dataset can be found...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nTake a look at an example now:\\n\\n```py\\n\\u003e\\u003e\\u003e minds[\\\"train\\\"][0]\\n{'audio': {'array': array([ 0.     ...\"],[\"```\\n\\nNow you can convert the label id to a label name:\\n\\n```py\\n\\u003e\\u003e\\u003e id2label[str(2)]\\n'app_error'\\n```\\n\\n...\"],[\"```\\n\\nNow create a preprocessing function that:\\n\\n1. Calls the `audio` column to load, and if necessar...\"],[\"```\\n\\n## Evaluate\\n\\nIncluding a metric during training is often helpful for evaluating your model's pe...\"],[\"```\\n\\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\u003cTip\\u003e\\n\\nFor a more in-depth example of how to finetune a model for aud...\"],[\"```\\n\\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ...\"],[\"```\\n\\nGet the class with the highest probability, and use the model's `id2label` mapping to convert i...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage example\\n\\n```python\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transformers import AutoModel, AutoTokenizer\\n\\n...\"],[\"```\\n\\n\\u003cTip\\u003e \\n\\nThis implementation is the same as BERT, except for tokenization method. Refer to [BERT...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The process of selecting output tokens to generate text is known as decoding, and you can customize ...\"],[\"```\\n\\nPrinting out the `model.generation_config` reveals only the values that are different from the ...\"],[\"```\\n\\nEven if the default decoding strategy mostly works for your task, you can still tweak a few thi...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import AutoModelForCausalLM, GenerationConfig\\n\\n\\u003e\\u003e\\u003e model = AutoModel...\"],[\"```\\n\\nYou can also store several generation configurations in a single directory, making use of the `...\"],[\"\\u003e\\u003e\\u003e # You could then use the named generation config file to parameterize generation\\n\\u003e\\u003e\\u003e generation_...\"],[\"```\\n\\n## Streaming\\n\\nThe `generate()` supports streaming, through its `streamer` input. The `streamer`...\"],[\"```\\n\\n## Decoding strategies\\n\\nCertain combinations of the `generate()` parameters, and ultimately `ge...\"],[\"```\\n\\n### Contrastive search\\n\\nThe contrastive search decoding strategy was proposed in the 2022 paper...\"],[\"```\\n\\n### Multinomial sampling\\n\\nAs opposed to greedy search that always chooses a token with the high...\"],[\"```\\n\\n### Beam-search decoding\\n\\nUnlike greedy search, beam-search decoding keeps several hypotheses a...\"],[\"```\\n\\n### Beam-search multinomial sampling\\n\\nAs the name implies, this decoding strategy combines beam...\"],[\"```\\n\\n### Diverse beam search decoding\\n\\nThe diverse beam search decoding strategy is an extension of ...\"],[\"\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(checkpoint)\\n\\u003e\\u003e\\u003e inputs = tokenizer(prompt, return_tens...\"],[\"```\\n\\nThis guide illustrates the main parameters that enable various decoding strategies. More advanc...\"],[\"\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(checkpoint)\\n\\u003e\\u003e\\u003e inputs = tokenizer(prompt, return_tens...\"],[\"```\\n\\nWhen using assisted decoding with sampling methods, you can use the `temperature` argument to c...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nIf you want to use a specific model from the [hub](https:\\u002f\\u002fhuggingface.co) you can ignore the t...\"],[\"```\\n\\nTo iterate over full datasets it is recommended to use a `dataset` directly. This means you don...\"],[\"```\\n\\n[[autodoc]] pipeline\\n\\n## Pipeline batching\\n\\nAll pipelines can use batching. This will work\\nwhen...\"],[\"```\\n\\n```\\n# On GTX 970\\n------------------------------\\nStreaming no batching\\n100%|████████████████████...\"],[\"```\\n------------------------------\\nStreaming no batching\\n100%|██████████████████████████████████████...\"],[\"```\\n\\nThere are no good (general) solutions for this problem, and your mileage may vary depending on ...\"],[\"```\\n\\nThis should be very transparent to your code because the pipelines are used in\\nthe same way.\\n\\nT...\"],[\"```\\n\\nThat should enable you to do all the custom code you want.\\n\\n\\n## Implementing a pipeline\\n\\n[Imple...\"],[\"### VideoClassificationPipeline\\n\\n[[autodoc]] VideoClassificationPipeline\\n    - __call__\\n    - all\\n\\n#...\"],[\"### TokenClassificationPipeline\\n\\n[[autodoc]] TokenClassificationPipeline\\n    - __call__\\n    - all\\n\\n#...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Image segmentation is usually addressed by training ...\"],[\"## Usage tips\\n\\n- [`CLIPSegForImageSegmentation`] adds a decoder on top of [`CLIPSegModel`]. The latt...\"],[\"## CLIPSegTextModel\\n\\n[[autodoc]] CLIPSegTextModel\\n    - forward\\n\\n## CLIPSegVisionModel\\n\\n[[autodoc]] ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce a new language representation model cal...\"],[\"* a special mask token with probability 0.8\\n    * a random token different from the one masked with ...\"],[\"- A blog post on [BERT Text Classification in a different language](https:\\u002f\\u002fwww.philschmid.de\\u002fbert-t...\"],[\"- [`FlaxBertForSequenceClassification`] is supported by this [example script](https:\\u002f\\u002fgithub.com\\u002fhug...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e...\"],[\"- A blog post on how to use [Hugging Face Transformers with Keras: Fine-tune a non-English BERT for ...\"],[\"- [Token classification](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fchapter7\\u002f2?fw=pt) chapter of the 🤗 Hugging Fa...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`BertForMaskedLM`] is supported by this [example script](htt...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`BertForQuestionAnswering`] is supported by this [e...\"],[\"**Multiple choice**\\n- [`BertForMultipleChoice`] is supported by this [example script](https:\\u002f\\u002fgithub...\"],[\"🚀 **Deploy**\\n- A blog post on how to [Convert Transformers to ONNX with Hugging Face Optimum](https:...\"],[\"[[autodoc]] TFBertTokenizer\\n\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Bert specific outputs\\n\\n[[autodoc]] models...\"],[\"## TFBertForPreTraining\\n\\n[[autodoc]] TFBertForPreTraining\\n    - call\\n\\n## TFBertModelLMHeadModel\\n\\n[[a...\"],[\"[[autodoc]] FlaxBertForNextSentencePrediction\\n    - __call__\\n\\n## FlaxBertForSequenceClassification\\n\\n...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Exporting a model requires two things:\\n\\n- model instantiation with the `torchscript` flag\\n- a forwar...\"],[\"```\\n`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension...\"],[\"```\\n\\nWe recommended you trace the model with a dummy input size at least as large as the\\nlargest inp...\"],[\"# Initializing the model with the torchscript flag\\n# Flag set to True even though it is not necessar...\"],[\"```\\n\\n### Loading a model\\n\\nNow you can load the previously saved `BertModel`, `traced_bert.pt`, from ...\"],[\"```\\n\\n## Deploy Hugging Face TorchScript models to AWS with the Neuron SDK\\n\\nAWS introduced the [Amazo...\"],[\"### Implications\\n\\nTransformers models based on the [BERT (Bidirectional Encoder Representations from...\"],[\"### Converting a model for AWS Neuron\\n\\nConvert a model for AWS NEURON using the same code from [Usin...\"],[\"```\\n\\nYou only need to modify the following line:\\n\\n```diff\\n- torch.jit.trace(model, [tokens_tensor, s...\"]],\"hovertemplate\":\"source=transformers\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"transformers, circle\",\"marker\":{\"color\":\"#FFA15A\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"transformers, circle\",\"showlegend\":true,\"x\":[-3.37502,0.12152849,0.96784455,-1.7704188,-1.473309,-1.1042244,-1.1673709,-1.7150873,-3.5414855,-3.0243351,-4.8172383,1.2868958,-8.805158,-11.033769,-9.423252,-9.352887,0.62179613,1.2772467,-0.08342651,0.81540036,0.93945223,0.52537495,0.9310064,0.6266809,0.96152645,0.7231187,6.8839464,6.719237,7.323478,8.134105,7.709302,7.7567945,7.850246,6.783284,6.6321964,6.824562,6.9389515,14.809583,14.809298,14.808719,14.809951,14.809667,14.802536,14.808827,14.809415,14.810231,14.811809,14.810173,14.8105955,14.8088,14.807372,7.011136,7.9221764,-6.94278,7.8042045,7.6448426,7.566988,-3.705022,7.6122003,0.84257674,7.8318863,-2.6358702,-2.3845756,6.882407,-1.7607924,7.5473514,6.757864,6.274074,7.6884804,7.1608524,7.6798196,2.884426,2.9053578,7.2069335,7.6905713,7.5885286,-5.6171365,-5.6083465,-6.243785,-5.7601614,-5.651808,-5.671022,-5.639029,-5.54488,-5.963777,-5.672371,-5.6478295,-5.922709,-5.670428,-5.6647925,-6.131902,-5.6836534,-5.4385343,-6.054932,-5.6948953,-5.638743,-5.6599507,-5.657772,-5.750586,-5.730722,-5.692123,-5.6611395,-5.6007323,-5.5451436,-5.6309814,-6.3918014,-5.738301,-5.5006833,-5.6213455,-5.657097,-5.6284,-5.7658677,-5.577258,-5.440471,-5.5949554,-6.0085664,-5.633917,-5.4056497,-5.677788,-5.713773,-6.256434,-5.5073743,-5.330611,-5.6315484,-5.4289045,-5.933541,-5.703953,-5.4713073,-5.4074025,-5.627139,-5.620258,-5.392662,-5.366062,-5.492056,-5.5541162,-5.4954224,-5.344362,8.053988,7.9510436,7.6173263,-6.5928936,-1.8520385,4.1878324,4.32666,-1.0266371,1.964892,2.0927658,2.3149962,4.142398,3.822984,-8.888498,-9.295812,-6.2240376,-8.8777,-8.559153,-6.245172,-5.6887145,-10.620131,-11.365411,-6.2845035,-6.2543807,-6.2603416,-8.417813,-8.552621,-7.253891,-10.918743,-4.2120333,0.48636323,6.6987047,-5.6991105,-5.835959,-5.8061295,-5.1245837,-5.730817,-6.3020744,-6.1962113,-6.2984786,-6.1114326,-5.7700696,-5.6940417,-10.633752,6.298444,7.031469,-7.545749,-7.6178737,-6.822003,-8.20851,6.7084265,-6.093421,-6.3422017,5.981288,-6.026349,-5.6512637,-5.4577813,-5.944753,-2.0863078,-5.7193565,-6.2944794,-6.1596885,-6.2529883,-6.0974183,-9.742395,6.411456,-7.4133525,-5.5587273,-3.3420393,-5.0477705,-4.57339,-6.0685434,-5.8742943,6.3221345,-1.423173,-1.403206,-6.997325,-7.237556,-5.4746265,-11.374831,-5.8999367,-2.6671107,-0.49653107,-4.5881433,-3.2413285,-3.3226197,-2.353561,-0.95408624,-2.9250293,-2.816906,-5.310258,4.5947824,2.6941717,-2.9220543,-1.0019758,-0.4538051,-2.267822,-1.4398541,0.0429045,-2.5066054,-0.9403191,-1.6957346,-2.5992362,5.0201826,5.208711,-2.4398434,-0.93814385,-2.0127742,-0.7628698,-0.666557,-0.83771414,-0.39115354,5.85161,-0.615799,-1.1299976,-0.65099686,-1.6023512,-1.8189752,-0.91874766,-2.1745927,-2.8311226,-2.1447284,-0.061986115,2.933489,-2.8260975,-6.2271557,-4.9307747,-4.153025,0.8032421,-5.5735936,-5.436863,-3.9052675,-3.4592059,3.19339,-5.053123,-2.4000196,6.36297,-3.6967554,-5.3333635,-7.8628206,-7.85852,-9.959597,-4.629309,-4.3485703,-3.6761987,-1.7666637,-1.7605901,-2.8578374,-8.908076,0.1181092,3.5338962,2.4919753,1.6459717,-0.28978783,-1.5397794,-0.9054823,0.68575996,0.3008434,1.0347217,1.3850573,1.2515306,-1.2241685,-0.057015464,0.052858736,0.5376189,-1.1591599,-0.07790235,-5.6460567,-5.655329,-3.1402302,-4.372969,-5.463188,-5.0681906,-4.0149307,-11.13221,7.586444,7.3704643,6.6396384,8.108103,8.003813,6.842973,6.6441627,6.79076,7.179466,14.811061,14.809285,14.808249,14.811759,14.8088455,14.802368,14.81025,14.811712,14.81108,14.810387,14.810062,14.81059,14.810674,14.806029,7.02419,7.9012547,8.103364,-2.412881,8.326992,8.226561,2.8278184,7.2708473,8.060253,-5.630258,-5.624079,-6.1616597,-5.7998657,-5.680507,-5.767198,-5.735773,-5.6066422,-5.751625,-5.99298,-5.6575165,-5.9140053,-5.713763,-5.637395,-5.9691453,-5.599386,-5.5870047,-5.734365,-6.050555,-5.6961956,-5.713333,-5.629644,-5.7113447,-5.715583,-5.7230515,-5.6475654,-5.590093,-5.435452,-5.5204544,-5.6718946,-6.267724,-5.6860824,-5.4712634,-5.635091,-5.657178,-5.605986,-5.7119102,-5.6507745,-5.5998473,-5.5417295,-5.649056,-5.5597873,-5.6562796,-6.125816,-5.6244287,-5.491194,-5.639884,-5.7300262,-6.2664514,-5.5813403,-5.4074326,-5.395361,-5.4546585,-5.769594,-5.808094,-5.669987,-5.401285,-5.566233,-5.626562,-5.6174054,-5.392906,-5.358078,-5.620985,-5.4880886,-5.469516,-5.447279,8.164564,-6.665757,-3.4916925,-3.4265397,-2.3758008,-2.9387956,-1.7768654,-1.3711709,-1.3641399,-3.2763193,-2.1642237,-2.0717256,-1.9644015,-2.794518,-2.4211435,-2.0912147,-2.3637278,-2.134617,-2.239379,-2.1165206,-2.624915,-2.0907662,-2.8835948,-3.3033507,-1.4456959,-3.6163905,-2.0301328,-2.3576748,-2.0762818,-7.5932703,-7.719315,-2.1183085,-7.1208897,-2.6601007,-10.616685,6.768988,-5.7494273,-5.5238132,-4.3516164,-4.1349573,-4.5789256,-3.9079468,-3.3614738,-11.180255,-11.448945,-11.363632,-8.797762,-1.8468045,-9.403524,-9.015473,-8.045987,-8.561246,-2.9783888,-7.91918,-7.578185,-7.2567086,-7.191013,-7.282272,-2.3846366,-10.618847,-9.286962,-9.107871,-10.278673,-5.0214972,-2.9041624,-3.0296512,-3.024167,0.17720053,-8.903213,-9.170858,-8.051674,-5.6242657,-7.601561,-7.554646,-7.5411506,-7.9622254,-7.746544,-7.812714,-5.4825993,-5.0559487,-1.6237198,-1.2975554,-10.444488,-10.636159,-10.790434,-5.626525,-5.7489843,-4.918857,-9.434442,-9.490193,-8.048815,-7.5188885,-7.051486,-6.8706627,-7.037121,-7.0475745,-1.7870661,-10.171879,-0.41118637,0.2673142,0.26428577,1.0030059,-9.472444,-9.285983,-10.306559,-3.452789,-0.3471808,-7.5070744,-7.6807976,-3.4869897,-3.7614841,2.773668,-2.8417583,-3.0487518,-3.2857463,-2.5710328,-3.0966191,-0.85944456,-0.78350645,-1.1947633,-0.66019094,-0.14850529,-0.56505287,-1.0657471,-1.0438131,-0.824964,-5.9001207,-2.9599915,-5.9296327,-4.063625,-0.9801522,-3.31026,-2.062353,-1.5836617,-9.482939,-9.558445,6.701199,-5.172133,-5.535074,-1.0558836,-3.167793,-2.5135586,-4.263915,-4.4407463,-4.279503,-10.787373,-7.6285543,-9.151,-8.497413,-7.8667946,-7.7969027,-8.680936,-4.1666803,-11.318239,-9.782248,-8.538881,-4.707861,-0.25440332,-4.307264,-3.538975,-3.469353,-2.1461613,-2.2844121,-2.2597976,-5.448582,-7.941612,-7.995219,-9.595784,0.6473828,0.85392976,1.156827,-7.7076707,-7.7416453,-7.399706,-7.3741918,-0.9905033,-8.009243,-8.137405,-1.5181571,-1.1601366,-0.25757965,-1.4079618,-0.66411936,-1.598771,-0.51763535,-5.5494366,-3.995371,-10.959973,-9.622824,-9.724132,-9.149386,-9.440602,-8.478262,-8.517824,-3.8922276,-10.878821,-7.811947,-7.632482,-7.0151877,-7.1791525,-1.9556811,-2.1109285,-2.0222282,-2.122842,-2.22226,-3.9871812,-2.273639,-3.8356822,-3.686846,-3.4675708,-1.4217578,-2.0357697,0.1230684,0.5292002,-2.88973,-9.543417,-9.775546,-10.394869,-7.9897795,-8.429888,3.0373383,0.43133512,-0.21171871,-0.5205113,-0.3768281,-0.35451743,0.060978707,-1.1201577,-8.3663225,-0.75357413,-3.93222,-0.0701918,-0.3321203,-2.249467,-1.9855294,-1.6809925,-1.6449649,-6.269256,-6.357855,-10.69463,6.749415,8.160212,7.950235,7.9925933,6.7276134,6.6268845,6.6927185,7.0629,7.0673294,7.4813714,-4.080206,5.639698,8.106835,6.634294,-1.8476248,6.974556,7.6394887,7.750262,2.7957978,3.287595,7.3647933,7.883071,-5.6368985,-5.597151,-5.652615,-5.70216,-5.7598033,-5.577116,-5.527066,-5.902253,-5.7169924,-5.6547303,-6.0100694,-5.675784,-5.5612035,-6.1471453,-5.6505136,-5.595324,-6.0218325,-5.6612983,-5.7315326,-5.732353,-5.6841063,-5.742354,-5.613369,-5.5262785,-5.4989867,-5.672897,-6.207998,-5.6725307,-5.4169836,-5.642235,-5.5742702,-5.748243,-5.64701,-5.6704373,-5.81112,-5.5881844,-5.5355277,-5.7105017,-5.44217,-5.6835537,-5.813507,-5.64723,-5.4513297,-5.731452,-5.7223253,-5.951112,-6.161096,-5.5456724,-5.3844733,-5.325885,-5.274446,-5.8475337,-5.7921405,-5.6538754,-5.3903275,-5.370445,-5.6176124,-5.6430273,-5.593199,-5.399459,-5.3824887,-5.629384,-5.549746,-5.5518637,-5.334863,6.666752,8.127109,8.087927,-4.8488507,-6.6634603,-1.6442188,1.1162388,1.1322794,-0.601932,0.14106755,1.8494763,1.5708852,2.1052628,1.3676559,-0.40695834,5.0766244,2.5584528,1.3914187,-7.4937553,-8.245547,-0.3171708,-1.802029,-2.7655983,-5.524441,-10.605966,-9.450674,-8.84238,-0.14429818,-8.493826,-7.9825673,-7.7677536,2.7129028,-1.2984812,-1.665205,0.8344105,-1.696208,0.33876562,-1.6717786,1.0946287,6.4679103,-1.7550262,-7.7735147,-1.6030799,0.70626074,-1.0682142,-2.1360996,-3.092947,0.50993556,-1.6980495,-3.6484952,-2.6538417,6.786286,8.159739,-5.6084046,4.7268376,-5.2254176,6.6347194,6.2015104,7.0473194,-9.332223,-6.307086,-3.7374887,-3.4474409,0.67455137,0.3361315,-1.8885878,-2.4676864,2.6252792,6.9393187,-3.118865,-5.5398016,-6.1743693,-5.380869,-5.83749,-5.8806906,-5.6186876,-5.7588086,-5.6176023,-5.6297216,-6.0313864,-5.651544,-5.5037355,-5.854096,-5.971714,-5.660851,-5.613649,-5.7370353,-5.7262855,-5.6491413,-5.596035,-5.6689754,-5.3788786,-5.5206394,-5.700413,-6.2588778,-5.7439013,-5.4313245,-5.6540875,-5.614589,-5.6421385,-5.590346,-5.6007805,-5.6032467,-5.456118,-5.7006903,-5.539476,-5.589523,-5.8485465,-5.6059732,-5.4706984,-5.6605163,-5.7147994,-6.2530937,-5.5700254,-5.32169,-5.389697,-5.409435,-5.830612,-5.7216625,-5.592705,-5.36694,-5.5953994,-5.6658893,-5.562673,-5.3216767,8.058471,-6.621208,-4.8136225,3.5999882,1.9827617,-2.1547165,-2.8017266,1.1743097,0.21419784,-0.0049729287,-1.3905478,-2.477119,3.131945,-0.36682606,-0.43992528,-1.4063704,-1.6224505,0.06484889,-0.5954866,-1.3547534,-0.47534117,-1.3902756,-0.20188874,-0.31350958,-0.81482536,-1.7608773,-1.9319816,-1.6100044,-4.6595488,-7.1675344,-7.0803275,-7.174217,-7.125229,2.7259676,3.627522,-1.6027426,-1.5675955,0.2193772,1.2299254,-0.60236114,-0.014151348,0.3710729,-2.2896075,-1.5060922,-7.301946,-7.389588,-10.8586035,6.6787004,-6.291227,-5.5130424,-9.267795,-9.406969,-7.799071,-11.050325,-9.382935,0.67524445,-3.1814506,-0.6453544,-1.285482,-0.9898934,-1.4295385,-1.0087707,-9.494044,-9.676862,-8.537827,-10.6322975,-8.6964445,-7.1374817,1.7701485,-0.18567342,2.034715,0.47293,-9.104065,-8.826908,-9.944118,-10.768945,-8.93878,0.53730613,0.68350846,0.84518003,-0.81440413,-8.61521,-8.651709,-8.651569,-8.186819,-10.51439,-10.517595,-10.68909,-5.501481,-4.18295,-2.9443967,-5.417452,6.669781,6.173197,6.710824,-4.972756,1.5510459,-1.4960636,1.3907264,1.8378637,0.14748372,0.45405257,-6.3382335,-6.338316,-6.025875,-8.646829,2.634004,-0.22211798,-1.5536487,-1.7649618,-1.583841,-1.5179214,-1.5757685,-0.23541427,-1.2654911,-1.0227348,-0.8636803,-1.1870853,-1.2383497,-1.8569529,-0.5901584,-0.10497618,-0.2737188,-1.4080875,-0.99003017,-0.51774913,-1.1725686,-1.3897947,0.37856784,-0.473587,-1.1588278,-3.4292166,-2.000408,-2.1443973,6.533601,-0.120147265,-2.5566533,6.647437,4.8367844,3.2549365,2.287851,2.2857065,2.6903386,3.3248105,4.919407,5.5151567,5.5288606,5.9005136,-1.030658,-6.5003304,-6.344195,-4.8671985,-10.162157,-11.11338,4.7912726,2.904554,3.937123,4.184433,-1.6941211,-2.122768,-1.8902537,-2.346425,-4.36064,-4.2603555,-4.673746,-4.2871776,-4.2041516,-4.369677,-3.9887505,-3.3689504,-4.2993565,-3.8744347,-6.207066,-9.7105255,-9.41757,-8.878564,-5.7683907,-5.9436874,-5.965729,-7.851358,-8.768924,-8.728142,-8.085968,-1.5361167,1.2152327,1.2276176,1.3821733,1.3178875,1.2690667,1.0915706,-9.477012,-9.958434,-5.3636217,-1.8379061,-0.56185484,-1.5054482,-3.712886,-6.212519,-7.6351433,-7.3395324,-9.592023,0.37882742,-9.581696,-9.515096,-9.64083,-3.559273,-3.5522792,-0.23775779,1.898359,-2.0593622,-6.010708,-5.13763,-3.592514,-7.5794244,-5.8602533,-3.647241,-5.355355,-0.6511123,-0.7076403,-4.2515326,-1.026526,-4.861521,-3.612353,-4.898516,-6.8412375,-10.822547,-5.3017764,-8.296801,-2.1243386,-2.2469823,-2.349983,-2.1737847,-2.5145936,-2.0328846,-2.3008287,-5.1342673,-5.6063647,-5.085861,-11.214462,-11.445364,6.4950023,-5.23962,-5.155934,-1.9096656,-1.3979943,-3.6796684,-4.365501,-2.54024,0.49029845,-3.1210027,0.5159661,3.7637541,-8.776716,-8.697729,-8.738151,-8.231857,-8.459272,-10.563068,-11.195436,6.1214013,5.191837,2.708985,2.9590383,-3.572044,5.0866375,4.2199216,4.313689,5.234297,4.9493623,-2.4036546,2.306688,2.256714,4.602392,-8.571338,-8.770359,-8.366232,-0.7492742,-0.27220175,-0.528078,-8.57511,-8.001369,-3.525601,-2.5505445,-2.5780082,-2.739381,-2.4847946,-7.9214125,-7.9497366,-9.739933,-5.762315,-5.4508157,-1.8576738,-5.314286,-4.1205053,-3.8695383,-4.0289254,-3.956521,-3.361711,-4.257166,-4.6440935,-2.2485423,-1.8236643,-2.2129023,-2.7243617,-3.181972,-2.1849382,-3.451696,-3.5222647,-2.8766139,0.73014814,-0.584762,-5.067272,-5.0332546,-2.253591,2.454376,-2.8330712,-2.6720877,-3.5730987,-3.7137363,-3.1742802,-8.179223,0.8703043,-0.086583816,-1.0745476,-5.766829,-5.374756,-4.7355614,-3.9205317,-1.8913596,-3.452422,-11.276077,-6.019327,-5.8739147,-1.6456037,-3.817078,-5.4314213,-2.6767297,-5.0635667,6.733854,6.844636,-5.415858,-4.8548546,-4.4400935,-4.228941,-4.8641286,-3.968898,-3.197008,-11.138276,-11.403272,-3.3179884,4.8512454,2.8771167,7.38927,7.3761225,7.411167,7.4209204,7.453972,7.4264245,4.19304,1.7468272,-7.6858172,-8.307986,-10.569314,-4.1988983,-3.7436664,0.5055698,-0.62279516,-0.2986088,-0.7465462,-0.7032868,-0.8023147,-0.65360487,-0.7047841,8.192234,-6.187445,-4.9055467,6.8679485,6.6866164,6.4339314,7.0512457,-8.113807,7.6535854,7.796813,0.54536647,7.8330503,-2.3930519,-2.0509684,-6.141574,-5.567243,2.8682873,7.090307,7.8201694,-5.736365,-5.7405686,-5.582604,-6.2638535,-5.5802836,-5.6365085,-5.611629,-5.6322794,-5.877453,-5.4587307,-6.182084,-5.780437,-5.3516006,8.0721,7.8243976,-6.601917,-5.503988,0.4129696,-3.0870032,-2.317342,-3.8054357,-1.4354535,-1.6125596,-1.468527,-1.2472351,-1.0894908,-0.17304961,-0.20587975,-1.6821183,-1.359274,-1.2153035,-1.5591943,-0.45644066,-1.5406325,-1.4730015,-1.2710744,-0.26390207,-1.5311044,-1.5748949,-1.5278952,-2.0693355,-1.6031315,-1.6001221,-1.549391,-1.5491011,-1.4404684,0.14919561,0.109127544,0.17239363,-3.842306,-0.775458,0.63145155,-8.60133,-8.482664,5.1011734,-5.7796335,-3.9828892,-1.8571473,-3.2958815,-5.8568773,-5.786264,-6.8919044,-9.532106,-9.681466,-11.470476,-11.567435,-3.5686872,-3.1956742,-4.5363483,5.058798,-7.726299,0.516049,-7.72333,-7.5526543,-2.7405708,-3.1972663,-6.75704,-7.6280475,-7.424191,-7.402175,-7.4864526,-7.193784,-7.2917123,-6.976269,-7.065259,-6.910047,-6.478657,-7.0427485,-7.3276763,-6.5176225,-7.9375296,-7.7663517,-7.709874,-7.0995727,-7.1487713,-6.5417385,-10.305031,-11.191362,-2.0093956,-2.6566534,-4.523856,-1.6282586,1.088235,-1.3278599,-4.835052,-6.030724,-5.3410482,-0.1857588,0.09324622,4.1677628,3.7003126,7.5698915,7.457558,8.15116,7.832856,7.81216,6.8473525,6.638516,7.6750207,7.0288944,14.810378,14.8080435,14.810004,14.809668,14.809325,14.801125,14.809348,14.808949,14.808616,14.807505,14.811684,14.807584,14.811509,14.808165,7.183184,7.5841627,-3.7007012,6.648446,-2.4079478,7.6678805,7.672125,7.6985116,7.659524,2.9651408,6.4660463,7.41207,7.6812625,-5.6172853,-5.568644,-5.648423,-5.5041203,-5.424521,-5.635342,-5.491279,-5.482626,-5.2872725,-5.399865,-5.4179015,-5.575265,-5.235707,-5.5857625,-5.2425766,-5.3124375,-5.4147778,-5.3769255,-5.734244,-5.255387,-5.300666,-5.570739,-5.442362,-5.7984395,-5.3140635,7.596115,7.7317543,-5.721871,-5.2733693,-5.3997517,-5.2643023,-5.2617054,-5.6673465,-5.817025,-5.594885,-5.539011,-5.56414,-5.5675874,-5.4866557,-5.4501357,-6.2264605,-5.5169353,-5.3137403,-5.507948,-5.48005,-5.347449,-5.5026016,-5.6851745,-5.5031133,-5.5608754,-5.4809732,-5.4554033,-5.4634,-5.4381194,-5.515822,-5.6225543,-5.4591446,-5.6004004,-5.397168,-5.495693,-5.2690988,-5.4572263,-5.5333548,-6.029706,-5.486663,-5.4136033,-5.340469,-5.394872,-5.448532,-5.3248844,-5.371315,-5.583594,-5.476981,-5.2235556,-5.4848437,-5.41241,-5.4355683,-5.4883904,-5.45467,-5.462471,-5.2679462,-5.437596,-5.5062175,-5.5230584,-5.2048416,-5.153592,7.648721,7.751074,7.809543,-6.699335,-5.7570844,-5.5137873,-11.079646,-11.222489,-0.9622035,-1.0641367,-0.21982047,0.061445165,0.067462005,0.021204736,-0.29612094,-0.62133294,-0.44489482,0.30743685,0.23874028,0.5516818,0.36648932,-0.028019797,-0.5174306,0.55152667,-0.21941982,-0.00056642044,-0.17833172,-0.4695699,-1.411476,-0.7807834,-1.0805428,-1.0150708,-0.92829365,-0.38383427,-0.92006415,-0.6497762,-0.6011064,-0.4439367,-0.26256192,-0.2522911,-0.31303877,-0.2486612,-1.0375848,-1.3793521,-1.3597621,-0.65674233,-0.20469081,-0.26013255,0.08634608,0.005797732,-0.32034594,-0.8857716,-0.28574443,-0.9084873,-0.22827893,-0.17580025,-0.1570534,-0.005720375,-0.25996128,-0.27037635,-0.5096986,-0.72646964,-0.30450848,-0.61876845,-1.266599,-1.6042715,-1.1819909,-1.1369592,-0.766068,-0.30492288,-0.9225822,-0.46770835,-0.7201988,-0.3736942,-0.62819564,-0.73299205,-0.6110225,-0.49414974,-0.41905427,-0.4862777,-0.39695147,-1.074319,-4.432281,6.3919744,-5.4866915,-9.15432,-9.500795,-10.345997,-4.8970633,0.5311322,-5.61447,-5.5387177,-5.5513735,-11.231091,-5.8318496,-5.8110967,-5.5344973,-11.217676,-5.9406166,-5.997606,-5.1962256,-11.193169,-3.913463,-3.508447,-5.740096,-5.9797854,-3.6847415,-3.793943,-4.8921857,-2.549723,-4.6125603,-4.82185,-4.3043118,-9.222018,-9.23529,-9.216193,-8.039283,-6.0673175,-5.667302,-5.549252,-5.1636286,-11.203069,-9.687626,-10.017544,-3.7326708,-9.584013,-8.437035,-6.213267,-6.167005,-5.7535315,-1.5236127,-7.23188,-8.7177305,1.9145694,3.7401776,1.930513,0.49170136,-8.659691,-0.04209825,0.8294928,0.7809716,4.7095675,-6.4885306,-6.458113,-7.070638,-10.110089,-3.8328178,-1.4948056,-3.3428018,1.0514952,6.5328336,7.371197,7.4072413,7.560247,7.4224257,4.186779,0.23362407,-1.2908608,0.04786195,-0.5479856,-0.522072,0.18276884,0.35977444,0.11324444,3.3736591,-5.30209,-0.5089211,-0.49659485,-0.56739306,-1.0229902,-1.2944797,-2.2021463,-2.8825345,-0.49455047,0.29565898,-7.5366273,-7.5464797,-1.5551934,-6.122619,-4.1037436,-5.6647487,-5.6386814,-5.605843,-10.678257,-11.152676,-8.091174,-8.201949,-1.7909893,-9.678392,-9.790142,-9.197451,5.5590415,-5.0962124,2.4842246,0.3195451,-0.19008134,-2.9096107,-0.8296093,-0.3778436,-0.23372059,0.066698335,0.2706508,-0.12501882,0.0001880208,-1.6905785,-5.889646,-6.1134152,-4.408835,-3.1704485,-3.8996038,-6.009703,-6.429291,-10.600752,-7.970109,-7.929182,-7.7341537,-7.465751,-7.5659556,-7.407897,-2.466874,-7.3663926,-7.73648,-3.8277102,-2.766169,-2.930251,-3.2094827,-6.802175,-0.85359937,-0.7318378,-0.75035214,-5.641369,-7.1349416,-7.129296,2.6075034,3.122851,6.006188,-3.521339,-0.13361333,5.944354,-1.3438523,-1.559062,-1.8361413,1.2389106,-1.1958055,-0.33556482,-3.0178518,-2.328237,-1.5199653,-1.7271614,-5.8509803,-5.068594,-7.2015004,5.067643,3.7164586,2.4522338,2.3213437,0.6328543,2.8074787,2.5718424,2.688685,2.6127148,2.5546875,0.78705084,0.15979165,1.6057189,1.4292039,2.2917628,2.77387,2.5046313,2.3118625,2.4150412,2.580446,2.8111384,2.420449,2.314232,2.2919838,2.049397,1.6029247,2.2598903,2.0049744,2.3571715,2.499435,4.5595646,3.1268926,3.0444598,-9.637,-9.725253,-0.25510558,-7.983935,-5.2366214,-6.1431203,-6.0759516,-6.2178984,-6.6747813,-6.0942,-6.084681,-6.1241536,-6.219455,-1.7380291,-5.8616376,-6.1912684,-6.372985,-5.638453,-6.1120257,-5.156332,-6.146326,-5.811703,-6.058247,-5.964334,-5.9192343,-6.0938954,-6.1252136,-6.147642,-5.198963,-3.884554,8.076852,-6.074777,-5.1349235,6.807682,6.561336,6.5837035,7.0312414,7.823953,7.665746,7.70976,7.77356,7.8068376,-2.4162426,-2.2509987,-5.914384,-5.440938,2.7553713,7.173391,7.630841,-5.6430073,-5.663801,-5.732809,-5.3908834,-5.4785147,-5.996223,-5.644142,-5.572459,-5.538935,-5.7545238,-5.6585655,-5.541534,-5.3621597,-5.5989833,-5.5768447,-5.842432,-5.700027,-5.371475,-5.6596494,-5.573097,-5.370842,-5.338042,-5.569001,-5.5301733,-5.38545,6.585939,7.706556,7.9075994,-6.557276,-8.698795,-9.313982,-9.101163,-10.689128,-9.674674,-9.855342,-10.239906,-5.46793,6.157219,-7.164292,-7.298943,-7.1004558,3.131819,-2.7699633,-0.50073934,-1.6824615,-1.4612026,1.1330248,-1.1773417,-0.32562795,-2.418315,-3.0806112,-3.0928378,-7.7246404,-7.565979,-5.076933,-2.7675884,2.3539212,0.03609006,-8.257597,-7.4228063,-7.505185,-10.977826,-7.4628835,-7.3061895,-7.1424394,-10.219737,-8.77813,-8.788053,8.158813,-8.831441,-5.990525,-5.859658,-2.8095574,-10.6086445,3.9996169,-5.5910006,-6.6068597,-10.650023,-5.727352,-5.98386,-3.0239089,-1.3595872,-4.747426,-7.9164863,-7.892452,-7.942462,-7.9045024,-8.412273,-8.1703615,-10.768106,-10.69529,-4.114244,0.5900836,-5.4102917,-5.6228094,-11.159796,-5.4257035,5.2030993,2.8574715,3.209304,-1.5982087,0.16362405,0.1785694,-0.22290336,1.1828196,-0.0044033956,0.309373,-0.3390842,-2.8376782,-2.5461621,-5.808621,4.7586207,5.34592,3.2892256,4.3733363,1.0862707,1.3524086,2.4833906,2.1153407,-6.1969686,2.1019197,0.77024984,-8.598883,-8.723396,-10.662213,-5.7041883,-4.8824997,-5.521091,-5.4862194,-4.0063047,6.740206,-7.9326887,-5.127526,-2.0244112,-7.413068,-8.674828,-6.431012,-7.516308,-6.8990417,-2.5966449,-3.1070585,-1.7152084,-4.651579,-7.080006,-4.150099,0.4382383,-5.7419186,-4.5179133,-3.086488,-0.92435515,-3.1950097,-2.892851,-3.2590063,-0.17561167,2.0296366,2.6651802,2.2877502,3.0940354,0.4761284,-7.172688,-7.2955513,-6.9731207,-3.571091,-5.6971483,-3.5365717,-6.7861543,0.95483875,-7.2502556,-7.3632326,-7.368538,-7.2075906,0.8576603,-1.0689971,-1.2753832,-6.4257407,0.024500119,-7.3137407,-7.3603063,-7.273719,-7.8757105,-7.9153233,-8.365997,-4.8460717,-3.7932365,-4.6859345,-4.2178555,-4.2240624,-5.3745728,-3.700969,-4.0247846,-1.1460578,-2.9773726,-0.7842919,-3.1540427,1.5493615,-3.487226,-2.8357253,-0.94578207,-1.4336337,-1.1935251,0.98459005,-3.097298,-0.7839228,-0.57182175,-0.32213327,0.21712306,-2.9735966,-0.844339,-0.6358716,-1.3497255,-1.2181355,-2.6730995,0.11073563,-0.1082334,-2.8856878,-0.193047,-3.0022094,-11.209418,-5.246586,6.4727874,-4.983164,-7.2381682,-7.2709193,3.586901,-2.9387946,-1.341168,-1.5074121,-1.0993937,-1.0406415,0.19598034,1.1746433,-1.7228028,-0.024148434,0.54718536,-1.1463587,-0.7704069,-4.19889,-3.5871477,-3.239124,-2.172446,0.4002611,-2.8987067,0.3401322,0.049808316,-5.917722,-5.762554,-10.881854,-8.661009,-9.187077,-8.538161,0.6262755,-7.948674,-7.883292,-7.7487793,-9.381171,-9.367928,0.7052383,-9.682842,-9.75352,1.2754245,-9.012658,1.6498317,1.0354506,1.6903557,1.6677058,3.7174296,1.1093704,0.5965902,0.799361,0.71388084,0.2736902,0.26515812,0.2480715,-0.561563,0.8832008,0.11247655,-0.79832625,0.30916813,0.7007761,0.5275561,1.0606946,-8.54703,-8.611081,-7.7301736,-0.09220478,-10.35247,-2.667128,-1.2154663,-1.4211334,-0.43740854,-1.8486463,-1.3092473,-3.8713307,-4.2511435,-5.0226097,-0.4429372,0.06578272,-1.3293563,-4.0604715,-5.2895913,-0.5842415,-0.060959015,-0.14905143,0.083342224,-0.86725545,0.67066175,0.6548622,0.65630686,0.9694292,-4.779178,1.0429918,0.9781388,1.0838873,1.1866995,-2.0202193,2.3578212,2.5950122,11.322031,1.130716,-3.0078804,-2.1014223,-1.4723607,-1.0724754,-7.3237095,-0.23214301,-0.68750536,-1.2956017,-7.424204,-1.0699624,-1.7893661,-1.3281723,-7.5557213,-0.24187765,-2.4163113,-1.5705103,-7.4581203,-1.0756794,-2.2635145,-1.480263,-6.2463765,-6.184764,-0.8076257,-5.6345973,6.747702,-5.7024345,-4.972135,-5.182281,-10.42756,-11.108241,-5.43394,-10.81237,-2.7638118,-3.1884158,2.618907,3.3963122,2.4559486,1.9857955,4.368789,4.8163114,-4.7332363,-2.4815989,-1.5943439,-3.8739688,-11.311584,-4.778802,-5.540267,-9.513505,-1.5329988,2.9202175,1.2102891,1.5606217,1.2334503,1.3750626,0.3332471,0.66770357,1.5389081,0.89270866,0.48495996,5.754083,0.55122197,0.5600853,-0.3388532,0.87030935,0.9740336,2.1130207,2.7610574,0.35808104,-0.14839391,0.35850182,-0.24311346,0.16272281,1.2184027,-7.6431756,-7.6091313,-7.603833,-7.7046547,-7.5321474,-7.496369,-7.2698054,2.521662,2.305284,-7.1207337,-7.3841505,2.9805796,-7.3198137,0.08019225,0.75169986,2.9186277,3.6511693,-6.967635,-7.0541587,-7.471096,-7.134433,-7.2042847,-0.7275476,-7.206781,-6.8580294,-7.67333,-7.7367992,-7.64167,-5.6132536,-4.382299,-9.423785,-9.210618,-9.524443,0.22688498,-9.5519,-6.644119,-6.807644,-6.482359,-6.7576537,-6.6020517,-6.77226,-6.8009734,-6.344496,-6.5580606,-1.4114377,-2.9094374,3.1894815,2.8262002,3.5415082,2.8869483,2.0712676,2.8166358,2.9399984,2.7712152,2.7604604,2.3305626,-1.2883385,-7.0226984,-6.405507,-4.8760805,-1.1529962,-0.5969597,-0.69973683,-1.3771588,-0.9597355,-1.2641566,-1.1789439,-0.71495605,-1.1646739,-1.2013558,-1.231145,-0.8660822,-1.2173408,-1.272569,-0.4113795,-1.3603956,-6.5773263,6.415186,-6.4354463,-6.1477947,-6.554427,-6.343877,-5.274728,-6.27418,-6.319352,-6.0712886,-6.2642727,-6.4006424,-6.2649016,-5.543163,4.900233,4.4228377,4.2783065,-2.785611,-2.7049396,2.5528517,4.4210696,4.8822246,3.8172264,3.2017014,-5.3392115,-7.3935847,4.4020452,-6.2767124,-6.378366,-8.69832,-8.758563,-6.9633107,-10.511565,-5.5154476,1.4001118,-9.33105,-9.667194,-9.061567,5.7385244,-2.7647135,-10.6111,-9.351544,-8.791751,-2.737441,-8.894062,9.346961,9.325629,9.391103,-8.998237,-3.9056754,-7.0317874,-0.62243205,4.9762325,-2.2845948,-4.381931,7.1827564,1.5786824,-0.92138773,-1.3700525,-3.9477913,-7.070724,-3.120337,-1.7108259,1.6183925,-2.069717,-1.3963131,-2.1082354,-1.447107,-2.2655683,-2.2922938,-2.0704472,-3.7034438,-3.4435189,-3.7264607,-3.7619326,-3.6308856,-3.5187297,-3.7574263,-3.2821422,-3.113271,-3.4311438,-3.3551412,-3.5865633,-2.8028758,-3.5502148,-4.4717855,-5.8617344,-5.8594294,-10.737101,-11.305129,-8.094908,-2.614156,-6.418807,0.21696743,-1.000008,-2.8246765,-1.3765029,-4.6853657,-1.5829103,-4.7941117,6.546215,-5.3098016,5.665192,-3.5496237,-4.4846025,-4.67279,-4.7325954,-3.703513,-3.0431972,-4.7504544,0.2790145,-3.5794194,-6.161903,-6.6958356,-5.823465,-4.9011135,6.2724085,-6.415687,-5.6312723,-3.5316684,-2.332838,-2.6728654,-1.2608758,-5.512395,-6.8439336,-7.216395,-7.081229,-10.834086,-7.591338,-7.572031,3.0571296,-7.2554007,-7.4189234,-7.2898526,-1.4532121,-0.0053591635,-0.3464991,1.1129148,-0.035652,-7.1496925,-7.2556405,-7.4712696,-5.630488,-11.190946,-11.367623,-7.8558216,-7.739293,-5.2613053,-5.553087,-0.71100104,-5.596049,-3.4368794,-2.182005,-2.0753932,-2.5994077,2.4891808,-3.0738037,-2.2994406,-1.389384,-1.4155291,-2.2428067,-2.7673268,-2.2090526,-1.6529886,-5.4386196,-2.0639176,-1.5003561,-7.8835654,-1.582451,0.58470935,-9.039009,-2.2182758,-1.4110073,0.63574165,-9.192711,0.49111527,-1.5876458,2.740383,-5.7865868,-5.4242783,-4.751389,-5.3491926,-5.6469245,-0.27690688,0.5997121,-6.7601557,-6.913543,-7.248738,-1.112624,-5.5463924,-8.436492,-9.721152,-0.93327236,-1.1859581,-0.07177665,-0.41336665,-0.4155943,-10.3572445,-10.529507,-9.427921,-10.709088,-5.2809153,-5.3064346,-11.292151,-3.4747045,-5.433638,-5.6841307,-10.688312,8.095572,7.703794,6.7732778,6.659395,7.1186566,6.971728,14.810333,14.808684,14.80836,14.809822,14.809693,14.79915,14.809284,14.810497,14.811167,14.812172,14.8103895,14.81172,14.8122015,14.803899,7.0977902,7.9538646,8.011936,-3.632257,7.8005238,7.9607515,-1.631606,7.864033,7.336667,7.8391724,-5.667596,-5.762309,-5.7000556,-5.6773653,-5.571158,-5.2931657,8.0309725,-6.759201,-5.673499,-5.940867,-2.3078008,-1.9620678,-2.7708333,-1.0771282,-2.012706,4.5162425,-5.9512615,-2.039748,-3.9440987,7.4372077,6.8823133,8.1749325,8.012459,6.835826,6.606894,7.258816,6.9800134,7.799588,7.9941325,-2.360753,8.303841,8.207094,2.7604938,7.2372866,8.034263,-5.963891,-5.6549754,-5.7281823,-5.7048206,-5.6574445,-5.611335,-5.9859056,-5.756704,-6.2771215,-5.427463,-5.942446,-5.619853,-5.553039,-5.5366435,-5.301872,8.244878,-6.6989408,-8.180791,-8.247891,1.1857675,1.6363182,-5.7845774,-4.075766,-11.180147,-0.7428982,0.024245912,-7.497738,-7.5376897,-9.19959,-9.332108,-8.146385,-5.6095614,-5.6681714,-6.807614,-5.777235,-5.4357047,-5.035762,-10.627196,-5.632926,12.113964,-3.8364851,-2.027855,0.108616605,-0.96893924,-0.21313748,2.8046787,0.12531205,-2.1183395,-7.774852,-7.5676274,-6.8900633,-7.457337,-7.609296,-7.6823196,-7.193777,-7.62198,-7.38621,-6.6299043,-0.6840034,6.6779838,-2.7552004,-6.766928,-6.728244,6.1430273,-6.9836855,-7.2340665,-6.8533535,-6.8431125,-6.7744865,-6.873014,-5.294898,-2.8406572,-6.482385,-7.515427,-7.136457,3.0965626,-7.265179,-7.182011,-0.52230704,-7.3961973,-7.4847674,-7.41027,-7.5420938,-7.5205607,4.4552627,-7.1625547,-7.8430834,-7.4776497,-7.733806,0.55534637,-7.5849605,-7.736267,-1.4088094,19.540535,19.531412,19.58052,19.575346,19.590853,19.519114,-0.16728675,-6.6682487,-9.453469,-9.594441,-9.701463,2.7818396,-5.2019014,-3.052186,0.901206,-4.2964115,-3.1888113,0.057642583,-4.8402796,-2.5504825,-2.804332,0.06434235,-4.8740244,-3.0138905,0.7122322,-2.1246903,-0.60282475,0.7744772,-0.24988085,0.4751916,-5.390929,-5.4790373,-10.7144785,-7.454929,-7.321751,-7.3932796,-7.2441664,-7.113161,2.1031814,-7.0363855,-7.014648,-3.6005936,-7.351217,2.5880752,2.4064405,2.69528,-7.4766865,4.0866466,4.2457337,-7.327849,-7.1368704,-0.3198451,4.3677216,-0.137429,-3.9582887,1.8498752,-7.393231,-7.237525,0.48159763,-0.38673988,-4.060417,-7.07199,-7.201455,-7.21238,-7.1441374,-6.25636,-6.619429,-7.4028444,-6.3317604,-7.560241,-5.646827,-2.132915,-4.948167,0.60805756,-0.42063323,0.38458887,2.3153982,2.822167,-9.34713,-9.492932,-8.7151575,-5.8374877,-5.746396,-4.819817,-2.268574,-2.8349638,-9.804036,-9.751721,-9.819426,-9.885346,-3.8141217,-2.3563986,1.3178087,-5.2096624,-4.680566,-0.36878407,-4.7817,0.02846857,-3.9851534,-0.6630089,-8.009024,-8.017436,5.6693983,5.896294,5.9676514,-1.623406,1.8807389,1.1570708,-0.6228842,-1.6480755,5.041285,-7.2030134,0.6145243,2.629112,-4.3135304,-6.196931,-4.295093,-3.917734,-2.4241748,-1.0429502,-0.6288117,-5.7140064,-4.5752606,-8.167271,1.1692342,-1.961144,-10.6052475,-6.3307858,-6.17955,-5.836756,-1.209505,-2.378991,-7.896938,-7.675468,-7.5087533,-7.4094877,-7.548443,-9.670797,-9.498861,-9.677943,-7.304527,-1.3904513,0.5840207,0.519772,-4.878723,-4.0148354,-4.7286625,-4.542074,-4.059108,-4.2732787,-4.6518946,-4.330602,-4.790573,-4.6987777,-4.871338,-4.764603,-4.73836,-4.884074,-4.704453,-4.784563,-4.7595954,-4.8280787,7.41666,8.066971,6.664884,7.6531143,6.809512,6.7087436,6.4902697,7.0442185,14.810979,14.803537,14.81061,14.810729,14.80846,14.802903,14.807773,14.809352,14.811851,14.811313,14.808202,14.811029,14.806585,14.807331,7.027141,7.6383343,7.594427,7.403352,7.754427,-2.624296,-1.9913503,7.3919473,7.7254586,2.9023664,7.1567545,7.8087397,-5.6882744,-5.555178,-6.26776,-5.80158,-5.7662296,-5.689576,-5.76024,-5.6195107,-5.5418367,-5.890804,-5.6819882,-5.679871,-5.9421415,-5.68738,-5.629536,-6.1711445,-5.6614327,-5.6216054,-5.845499,-5.9551725,-5.6900463,-5.7113514,-5.6603813,-5.719996,-5.715177,-5.7739043,-5.732939,-5.9470315,-5.3507013,-5.5405807,-5.672948,-6.306129,-5.7349777,-5.418111,-5.6685605,-5.6448684,-5.658457,-5.7030325,-5.6802278,-5.580914,-5.608973,-5.6490445,-5.6308517,-5.7271185,-5.6220274,-5.7713747,-5.605983,-5.4203577,-5.6696844,-5.750295,-6.2027903,-5.5802417,-5.483837,-5.3587165,-5.5973277,-5.544142,-5.9527774,-5.7133093,-5.6492763,-5.3641663,-5.580997,-5.6519427,-5.7909045,-5.5518804,-5.3855267,-5.3540645,-5.6466384,-5.608399,-5.5378213,-5.3824806,6.401126,7.9474964,8.046548,-6.677891,5.1435776,-7.5395327,-7.356942,-1.983901,-2.4665954,-1.8833197,-7.296333,-7.1975155,-10.582248,-5.856401,-5.7536025,-10.576592,-11.244634,-8.478796,-8.7402525,-10.609693,-10.961473,-5.699589,-5.826993,-2.4234297,-5.61204,-5.627758,-2.2238402,-3.0532093,-11.252967,-3.303077,-3.2606688,-2.1621313,-2.1493688,-1.1204202,-3.260601,-2.9759653,-4.6454983,2.417357,-3.1796875,1.4928057,-0.48553312,-2.3908522,-1.2958941,4.877904,-2.7393062,-0.9594997,-0.45194122,3.3070292,-2.7732723,5.118449,-3.4881103,-2.4818406,-2.0359058,-0.4429375,-2.1724508,-0.8288184,-0.8816126,-0.71257436,-0.6484054,-1.269708,-0.6406274,-1.7667899,-2.0456932,-2.2605872,-2.6806118,-2.8348975,-2.6954017,-2.3027358,-2.3678746,-3.1693778,-2.45071,-3.2952197,-0.09473286,-0.31294158,0.22211705,0.36164445,-0.6549061,-7.096893,-7.502581,-7.5883,-0.7286336,-7.269378,-6.952576,0.846674,0.6625059,-3.1557086,0.7181983,-1.5041962,-5.8321877,-5.8774967,-9.541068,-9.4221,-9.589481,0.23905659,-9.567587,5.700565,-9.616904,-10.270708,-9.564836,-9.346215,-10.740113,-5.531832,-5.1573415,-3.4633098,-3.4619834,-5.383751,-9.098415,-9.202085,-9.414335,-9.201061,-8.795563,-10.59875,-8.019147,-8.006,-7.9149723,-7.6429043,-5.7225313,-1.8282813,-3.185306,-8.308739,-8.231833,-6.0026283,-4.4955153,-7.3534966,-1.2329847,-0.33827755,-1.9625906,-0.8861183,0.27371207,-9.800286,-1.3918978,-1.444507,-10.992666,-11.084051,-10.886255,-11.034666,-10.903357,-10.980751,-10.968012,-10.990041,-11.065584,-11.053567,-11.116152,-1.4231136,-0.8586517,-0.15794598,-8.340129,-6.18596,1.8660038,-1.8521464,-0.402242,-7.8562145,-7.8374515,-10.964526,-8.887613,-9.635031,-8.913795,-8.564231,-7.570665,-7.193617,-0.61067796,-7.061117,-6.7993464,-6.9062877,-4.6438055,-10.619061,6.2042537,4.955245,-0.42464343,-0.90196854,-0.2511223,-1.6650016,-1.4255472,-2.010497,-8.062903,-8.458527,-4.590485,-3.2361588,-4.9547014,-4.4100738,0.7304742,-2.3583698,-4.5940824,-4.3313055,-9.515607,-9.642008,-3.6314523,-9.794597,-8.415864,-10.875456,-9.619244,-9.804942,-9.755779,-0.90772504,-9.0847645,-10.840953,-4.8664584,-7.135195,-7.1720653,-7.131082,3.210978,-1.8766452,-1.9048451,-1.4890631,-0.43706664,1.2229912,-0.8419742,0.030951822,-3.8481474,-2.4378166,-2.379384,-7.7126293,-7.369918,-7.4528403,-1.5428265,-7.145769,-7.1520643,-7.0848985,-7.032739,-6.6346254,-7.092064,-7.1757526,-7.1738615,-2.1920047,-2.0729706,-0.7531266,-0.5807839,-2.2791936,-6.140627,-5.8355184,-10.396064,-8.9924755,-8.732098,-7.673632,-8.741827,0.6727278,-0.116383724,3.4747007,-0.631711,-3.6612413,-3.2824214,-3.3807042,1.2957399,3.9600651,2.352691,2.679446,-0.033748917,-7.701884,-7.116834,-7.0552855,-4.1988215,-7.8251534,-8.881181,-8.97626,-8.471391,-7.9395895,-8.212814,-8.996378,-5.156787,3.1312864,-0.09710073,-2.6850326,2.9010172,-1.7096163,-0.8048065,0.7125689,-0.24018557,-2.3023882,-3.0290275,1.1655821,0.11766302,-2.3269613,-0.20006336,2.4840837,-1.5795918,1.9215492,-5.347076,-7.733647,-1.1556811,-0.51536876,-0.65478754,-4.4625735,-1.2019788,-0.63122356,0.21278726,0.13051932,0.40089858,0.24000706,0.0067879115,1.0084511,1.2219155,0.863019,-0.12537935,-0.142739,0.08550253,0.86375046,0.4625384,-7.757621,-7.663797,0.4536309,-7.5624127,0.44974265,-1.9417735,0.5917258,-1.8363804,-5.026079,-0.9325828,3.9845748,7.3902316,-5.1889606,-5.4210324,-5.428712,-5.3725066,-6.482867,-5.137438,-5.275555,-5.3206906,-5.513328,-6.221645,-5.3425207,-5.48888,-4.978965,-5.570161,-4.089177,-5.5132794,-5.051959,-5.031516,-7.3978186,5.7787104,-9.551186,-8.675938,4.6877522,-5.154723,-9.217123,-5.3601723,-6.9609303,-3.671484,-1.3944107,-1.9089283,-7.2549634,-7.588368,-7.3457217,-7.369841,-7.248675,-7.544493,-7.325196,-7.3695498,-7.565316,-7.2789946,-7.4078646,-7.3349648,-7.44026,-7.5364017,19.508833,-2.6185284,-2.4041371,-1.8862228,-1.9127392,-1.5750327,-2.0864058,-1.175866,-6.231276,-7.531658,-7.9580956,-7.9458785,-9.436203,-8.958104,-9.6089525,-9.459036,-9.562584,-9.763843,-9.904986,-5.8404713,-5.0350437,-5.6488447,-5.307333,-5.8890347,-5.190612,-5.649583,-5.505618,-11.046864,-2.7850845,-1.2352663,-9.500484,-9.569164,-9.050078,-8.91035,-9.529714,-8.981373,-0.047259375,-0.9227772,-0.6730983,-1.0922602,2.8845932,-9.701121,-0.35810703,-9.499392,-9.941757,-3.1550524,-0.7771416,-0.47498047,0.33480617,0.24812248,-0.46529275,-0.04543363,-0.11216107,-0.085311115,2.3352568,2.1298912,-5.253783,-5.265828,-10.68472,-4.9512777,2.4987347,3.3657799,4.9550977,7.0221553,7.0847907,6.9776783,7.053284,7.049034,7.0645638,6.986162,7.013224,7.035054,7.0250583,7.0660157,6.9706783,6.9262104,-1.7105447,-1.3725342,0.056224566,1.0989097,0.103014216,0.084717065,0.32926992,-2.5713696,-2.722121,-2.1045017,-1.8218249,4.553153,-4.983098,-5.6604967,-1.3290395,-1.3240812,-5.552017,-4.3800917,-2.1706452,-3.3575342,-2.954493,-2.9563367,-2.8806036,0.45764467,-2.6576593,-0.090802535,2.2327697,2.5917845,3.3088005,-1.143314,-5.9013824,-5.6592274,-2.6029007,-3.828742,-5.5031977,-1.992754,-3.9832542,-3.6645536,-5.533289,-11.276822,-4.7060485,-5.980199,-7.0992064,-7.431298,-7.5297766,-7.370808,-7.5839505,-7.6186147,0.47398075,-1.3360099,-4.674508,-2.5659769,-3.2324476,-8.149983,2.665962,2.2783678,-4.80347,-1.8365567,-2.5640814,-2.127501,-0.73789495,-2.1109335,0.20362741,-1.0155624,-1.3483224,-0.7252395,-0.31658387,-0.4791549,0.25430098,-0.06645905,0.0025263862,-6.0035195,-5.4713902,-4.062934,6.8291383,-4.982693,-5.291129,-4.726644,-4.630266,-11.1271515,-11.205214,-4.506527,-4.1079288,0.2625164,-1.0784848,5.437791,0.32896158,0.17651011,1.6225785,-9.609335,5.4472938,-0.26225644,0.3273401,2.260742,0.09352648,-0.09678843,-0.538473,-0.36904603,-0.36431906,-0.34962228,-0.5130082,-0.19983079,-0.036114607,0.09305709,-0.42277068,-4.3282905,-3.0704281,-3.0810053,0.37464142,-2.1506128,6.7729363,-5.959068,-5.4186683,-5.5362353,-5.0676446,-1.790052,-2.6965854,-2.1102545,-1.9936926,-1.4712598,-3.2209005,-2.482205,-3.204724,-2.5206048,-4.3101068,-3.2957618,-4.8872223,-5.5381393,-4.7582335,-4.012638,-11.286649,-7.57805,-7.4384785,-2.1760798,-3.4550972,-1.9926414,-2.508569,-1.049989,-3.8686917,1.2527001,0.78112996,0.8882012,-2.767014,-6.1555104,-5.7006183,-5.4266925,-10.741311,-11.149262,-7.4822526,-7.0024056,2.523451,0.26462924,-1.6512492,0.83903915,-0.41258088,-1.9299333,5.333306,-0.67583996,6.0933614,5.8606477,5.9459996,5.7573204,6.0439057,6.141101,6.139075,-2.7984164,-1.900786,-1.9293439,1.5673757,-0.3227097,-4.363566,1.115176,1.3881772,-0.61179215,-5.6130595,-4.987638,-5.533995,-5.2234087,-5.0910745,-4.2600603,-4.902128,-4.0890484,-2.9726295,-11.134536,-11.346698,-4.6750603,-7.2066784,-7.07985,-7.20385,2.9082968,3.6800437,-2.2611098,-2.8622563,-2.143245,-1.4728012,-0.53491706,0.20449013,0.058368847,-0.6067486,1.2797626,-1.1442858,-0.025102364,0.46234855,-2.3737664,-0.88661736,-0.69849527,-2.086595,-5.4909177,-3.1856966,3.566804,-0.26974782,6.471098,-3.4548397,-8.093544,-7.9024715,-7.6445017,5.407292,-2.3832831,-0.2168827,-1.1866863,-0.03956768,-0.73338306,-8.084377,-8.198282,-0.6488693,-7.9878044,-7.1510787,-9.43695,-9.640196,-9.1169405,-8.881558,-0.9291244,-8.509301,-8.709976,-9.699039,-7.677747,-7.386082,-9.658247,0.9076644,0.89254385,0.40866855,1.5229107,-3.6546648,-5.1661015,-5.56894,-4.8057156,-4.003045,-4.240686,-2.0189228,-5.1066675,-8.864351,-10.345686,-6.8444858,2.948336,0.6406711,0.099682316,-0.4590584,0.77925074,0.14598347,-0.3585387,0.5241638,0.2754473,0.34497413,-0.05193138,0.34775573,-0.5827919,0.26403266,0.48765168,-0.38659978,-7.672539,-7.0753627,-7.1191406,4.4658165,-10.4815645,-11.248053,-1.619405,-0.5140205,-2.086966,-2.2510571,6.5579953,-4.936639,-5.2689657,-5.554568,-3.5333128,-1.4523985,-1.3074772,-10.513596,-10.639927,-10.756552,-10.738926,-10.785743,-5.6755404,-5.79921,-4.1712127,-4.1113687,-4.7250185,-4.0437703,-11.001942,-7.82147,-7.6830444,-7.478129,-10.396364,-1.0240241,-0.6959557,0.5753414,2.1662219,-8.640328,-8.538188,-4.416571,-1.1239303,-10.742279,-8.372982,-7.1601486,-10.980342,-9.519318,-9.624431,0.5276671,-6.3671722,-8.980471,-8.984402,-8.917835,-6.150682,-9.625375,-8.537536,-8.515038,-4.19587,0.67511183,-0.3619404,-0.16725686,-0.3468165,0.14079829,0.33285302,0.09716565,-4.444818,1.2250173,2.4556053,2.21772,1.4265575,2.380498,1.3552735,-2.2775087,-2.722296,-7.882148,-7.7659235,4.399381,-5.041499,-1.7296662,-9.40158,-9.697952,-7.783554,-8.646062,-8.713894,-0.3257789,-7.917491,-5.2169533,-3.2512345,-2.0419638,-1.3685262,1.5173036,-7.0899158,-3.231074,-2.8294094,-2.451769,-2.0347733,-2.4422123,-2.240095,-1.7627708,-1.5204612,-1.7832351,-0.9093616,-7.4419637,-7.3810773,-7.409442,-7.4369636,0.69351053,1.2271962,0.89548326,0.755008,0.9411636,-7.114945,-7.446092,-7.465638,-5.0344315,-4.521403,-3.315099,-5.047063,-1.7930266,-3.4223318,-2.617318,-3.489929,-3.0310278,-1.689768,-2.1615448,-4.689398,-2.6546848,-5.1910357,-4.334326,-1.7834958,-4.127818,-1.6111821,-2.1628087,-2.1844413,-2.1823616,-1.9284955,-4.3664346,-3.974682,-5.3349843,-4.7626433,-5.4660835,-7.1158338,-8.379595,1.0103772,0.424811,1.0519701,2.6759918,-3.4131885,-0.267496,-3.469231,1.1736205,0.7870377,0.8467414,1.2154789,1.501984,-2.2066834,-0.77588904,-0.90539765,-0.8901309,-0.8797337,-1.2507147,-0.19240099,-0.52237725,-9.700116,-9.747219,-9.862608,-10.114893,-9.590815,-9.7790575,-10.615795,-8.124372,-8.397579,-6.703337,-4.0341206,-4.015151,-4.134687,-11.267905,-6.221141,-5.7518497,-10.462124,-5.7923613,-2.6040134,6.679842,3.5029042,-0.6540981,-0.7371235,0.21739317,0.5966641,1.4129381,-0.53513885,-1.029438,-0.61284804,-3.834114,-1.9354761,1.8065126,0.44209507,2.4255426,3.1506162,-7.588793,1.4523375,1.2030662,0.48824924,0.092776515,0.52719647,0.7731403,-7.2306056,-7.015146,-9.491414,-9.47465,-9.986127,6.793444,-5.706211,-5.306203,-5.478255,-11.252269,-11.374999,-8.617758,-8.69083,-0.2776846,-10.4177685,-8.579888,-8.5035,-5.2721105,-2.9677026,-2.6139684,-0.16267547,-0.9480394,-1.3985262,-1.5128936,-0.36598274,-2.659374,-0.98871624,-7.435682,-8.484873,0.6167603,-0.3008536,4.5845156,-8.096668,-8.000474,-8.147045,2.213675,6.3504205,-8.461551,-1.219651,1.8491695,-8.684784,-5.736037,-2.8751247,-0.98799413,-0.9274859,-0.8164177,-1.4843034,-0.58891904,-0.6788636,-10.583894,-4.4261456,0.039881,0.2616226,-5.383298,-5.794306,-10.743621,-5.6472325,-5.599611,-1.0771704,-3.0662353,-7.7690353,-7.93223,-5.082611,-4.576719,-5.7023983,-7.1708694,-7.628283,-4.4735875,-5.590415,-10.918224,-11.194641,-2.3069828,-3.519188,-2.919499,-8.471365,-8.542053,-9.736375,6.7721014,-5.019003,-4.785758,-5.3674216,-4.3626504,-4.196758,-4.6111503,-3.8890777,-3.5592906,-3.9050195,-1.5389812,-11.002015,-11.379373,-3.44701,-4.807298,-6.657231,-2.6158335,-2.0875657,-1.7327777,-1.6234623,-3.3048396,-3.2130702,-3.2415533,-7.6820474,-1.4749124,-1.4140877,-6.901955,-7.1236486,-7.2440085,-7.264656,-0.57044137,-6.878407,-5.4155188,-1.7227734,-1.7673154,-7.186129,-7.090532,-6.345246,-4.0926595,-5.806672,-7.045725,-6.3741617,-5.1443295,-10.865011,-7.816478,-8.055621,-0.28773636,-1.123211,-7.613137,0.5630236,-3.593682,-3.7052817,0.38518962,-4.4484754,-0.59303993,-0.025995642,-0.29404962,0.3098801,-7.0118623,-5.8087044,-5.97326,-3.1714346,-2.4148693,-5.5140004,4.0750813,1.8632673,1.5755438,0.21660061,0.6872064,0.76335627,0.32751662,0.6448769,-4.024736,-2.8800564,-3.4267402,-5.955785,-4.4353943,3.7882807,2.5223145,-3.3783214,-1.3288273,-1.4055085,-2.6701193,-2.066645,-3.245062,5.113261,5.136149,-2.7081885,-0.8768625,-2.5703127,-1.2446663,-0.5459777,-1.7120624,-1.5255214,-1.4833081,-2.6830866,-2.8727918,-2.5597138,-0.6914218,-2.675073,-5.333955,-5.5865345,-7.2686567,-1.3437706,-3.7389328,-4.5853953,-2.5824144,-2.071954,-5.343529,-4.6647897,-4.2781224,-10.683837,-7.95859,-7.835053,-7.5172024,-9.100012,-9.395618,-9.261392,-10.817851,-5.617231,-8.724269,1.0906916,-5.121648,-8.673412,-8.794787,-10.490895,-2.5626433,-0.77271926,-5.0663643,-11.27756,-10.674894,-10.518462,-9.473203,-11.022072,-5.6753283,-4.9719973,-11.302194,-3.899987,-2.6590028,-1.3689462,0.404867,-2.1778183,-0.043429893,-4.053983,-2.5905254,-0.23391268,-0.57453257,-4.659607,-7.8306684,-7.508897,3.217485,1.0022324,-7.4744053,-7.4805627,0.21800718,-0.50250053,1.2799901,-7.4222736,-7.393617,-0.7063025,-5.373474,-1.9087187,-3.1626582,-4.831938,-3.2468927,-3.1657262,-3.614357,-1.0403633,-0.67020994,-3.1451325,-2.4813595,-3.606047,-3.9429774,-3.3483698,-3.518291,-3.6709552,-3.0913773,-2.446145,-3.8492084,-2.1661415,-3.5416183,-4.2132015,0.34296188,0.56050265,-0.055780422,-1.2614965,-0.7542318,-0.86734015,-0.5541475,1.5046066,-4.5170617,1.4375337,-9.200325,-9.152006,-9.002615,-10.532995,-5.687601,-5.7286906,-4.9120283,-5.062799,-4.4924235,-3.4037523,-4.7080245,7.295865,-4.7017846,-4.10945,-4.271369,-4.356136,-10.54074,-11.241352,-11.387364,-0.89254254,-1.3205211,-0.25856978,-2.8575914,-2.2041068,-2.3968108,-3.737714,-4.063789,-2.1911867,-1.037757],\"xaxis\":\"x\",\"y\":[0.5096532,1.3506017,1.130752,-0.61940116,-1.5351205,-2.9834967,-2.9384034,-2.9268742,-0.06317416,0.94814706,-0.27517894,0.0034982143,-0.39689207,-3.5481184,-0.40384918,-0.37428448,-1.6376356,-2.3108678,-6.155354,-1.8762579,-2.078356,-2.345928,-2.4287913,-2.410245,-2.591227,-2.1308846,-4.016597,0.22970392,-0.7655864,-0.8278829,-1.1063516,-1.3841275,-1.2348391,-4.8346844,-5.0938253,-4.2491817,-4.361199,10.396718,10.388831,10.392155,10.396764,10.39542,10.371252,10.392494,10.390075,10.392955,10.390073,10.393682,10.400307,10.399703,10.39295,-3.5340261,-1.2529012,-7.4791136,-1.3814255,-1.4293429,-1.197597,-2.895645,-1.4093033,-5.2996364,-1.0950598,-2.5415184,-1.8744255,-3.9498968,-0.5859892,-1.9003247,-4.0671754,-3.674596,-1.8370504,-1.9590107,-1.5279924,1.1977749,1.3311808,-0.5502687,-1.174034,-1.1720722,21.659945,21.503954,21.536938,20.89055,21.08295,21.474363,21.1714,21.34714,20.603296,20.951765,21.462742,21.453302,21.165237,21.233364,21.425138,21.330656,21.249895,20.884657,20.590147,20.79906,21.271446,21.322718,20.756014,20.734211,20.956001,21.66239,21.31368,21.008703,21.08313,20.273058,21.174553,21.589869,21.27736,21.242079,21.188837,21.34168,21.366756,21.065123,21.335371,21.304672,20.970821,21.052149,21.434242,21.383224,21.574512,21.497631,22.132736,21.347267,21.205269,20.877584,21.339445,21.791077,21.897137,21.374771,21.336823,21.92399,21.946007,21.459862,21.500393,21.724762,22.053988,-1.0146086,-1.0709904,-1.2582437,0.48107502,0.60468674,-0.655464,-0.6703407,-0.5270438,0.26512763,0.10013689,-0.048075456,-0.8302089,-1.7119594,-0.88012904,-0.69147086,0.129198,-0.42187363,0.5939574,-0.79833156,-1.778735,-3.9117832,-3.8882873,-3.917112,-4.0372977,-3.88347,-0.6067559,-0.44612652,-1.9034877,-3.9487503,-1.2061727,-4.063868,-0.11823937,-0.05186138,-0.35301432,-0.53865665,-1.1304255,-0.4814293,-0.3201747,-0.35542437,-0.35188824,-0.3021677,-0.43755904,-0.6680715,0.009254818,-4.4508033,-4.3904967,0.17474672,-0.14975683,-0.14408383,0.2819189,-4.437966,-0.52066314,-0.6922547,-4.1458592,-0.38825607,-0.26087537,0.22052112,-3.7752106,1.5687612,-0.5420687,-0.39502636,-0.44139504,-0.23470126,-0.31304714,-0.4958574,-4.3948197,0.010752592,-0.6443197,0.8784474,-0.26889792,-0.13182445,-0.40361598,-0.34321705,-3.7346246,0.5132192,-0.7354375,-2.1075566,-2.2755866,-2.468524,-3.9337106,-1.0348957,2.2836952,0.44559547,-3.5888789,-0.8695179,-0.28266966,-0.5647241,-0.46067163,-0.36753848,-0.54214346,-2.1592367,-0.19631977,0.5781486,-0.71363986,-0.76929754,-1.2915555,-0.70582205,-0.68325555,-4.2605524,-0.5399703,-1.105755,-1.850339,-0.45429987,-0.21518165,-0.70149016,-1.2875253,-1.3051543,-0.8880235,-0.5166514,-0.9340013,-2.0530086,-4.1922827,-5.742192,-0.47847003,-0.39424044,-0.18357913,-0.533122,-0.7835303,-0.66278374,-2.5842764,-3.566926,-1.8288772,-1.4190778,-1.4448875,-0.4833752,0.86018324,-1.5773617,-3.9055805,1.5143272,-0.6547281,-0.5213725,-1.4525892,-4.2781134,-0.8665187,-1.0392226,-5.2755556,-4.5998073,-3.3193116,-1.7215635,-6.1288314,-5.956613,-4.2531033,-2.1066391,-2.5651085,-2.666576,-3.7609918,-3.7404947,-3.0694082,-0.4221404,7.9344406,-1.3745269,-4.03432,-3.6186817,-1.4446759,-0.8999912,-0.912278,-2.5014994,-3.7983441,-3.8397532,-3.7709777,-2.6763995,0.46299255,0.08863263,-5.3351283,0.42762393,-0.12182196,-1.8338795,-1.2341694,-1.7756801,-4.363084,-1.7865268,-0.5570245,-1.4263308,-2.1545393,-3.8425329,-1.4091452,-0.74587405,-2.9259396,-0.7982507,-1.2802466,-4.7489247,-5.1222878,-4.5509443,-4.4972854,10.397635,10.393256,10.391746,10.39626,10.393705,10.371429,10.392358,10.387844,10.391188,10.392284,10.3943615,10.401144,10.400587,10.386635,-3.7224576,-1.3262641,-1.4889724,-2.116875,-1.3134192,-1.7504898,1.2116885,-0.5402519,-1.2329546,21.507536,21.534636,21.465096,20.912697,21.020235,21.505526,21.248766,21.500578,20.93984,21.352638,21.04946,21.490723,21.45471,21.378489,21.343697,21.144482,21.31215,21.287354,20.600624,20.43901,21.079697,21.301348,20.940443,20.844446,20.578892,21.432291,21.596241,21.03277,21.0498,21.022488,20.222439,20.98553,21.759964,21.33026,21.379435,21.248613,20.932987,21.495972,21.365881,21.123331,21.24881,21.328894,21.534824,21.102991,20.994745,21.01415,21.42218,21.334475,21.54285,21.42549,21.896635,21.949572,21.257923,21.039318,21.203608,21.383562,21.961075,21.453583,21.380032,21.393675,21.919493,22.00307,21.361174,21.628113,21.87913,21.836313,-1.0268538,0.7741987,2.9752033,2.7004352,1.7288057,2.213705,1.4735748,0.68144375,-0.102263846,2.6396682,1.5967118,1.9173812,1.4767073,2.0335975,1.8907048,1.6078886,1.4546012,1.7417907,2.2062278,1.5622883,2.2227795,1.7264668,2.2404194,2.6546893,5.625999,2.2570112,3.417119,3.4347513,3.6375554,-5.8645635,-5.875372,-2.7707374,-6.8660636,-3.9181,-3.8434906,0.67544115,-1.5367126,-1.5629251,-1.7621107,-1.9701071,-1.8147286,-1.4725,-0.7962758,-3.84636,-3.9530017,-3.850437,-0.7188655,6.827829,-0.5950472,-0.7137325,0.5497558,0.79864496,7.3604918,-6.7289615,-7.0082583,-7.2990627,-7.194492,-7.2075996,-5.18583,-3.7129827,-0.41754398,-0.24691129,-2.219652,-1.1281002,0.22213171,0.11177628,0.26716816,-5.299299,-0.42328572,-0.44673246,0.84713393,0.35191828,-2.5177054,-2.7900615,-2.9472654,-2.648592,-2.4411368,-2.5075479,-2.5266328,-0.15832265,-1.2077463,-0.97578865,-4.055934,-3.991225,-3.9101124,-1.6268371,-1.9188192,-1.7211529,-0.67703724,-0.8506708,0.76233727,-5.9069233,-5.6093473,-4.985728,-6.1084185,-6.145011,-3.420665,-4.205819,3.01288,0.8007124,1.8740076,2.916286,-1.1911603,-1.0831207,-1.8304036,0.30290017,-0.7087507,-5.150304,-5.0536,-2.9422004,-2.9366817,-4.1140084,-2.9556842,-3.0691462,-2.9751248,-4.2790995,-2.9445283,-2.8742888,-2.4309535,-1.155649,-0.6346481,0.79299,-3.7410016,-0.52914464,-0.90963113,-3.021407,-2.714061,-1.6971223,-3.2747307,-2.7916675,-0.4859607,-2.1379545,-3.4830635,-1.1034999,-1.1524023,-1.0511286,0.6665102,1.5478712,0.9951036,0.07503805,0.9307177,-2.6603818,0.99396217,1.5523369,1.8652645,-3.7071893,0.672491,-0.2386904,0.82882726,-5.9787626,-6.2274323,-5.17351,-0.68296665,-0.15988892,-0.3303446,0.8413078,-1.080901,0.7786573,-1.3004365,2.664268,2.0296328,1.5341489,1.641293,1.3833485,-2.5406492,-6.1847777,-5.917757,-0.43688682,-1.4820232,-1.6117022,-2.4935362,-6.200911,-6.3034215,-7.066393,-7.528309,0.32671815,-0.3171007,-0.18399999,-0.24414074,-2.7753146,-2.6068008,-3.171104,-2.9877722,-3.21311,0.53504515,-1.4349223,1.8970116,-3.7298396,-0.77020395,-0.83955336,-0.844931,-0.57230616,-0.59151495,0.29834124,1.6934122,-3.0805342,-6.039729,-6.1214595,-6.50599,-6.258818,-4.1580195,-4.442136,-4.297625,-4.3110037,-4.2325006,-0.5985604,2.2772474,-0.32929382,-0.7850936,-0.21790233,1.9797838,2.209056,1.157878,2.2457905,0.15367208,-0.8522385,-0.9305482,-2.4082668,-0.55818737,-0.47983947,-1.5386957,-4.0802264,-3.629391,-3.8649435,-3.0655963,-3.8054285,0.43616882,-2.3420057,-0.5666276,-2.5324032,-0.62622756,0.21764891,2.004278,2.3461854,1.4863514,2.5081346,2.8052084,-3.8421013,-3.915458,-3.5830574,0.76079506,-0.8274491,-1.1488583,-1.2351209,-4.8511796,-5.188674,-4.600584,-4.375764,-3.8427126,-0.8568546,-2.570974,-2.2381468,-1.1937283,-4.435983,-0.70248646,-3.7399187,-2.0255952,-1.3318161,1.2836069,0.6064716,-0.52447474,-1.0887458,21.569105,21.573235,20.954784,21.3419,21.028645,21.38494,21.502,20.724573,21.010801,21.034399,21.350111,21.261572,21.48166,21.369976,21.359539,21.363222,20.83447,21.248165,20.855812,20.651505,20.904774,21.509403,21.424082,21.042677,21.021107,21.00835,20.293102,21.094095,21.815945,21.296368,21.259857,21.076822,21.114286,21.01286,21.404053,21.257338,21.149006,21.135595,21.223291,21.432297,21.363981,20.903261,20.961868,21.475983,21.2141,21.361685,21.39392,21.71117,21.93843,21.977346,21.142157,21.017153,21.23278,21.434921,21.933067,21.983067,21.400507,21.392975,21.389338,21.91106,21.864426,21.449259,21.572706,21.647678,22.035198,-3.7586432,-1.0254942,-1.0072523,-0.4342109,0.5679389,2.8846662,1.1246223,1.3512841,2.3125577,2.352358,0.8338668,0.4918719,0.6328779,0.108876444,2.2257483,-3.3832548,0.07004415,0.6379541,-0.26373962,-0.25196785,0.12648548,-3.1776772,2.2773645,-0.42523968,-3.7471936,-0.660047,-0.50544494,-1.5644408,0.7869342,0.07309404,-0.007098382,0.7516556,0.9222638,1.2981125,-2.0359542,-3.2606926,-2.2518249,-3.1891818,-2.188373,-4.867666,-3.183988,-0.5887336,-3.3836043,-2.3196356,-3.08044,-3.5353281,-4.1230316,5.8616486,-3.1790314,-2.227144,-3.2088966,0.12964341,-0.75592536,-0.39649728,0.33171764,-1.9122379,-5.2041516,-5.093939,-3.780353,-0.35029247,0.21155713,-2.7735715,-2.427714,-2.43336,-1.6536641,-2.6533856,-0.34055778,0.9426077,-0.35754663,0.51855654,21.664984,21.549534,21.836323,20.711258,21.384405,21.043858,21.518372,21.294407,21.302855,21.384727,21.27501,21.268507,21.141176,20.602158,21.047928,21.348557,20.803167,20.793358,20.708363,21.49681,21.453089,20.967436,20.928524,20.968693,20.181944,21.041044,21.914707,21.268274,21.360346,21.14245,20.838524,21.452133,21.256102,21.101423,21.170406,21.274132,21.516037,21.118942,20.978788,21.074915,21.405214,21.392237,21.54358,21.421383,22.09825,21.953236,21.142889,21.078997,21.221624,21.402826,22.039658,21.409264,21.241247,21.297888,22.090548,-0.97690123,0.6846225,-0.120202735,-1.7041525,-4.1247373,-2.891348,-1.548668,0.15357861,-4.8765216,-6.997128,2.4921668,-4.8934107,-1.2709687,2.7468345,0.8767752,0.3183849,2.8318539,1.942075,1.7345595,2.771411,1.8930945,2.1645916,2.4313605,2.3364391,1.9313534,2.978086,2.6460612,2.9650857,-2.1466038,-1.1448715,-1.2559592,-1.1883166,-1.2105775,0.95882344,-1.334998,-4.5023327,-4.3231554,-6.1625447,1.0271319,-0.6608243,-0.97507274,-0.17748353,-2.8727114,-3.2371278,-2.2365866,-2.3989727,-3.7012022,0.54942113,-2.1661723,-1.6866437,-0.19120187,-0.25723094,0.914854,-3.8121552,-0.5336036,1.8923901,0.1721719,0.04525811,1.4172873,0.23444718,0.9459366,-0.31292975,-0.83938354,-0.73620296,0.3974789,-2.8914106,-0.32131344,-0.50670683,-0.13583142,-0.97606784,-3.8017833,3.3469613,-0.52073544,-0.454016,-0.14886911,-3.3486974,-0.24585624,-1.6281765,-1.8843822,-1.9413986,-2.7928932,-1.5466844,-1.5406064,-1.7584163,-1.5270957,-3.4440806,-3.143767,-3.376577,-1.1741254,-2.099441,-3.6053169,-1.7068717,-3.5058746,-2.538833,-3.857995,-0.97402227,-4.003735,-1.57296,-4.363475,-4.5330477,1.3370814,2.241427,-3.9811378,-3.9810827,-3.9276235,-0.21356943,-3.8550704,-1.1341088,3.296358,2.7449684,2.8929267,2.455345,2.7233455,1.1234385,2.381027,2.2992034,1.942356,2.0267856,1.839827,2.4240556,1.0603223,-0.4790109,0.45481768,2.828032,3.0589838,1.9236155,2.2825758,2.2301111,1.602228,2.4929035,2.3925695,2.3000457,3.3141878,3.4051962,0.10628119,-4.031069,-4.723142,0.32973224,-2.3829603,-0.3307776,-1.5177007,-1.2437332,0.19407871,-2.1564896,-2.3673286,-2.4744675,-2.463957,-2.8596141,2.903647,-1.1606771,-2.3060784,-1.5312618,-3.6388292,-3.8423603,0.13442679,-0.040839177,-0.26462582,-0.33325562,-0.8838238,-1.3117456,-2.7959158,-1.9275196,0.044352643,-0.1830329,-0.76809996,-0.15553156,-1.0750427,-0.4281242,-0.31304246,-0.06032593,-0.46985292,0.41585824,-2.0970151,-0.89663,-0.698226,-0.77226806,-1.9124022,-2.3679862,-2.3960183,-6.036946,-0.45290712,-0.3500913,5.0804005,3.14398,2.4484315,2.5334733,3.1190786,2.948192,3.0238125,2.9492445,-0.755975,-1.0255028,0.5194624,-0.11479731,0.005825217,-0.6701649,2.1674445,-0.67609173,-6.755148,-6.2164097,-0.37594578,-3.03678,-0.1905399,-0.059162673,-0.027972803,-2.8761594,-3.3161871,-3.3415496,-5.4622483,-2.741195,-2.1655939,-1.9572961,-2.7684982,-0.6771774,-0.9943463,0.45615792,-0.65697163,2.6460073,0.80065733,-1.1241832,0.75379854,-1.6274648,-2.6072998,-2.2989628,-2.3437703,-3.6979256,-1.4808866,-0.071619555,-2.570525,-3.0166123,-3.4620175,-3.239172,-3.6887784,-3.0357966,-3.0115552,-2.4902236,-1.5020492,-2.2504961,-3.8659377,-3.9189224,0.6056733,-0.0842134,0.013798677,-2.556578,-0.36029485,2.1563742,-2.0342329,-2.0945878,1.1122442,-0.6878727,0.4690445,0.20542431,-0.25353932,-0.3164373,-0.3856217,-0.0049637463,-0.10544978,-3.5571945,-3.8375406,-0.18558714,-0.31699505,0.61731756,0.11254735,-0.008130216,-0.4511201,0.011942333,-0.16198792,-0.96853256,-2.340518,-0.7844738,-0.026616639,0.0016203934,-0.45177478,-0.8854811,-0.50929296,-0.5255886,-2.864294,-2.8206403,-1.4997479,-0.58870536,-6.4238834,2.4019854,1.5285219,1.8003901,0.63257843,1.3631431,-6.3720813,-6.284299,-4.4594464,-1.4663361,-1.671536,-3.2749007,-1.7620945,-3.5665915,-3.6885378,-3.1561613,-3.7780716,0.13469166,-2.124136,-2.609195,-1.8716317,-1.7073005,-2.2136166,-3.2693152,-3.3519409,-3.2818549,-3.1200595,-3.3191197,2.5406752,-3.6187465,-3.9657254,1.545418,0.9371897,1.2552165,-0.83592075,-3.7199104,-3.6927018,-3.3960762,-0.4669829,-1.4078416,-0.17191006,-2.0137432,-1.6098495,-0.7255392,-0.6575057,-0.33230984,-1.789937,-1.5239042,-1.1099643,2.1091895,-3.8837528,-1.1116836,-1.9503808,-1.2734787,-3.6056993,-1.8208672,-1.2309617,0.8699022,0.7185634,0.678092,-1.3210298,-1.6282465,-1.9741315,-1.9259887,-1.7378193,-1.5237929,-1.3401483,-3.8309686,-3.9315588,0.46446577,-0.6589877,0.19919352,-1.8543537,-1.0535752,-0.9639172,-0.8731409,-1.0043911,-0.99522316,0.023881558,0.9144243,0.021406248,-0.12681969,-3.0954978,-2.529028,-2.6258159,1.954222,1.7601236,1.8789291,2.198307,0.70549774,1.9891722,2.198732,1.6736078,-0.8135648,-0.38014024,0.008370707,-4.7749543,-5.241042,-4.322977,-3.785921,0.3799729,-0.9867975,-1.1370492,-2.3788862,-0.9207306,-1.8747497,-1.5061098,-0.18348606,0.14857443,1.2817421,-0.4105252,-0.967465,20.808031,20.749454,21.517393,20.268639,21.18702,20.810469,21.258371,21.530777,21.066528,21.129396,21.48706,21.208406,22.041449,-0.86738664,-0.9829417,0.5576407,-0.84759885,0.0576298,-3.1157913,3.075027,2.0480382,3.0406663,3.0851045,2.8866417,2.8421955,2.5378015,2.2620313,2.0524728,3.186157,2.9303582,2.4732006,3.0604298,2.5777543,2.9566002,2.9761488,2.996645,1.6404525,2.0750628,2.8419306,3.073398,2.721783,2.9619744,2.937072,3.0330722,2.8437157,2.667892,2.3596947,2.4508522,1.9951148,-1.171757,0.6584832,1.0435225,-0.34495658,-0.48188463,-0.71655804,-2.5627549,-2.254037,-1.6064564,-2.4619088,-0.7730757,-1.5483077,-0.28535065,-0.5750569,-0.75193447,-0.37538442,-0.40149182,-5.6625605,-5.062923,-1.2915996,-3.3249269,-6.481277,2.225916,-6.971267,-7.132844,-4.231266,2.3809361,-7.6202874,-5.504638,-5.2738557,-5.500506,-5.563028,-4.628073,-5.7678723,-6.2801485,-6.5943327,-6.274409,-5.8883424,-5.922709,-5.4607763,0.08726586,-6.0816126,-6.303977,-6.167472,-7.0316596,-7.0573764,-6.7611504,-4.041249,-3.8787527,-5.662887,-5.3598847,-0.54725516,-1.2781892,-4.0502725,-3.6497805,0.53545177,-0.28520384,-1.1444585,4.23426,3.373913,-0.37919095,-1.5539045,-1.0940751,-1.2847444,-0.79392874,-1.5191174,-1.2259997,-4.6807013,-5.148038,-2.3058164,-4.552267,10.396447,10.3908,10.391042,10.394869,10.393866,10.371289,10.391688,10.393291,10.393553,10.391598,10.39002,10.4022255,10.40004,10.396187,-3.888226,-1.2920251,-2.9750185,-1.5092677,-1.8352768,-1.1101236,-2.368751,-2.303487,-1.7526635,1.2158178,-1.0660049,-1.0232383,-1.157111,21.099787,21.403524,20.654425,20.526249,20.501883,20.7749,20.75611,20.601006,20.334135,21.232859,21.06178,20.47516,20.392822,20.728598,20.27901,20.329596,20.520536,20.439278,21.316195,20.278809,20.355701,21.091393,20.814732,21.017061,20.36538,-2.3950796,-2.2821374,20.668459,20.346436,20.664476,20.303774,20.264019,20.63615,20.552177,20.575294,20.656553,21.327034,21.041643,20.83509,20.623499,20.197603,20.618158,21.162628,20.528965,21.012938,20.480038,20.76047,20.61874,21.0277,21.01969,20.602327,21.046297,20.72432,20.913132,20.853527,21.423828,20.511719,20.561668,20.689035,21.013308,20.342144,20.594208,20.507511,21.203638,21.403374,20.7563,20.550938,20.862165,20.95056,20.46116,20.57997,21.073694,21.686243,20.287497,21.174147,20.667892,20.757736,21.02245,20.834003,20.806843,20.365084,20.701319,21.022366,20.634472,20.375269,20.175903,-2.4256425,-1.240367,-1.1797307,0.49714574,-1.4513532,-1.7642735,-3.8112407,-3.8600554,3.0850034,2.9315248,2.0400994,2.2109892,2.4410112,2.5669644,2.8151157,2.664405,2.743035,2.4275303,2.4182343,2.3808138,2.5235329,2.620622,3.0610397,2.5167332,2.9431682,2.917963,2.8248584,3.0707624,3.249969,3.1545177,3.046501,2.8772147,3.2418365,2.9337466,3.1263106,3.0868373,3.1547806,2.8593135,2.9985988,2.8441548,1.3736134,2.838157,3.066686,3.0720894,2.9986048,2.7730746,2.8076036,2.8568177,2.6605382,2.467277,2.7487135,1.9200794,2.840072,2.57101,2.7675295,2.5258422,1.5457462,0.5256554,0.9733441,2.207335,0.83093643,-0.14116585,2.3823404,2.8624368,3.0317194,2.4510813,3.030933,2.7851355,2.8731544,2.1914637,3.0024052,-0.92703867,2.5882688,2.7495756,0.13410178,2.882692,2.693931,2.464289,2.5882702,2.194353,1.8642715,3.1726923,-0.7998956,-2.146807,-2.5426524,-0.6609073,-0.74629575,-0.87540334,-1.2220849,0.6313606,-1.5822557,-1.4605907,-2.593108,-3.830524,-0.905593,-1.9533966,-1.6832433,-3.8682406,-1.3838289,-1.9079267,-1.7207189,-3.8548732,-5.5594716,-4.215201,-0.55182934,-1.7427303,-3.909573,-3.7001104,1.333797,0.290113,1.3687882,0.97384214,1.2598511,-0.7976456,-0.7943802,-0.7122389,0.76652795,20.57127,0.09378343,0.024077393,-0.8475354,-3.852976,-0.7384668,-0.84896016,1.2281884,-0.45562348,0.8482367,-3.7468047,-3.8719358,-3.514002,-2.1904664,1.2720916,0.25627974,-3.160579,-2.6121442,-3.8851123,1.1631447,0.36841354,2.534898,3.1893,-2.329222,0.0820243,-1.5540521,-2.3659568,-2.3197274,-3.9591439,0.19602415,-0.3067699,0.3817218,-0.2232269,-3.3075933,-1.1286778,-0.9680182,-0.8972776,-0.9939191,0.11472081,0.2556976,-1.7767358,1.0749857,1.9715158,1.802174,0.5970511,-3.3922222,0.7181942,-0.17146836,-0.95824695,-1.5332407,-1.0314779,-0.6678238,-0.605245,-1.3243395,-2.08312,-3.8664258,-2.082883,-1.9574841,-7.1041017,-6.756055,-1.8805718,-1.4490007,-2.7020407,-1.6435388,-1.6969415,-1.9520855,-3.8127844,-3.83567,-0.13837563,-0.14894184,-2.5076962,-0.7359361,-0.8535082,-0.009877188,-2.8704252,-1.0810643,-3.1044507,0.98186946,2.130146,1.8839933,0.39141804,1.9534672,-4.3990746,2.1154387,-3.628172,-3.9410996,0.62654656,2.207391,-1.3783367,-1.8525548,1.6823214,-3.6626844,-3.5067453,-1.4907125,-1.781123,-3.8632247,-6.554575,-6.633668,-7.02156,-7.3333244,-7.037971,-7.3755326,-2.1478977,-6.5473223,-6.578539,-1.5546051,-2.0975459,-0.7911101,-3.877843,-6.7778788,-1.0068296,-0.33491766,-0.5931925,-1.5609672,-1.0953262,-1.1645124,0.8672812,-1.760551,-5.178544,-3.1328433,-4.5394177,-5.2535415,-4.4997835,-4.1570945,-3.8951025,1.0669159,-0.24378899,-0.6303553,-1.3088297,-2.8463757,-3.2242503,-3.1196392,-1.268165,-0.6788625,-4.4658227,0.40328977,-0.1274857,-0.22382702,-0.17435844,0.4774238,-0.29469582,-0.20927992,-0.25284487,-0.18178017,-0.2708742,1.4392593,1.946943,0.5755128,0.5872206,-0.108377844,-0.72149765,-0.25709635,-0.22797953,-0.2747425,-0.46104664,-0.42396635,-0.40876985,-0.19471484,-0.20048712,0.32555875,-0.41883367,-1.3761411,-1.2266324,-0.5710138,-0.25867665,-0.44703603,-0.36907488,-0.6971622,-0.65547836,-0.6779842,-0.8421798,0.87196034,0.4453386,0.7889994,0.57268685,0.67905366,-0.4938236,0.72712886,0.5869258,0.71642536,0.7715077,6.7115817,0.580427,0.65636575,0.9984706,0.42905855,0.7491482,-0.5939201,0.6779869,1.3856138,0.723503,0.5585292,0.6373695,-0.061932746,0.7012887,0.7721921,-0.1934537,1.9597082,-0.818605,-0.24191768,-0.0018148639,-4.816759,-5.233851,-4.356846,-3.7781787,-1.1031289,-1.0349271,-1.0148388,-1.1179032,-1.000737,-2.0389655,-2.0961387,-0.15578042,0.054132733,1.3645849,-0.45351127,-1.042544,21.349403,21.259357,20.800392,20.95753,20.969673,20.539974,21.104288,21.052233,21.135199,21.31333,21.334776,21.467627,22.021074,21.268448,21.200583,21.080526,21.284632,22.01065,21.374342,21.354595,21.963968,22.084509,21.405668,21.573946,21.988914,-4.9222217,-0.8623188,-0.98374474,0.4752486,-0.42487127,-0.6731757,-0.2161737,-3.4355705,-0.8531976,-0.8900405,-0.6233524,-1.4256806,-2.8592172,-1.1871039,-1.07851,-1.1048687,-1.6891105,-3.622304,-4.367906,-4.357007,-4.0883174,1.0978123,-0.37029505,-0.7296624,-1.6383873,-3.0334952,-3.3595312,-6.006194,-6.032244,-1.1212223,-1.3365659,-0.15169972,2.0854716,-1.9410751,-2.373078,-2.1286364,-3.8267002,-5.993162,-6.610158,-7.292053,-4.1577973,-0.30967554,-0.43197936,-3.1597488,0.16884694,-1.3528327,-1.644838,-1.426026,-3.7363198,-0.044093575,-1.9791926,-2.3114364,-3.8887222,-0.7597152,-2.1964605,1.8738536,-2.5094147,-1.4747263,-5.928422,-5.8429093,-6.0604405,-5.696488,0.123099,-5.5339117,-3.4119427,-3.0732768,-2.4496896,2.6321125,0.59742177,0.9050908,-3.7547355,-1.6884317,-2.9145157,0.9813443,-1.6977353,-4.3210535,-5.9510055,-6.566949,-0.75346255,0.92740977,-0.1755487,-0.52743965,-0.6032055,-2.4837408,-3.1724358,-0.16474232,-2.9581807,-2.7419116,-0.48977274,0.45614198,-2.0624616,-2.064504,-1.399045,-1.6042135,-1.090064,-1.4041973,-2.1246254,-0.4415943,-0.3774398,-3.3236434,-0.45723975,-2.1813512,-2.4176204,-0.6794334,1.2977821,-1.9513425,-5.7987666,-2.1639326,-3.6133559,-5.1465783,-0.7385869,-2.852386,-2.44924,-3.378283,-3.3692546,-5.105532,-1.7261922,-2.9500291,-4.8364334,-2.222043,-0.908135,-3.0921566,-1.5676248,0.28092813,-4.43318,-4.6713815,-2.0022156,0.73045474,-0.5090715,-2.3782067,-3.1525817,-4.1001472,-2.082689,2.6983404,-6.119997,-6.7196507,-6.8886867,-5.434151,-6.424374,-5.4788265,-6.886318,-4.509398,-7.0035114,-6.997395,-6.94533,-6.8329453,-3.6750374,-3.691298,-3.4742167,-6.4159894,0.11100463,-6.594616,-6.9345117,-6.7128534,-6.0526195,-6.1016603,-5.47432,-2.1127555,-2.5069854,-2.073092,-2.406835,-2.2390444,-2.1305122,-2.4403124,-1.9269103,-0.6906708,-2.2684886,-0.6905654,-2.4473536,-2.721044,-2.6565742,-2.8982837,-4.101756,-3.5561113,-4.0430746,-4.3822217,-2.9994411,-3.6120126,-3.8842468,-4.1393065,-3.9615672,-2.8458865,-1.6359906,-1.2733474,-0.9659044,-0.96433,-2.7229068,-4.428145,-4.7964454,-2.942638,-4.3847647,-2.8050916,-3.8245516,-0.4193281,-2.3146136,-0.98423815,-1.1518533,-1.1937908,-1.2450029,-4.4108367,-4.529536,-4.1368427,-3.66396,-3.8350234,-6.1614013,1.0181113,-0.48429355,-1.1201149,-0.20352072,-0.5161615,-3.0628414,-0.72914535,-0.3522354,0.12053328,0.47469163,1.7064141,-0.69320375,2.1037877,2.2162569,-1.4761401,-1.6666418,-3.6946776,-0.58436435,-0.6390879,0.85790527,-1.989765,-2.2408655,-1.9854552,-2.7305655,0.006943332,0.08627981,-2.6395118,-0.18659438,-0.21190086,-3.083589,0.14566565,-3.1734893,-3.8302894,-3.0096962,-3.1237185,-2.604681,-2.7757375,-2.6415358,-2.3480604,-2.608676,-6.1124253,-6.25666,-6.374652,-0.9739488,1.0274602,-0.11449016,-1.9133657,-0.5510748,-1.8349853,-2.1681802,-2.4191835,-0.37363738,-0.3696062,-0.6619089,-1.9836549,-3.9387627,-1.9782236,-2.6411219,-0.4417955,-3.567153,-2.0737154,-2.9846206,-0.22756426,1.7584658,1.0167737,0.094118945,0.46157828,-0.98407054,1.871245,0.3149294,-2.8428106,-2.9651966,-2.779349,-2.878074,-3.0422318,-2.404707,-2.5285227,-2.4766724,-2.2984908,-4.182168,-2.1007254,-2.1294193,-2.140302,-1.8699028,-2.2313135,0.24076943,-1.5995466,-4.956235,-1.9298913,2.0513415,0.05663887,0.053240597,1.4067767,-2.1726882,1.7257934,-3.1595833,1.3041788,-2.130143,1.9179122,1.2102159,1.3958267,-2.2322032,1.8708177,-1.2298766,1.2038989,-2.1326363,1.8577304,1.6266763,1.6197556,-3.8891008,-3.938729,-0.01105009,-3.9867616,0.5887095,-1.5321785,-2.1370716,-1.6365768,-3.8244677,-3.8841867,-1.4472537,-3.7946246,-0.07689191,0.28438437,0.93217546,0.22106186,0.37736115,-0.08274249,-0.9242864,-1.0760541,-1.499387,-3.524049,-0.10791757,1.8852628,-3.8359635,-1.0084014,-1.3871976,-0.32231757,-0.54319465,-1.4754195,-3.3107884,-2.971868,-3.025585,-3.038378,-1.7171769,-2.19728,-3.2239976,-2.925478,-3.2127175,-5.9239817,-2.9008965,-2.6439178,-0.85866004,0.9381626,-2.8592029,-3.4441886,-2.8377578,-2.1439884,-4.8093796,-7.291625,-1.2848456,-2.1107788,-2.807332,-6.277229,-6.104789,-6.0701947,-6.3435283,-6.4955974,-6.1438093,-6.093686,-1.3515973,-1.7360452,-6.6202826,-6.7390094,0.19346899,-6.887034,2.1162868,1.4759012,-1.3677218,-1.6401683,-6.852776,-6.7058334,-6.4748716,-7.007154,-6.8025985,-4.0498314,-6.728074,-6.261588,-5.886286,-6.1652765,-6.2713304,-0.86484593,0.86387867,-0.3549314,-0.39907786,-0.32036072,-2.4466085,-0.38739133,0.65984696,0.7526854,0.59356433,0.8420266,0.42265695,0.5765744,0.37411138,0.2666832,0.56039065,-0.5588705,0.7175256,-2.0113525,0.8083775,0.36133596,0.32248688,0.4001129,0.89122766,0.7421192,0.26812342,0.73699534,-0.42251784,-0.6016577,-0.8835034,-0.629467,-1.0539962,-0.69212216,-0.6239621,-0.9407814,-1.4550971,-0.92777836,-1.0306535,-0.38180786,-1.0382942,-0.80948454,-0.74698585,-0.94348735,-0.64687765,-0.5672308,-1.1979735,-0.08290707,0.009173378,1.6289809,-4.686003,1.6669769,2.1157687,2.492454,1.6967684,-0.3090539,1.4238849,1.6461842,0.973132,1.5577196,1.5584332,1.5377944,0.68846285,-0.619732,-0.5270633,-0.5124709,-2.7452323,-2.7534175,-0.22354971,-0.8247404,-0.73768824,-0.7517274,0.2806047,-0.9688037,-5.814621,-0.9294534,2.2443004,3.0471053,-0.5027947,-0.44527996,-0.30312613,-3.5421143,-1.9934008,-3.5750012,-0.7976658,-0.8694299,-0.5300908,-2.8920023,0.58967507,-3.7419496,-0.7404237,-0.6290732,7.848674,-0.6322979,-4.440629,-4.377143,-4.6217937,-0.55259603,-0.9903636,-3.9080245,0.3206549,-3.572372,2.7638602,0.87119746,-1.9092839,0.94539386,1.5876172,-0.6878311,1.759681,-2.0108225,1.0367451,-2.9490595,0.62160087,3.1991484,1.5416436,1.6449244,2.0611484,2.3417594,0.75548255,0.9610054,-5.617518,-5.4287744,-5.137077,-5.321552,-5.684189,-5.3151283,-5.368656,-6.0338273,-5.997452,-5.6116123,-5.83081,-5.6757064,-6.0078397,-5.806678,-3.868281,-1.2253402,-1.6661344,-3.7922087,-3.874321,-0.5882822,-1.6182206,-3.1163504,-1.3632482,-0.5927012,-1.6992679,-1.1245548,-1.3608526,3.0037494,-1.3080232,-4.5520515,1.2216953,-5.918483,-0.91865927,-1.1371964,-1.1183869,-1.2173889,-3.3600862,-2.0424194,-1.8706937,2.2251496,-0.9380118,0.1944005,0.44268024,0.017206771,-0.7369306,-3.6267815,0.42998984,-1.1707852,-0.48856065,-3.2332911,-2.9807272,-0.21004474,-2.6357641,-2.0642605,-2.3814259,-2.2092185,-3.703226,-5.9864984,-6.5760493,-1.5146157,-7.372298,-7.6070333,-7.1685276,-3.8006234,-6.0381484,-0.8398575,1.0083879,0.103609875,-6.5738473,-6.8451705,-2.195182,-2.013847,-3.8356252,-3.916463,-6.071523,-6.1988425,-0.78701293,0.50560206,0.40587047,-1.8393319,0.2135488,-0.50535274,-0.39446992,0.06763011,0.5785647,-0.28710473,-1.0260073,-0.121446416,-0.53296155,-0.9266509,-0.6357661,-0.92088705,-0.69259304,2.0519776,-0.51474273,-0.81372476,-0.21465376,-0.7011164,3.55512,-0.73362345,3.7839577,2.592303,2.4677656,-0.78233474,1.5714512,4.9891405,-2.519221,-1.6397629,-1.9881102,1.2950853,-0.40197513,-1.415439,8.176061,8.30017,-1.530008,-2.2892442,-2.0280387,-3.6692414,-1.7324954,-1.8378077,-2.8262196,-0.017149469,0.079254314,0.31168684,0.28623497,0.17865619,-0.61692536,-0.15661272,-0.002380257,-3.3204112,0.9749374,0.1468517,-3.8670053,2.8181598,-1.1927552,-1.8180944,-3.7415323,-0.7703238,-1.3466094,-4.7591963,-5.1101093,-4.4724975,-4.483285,10.394366,10.389842,10.392558,10.393668,10.392322,10.37737,10.394374,10.39272,10.390847,10.390559,10.395402,10.398731,10.3952875,10.386957,-3.5994205,-1.23319,-1.2225854,-2.9089618,-1.3559973,-1.1237355,-0.7462433,-1.1024101,-0.69866025,-1.057777,21.316256,20.786098,21.28243,21.124851,21.43097,22.095682,-1.0492079,0.44418192,-0.48171481,-2.08841,-3.226182,-3.4736593,-3.2604103,-3.127007,-4.2402062,-3.7383428,-1.6016784,-2.3603213,-4.635811,-1.1522237,-1.594342,-0.68711245,-1.257683,-4.807755,-5.2337675,-4.4285893,-3.764948,-1.1514357,-1.5478661,-2.1398647,-1.0334561,-1.7721816,1.2599603,-0.44797453,-1.19669,21.392435,21.245262,20.800976,20.46911,21.373243,21.257906,21.127033,21.341007,21.516706,21.857527,21.130634,21.412037,21.345957,21.525875,22.130215,-1.0275482,0.55043405,-0.19026971,-0.14909686,-3.948251,-3.7677016,-0.36187652,-2.1258283,-3.8467677,-0.13834506,-0.3027552,-6.48225,-6.6803846,-0.6162697,-0.8439891,0.7894637,0.34933898,0.80141443,-1.4168389,-1.6034746,-2.399857,1.2900553,-3.4471302,0.08629315,-5.2538695,0.4229796,-0.15461075,-2.194677,-1.0701993,-1.724689,0.010555916,-6.1325417,0.2590499,-5.954928,-6.4691954,-6.810679,-6.9874244,-6.98618,-7.112448,-6.999315,-6.998627,-7.0011883,-7.6308937,2.2700012,-2.5425305,3.060028,-7.602146,-7.555013,-3.4388685,-7.057562,-6.9630256,-7.3374043,-7.3187537,-7.3960347,-7.431009,-7.2679024,3.105184,-7.6402717,-6.448482,-6.6653047,-0.38218814,-6.3257623,-7.027755,-1.1015033,-6.452972,-6.811072,-6.198194,-6.793779,-5.907545,-1.0878053,-6.801173,-6.126822,-6.293323,-7.0800343,2.0663557,-7.053404,-7.2480545,2.489362,8.164894,8.15212,8.226799,8.2164755,8.239417,8.134201,1.1636424,-7.450558,-0.9756384,-0.94123435,-0.3920937,0.35595563,-1.3123866,-4.090331,2.3717155,-0.75626767,-4.775753,0.15695801,-0.84059095,-4.3787885,-5.0153694,-0.108705275,-1.0974226,-4.211978,2.4077206,1.5444708,1.3103555,2.286695,1.8715078,1.9282093,-1.2486548,-1.514192,-3.8808672,-6.430963,-6.455864,-6.703335,-6.6299314,-6.79767,-4.2808747,-6.8520107,-6.464879,-5.486501,-6.728511,0.52748245,0.61858815,-0.1996172,-6.5645165,-1.1541708,-0.31843212,-6.795987,-6.713406,2.1503983,-0.38022307,1.7731016,2.1214502,0.7494303,-5.994237,-6.751518,-4.9700084,-1.8355147,-5.5173154,-6.8637905,-6.6892705,-6.6942635,-6.7425065,1.8488046,2.018292,-6.658298,1.1519886,-6.214741,-1.4288685,-2.0988257,-2.0594215,0.8228299,1.8524394,1.7039977,0.5342166,-0.024470605,-0.8173166,-0.7297269,0.79280645,-1.3465447,-2.2390912,-2.1432748,-3.152406,-4.143684,-0.26351917,-0.3410362,-0.55102205,-1.0488853,-2.7597172,-3.691455,-1.842083,-1.3995966,-2.0930204,2.9073944,-1.4762183,-2.9854765,-2.4532838,-3.8032992,4.9330134,5.2646537,0.025154274,-1.5472776,-1.2867005,-0.6921151,-1.2526432,-1.5779115,-3.7202764,-3.8301091,-0.8268604,-7.0246654,-1.5655217,-1.1385113,-1.5072123,-0.63128585,-0.405362,-2.956694,-2.433662,-2.8002,-2.6699424,-1.6866198,-1.8118875,-0.28224173,-3.8986049,-3.526174,-3.7902362,-1.9933178,-2.0713975,-2.4515307,-2.3710907,-3.8392167,-2.609503,-2.8225496,-3.2365592,-3.2319033,-3.2779908,-0.019413833,-0.17295092,-0.3484831,0.9426841,1.288284,2.052848,2.5289636,-3.6209092,-3.8422544,-4.022829,-3.7415695,-3.3619938,-3.5229213,-3.8607743,-3.5166144,-3.7690413,-4.12355,-4.0192766,-3.9330537,-3.7958877,-3.6114988,-3.9522262,-3.8269207,-4.119144,-4.1241574,-1.1133848,-0.78580993,-1.2678766,-1.2695031,-4.869593,-5.168998,-4.937089,-4.507078,10.399048,10.384956,10.389394,10.396984,10.393764,10.372486,10.384789,10.391803,10.388222,10.391403,10.391885,10.397034,10.401986,10.395854,-3.8539946,-1.3207198,-1.0903105,-1.2980753,-1.0343747,-2.176997,-0.96289957,-2.8949635,-1.388188,1.3231344,-0.40059212,-1.1393096,21.451777,21.571985,21.442825,20.980612,20.98789,21.148384,21.417305,21.296234,21.42833,20.622923,20.968801,21.145533,21.442429,21.235462,21.390966,21.47943,21.228334,21.331163,21.106466,20.634245,20.639418,20.958931,21.23795,20.899384,20.824236,20.496864,21.1282,21.52542,21.067612,20.942747,21.019388,20.328342,20.99235,21.844505,21.139826,21.265125,21.114786,20.761927,21.24443,21.276379,21.024473,21.30632,21.277012,20.99899,21.694159,21.031878,20.93812,21.014957,21.360329,21.24089,21.427261,21.411903,21.639967,21.976442,21.269585,21.173204,21.02326,21.148247,21.3538,21.998852,21.436985,21.278961,21.376245,21.38555,21.93296,22.02677,21.224503,21.482067,21.6201,21.948343,-3.2283742,-1.0746869,-0.98520035,0.40319932,0.1556105,-6.0510306,-6.648079,2.9825797,3.2093415,3.593069,-6.6716456,-6.495044,-3.6708252,-1.4658452,-1.8025951,-3.9206727,-3.890879,-0.449457,-0.36021832,-3.270854,-3.805125,-1.4436907,-1.7554955,-3.0417423,0.6450318,1.0798941,-2.9128032,-0.6833855,-3.8469906,0.1702598,-0.21842188,-0.418622,-1.2539746,-0.4828222,0.1604239,-0.40406656,-1.340828,0.7381871,-0.93763125,-0.4825094,-0.0400433,-0.7159706,-0.73415774,-5.713632,-0.9427182,-1.0377227,-1.4643755,0.20597668,-0.6470758,-0.49219778,-0.7979955,-1.3344201,-0.9026195,-1.0633595,-1.0766416,-0.5517782,-0.9448886,-2.661532,-0.5208049,-0.43669373,0.07105465,-0.75108236,-0.95791024,-1.0377053,-1.6905386,-3.8978107,-1.5463805,-1.4067798,-0.99789876,-0.76224315,0.024356501,1.8677593,0.8744522,1.039604,0.6417255,2.7321637,0.83664006,-5.802359,-6.4131036,-6.5498686,1.3154355,-7.192086,-6.60765,-4.143657,-1.6270918,-3.0804625,-2.1580684,1.3025734,-1.4917521,-1.747061,-0.49049643,-0.41639718,-0.4189371,-2.36045,-0.67066324,-1.5945427,-0.19269563,-0.22559327,-0.23906186,-0.098890215,-3.0768852,-0.78916806,-0.73394233,-4.8964653,-3.5922818,-2.0919368,-0.8226806,-0.7613217,-0.57922,-0.5366739,0.3213761,-2.712907,5.135449,4.7432513,-5.9722257,-6.1266036,-1.5663047,-2.4021654,-3.9517791,-0.43481836,-0.3022212,0.11887705,-1.4765917,-0.45578536,-3.1117842,0.5868485,1.0394474,4.1486044,-2.0181653,0.008627812,4.466146,4.258056,0.36616,0.30910438,0.27347162,0.31638965,0.39989108,0.25011337,0.27841207,0.29126516,0.30478802,0.28322366,0.3502138,2.6765158,2.5312579,1.7582116,-0.22702317,0.17975275,-0.6553368,-4.3448286,0.665915,-6.126666,-6.0846305,-3.805932,-1.4139309,-0.89839256,-0.6073284,0.74803853,-5.42152,-5.061216,-0.22190107,-6.5385847,-6.077703,-5.8018494,-0.85801685,-3.5532484,-0.061630808,-0.08522114,2.1471488,1.8153818,-0.07319274,-2.7825189,-2.83834,-1.7289902,-0.29343385,-0.29522347,-2.1708233,0.20033719,-1.3254328,-0.40305212,2.068091,-4.681795,0.09067658,-1.3621932,-0.79547685,-0.80647796,0.8121434,-0.6020009,0.80324346,-3.233092,-0.7611168,-0.7996338,-0.31404915,-1.378337,-0.009199671,-3.2709749,-1.1819848,-1.0919126,-1.2227854,-1.111849,-1.47526,-3.0963056,-4.419511,-4.744189,-3.9478905,1.0317218,-0.29102236,-0.30977458,-1.2195803,-2.6618826,-3.4930544,-5.841048,-6.0682936,-6.3658204,-0.85470563,-6.8677793,-6.7875977,-6.3871913,-6.7332506,-6.1522026,-6.802834,-6.7994413,-6.4290004,2.6897647,2.784061,1.7622156,1.7893848,1.0895013,-1.176749,-1.6521732,-3.9584951,-0.36681184,-0.42830306,0.02665215,-0.21955714,-1.7890878,-0.95087254,0.08479824,-0.48638144,-5.687051,-5.1784463,-5.170527,0.23982394,-2.3916728,-0.48316398,-2.9720583,2.0863612,-5.932242,-6.2527075,-6.8508863,-4.0752215,-5.827764,-1.2381725,-1.1955938,0.79429734,-0.3448531,-0.18101084,-0.49538824,-1.4114219,-3.089226,1.9314307,2.1637552,-0.25851023,1.0528073,2.3595111,0.14076263,1.4506135,2.5908837,2.1581712,0.47795674,2.157433,1.9394733,1.5835565,-0.59006166,1.6736767,-3.6907086,-1.2305121,-0.39698958,0.06655286,0.2114448,-1.3554064,-2.645525,-0.004125239,0.7588083,0.5900059,0.3109276,0.9099646,0.5384067,-0.09359121,0.21677317,-0.25327328,0.408457,2.3281438,2.5210187,2.923934,2.8379672,2.8107212,-6.2952313,-6.567397,2.5450113,-6.5971,2.5755262,2.8636117,2.4139228,2.9064882,-1.720402,-0.6765955,-3.6693597,-0.9334132,-0.7770266,-1.0203509,-0.22754258,-1.1428838,1.2087823,-1.296357,-1.227733,-1.27626,0.9999745,-1.1637671,-1.3066043,-1.2069129,-1.494589,-2.1315553,-2.0500083,-0.939009,-1.2604668,-1.3061808,-5.5253057,-4.5385814,-0.24045867,-0.10926598,-4.1449666,-1.7003379,-0.5642052,-1.4500761,-1.025098,0.48603317,-0.57344586,-1.087273,-1.0165957,-0.69506395,-0.62614554,-0.9206383,-1.0574216,-0.7674308,-0.9451888,-0.73663205,-0.78783435,-1.0670737,-0.71799093,-0.6909794,-0.75471145,-0.7894875,8.123548,1.0517765,0.8824802,0.5108827,0.3753373,0.26188907,0.5199404,-0.59183043,-0.85910726,-5.2822275,-5.998126,-5.4523687,-0.4957323,-0.7087895,-0.5555018,-0.8192818,-0.4697988,-0.60333276,-0.3324928,-2.0038476,-2.185341,-2.3810275,-2.5434186,-2.94984,-1.657465,-1.5253494,-1.6004552,-3.8442607,0.7914901,0.078028165,-0.39855108,-0.50729895,-0.93840134,-1.3088394,-0.28244725,-0.7963516,-1.6047571,5.4717426,-0.63068354,-0.497214,-4.7262597,-0.29521552,-1.8208482,-0.34546947,-1.2457439,-0.15768242,-0.52845025,-0.47879997,-0.5115237,-1.4396598,-0.41481248,-0.48469085,-0.5329475,-0.55470914,-0.045568563,-0.49500233,-1.238898,-1.5778708,-3.9369376,-1.6753774,0.84119886,-1.5154111,-4.3537564,-5.4043994,-5.3690934,-5.3599854,-5.3831635,-5.4071484,-5.3780236,-5.313747,-5.3712707,-5.334656,-5.3683124,-5.405321,-5.1138277,-5.305455,-4.3653073,-4.052328,-5.963943,0.9136244,-0.17194843,-0.6178209,-0.3115951,-2.61431,-3.3668141,2.2596903,2.8300726,-3.7567925,-0.39807802,-0.8088109,2.322038,-0.15730464,-2.4832711,-0.69708407,-2.9281683,-1.875443,-1.4965487,-3.577077,-1.1578377,2.3152945,0.24777545,-1.4905392,-2.306015,-3.0884583,-1.9398926,0.80054116,-1.1575757,-2.2825682,-2.7507832,-3.8667638,-2.138857,-1.710961,-3.6172862,-3.930547,-2.4961476,-3.8793724,-0.9955714,-4.0064945,-6.276655,-6.147657,-6.549063,-5.873767,-6.584537,-7.0287538,1.6980108,2.4980175,-1.5110464,-2.6002772,-3.3745787,-0.17734867,-0.76910037,-0.70231974,-0.9784354,-0.05152757,-0.05731931,-4.2799997,-0.3678517,-0.89799535,-6.1339216,-1.1091157,-1.1019496,-2.20724,-2.444046,-0.10955557,1.0230503,1.5785397,-5.2359176,-1.0182217,-2.4092655,-3.0038035,-1.3522763,-0.9590615,-0.42162848,-1.8792853,-1.5916288,-3.8009958,-3.9571679,-1.2751942,-1.4915267,0.25383455,1.7760844,-4.267805,-4.475649,-5.5263124,-2.9679546,-0.5921418,-2.808111,2.1744905,1.8592895,0.6309387,2.2025476,-0.2018964,-2.2317123,-1.8362489,-1.3963464,-2.0781565,-1.5826235,-1.7102969,-1.1885763,-0.774458,-1.2342534,0.40538818,1.8425734,-0.76700336,1.6113728,-6.793506,0.7453128,-2.051053,-1.6960169,-1.3005908,-2.3003466,-3.0886242,-3.230429,-3.3352695,-3.5886993,-2.9944785,-1.5423771,-3.0888329,-3.3696241,-3.3374405,-0.29430875,-2.7841449,-1.0170379,-0.67407584,-1.7749836,-1.70389,-3.8799121,-6.1070046,-6.5310755,1.5229514,-2.424152,-2.3617198,-2.8592255,-3.1403403,-2.0681577,-0.3451641,-0.25599277,-0.49245316,-3.1919248,-1.305807,-1.7136605,-2.0759385,-3.8484323,-3.8696342,-0.51215625,-0.64391065,-1.5807226,-3.8188019,-3.9122696,-2.6058335,-3.6089861,-3.9659293,-4.731034,-3.9484198,-4.955713,-4.717252,-4.778805,-4.80582,-4.7657223,-4.683591,-5.19279,-5.0127745,-4.4331217,-4.6730647,-3.2230065,-3.6583138,-1.3055573,1.0025685,0.17025243,-2.2910948,-1.5074512,-0.5871646,-1.4697995,-1.3806875,-1.436662,-1.776535,-1.9143806,-1.4899144,-1.2861733,-3.8333886,-3.9137967,-1.7089279,-1.0882264,-1.2485315,-1.1484258,0.69275784,-1.3003688,-4.2513394,-5.0865917,-4.7549214,-4.607803,-4.2500563,-6.6231933,-5.658637,-0.4353891,1.0138979,-0.4695999,-1.0831404,-0.22629099,-2.539142,-2.7900512,-3.0981874,2.8314922,1.8316727,2.3852067,-1.2953836,0.48594856,0.44392553,1.5844114,-5.788904,-6.167406,-6.5727396,-2.5704837,0.6305363,-3.549312,1.1211039,0.12819211,0.60571796,-0.104766116,-0.117524944,-0.78421134,0.027908096,-0.6366514,-0.16985545,-0.3740872,0.2861983,0.18810673,-2.2657537,0.33530024,0.42884088,-1.0765816,-5.9595604,-5.640672,-4.5236497,-1.7434263,-1.7366706,-1.6309783,-3.1470537,-5.6966496,-1.6843339,-1.735501,-2.195004,-2.4742997,-2.6431236,-3.1124809,-2.1065748,-0.01632971,-0.50011355,-0.21099748,-1.8131701,-2.502868,-3.7375176,-1.317367,-2.821789,-6.20494,-1.1706196,1.0357765,0.47187558,-0.1020576,-2.32839,-0.9077261,-0.57993066,-1.5204396,-1.6397115,-2.4943998,-6.020272,-6.4122596,-6.674912,-0.16259052,-3.9546769,-3.8785536,1.1570235,-0.8582636,1.9985794,-3.1583683,-1.14252,0.99447525,-1.5834477,-2.7967107,-1.203728,-2.2083306,-1.8277569,-3.4297438,-3.5143454,-3.7579458,-3.8068297,-3.7980394,-1.5441303,-1.7004501,-1.1837612,-2.0357304,-1.8688979,-1.6500877,-3.777681,-6.096315,-6.5130305,-7.241994,-4.025896,0.052055545,-0.52862537,1.8876145,0.43492255,-0.38702786,-0.46702325,-2.0928893,-2.607394,-3.710959,-1.9849933,-2.4016848,-3.6978772,-0.09288183,-0.16316524,8.592737,0.73550063,-0.1902203,0.05383151,0.23909605,0.37941164,-0.89049244,0.5247445,0.7384535,-1.4039178,-0.19706005,3.0062773,2.769626,2.173391,1.0732199,2.6398795,2.9051425,2.01695,0.520837,-0.85462457,0.5078249,0.71477926,-0.9331963,0.19108225,2.0390694,-0.9381948,-5.984604,-6.1772094,0.24408716,0.61364406,-0.014293927,-0.3399477,-0.41005346,1.0482233,-0.333292,-0.28997135,-2.8367186,0.710936,-0.20220974,0.15505639,0.11061626,-0.6718769,-0.3814326,-5.6924124,-3.3742294,-4.7503343,-4.484834,-3.9919453,-4.4715953,-4.2639875,-2.8109813,-2.838472,-3.5201015,-2.9881458,-7.3785233,-7.657525,-7.3614855,-7.4016013,-2.269535,-2.2666557,-2.5599353,-2.538184,-2.4443643,-6.6750517,-7.4923625,-7.524646,0.0038930571,-1.653522,-2.6899126,-2.3968828,-1.768998,-2.906778,-3.7023706,-3.203419,-3.3488238,-3.0054622,-3.376483,-0.1729728,-3.5030012,0.011370246,0.35382432,-0.43399477,-0.6212689,-1.5252376,-1.9267644,-1.9092889,-2.918312,-1.8578498,-1.7838802,0.5823238,-0.9971782,-1.493439,-0.80639595,-6.816624,0.25558597,-4.3185616,0.8718394,0.42305404,0.5357543,-0.12194303,1.9512874,-0.20873432,1.1228691,0.50446224,0.5222807,0.44005936,0.01567293,2.7936828,2.757683,2.7489169,2.720237,2.6984425,2.455084,2.1767888,2.47308,-0.28252828,-0.19389726,-0.6486317,-1.7409116,-0.8276723,-0.7918258,-3.1489751,-0.13856603,-0.25016683,-0.070736155,-2.1439912,-1.969151,-1.7332952,-3.7865105,-0.810658,-1.2178885,-3.9089823,-1.3982804,-2.4628947,-2.4925222,-0.30092114,-0.33876872,1.4841671,-3.2881036,-1.0893222,-0.9793809,-1.6186606,-0.5429877,-0.27533436,-0.2371414,0.74052095,-1.1558437,1.0344367,0.050834704,0.99925685,0.0050110337,-3.0774932,-2.9312437,-2.2254844,-5.8643503,0.5668972,-1.8681427,-0.95778906,-0.9435642,-0.5900027,-0.48896888,-1.3553103,0.6935795,-1.9812622,-1.5485127,-2.5382407,-3.8650157,-3.9099793,-0.3467813,-0.41020343,-2.8959093,-3.4087205,-0.37828788,-0.32082,0.34935513,0.26160762,-1.0280896,-0.8213381,-0.60735357,-0.4387003,-0.8256718,-1.0015013,2.215972,7.2678013,-0.70474297,-0.27848947,8.242309,-2.5439897,-3.4230108,-0.15710416,-0.22400118,-0.11982392,0.55881023,-3.2381933,-0.53496355,-2.554921,-3.6435087,-0.6132111,-0.01990118,-2.3505611,-3.0796952,-2.8761857,-3.0432858,-3.3996427,-3.0232737,-3.1023605,-3.4231775,-1.7205031,0.7341068,2.2415118,-0.7662581,-2.0803144,-3.6377501,-1.3687292,-1.892162,-1.4463066,-3.375745,-6.0923934,-5.8533034,-1.4684387,-1.1748399,-0.06663444,-2.4126012,-2.2749777,-1.9611018,-2.6356127,-3.8648367,-3.8505957,0.6575967,-0.4921609,-0.0544004,-0.6657362,-0.50273323,-1.1276455,0.66398734,-1.4479024,-1.4530063,-1.2927834,-1.8034344,-1.9585193,-1.8954074,-1.6926385,-0.4369767,1.8064632,-1.098528,-3.8125432,-3.925845,-0.1680039,1.134237,-1.4041945,2.5689757,3.118481,1.9751071,1.6410671,2.8323667,2.7271218,2.942835,-2.1831243,2.333475,2.9208832,-1.702757,-2.8924835,-2.830834,-2.685099,-3.5248365,-2.1118295,-2.3808935,1.7275664,2.2083347,-1.7555616,-1.8774748,-0.71361494,1.9307979,0.20259127,-2.275491,-1.7894312,-1.0243287,-3.819687,-0.2300077,-0.22494341,0.41835344,6.24408,-6.4683056,1.9900565,-5.0975432,-5.458253,2.0906367,-5.821108,2.6088433,2.5188165,2.8353894,2.116705,-6.794889,-1.4509711,-1.8050445,-3.068965,-1.6588787,-2.512969,0.17564203,-0.5357735,-0.52213097,-0.04329413,-0.2411161,0.048074014,0.25823775,0.11821578,0.08953886,-0.471688,-0.33446762,-1.8881419,-0.050695553,0.09745986,0.78069484,-0.19062033,-0.60125583,-0.70118475,-0.5681235,-0.39643383,-0.03099139,-0.27066186,-0.6280256,-1.4915662,-0.38878706,-0.072232015,-0.34460998,0.16086707,-0.498174,-0.8096084,-0.59765536,-1.9334469,-3.950053,-2.3579667,-0.9901798,-0.5189014,-0.20114544,0.6464633,-1.9060427,-1.0077052,2.0970914,-0.0034974208,-1.2429368,-1.7840568,-1.2632881,-1.4157267,-2.4836135,-3.5487633,-6.5950737,-6.422372,-6.5163956,-1.2628322,-1.1326128,-0.401039,-3.7447128,1.2062693,-0.35513234,-1.9292449,0.17067106,-0.58537763,-0.40144068,-3.5296922,-0.6683714,-0.5825302,-1.9070582,-3.923104,-3.1830487,-3.3334606,-4.390277,-3.8167124,-1.5644042,-1.955914,-3.8993573,-2.8042452,-3.6328216,0.7376248,-4.147728,0.67541444,-6.022625,-2.1974084,-4.108847,1.5593432,1.2333988,-1.9498835,-6.0274396,-6.6015234,-1.5799587,-4.3574266,-7.4390244,-7.4399023,-6.2249656,-1.113127,1.0158604,-6.908712,-6.910302,-3.028028,-1.3534968,-2.3932326,-3.8645074,-2.0008857,-3.191948,-3.0514522,-3.507906,-1.00232,-1.0185511,-3.2325482,-3.0112555,-3.404855,-3.3454432,-3.4948921,-3.6566005,-3.7706096,-3.7116675,-2.393738,-3.6134505,-3.203893,-3.6073017,-2.281582,-1.8646746,-4.3319106,-3.482864,2.6911063,2.6313868,2.2488766,-1.6714963,-0.6188234,-2.270949,-0.5234135,-0.062134493,-0.092448674,-0.058517188,-3.539137,-1.6269236,-1.7605323,-1.548509,-1.1179235,-1.4473886,-3.1593437,-1.2746203,-1.4827285,-1.8304867,-1.4933395,-0.68041706,-0.03640655,-3.7966402,-3.8694694,-3.9252126,0.5928129,-0.11471255,-3.0633094,-2.2429683,-1.2100718,-1.1360772,1.7836032,-0.47084877,-1.2915832,-1.2371976],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Git over SSH\\n\\nYou can access and write data in repositories on huggingface.co using SSH (Secure Shel...\"],[\"```\\n$ ssh-add ~\\u002f.ssh\\u002fid_ed25519\\n```\\n\\nIf you chose a different location than the default to store you...\"],[\"Using Flair at Hugging Face\\n\\n[Flair](https:\\u002f\\u002fgithub.com\\u002fflairNLP\\u002fflair) is a very simple framework f...\"],[\"```\\n\\nIt outputs the following:\\n\\n```text\\nSentence[6]: \\\"George Washington ging nach Washington.\\\" → [\\\"G...\"],[\"Widget Examples\\n\\nNote that each widget example can also optionally describe the corresponding model ...\"],[\"```\\n\\n### Summarization\\n\\n```yaml\\nwidget:\\n- text: \\\"The tower is 324 metres (1,063 ft) tall, about the ...\"],[\"```\\n\\n### Text Generation\\n\\n```yaml\\nwidget:\\n- text: \\\"My name is Julien and I like to\\\"\\n  example_title:...\"],[\"```\\n\\n### Feature Extraction\\n\\n```yaml\\nwidget:\\n- text: \\\"My name is Sylvain and I live in Paris\\\"\\n  exam...\"],[\"```\\n\\n### Voice Activity Detection\\n\\n```yaml\\nwidget:\\n- src: https:\\u002f\\u002fcdn-media.huggingface.co\\u002fspeech_sa...\"],[\"```\\n\\n### Text-to-Image\\n\\n```yaml\\nwidget:\\n- text: \\\"A cat playing with a ball\\\"\\n  example_title: \\\"Cat\\\"\\n-...\"],[\"```\\n\\n## Other\\n\\n### Structured Data Classification\\n\\n```yaml\\nwidget:\\n- structured_data:\\n    fixed_acid...\"],[\"Configure the Dataset Viewer\\n\\nThe Dataset Viewer supports many [data files formats](.\\u002fdatasets-addin...\"],[\"Adding a Sign-In with HF button to your Space\\n\\nYou can enable a built-in sign-in flow in your Space ...\"],[\"```\\n\\nYou can check out the [configuration reference docs](.\\u002fspaces-config-reference) for more inform...\"],[\"Those scopes are optional and can be added by setting `hf_oauth_scopes` in your Space's metadata:\\n\\n-...\"],[\"Basically, you need to:\\n\\n- Redirect the user to `https:\\u002f\\u002fhuggingface.co\\u002foauth\\u002fauthorize?redirect_uri...\"],[\"User access tokens\\n\\n## What are User Access Tokens?\\n\\nUser Access Tokens are the preferred way to aut...\"],[\"If you are a member of an organization with read\\u002fwrite\\u002fadmin role, then your User Access Tokens will...\"],[\"## How to use User Access Tokens?\\n\\nThere are plenty of ways to use a User Access Token to access the...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\nTry not to leak your token! Though you can always rotate it, anyone will b...\"],[\"ZenML on Spaces\\n\\n[ZenML](https:\\u002f\\u002fgithub.com\\u002fzenml-io\\u002fzenml) is an extensible, open-source MLOps fram...\"],[\"Visit [the ZenML documentation](https:\\u002f\\u002fdocs.zenml.io\\u002f) to learn more about its\\nfeatures and how to ...\"],[\"![Choose the ZenML Docker template](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"Once you have your ZenML server up and running, you can connect to it from your\\nlocal machine. To do...\"],[\"```\\n\\nYou can also use the Direct URL in your browser to use the ZenML dashboard as a\\nfullscreen appl...\"],[\"\\u003cTip warning={true}\\u003e\\nIf you wish to use a cloud secrets backend together with ZenML for secrets\\nmana...\"],[\"## 🤗 Feedback and support\\n\\nIf you are having trouble with your ZenML server on HuggingFace Spaces, y...\"],[\"Using Spaces for Organization Cards\\n\\nOrganization cards are a way to describe your organization to o...\"],[\"Repository Settings \\n\\n## Private repositories\\n\\nYou can choose a repository's visibility when you cre...\"],[\"If these are use cases you need help with, please send us an email at **website at huggingface.co**....\"],[\"--\\n# Example metadata to be added to a dataset card.  \\n# Full dataset card template at https:\\u002f\\u002fgithu...\"],[\"- {bcp47_lang_1}  # Example: en-US\\npretty_name: {pretty_name}  # Example: SQuAD\\nsize_categories:\\n- {...\"],[\"# Optional. This part can be used to store the feature types and size of the dataset to be used in p...\"],[\"```\\n\\n# Optional. If you want your dataset to be protected behind a gate that users have to accept to...\"],[\"Valid license identifiers can be found in [our docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002frepositories-li...\"],[\"Repository limitations and recommendations\\n\\nThere are some limitations to be aware of when dealing w...\"],[\"Under the hood, the Hub uses Git to version the data, which has structural implications on what you ...\"],[\"- **Repository size**: The total size of the data you're planning to upload. There is no hard limit ...\"],[\"- **Number of commits**: There is no hard limit for the total number of commits on your repo history...\"],[\"Dask\\n\\n[Dask](https:\\u002f\\u002fgithub.com\\u002fdask\\u002fdask) is a parallel and distributed computing library that scal...\"],[\"```\\n\\nThis creates a dataset repository `username\\u002fmy_dataset` containing your Dask dataset in Parquet...\"],[\"Access control in organizations\\n\\n\\u003cTip\\u003e\\n\\nYou can set up [Single Sign-On (SSO)](.\\u002fsecurity-sso) to be ...\"],[\"Billing\\n\\nAt Hugging Face, we build a collaboration platform for the ML community (i.e., the Hub), an...\"],[\"Any feedback or support request related to billing is welcome at billing@huggingface.co.\\n\\n## Invoici...\"],[\"Streamlit Spaces\\n\\n**Streamlit** gives users freedom to build a full-featured web app with Python in ...\"],[\"```\\n\\nYou can edit the `sdk_version`, but note that issues may occur when you use an unsupported Stre...\"],[\"## Add the dependencies\\n\\nFor the **Hot Dog Classifier** we'll be using a [🤗 Transformers pipeline](h...\"],[\"```\\ntransformers\\ntorch\\n```\\n\\nThe Spaces runtime will handle installing the dependencies!\\n\\n## Create t...\"],[\"```\\n\\nThis Python script uses a [🤗 Transformers pipeline](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fpi...\"],[\"```\\n\\n\\u003c!-- The height of this iframe has been calculated as 236 + 64 * 2. 236 is the inner content he...\"],[\"- `id` is set to `\\u003ciframe \\u002f\\u003e` that is used to specify the auto-resize target.\\n- The `iFrame Resizer`...\"],[\"```\\n\\nAdditionally, you can checkout [our documentation](.\\u002fspaces-embed)....\"],[\"Next Steps\\n\\nThese next sections highlight features and additional information that you may find usef...\"],[\"To learn about Git branching, you can try out the [Learn Git Branching interactive tutorial](https:\\u002f...\"],[\"**Note that you will need to [install Git LFS](https:\\u002f\\u002fgit-lfs.github.com\\u002f) and the [`huggingface_hu...\"],[\"```\\ngit clone git@hf.co:me\\u002fmyfork\\n```\\n\\n3. Fetch non-LFS files:\\n\\n```\\ncd myfork\\ngit lfs install --skip...\"],[\"Run with Docker\\n\\nYou can use Docker to run most Spaces locally.\\nTo view instructions to download and...\"],[\"Advanced Topics\\n\\n## Contents\\n\\n- [Using OpenCV in Spaces](.\\u002fspaces-using-opencv)\\n- [More ways to crea...\"],[\"Using spaCy at Hugging Face\\n\\n`spaCy` is a popular library for advanced Natural Language Processing u...\"],[\"```\\n\\nTo find the link of interest, you can go to a repository with a `spaCy` model. When you open th...\"],[\"```\\n\\nYou can then check if the command has been registered successfully\\n\\n```bash\\npython -m spacy hug...\"],[\"```\\n\\n| Argument             | Type         | Description                                            ...\"],[\"```bash\\nhuggingface-cli login\\npython -m spacy package .\\u002fen_ner_fashion .\\u002foutput --build wheel\\ncd .\\u002fo...\"],[\"```\\n\\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browse...\"],[\"Audit Logs\\n\\n\\u003cTip warning={true}\\u003e\\nThis feature is part of the \\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fenterpr...\"],[\"Spaces Overview\\n\\nHugging Face Spaces make it easy for you to create and deploy ML-powered demos in m...\"],[\"Under the hood, Spaces stores your code inside a git repository, just like the model and dataset rep...\"],[\"| **Hardware**        \\t| **GPU Memory** \\t| **CPU** \\t| **Memory** \\t| **Disk** \\t| **Hourly Price** \\t|\\n...\"],[\"| **Storage tier**     \\t| **Size**             \\t| **Persistent** \\t| **Monthly price** \\t|\\n|----------...\"],[\"Note: Find more detailed and comprehensive pricing information on [our pricing page](https:\\u002f\\u002fhugging...\"],[\"Variables are publicly accessible and viewable and will be automatically added to Spaces duplicated ...\"],[\"Some Spaces might have environment variables that you may need to set up. In these cases, the duplic...\"],[\"In case [OAuth](.\\u002fspaces-oauth) is enabled for your Space, the following variables will also be avai...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\ntitle: My lovely space\\nemoji: 🤗\\ncolorFrom: blue\\ncolorTo: green\\nsdk: docker\\npinned: false\\nmodels:...\"],[\"Search\\n\\nYou can now easily search anything on the Hub with **Full-text search**. We index model card...\"],[\"## Filter with ease\\n\\nBy default, models, datasets, & spaces are being searched when a user enters a ...\"],[\"[paddlenlp-banner](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fhub...\"],[\"4. Easily deploy your model as a Gradio app on Spaces.\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg src=\\\"...\"],[\"```\\npip install -U paddlenlp\\n```\\n\\n## Using existing models\\n\\nSimilar to `transformer` models, the `pa...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click `Use in paddlenlp` and you will ...\"],[\"Reference\\n\\n## Deep Learning Container\\n\\nBelow you can find a version table of currently available Hug...\"],[\"**Example 1: PyTorch Training:**\\n`763104351884.dkr.ecr.us-west-2.amazonaws.com\\u002fhuggingface-pytorch-t...\"],[\"| 🤗 Transformers version | 🤗 Datasets version | PyTorch\\u002fTensorFlow version | type     | device | Pyt...\"],[\"| 4.10.2                  | 1.11.0              | TensorFlow 2.4.1           | training | GPU    | 3...\"],[\"## Inference DLC Overview\\n\\nThe Inference DLC overview includes all released and available Hugging Fa...\"],[\"| 🤗 Transformers version | PyTorch\\u002fTensorFlow version | type      | device | Python Version |\\n| ----...\"],[\"| 4.11.0                  | PyTorch 1.9.0              | inference | GPU    | 3.8            |\\n| 4.1...\"],[\"## Hugging Face Transformers Amazon SageMaker Examples\\n\\nExample Jupyter notebooks that demonstrate h...\"],[\"| Notebook                                                                                          ...\"],[\"| [02 getting started with TensorFlow](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fsagemaker\\u002f...\"],[\"| [07 Distributed Training: Data Parallelism](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fsag...\"],[\"| [12 Batch Processing with Amazon SageMaker Batch Transform](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteboo...\"],[\"| [18 AWS Inferentia](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fsagemaker\\u002f18_inferentia_inf...\"],[\"## Inference Toolkit API\\n\\nThe Inference Toolkit accepts inputs in the `inputs` key, and supports add...\"],[\"```\\n\\n**`sentiment-analysis`**\\n\\n```json\\n{\\n  \\\"inputs\\\": \\\"Don't waste your time.  We had two different p...\"],[\"```\\n\\n**`parameterized-request`**\\n\\n```json\\n{\\n  \\\"inputs\\\": \\\"Hugging Face, the winner of VentureBeat’s I...\"],[\"```\\n\\n**`HF_API_TOKEN`**\\n\\n`HF_API_TOKEN` defines your Hugging Face authorization token. The `HF_API_T...\"],[\"Pandas\\n\\n[Pandas](https:\\u002f\\u002fgithub.com\\u002fpandas-dev\\u002fpandas) is a widely used Python data analysis toolkit...\"],[\"```\\n\\nThis creates a dataset repository `username\\u002fmy_dataset` containing your Pandas dataset in Parqu...\"],[\"Datasets without language challenge\\n\\nRelated to https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fissues\\u002f986.\\n...\"],[\"```\\n\\nHaving this field filled in is essential for users to find datasets in their language and give ...\"],[\"1. Find a dataset that doesn't have the `language` field filled in. You can find a list of datasets ...\"],[\"4. Once you've identified the language(s) of the dataset, you can add the language tag(s) to the dat...\"],[\"## F.A.Q.\\n\\n### Does it make sense to add language metadata to all datasets?\\n\\nNo! This is why we have...\"],[\"```\\n\\n## Datasets without language field filled in...\"],[\"| status | pr_url                                                                                   ...\"],[\"|        |                                                                                          ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002funaidedelf87777\\u002fopenapi-function-invocations-25k\\u002fd...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcodeparrot\\u002fgithub-jupyter-code-to-text\\u002fdiscussions...\"],[\"|        |                                                                                          ...\"],[\"|        |  [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fvivym\\u002fmidjourney-prompts\\u002fdiscussions\\u002f1)          ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fiamtarun\\u002fcode_instructions_120k_alpaca\\u002fdiscussions...\"],[\"|        |                                                                                          ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fajaykarthick\\u002fimdb-movie-reviews\\u002fdiscussions\\u002f1)    ...\"],[\"|        |                                                                                          ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fvhtran\\u002fid-en\\u002fdiscussions\\u002f1#651ababdc4fdc1c93efb0f2...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdandrade\\u002fes-en\\u002fdiscussions\\u002f1#651ac2720047dc5f7aae8...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fjoelniklaus\\u002fgerman_rental_agreements\\u002fdiscussions\\u002f1...\"],[\"|        |                                                                                          ...\"],[\"|        |                                                                                          ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fIskaj\\u002fdutch_corpora_parliament_processed\\u002fdiscussio...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmanu\\u002ffrench_librispeech_text_only\\u002fdiscussions\\u002f1)  ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fstas\\u002fwmt16-en-ro-pre-processed\\u002fdiscussions\\u002f1#651ab...\"],[\"|        |                                                                                          ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fW4nkel\\u002fturkish-sentiment-dataset\\u002fdiscussions\\u002f1#651...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fvhtran\\u002fde-en\\u002fdiscussions\\u002f1#651abad1b61121b12838a02...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyongsun-yoon\\u002fopen-ner-english\\u002fdiscussions\\u002f1#651abb...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002findiejoseph\\u002fwikipedia-en-filtered\\u002fdiscussions\\u002f1#65...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fthesistranslation\\u002fdistilled-ccmatrix-de-en\\u002fdiscuss...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ferebos\\u002fgermanZickleinLLAMA2Dataset\\u002fdiscussions\\u002f1) ...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fthesistranslation\\u002fdistilled-ccmatrix-en-es\\u002fdiscuss...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fthisserand\\u002fhealth_care_german\\u002fdiscussions\\u002f1)      ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fneil-code\\u002fsubset-data-en-zh\\u002fdiscussions\\u002f1#651ac98a...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002ft5-qe-2023-enmr-da-sys-test\\u002fdiscuss...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002ft5-qe-2023-ente-da-test\\u002fdiscussions...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002ft5-qe-2023-enhi-da-test\\u002fdiscussions...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-enta-da-sys-test\\u002fdi...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-engu-da-sys-test\\u002fdi...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-enmr-da-test\\u002fdiscus...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-enta-sys-test\\u002fdiscu...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-enhi-sys-test\\u002fdiscu...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-enta-test\\u002fdiscussio...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-engu-test\\u002fdiscussio...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fChanceFocus\\u002fflare-multifin-en\\u002fdiscussions\\u002f1#651ac6...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fVFiona\\u002fcovid-19-synthetic-it-en-10000\\u002fdiscussions\\u002f...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyezhengli9\\u002fwmt20-cs-en\\u002fdiscussions\\u002f1#651ac588394b6...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyezhengli9\\u002fwmt20-en-ps\\u002fdiscussions\\u002f1#651ac54adeec0...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyezhengli9\\u002fwmt20-ps-en\\u002fdiscussions\\u002f1#651ac4fcf0354...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyezhengli9\\u002fwmt20-ja-en\\u002fdiscussions\\u002f1#651ac48fd03e9...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyezhengli9\\u002fwmt20-de-en\\u002fdiscussions\\u002f1#651ac41a1c53e...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fshreevigneshs\\u002fiwslt-2023-en-pt-train-val-split-0.2...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fshreevigneshs\\u002fiwslt-2023-en-es-train-val-split-0.1...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcahya\\u002finstructions-en\\u002fdiscussions\\u002f1#651ac25fbf3fb2...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fNadiaHassan\\u002far-en\\u002fdiscussions\\u002f1#651ac1936a6b822b88...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fvocab-transformers\\u002fwiki-en-passages-20210101\\u002fdiscu...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fMakxxx\\u002ffrench_CEFR\\u002fdiscussions\\u002f1)                 ...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fgollumeo\\u002ffrench-litterature\\u002fdiscussions\\u002f1)        ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ffathyshalab\\u002fgermanquad_qg_qg_dataset\\u002fdiscussions\\u002f1...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fgermank\\u002fhh-rlhf_with_features\\u002fdiscussions\\u002f1)      ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fphilschmid\\u002fprompted-germanquad\\u002fdiscussions\\u002f1)     ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcansen88\\u002fturkishReviews_5_topic\\u002fdiscussions\\u002f1#651a...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002forhanxakarsu\\u002fturkishPoe-generation\\u002fdiscussions\\u002f1#6...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fHarsit\\u002fxnli2.0_train_turkish\\u002fdiscussions\\u002f1#651aeb1...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ferytrn\\u002fturkishReviews-ds-mini\\u002fdiscussions\\u002f1#651aeb...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fsebinbusra\\u002fturkishReviews-ds-mini\\u002fdiscussions\\u002f1#65...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fbosnakdev\\u002fturkishReviews-ds-mini\\u002fdiscussions\\u002f1#651...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fVeyselbyte\\u002fturkishReviews-ds-mini\\u002fdiscussions\\u002f1#65...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fMemis\\u002fturkishReviews-ds-mini\\u002fdiscussions\\u002f1#651aec9...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fAnanthZeke\\u002ftamil_sentences_sample\\u002fdiscussions\\u002f1#65...\"],[\"Advanced Topics\\n\\n## Contents\\n\\n- [Integrate your library with the Hub](.\\u002fmodels-adding-libraries)\\n- [...\"],[\"Storage Regions on the Hub\\n\\nRegions let you decide where your org's models and datasets will be stor...\"],[\"For companies in the EU, that means you can use the Hub to build ML in a GDPR compliant way: with da...\"],[\"Managing Spaces with Github Actions\\n\\nYou can keep your app in sync with your GitHub repository with ...\"],[\"```\\n\\nFinally, create an Action that automatically checks the file size of any new pull request:\\n\\n\\n``...\"],[\"Webhook guide: Setup an automatic metadata quality review for models and datasets \\n\\n\\u003cTip\\u003e\\n\\nWebhooks ...\"],[\"```\\n\\nThis metadata contains essential information about your model or dataset for potential users. T...\"],[\"```python\\nfrom huggingface_hub import DatasetCard, ModelCard\\nfrom huggingface_hub.utils import Entry...\"],[\"```\\n\\nThis function will return a Python dictionary containing the metadata associated with the repos...\"],[\"```\\n\\nOnce we have this dictionary, we can create our metadata report. In the interest of brevity, we...\"],[\"```\\n\\n\\u003cTip\\u003e\\n    `:=` is the Python Syntax for an assignment expression operator added to the Python l...\"],[\"## Create a new Bot user profile\\n\\nThis guide creates a separate user account that will post the meta...\"],[\"KEY = os.environ.get(\\\"WEBHOOK_SECRET\\\")\\n\\napp = FastAPI()\\n\\n@app.post(\\\"\\u002fwebhook\\\")\\nasync def webhook(req...\"],[\"```\\n\\nThe above function will receive Webhook events and creates or updates the metadata review repor...\"],[\"## Conclusion and next steps\\n\\nWe now have an automatic metadata review bot! Here are some ideas for ...\"],[\"🟧 Label Studio on Spaces\\n\\n[Label Studio](https:\\u002f\\u002flabelstud.io) is an [open-source data labeling\\nplat...\"],[\"By default, Label Studio is installed in Spaces with a configuration that uses\\nlocal storage for the...\"],[\"* `LABEL_STUDIO_DISABLE_SIGNUP_WITHOUT_LINK`: Setting this value to `true` will\\n  disable unrestrict...\"],[\"* `STORAGE_PERSISTENCE`: Set this to `1` to remove the warning about ephemeral\\n  storage.\\n\\nRestart t...\"],[\"* `STORAGE_TYPE`: Set this to `azure`.\\n\\n* `STORAGE_AZURE_ACCOUNT_NAME`: `\\u003cYOUR_STORAGE_ACCOUNT\\u003e`\\n\\n* ...\"],[\"Webhooks\\n\\n\\u003cTip\\u003e\\n\\nWebhooks are now publicly available!\\n\\n\\u003c\\u002fTip\\u003e\\n\\nWebhooks are a foundation for MLOps-r...\"],[\"You can view the history of payloads sent in the activity tab of the webhook settings page, it's als...\"],[\"```json\\n{\\n  \\\"event\\\": {\\n    \\\"action\\\": \\\"create\\\",\\n    \\\"scope\\\": \\\"discussion\\\"\\n  },\\n  \\\"repo\\\": {\\n    \\\"type\\\"...\"],[\"```\\n\\n### Event\\n\\nThe top-level properties `event` is always specified and used to determine the natur...\"],[\"In the current version of webhooks, the top-level property `repo` is always specified, as events can...\"],[\"```\\n\\n`repo.headSha` is the sha of the latest commit on the repo's `main` branch. It is only sent whe...\"],[\"```\\n\\n## Webhook secret\\n\\nSetting a Webhook secret is useful to make sure payloads sent to your Webhoo...\"],[\"## Debugging Webhooks\\n\\nYou can easily find recently generated events for your webhooks. Open the act...\"],[\"Dataset viewer\\n\\nThe dataset page includes a table with the contents of the dataset, arranged by page...\"],[\"## Access the parquet files\\n\\nTo power the dataset viewer, every dataset is auto-converted to the Par...\"],[\"## Configure the Dataset Viewer\\n\\nTo have a properly working Dataset Viewer for your dataset, make su...\"],[\"The Model Hub\\n\\n## What is the Model Hub?\\n\\nThe Model Hub is where the members of the Hugging Face com...\"],[\"Signing commits with GPG\\n\\n`git` has an authentication layer to control who can push commits to a rep...\"],[\"For a more in-depth explanation of how git and GPG interact, please visit the the [git documentation...\"],[\"```\\n\\nGPG will then guide you through the process of creating a GPG key pair.\\n\\nMake sure you specify ...\"],[\"Spaces Changelog\\n\\n## [2023-07-28] - Upstream Streamlit frontend for `\\u003e=1.23.0`\\n\\n- Streamlit SDK uses...\"],[\"- Read more doc about: [Docker Spaces](.\\u002fspaces-sdks-docker)\\n\\n## [2022-12-14] - Ability to set a cus...\"],[\"- All `1.x.0` versions are now supported (up to `1.9.0`).\\n\\n## [2022-05-16] - Gradio 3 is out!\\n\\n- Thi...\"],[\"## [2021-08-10] - Upgrade Streamlit to `0.83.0`\\n\\n- [Streamlit changelog](https:\\u002f\\u002fgithub.com\\u002fstreamli...\"],[\"Licenses\\n\\nYou are able to add a license to any repo that you create on the Hugging Face Hub to let o...\"],[\"\\u003c!-- region licenses --\\u003e\\nFullname | License identifier (to use in model card)\\n--- | ---\\nApache licen...\"],[\"Creative Commons Attribution Share Alike 3.0 | `cc-by-sa-3.0`\\nCreative Commons Attribution Share Ali...\"],[\"Etalab Open License 2.0 | `etalab-2.0`\\nEuropean Union Public License 1.1 | `eupl-1.1`\\nGNU Affero Gen...\"],[\"Llama 2 Community License Agreement | `llama2`\\nUnknown | `unknown`\\nOther | `other`\\n\\u003c!-- endregion --...\"],[\"In case of `license: other` please add the license's text to a `LICENSE` file inside your repo (or c...\"],[\"Hugging Face Hub documentation\\n\\nThe Hugging Face Hub is a platform with over 350k models, 75k datase...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-orange-100 bg-gradient-to-br from...\"],[\"\\u003cdiv class=\\\"flex items-center py-0.5 text-lg font-semibold text-orange-600 dark:text-gray-400 mb-1\\\"\\u003e...\"],[\"\\u003ca class=\\\"transform !no-underline transition-colors hover:translate-x-px hover:text-gray-700\\\" href=\\\"...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-indigo-100 bg-gradient-to-br from...\"],[\"\\u003cdiv class=\\\"flex items-center py-0.5 text-lg font-semibold text-indigo-600 dark:text-gray-400 mb-1\\\"\\u003e...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fm...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fm...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-red-100 bg-gradient-to-br from-re...\"],[\"\\u003csvg class=\\\"shrink-0 mr-1.5 text-red-400\\\" xmlns=\\\"http:\\u002f\\u002fwww.w3.org\\u002f2000\\u002fsvg\\\" xmlns:xlink=\\\"http:\\u002f\\u002fwww...\"],[\"7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fd...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fd...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-blue-100 bg-gradient-to-br from-b...\"],[\"\\u003csvg class=\\\"shrink-0 mr-1.5 text-blue-500\\\" xmlns=\\\"http:\\u002f\\u002fwww.w3.org\\u002f2000\\u002fsvg\\\" xmlns:xlink=\\\"http:\\u002f\\u002fww...\"],[\"7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.08...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fs...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fs...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-green-100 bg-gradient-to-br from-...\"],[\"\\u003cdiv class=\\\"flex items-center py-0.5 text-lg font-semibold text-green-600 dark:text-gray-400 mb-1\\\"\\u003e\\n...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fo...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fa...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\n## What's the Hugging Face Hub?\\n\\nWe are helping the community work together towards the goal...\"],[\"## Models\\n\\nYou can discover and use dozens of thousands of open-source ML models shared by the commu...\"],[\"The [🤗 `datasets`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002findex) library allows you to programmaticall...\"],[\"## Organizations\\n\\nCompanies, universities and non-profits are an essential part of the Hugging Face ...\"],[\"Using ESPnet at Hugging Face\\n\\n`espnet` is an end-to-end toolkit for speech processing, including aut...\"],[\"Here is an inference example:\\n\\n```py\\nimport soundfile\\nfrom espnet2.bin.tts_inference import Text2Spe...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be ...\"],[\"Model Card components\\n\\n**Model Card Components** are special elements that you can inject directly i...\"],[\"Annotated Model Card Template\\n\\n\\n## Template\\n\\n[modelcard_template.md file](https:\\u002f\\u002fgithub.com\\u002fhugging...\"],[\"* The **project organizer** is necessary for filling out [Model Details](#model-details) and [Uses](...\"],[\"* **Shared by [optional]:** `shared_by`\\n\\n_List (and ideally link to) the people\\u002forganization making ...\"],[\"_Explain how the model can be used without fine-tuning, post-processing, or plugging into a pipeline...\"],[\"## Training Procedure [optional]\\n\\n\\n### Preprocessing\\n\\n\\n`preprocessing`\\n\\n_Detail tokenization, resizi...\"],[\"**Section Overview:** This is an experimental section some developers are beginning to add, where wo...\"],[\"**Section Overview:** This section defines common terms and how metrics are calculated.\\n\\n\\n`glossary`...\"],[\"Organizations\\n\\nThe Hugging Face Hub offers **Organizations**, which can be used to group accounts an...\"],[\"Using 🤗 Datasets\\n\\nOnce you've found an interesting dataset on the Hugging Face Hub, you can load the...\"],[\"Appendix\\n\\n## Appendix A: User Study\\n_Full text responses to key questions_\\n\\n### How would you define...\"],[\"### What do you like about model cards?\\n\\n* They are interesting to teach people about new models\\n* A...\"],[\"### Other key new insights\\n\\n* Model cards are best filled out when done by people with different rol...\"],[\"* Google Cloud: [Face Detection](https:\\u002f\\u002fmodelcards.withgoogle.com\\u002fface-detection), [Object Detectio...\"],[\"* [Duke PULSE Model Card](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2003.03808.pdf)\\n* [Stanford Dynasent](https:\\u002f\\u002fgithub...\"],[\"### MODEL CARDS FOR LARGE LANGUAGE MODELS\\nLarge language models are often released with associated d...\"],[\"### MODEL CARD GENERATION TOOLS\\nTools for programmatically or interactively generating model cards i...\"],[\"Notifications\\n\\nNotifications allow you to know when new activities (Pull Requests or discussions) ha...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"![Notifications settings page](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"How to configure SAML SSO with Azure\\n\\nIn this guide, we will use Azure as the SSO provider and with ...\"],[\"### Step 2: Configure your application on Azure\\n\\nOpen a new tab\\u002fwindow in your browser and navigate ...\"],[\"Then under \\\"SAML Certificates\\\", verify that \\\"Signin Option\\\" is set to \\\"Sign SAML response and assert...\"],[\"```\\n-----BEGIN CERTIFICATE-----\\n{certificate}\\n-----END CERTIFICATE-----\\n```\\n\\n\\u003cdiv class=\\\"flex justif...\"],[\"Gradio Spaces\\n\\n**Gradio** provides an easy and intuitive interface for running a model from a list o...\"],[\"## Create a new Gradio Space\\n\\nWe'll start by [creating a brand new Space](https:\\u002f\\u002fhuggingface.co\\u002fnew...\"],[\"```\\ntransformers\\ntorch\\n```\\n\\nThe Spaces runtime will handle installing the dependencies!\\n\\n## Create t...\"],[\"```\\n\\nThis Python script uses a [🤗 Transformers pipeline](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fpi...\"],[\"Cookie limitations in Spaces\\n\\nIn Hugging Face Spaces, applications have certain limitations when usi...\"],[\"Argilla on Spaces\\n\\n**Argilla** is an open-source, data labelling tool, for highly efficient human-in...\"],[\"\\u003cTip\\u003e\\n**IMPORTANT NOTE ABOUT DATA PERSISTENCE:**\\nYou can use the Argilla Quickstart Space as is for ...\"],[\"\\u003cTip\\u003e\\nFor quick experimentation, you can jump directly into the next section. If you want to secure ...\"],[\"The usernames, passwords, and API keys to upload, read, update, and delete datasets can be configure...\"],[\"The combination of these secret variables gives you the following setup options:\\n\\n1. *I want to avoi...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentatio...\"],[\"```\\n\\nThird, you need to read the dataset using the `datasets` library. For reading other file types,...\"],[\"```\\n\\nTo train a SetFit model with this dataset:\\n\\n```python\\nfrom sentence_transformers.losses import ...\"],[\"Using Stable-Baselines3 at Hugging Face\\n\\n`stable-baselines3` is a set of reliable implementations of...\"],[\"```\\npackage_to_hub(model=model, \\n               model_name=\\\"ppo-LunarLander-v2\\\",\\n               mode...\"],[\"File names and splits\\n\\nTo host and share your dataset, create a dataset repository on the Hugging Fa...\"],[\"```\\nmy_dataset_repository\\u002f\\n├── README.md\\n├── train.csv\\n├── test.csv\\n└── validation.csv\\n```\\n\\nIf you d...\"],[\"```\\n\\n### Multiple files per split\\n\\nSplits can span several files, for example:\\n\\n```\\nmy_dataset_repos...\"],[\"Integrate your library with the Hub\\n\\nThe Hugging Face Hub aims to facilitate sharing machine learnin...\"],[\"```\\n\\n2. Once you have successfully installed the `huggingface_hub` library, log in to your Hugging F...\"],[\"```\\n\\n   `notebook_login` will launch a widget in your notebook from which you can enter your Hugging...\"],[\"```\\n\\nUse the `cache_dir` parameter to change where a file is stored:\\n\\n```python\\n\\u003e\\u003e\\u003e from huggingface...\"],[\"```\\n\\nDoing so will also add a tag to your model so users can quickly identify models from your libra...\"],[\"```\\n\\nWhen you check your Hugging Face account, you should now see a `test-model` repository under yo...\"],[\"```\\n\\nIf you need to upload more than one file, look at the [utilities offered by the `Repository` cl...\"],[\"```\\n\\n    * For each task your library supports, modify the `app\\u002fpipelines\\u002ftask_name.py` files accord...\"],[\"```\\n\\n### Register your libraries supported tasks on the hub\\n\\nTo register the tasks supported by your...\"],[\"Aim on Spaces\\n\\n**Aim** is an easy-to-use & supercharged open-source experiment tracker. Aim logs you...\"],[\"Now, when you navigate to your Space's **App** section, you can access the Aim UI.\\n\\n## Compare your ...\"],[\"```\\n\\nThe experiments tracked by Aim are stored in the `.aim` folder. **To display the logs with the ...\"],[\"```\\n\\nThat’s it! Now open the App section of your Space and the Aim UI is available with your logs.\\nH...\"],[\"# Model `license:other` challenge\\n\\nRelated to https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fissues\\u002f985.\\n\\n#...\"],[\"```\\n\\nwhich display on the Hub as\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"```\\n\\n## How to contribute?\\n\\nHow to do it in practice? That's simple! We have listed models below tha...\"],[\"For each model, the workflow looks like this:\\n1. Choose a model in the list below. We suggest focusi...\"],[\"## F.A.Q.\\n\\n### What if the model has 2 licenses?\\n\\nThis use case can happen when a model is finetuned...\"],[\"|status|pr_url                                                                      |model_id|nb_dow...\"],[\"|------|----------------------------------------------------------------------------|--------|------...\"],[\"-|------------|-------------------------|-----------------------------------------------------------...\"],[\"-------------------------------------------|--------------------------------------------------------...\"],[\"----------------------------------------------|-----------------------------------------------------...\"],[\"------------------------------------------------------|---------------------------------------------...\"],[\"--------------------------------------------------------------------------------|-------------------...\"],[\"------------------------------------------------------------------------|...\"],[\"|Opened|[Hub PR](https:\\u002f\\u002fhuggingface.co\\u002fdecapoda-research\\u002fllama-7b-hf\\u002fdiscussions\\u002f130)|[decapoda-res...\"],[\"|Opened|[hub_pr](https:\\u002f\\u002fhuggingface.co\\u002felinas\\u002fchronos-13b-v2\\u002fdiscussions\\u002f3) |[elinas\\u002fchronos-13b-v2...\"],[\"|Opened|[HUB_PR](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fOpenAssistant-Llama2-13B-Orca-v2-8K-3166-GPTQ\\u002fdiscu...\"],[\"|      |                                                                            |[aipicasso\\u002fpica...\"],[\"|      |                                                                            |[sambanovasyste...\"],[\"|Opened|[HUB_PR](https:\\u002f\\u002fhuggingface.co\\u002fhuggyllama\\u002fllama-30b\\u002fdiscussions\\u002f1)                         ...\"],[\"|      |                                                                            |[TheBloke\\u002fLlama...\"],[\"|      |                                                                            |[georgesung\\u002flla...\"],[\"|      |                                                                            |[TheBloke\\u002forca_...\"],[\"|      |                                                                            |[jondurbin\\u002fairo...\"],[\"|      |                                                                            |[h2oai\\u002fh2ogpt-r...\"],[\"|      |                                                                            |[TheBloke\\u002ftulu-...\"],[\"|      |                                                                            |[decapoda-resea...\"],[\"|      |                                                                            |[luodian\\u002fllama-...\"],[\"|      |                                                                            |[learnanything\\u002f...\"],[\"|      |                                                                            |[waylandy\\u002fphosf...\"],[\"|      |                                                                            |[aipicasso\\u002fcool...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[aipicasso\\u002fcool...\"],[\"|      |                                                                            |[TheBloke\\u002forca_...\"],[\"|      |                                                                            |[valurank\\u002ffinet...\"],[\"|      |                                                                            |[TheBloke\\u002fOpenC...\"],[\"|      |                                                                            |[valurank\\u002fdisti...\"],[\"|      |                                                                            |[TheBloke\\u002fZaraf...\"],[\"|      |                                                                            |[Mitsua\\u002fvroid-d...\"],[\"|      |                                                                            |[TheBloke\\u002fLLaMA...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[TheBloke\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002fZarab...\"],[\"|      |                                                                            |[coqui\\u002fXTTS-v1]...\"],[\"|      |                                                                            |[TheBloke\\u002fLLaMA...\"],[\"|      |                                                                            |[TheBloke\\u002fOpenA...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[dfurman\\u002fllama-...\"],[\"|      |                                                                            |[Neko-Institute...\"],[\"|      |                                                                            |[tawfikgh\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002fCodeF...\"],[\"|      |                                                                            |[deerslab\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002fLlama...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[ibm\\u002froberta-la...\"],[\"|      |                                                                            |[TheBloke\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002fOpenO...\"],[\"|      |                                                                            |[nenkoru\\u002fllama-...\"],[\"|      |                                                                            |[TheBloke\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[zohaib99k\\u002fNous...\"],[\"|      |                                                                            |[TheBloke\\u002fAirob...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[unoooo\\u002fllama-7...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[TheBloke\\u002forca_...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Jsevisal\\u002frober...\"],[\"|      |                                                                            |[nenkoru\\u002falpaca...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fOpenC...\"],[\"|      |                                                                            |[TheBloke\\u002fZarab...\"],[\"|      |                                                                            |[Cartinoe5930\\u002fo...\"],[\"|      |                                                                            |[Neko-Institute...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fChron...\"],[\"|      |                                                                            |[TheBloke\\u002fqCamm...\"],[\"|      |                                                                            |[4bit\\u002fRedmond-P...\"],[\"|      |                                                                            |[Agtian\\u002fllama-3...\"],[\"|      |                                                                            |[amayprro552\\u002fjs...\"],[\"|      |                                                                            |[aoyoo\\u002fllama-7b...\"],[\"|      |                                                                            |[baffo32\\u002fllama-...\"],[\"|      |                                                                            |[camelids\\u002falpac...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[coyotte508\\u002fber...\"],[\"|      |                                                                            |[decapoda-resea...\"],[\"|      |                                                                            |[deepsbn\\u002fllama-...\"],[\"|      |                                                                            |[fragro\\u002fllama-7...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Jsevisal\\u002fdisti...\"],[\"|      |                                                                            |[khachdallak\\u002fll...\"],[\"|      |                                                                            |[Mithilss\\u002fllama...\"],[\"|      |                                                                            |[nonlinearshima...\"],[\"|      |                                                                            |[prodm93\\u002fllama_...\"],[\"|      |                                                                            |[ruibin-wang\\u002fll...\"],[\"|      |                                                                            |[shekharchatter...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fAirob...\"],[\"|      |                                                                            |[TheBloke\\u002fHerme...\"],[\"|      |                                                                            |[TheBloke\\u002fLLaMA...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[TheBloke\\u002fPuddl...\"],[\"|      |                                                                            |[TheBloke\\u002ftulu-...\"],[\"|      |                                                                            |[TheBloke\\u002fZarab...\"],[\"|      |                                                                            |[Thireus\\u002fVicuna...\"],[\"|      |                                                                            |[usamakenway\\u002fll...\"],[\"|      |                                                                            |[Zhejian\\u002fllama-...\"],[\"ChatUI on Spaces\\n\\n**Hugging Chat** is an open-source interface enabling everyone to try open-source ...\"],[\"\\u003ca href=\\\"Parameters\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"Model Cards\\n\\n\\u003cTip\\u003e\\n\\n[New! Try our experimental Model Card Creator App](https:\\u002f\\u002fhuggingface.co\\u002fspaces...\"],[\"Dataset, metric, and language identifiers are those listed on the [Datasets](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"This UI will allow you to add key metadata to your model card and many of the fields will autocomple...\"],[\"```\\n\\nYou can find the detailed model card metadata specification \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhugging...\"],[\"```\\n\\nThis metadata will be used to display the base model on the model page. Users can also use this...\"],[\"```\\n\\n### Specifying a task (`pipeline_tag`)\\n\\nYou can specify the `pipeline_tag` in the model card me...\"],[\"```\\n\\nIf the license is not available via a URL you can link to a LICENSE stored in the model repo.\\n\\n...\"],[\"```yaml\\n---\\nmodel-index:\\n  - name: Yi-34B\\n    results:\\n      - task:\\n          type: text-generation...\"],[\"```\\n\\nFor more details on how to format this data, check out the [Model Card specifications](https:\\u002f\\u002f...\"],[\"### Can I add custom tags to my model?\\n\\nYes, you can add custom tags to your model by adding them to...\"],[\"Uploading datasets\\n\\nThe [Hub](https:\\u002f\\u002fhuggingface.co\\u002fdatasets) is home to an extensive collection of...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocument...\"],[\"You can also look at the [Dataset Card specifications](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fblob\\u002f...\"],[\"## Using other libraries\\n\\nSome libraries like [🤗 Datasets](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002finde...\"],[\"WebDataset\\n\\n[WebDataset](https:\\u002f\\u002fgithub.com\\u002fwebdataset\\u002fwebdataset) is a library to write I\\u002fO pipelin...\"],[\"Pull requests and Discussions\\n\\nHub Pull requests and Discussions allow users to do community contrib...\"],[\"## View\\n\\nThe Discussion page allows you to see the comments from different users. If it's a Pull Req...\"],[\"## Pin a Discussion \\u002f Pull Request\\n\\nIf you have write access to a repository, you can pin discussion...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"Once the comment has been edited, a new link will appear above the comment. This link shows the edit...\"],[\"Read also [moderation](.\\u002fmoderation) to see how to report an abusive comment.\\n\\n## Can I use Markdown...\"],[\"```\\n\\n### Draft mode\\n\\nDraft mode is the default status when opening a new Pull request from scratch i...\"],[\"Using fastai at Hugging Face\\n\\n`fastai` is an open-source Deep Learning library that leverages PyTorc...\"],[\"```\\n\\n\\nIf you want to see how to load a specific model, you can click `Use in fastai` and you will be...\"],[\"Using SpeechBrain at Hugging Face\\n\\n`speechbrain` is an open-source and all-in-one conversational too...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click `Use in speechbrain` and you wil...\"],[\"Model Card Guidebook \\n\\nModel cards are an important documentation and transparency framework for mac...\"],[\"Our work presents a view of where we think model cards stand right now and where they could go in th...\"],[\"Models Frequently Asked Questions\\n\\n## How can I see what dataset was used to train the model?\\n\\nIt's ...\"],[\"The Hugging Face Hub is also home to Spaces, which are interactive demos used to showcase models. If...\"],[\"## What if I have a different checkpoint of the model trained on a different dataset?\\n\\nBy convention...\"],[\"Using OpenCLIP at Hugging Face\\n\\n[OpenCLIP](https:\\u002f\\u002fgithub.com\\u002fmlfoundations\\u002fopen_clip) is an open-so...\"],[\"```\\n\\nOnce loaded, you can encode the image and text to do [zero-shot image classification](https:\\u002f\\u002fh...\"],[\"```\\n\\nIt outputs the probability of each possible class:\\n\\n```text\\nLabel probs: tensor([[0.0020, 0.003...\"],[\"Libraries\\n\\nThe Datasets Hub has support for several libraries in the Open Source ecosystem.\\nThanks t...\"],[\"The table below summarizes the supported libraries and their level of integration.\\n\\n| Library       ...\"],[\"Using Stanza at Hugging Face\\n\\n`stanza` is a collection of accurate and efficient tools for the lingu...\"],[\"Panel on Spaces\\n\\n[Panel](https:\\u002f\\u002fpanel.holoviz.org\\u002f) is an open-source Python library that lets you ...\"],[\"Once you have created the Space, it will start out in “Building” status, which will change to “Runni...\"],[\"### 3. requirements.txt\\n\\nThis file defines the required packages for our Panel app. When using Space...\"],[\"User Studies\\n## Model Card Audiences and Use Cases\\n\\nDuring our investigation into the landscape of m...\"],[\"* **Stakeholder Perspectives**\\n\\nAs different people, of varying technical backgrounds, could be coll...\"],[\"1) For those who have not created model cards before or who do not usually make a model card or any ...\"],[\"**Insights:**\\n\\n* While a few respondents really liked this format, most found it overwhelming or as ...\"],[\"[Checkout the Appendix](.\\u002fmodel-card-appendix)\\n\\n \\u003c\\u002fTip\\u003e\\n\\n\\nAcknowledgements\\n================\\n\\nWe want...\"],[\"Docker Spaces Examples\\n\\nWe gathered some example demos in the [Spaces Examples](https:\\u002f\\u002fhuggingface....\"],[\"Datasets\\n\\nThe Hugging Face Hub is home to a growing collection of datasets that span a variety of do...\"],[\"Using GPU Spaces\\n\\nYou can upgrade your Space to use a GPU accelerator using the _Settings_ button in...\"],[\"## Hardware Specs\\n\\nIn the following table, you can see the Specs for the different upgrade options.\\n...\"],[\"### PyTorch\\n\\nYou'll need to install a version of PyTorch compatible with the built-in CUDA drivers. ...\"],[\"```\\n--extra-index-url https:\\u002f\\u002fdownload.pytorch.org\\u002fwhl\\u002fcu113\\ntorch\\n```\\n\\nYou can verify whether the i...\"],[\"```\\n\\n### Tensorflow\\n\\nThe default `tensorflow` installation should recognize the CUDA device. Just ad...\"],[\"```\\n\\n## Billing\\n\\nBilling on Spaces is based on hardware usage and is computed by the minute: you get...\"],[\"If you want your Space never to deactivate or if you want to set a custom sleep time, you need to up...\"],[\"How to Add a Space to ArXiv\\n\\nDemos on Hugging Face Spaces allow a wide audience to try out state-of-...\"],[\"And that's it! Your Space should appear in the Demo tab next to the paper on ArXiv in a few minutes ...\"],[\"```py\\n    from transformers import LayoutLMForTokenClassification\\n    \\n    layoutlm_dummy = LayoutLM...\"],[\"```\\n\\n    *Note*: Here's an [overview on building demos on Hugging Face Spaces](.\\u002fspaces-overview) an...\"],[\"Datasets Overview\\n\\n## Datasets on the Hub\\n\\nThe Hugging Face Hub hosts a [large number of community-c...\"],[\"## Privacy\\n\\nSince datasets are repositories, you can [toggle their visibility between private and pu...\"],[\"Using AllenNLP at Hugging Face\\n\\n`allennlp` is a NLP library for developing state-of-the-art models o...\"],[\"```\\n\\nTo get a snippet such as this, you can click `Use in AllenNLP` at the top right,\\n\\n\\u003cdiv class=\\\"f...\"],[\"```\\n\\n| Argument                    \\t| Type         \\t| Description                                   ...\"],[\"The `push_to_hf` function has the same parameters as the bash script.\\n\\n```py\\nfrom allennlp.common.pu...\"],[\"```\\n\\nIn just a minute, you can get your model in the Hub, try it out directly in the browser, and sh...\"],[\"Webhook guide: Setup an automatic system to re-train a model when a dataset changes\\n\\n\\u003cTip\\u003e\\n\\nWebhooks...\"],[\"First, let's create a Webhook from your [settings]( https:\\u002f\\u002fhuggingface.co\\u002fsettings\\u002fwebhooks).\\n\\n- Se...\"],[\"1. It spawns a FastAPI app that will listen to HTTP `POST` requests on `\\u002fwebhook`:\\n\\n```python\\nfrom f...\"],[\"```\\n\\n2.  2. This route checks that the `X-Webhook-Secret` header is present and that its value is th...\"],[\"```\\n\\n3. The event's payload is encoded as JSON. Here, we'll be using pydantic models to parse the ev...\"],[\"```\\n\\n4. If the payload is valid, the next step is to create a project on AutoTrain, schedule a fine-...\"],[\"```\\n\\nVisit the link inside the comment to review the training cost estimate, and start fine-tuning t...\"],[\"```\\n\\n## Configure your Webhook to send events to your Space\\n\\nLast but not least, you'll need to conf...\"],[\"Downloading models\\n\\n## Integrated libraries\\n\\nIf a model on the Hub is tied to a [supported library](...\"],[\"```\\n\\n## Using Git\\n\\nSince all models on the Model Hub are Git repositories, you can clone the models ...\"],[\"Getting Started with Repositories\\n\\nThis beginner-friendly guide will help you get the basic skills y...\"],[\"```\\n\\n**The content in the Getting Started section of this document is also available as a video!**\\n\\n...\"],[\"3. Enter your model’s name. This will also be the name of the repository. \\n\\n4. Specify whether you w...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"### Uploading a file\\n\\nIf you choose _Upload file_ you'll be able to choose a local file to upload, a...\"],[\"```\\n\\nYou can clone over SSH with the following command:\\n```bash\\ngit clone git@hf.co:\\u003cyour-username\\u003e\\u002f...\"],[\"```\\n\\nAnd you're done! You can check your repository on Hugging Face with all the recently added file...\"],[\"You can click on an individual commit to see what changes that commit introduced:\\n\\n\\u003cdiv class=\\\"flex ...\"],[\"Embed your Space in another website\\n\\nOnce your Space is up and running you might wish to embed it in...\"],[\"```\\n\\nFor instance using the [NimaBoscarino\\u002fhotdog-gradio](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fNimaBoscarin...\"],[\"Tabby on Spaces\\n\\n[Tabby](https:\\u002f\\u002ftabby.tabbyml.com) is an open-source, self-hosted AI coding assista...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Your Tabby Space URL\\n\\nOnce Tabby is up and running, for a space link such as https:\\u002f\\u002fhug...\"],[\"![Code Completion](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fhub...\"],[\"Data files Configuration\\n\\nThere are no constraints on how to structure dataset repositories.\\n\\nHoweve...\"],[\"Single Sign-On (SSO)\\n\\n\\u003cTip warning={true}\\u003e\\nThis feature is part of the \\u003ca href=\\\"https:\\u002f\\u002fhuggingface....\"],[\"Models\\n\\nThe Hugging Face Hub hosts many models for a [variety of machine learning tasks](https:\\u002f\\u002fhug...\"],[\"Using Asteroid at Hugging Face\\n\\n`asteroid` is a Pytorch toolkit for audio source separation. It enab...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click `Use in Adapter Transformers` an...\"],[\"Models Download Stats\\n\\n## How are download stats generated for models?\\n\\nCounting the number of downl...\"],[\"```json\\n{\\n    \\\"adapter-transformers\\\": {\\n        filter: [\\n            {\\n                term: { path...\"],[\"{\\n                term: { path: \\\"cfg.json\\\" },\\n            },\\n        ],\\n    },\\n    \\\"paddlenlp\\\": {\\n  ...\"],[\"],\\n    },\\n    \\\"diffusers\\\": {\\n        \\u002f\\u002f\\u002f Filter out nested safetensors and pickle weights to avoid d...\"],[\"Using RL-Baselines3-Zoo at Hugging Face\\n\\n`rl-baselines3-zoo` is a training framework for Reinforceme...\"],[\"```\\n\\nYou can define three parameters:\\n- `--repo-name`: The name of the repo.\\n- `-orga`: Your Hugging...\"],[\"Using OpenCV in Spaces\\n\\nIn order to use OpenCV in your Gradio or Streamlit Spaces, you'll need to ma...\"],[\"Uploading models\\n\\nTo upload models to the Hub, you'll need to create an account at [Hugging Face](ht...\"],[\"1. In the \\\"Files and versions\\\" tab, select \\\"Add File\\\" and specify \\\"Upload File\\\":\\n\\n\\u003cdiv class=\\\"flex j...\"],[\"The UI allows you to explore the model files and commits and to see the diff introduced by each comm...\"],[\"Models trained with 🤗 Transformers will generate [TensorBoard traces](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftr...\"],[\"Digital Object Identifier (DOI)\\n\\nThe Hugging Face Hub offers the possibility to generate DOI for you...\"],[\"After you agree to those terms, your model or dataset will get a DOI assigned, and a new tag should ...\"],[\"## Why is there a 'locked by DOI' message on delete, rename and change visibility action on my model...\"],[\"Secrets Scanning\\n\\nIt is important to manage [your secrets (env variables) properly](.\\u002fspaces-overvie...\"],[\"Downloading datasets\\n\\n## Integrated libraries\\n\\nIf a dataset on the Hub is tied to a [supported libra...\"],[\"```py\\nfrom huggingface_hub import hf_hub_download\\nimport pandas as pd\\n\\nREPO_ID = \\\"YOUR_REPO_ID\\\"\\nFILE...\"],[\"```\\n\\n## Using Git\\n\\nSince all datasets on the Hub are Git repositories, you can clone the datasets lo...\"],[\"Handling Spaces Dependencies\\n\\n## Default dependencies\\n\\nThe default Spaces environment comes with sev...\"],[\"Debian dependencies are also supported. Add a **packages.txt** file at the root of your repository, ...\"],[\"Inference API\\n\\nPlease refer to [Inference API Documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fapi-inferen...\"],[\"Specify `inference: false` in your model card's metadata.\\n\\n## Why don't I see an inference widget or...\"],[\"Enterprise Hub\\n\\nEnterprise Hub adds advanced capabilities to organizations, enabling safe, compliant...\"],[\"How to configure SAML SSO with Okta\\n\\nIn this guide, we will use Okta as the SSO provider and with th...\"],[\"### Step 2: Configure your application on Okta\\n\\nOpen a new tab\\u002fwindow in your browser and navigate t...\"],[\"On Okta, set the following settings:\\n\\n* Set Audience URI (SP Entity Id) to match the \\\"SP Entity ID\\\" ...\"],[\"```\\n-----BEGIN CERTIFICATE-----\\n{certificate}\\n-----END CERTIFICATE-----\\n```\\n\\n\\u003cdiv class=\\\"flex justif...\"],[\"Using 🧨 `diffusers` at Hugging Face\\n\\nDiffusers is the go-to library for state-of-the-art pretrained ...\"],[\"## Using existing pipelines\\n\\nAll `diffusers` pipelines are a line away from being used! To run gener...\"],[\"```\\n\\nIf you want to load a specific pipeline component such as the UNet, you can do so by:\\n\\n```py\\nfr...\"],[\"Security\\n\\nThe Hugging Face Hub offers several security features to ensure that your code and data ar...\"],[\"Using SpanMarker at Hugging Face\\n\\n[SpanMarker](https:\\u002f\\u002fgithub.com\\u002ftomaarsen\\u002fSpanMarkerNER) is a fram...\"],[\"```\\n```json\\n[\\n    {\\\"span\\\": \\\"Amelia Earhart\\\", \\\"label\\\": \\\"person-other\\\", \\\"score\\\": 0.7629689574241638, \\\"...\"],[\"```\\n\\nIf you want to load a specific SpanMarker model, you can click `Use in SpanMarker` and you will...\"],[\"Disk usage on Spaces\\n\\nEvery Space comes with a small amount of disk storage. This disk space is ephe...\"],[\"\\u003cTip warning={true}\\u003e\\n\\tWARNING: all data stored in the storage is lost when you delete it.\\n\\u003c\\u002fTip\\u003e\\n\\n##...\"],[\"Visit the [`datasets` library](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002findex) documentation and the [`h...\"],[\"Hugging Face on Amazon SageMaker\\n\\n![cover](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation...\"],[\"- Cost-effective: Training instances are only live for the duration of your job. Once your job is co...\"],[\"**One command is all you need**\\n\\nWith the new Hugging Face DLCs, train cutting-edge Transformers-bas...\"],[\"Take a look at our published blog posts, videos, documentation, sample notebooks and scripts for add...\"],[\"### Documentation\\n\\n- [Run training on Amazon SageMaker](\\u002fdocs\\u002fsagemaker\\u002ftrain)\\n- [Deploy models to A...\"],[\"- [All notebooks](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002ftree\\u002fmaster\\u002fsagemaker)\\n- [Getting Started...\"],[\"- [Image Classification with Vision Transformer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002f...\"],[\"Hub API Endpoints\\n\\nWe have open endpoints that you can use to retrieve information from the Hub as w...\"],[\"### GET \\u002fapi\\u002fmodels\\n\\nGet information from all models in the Hub. The response is paginated, use the ...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.list_models()`.\\n\\n### GET \\u002fapi\\u002fmodels\\u002f{repo_id} or \\u002fapi\\u002fm...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.list_datasets()`.\\n\\n### GET \\u002fapi\\u002fdatasets\\u002f{repo_id} or \\u002fa...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.dataset_info(repo_id, revision)`.\\n\\n### GET \\u002fapi\\u002fdatasets...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.list_spaces()`.\\n\\n### GET \\u002fapi\\u002fspaces\\u002f{repo_id} or \\u002fapi\\u002fs...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.update_repo_visibility()`.\\n\\n### POST \\u002fapi\\u002frepos\\u002fmove\\n\\nMo...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.whoami()`.\\n\\n## Collections API\\n\\nUse Collections to group...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.create_collection()`.\\n\\n### GET \\u002fapi\\u002fcollections\\u002f{namespa...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.list_collections()`.\\n\\n### PATCH \\u002fapi\\u002fcollections\\u002f{namesp...\"],[\"```\\n\\nThis is equivalent to `huggingface_hub.add_collection_item()`.\\n\\n### PATCH \\u002fapi\\u002fcollections\\u002f{nam...\"],[\"Displaying carbon emissions for your model\\n\\n## Why is it beneficial to calculate the carbon emission...\"],[\"```\\n\\n## How is the carbon footprint of my model calculated? 🌎\\n\\nConsidering the computing hardware, l...\"],[\"Train and deploy Hugging Face on Amazon SageMaker\\n\\nThe get started guide will show you how to quickl...\"],[\"```\\n\\nIf you want to run this example in [SageMaker Studio](https:\\u002f\\u002fdocs.aws.amazon.com\\u002fsagemaker\\u002flat...\"],[\"```\\n\\n## Preprocess\\n\\nThe 🤗 Datasets library makes it easy to download and preprocess a dataset for tr...\"],[\"```\\n\\n## Upload dataset to S3 bucket\\n\\nNext, upload the preprocessed dataset to your S3 session bucket...\"],[\"```\\n\\n## Start a training job\\n\\nCreate a Hugging Face Estimator to handle end-to-end SageMaker trainin...\"],[\"```\\n\\nBegin training with one line of code:\\n\\n```python\\nhuggingface_estimator.fit({\\\"train\\\": training_i...\"],[\"Dataset Cards\\n\\n## What are Dataset Cards?\\n\\nEach dataset may be documented by the `README.md` file in...\"],[\"```\\n\\nThe metadata that you add to the dataset card enables certain interactions on the Hub. For exam...\"],[\"* Visit the Paper page\\n* Filter for other models on the Hub that cite the same paper.\\n\\n\\u003cdiv class=\\\"f...\"],[\"Using sample-factory at Hugging Face\\n\\n[`sample-factory`](https:\\u002f\\u002fgithub.com\\u002falex-petrenko\\u002fsample-fac...\"],[\"```\\ngit clone git@hf.co:\\u003cName of HuggingFace Repo\\u003e # example: git clone git@hf.co:bigscience\\u002fbloom\\n`...\"],[\"```\\npython -m sample_factory.huggingface.push_to_hub -r \\u003chf_username\\u003e\\u002f\\u003chf_repo_name\\u003e -d \\u003cexperiment_...\"],[\"```\\npython -m sf_examples.mujoco_examples.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=\\u003cre...\"],[\"hub-docs\\n\\nThis repository regroups documentation and information that is hosted on the Hugging Face ...\"],[\"Datasets Download Stats\\n\\n## How are download stats generated for datasets?\\n\\nThe Hub provides downloa...\"],[\"Using TensorBoard\\n\\nTensorBoard provides tooling for tracking and visualizing metrics as well as visu...\"],[\"Organizations, Security, and the Hub API\\n\\n## Contents\\n\\n- [Organizations](.\\u002forganizations)\\n  - [Manag...\"],[\"Using timm at Hugging Face\\n\\n`timm`, also known as [pytorch-image-models](https:\\u002f\\u002fgithub.com\\u002frwightma...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click **Use in timm** and you will be ...\"],[\"# Create Transform\\ntransform = create_transform(**resolve_data_config(model.pretrained_cfg, model=mo...\"],[\"```\\n\\nThis should leave you with a list of predictions, like this:\\n\\n```py\\n[\\n    {'label': 'american_p...\"],[\"```\\n\\nThen, push your model using the `push_to_hf_hub` method:\\n\\n```py\\nimport timm\\n\\n# Build or load a ...\"],[\"```\\n\\n## Inference Widget and API\\n\\nAll `timm` models on the Hub are automatically equipped with an [i...\"],[\"```\\n\\n## Additional resources\\n\\n* timm (pytorch-image-models) [GitHub Repo](https:\\u002f\\u002fgithub.com\\u002frwightm...\"],[\"Image Dataset\\n\\nThis guide will show you how to configure your dataset repository with image files. Y...\"],[\"```\\nmy_dataset_repository\\u002f\\n├── train\\n│   ├── 1.jpg\\n│   └── 2.jpg\\n└── test\\n    ├── 3.jpg\\n    └── 4.jp...\"],[\"```\\n\\n## Relative paths\\n\\nMetadata file must be located either in the same directory with the images i...\"],[\"```\\nmy_dataset_repository\\u002f\\n├── test\\n│   ├── green\\n│   │   └── 2.jpg\\n│   └── red\\n│       └── 4.jpg\\n└─...\"],[\"Deploy models to Amazon SageMaker\\n\\nDeploying a 🤗 Transformers models in SageMaker for inference is a...\"],[\"```\\n\\nThis guide will show you how to deploy models with zero-code using the [Inference Toolkit](http...\"],[\"```\\n\\n**SageMaker environment**\\n\\nSetup your SageMaker environment as shown below:\\n\\n```python\\nimport s...\"],[\"```\\n\\n## Deploy a 🤗 Transformers model trained in SageMaker\\n\\n\\u003ciframe width=\\\"700\\\" height=\\\"394\\\" src=\\\"ht...\"],[\"############ pseudo code end ############\\n\\n# deploy model to SageMaker Inference\\npredictor = hf_esti...\"],[\"```\\n\\nAfter you run your request you can delete the endpoint as shown:\\n\\n```python\\n# delete endpoint\\np...\"],[\"```\\n\\nAfter you run our request, you can delete the endpoint again with:\\n\\n```python\\n# delete endpoint...\"],[\"```\\n\\nNow you can provide the S3 URI to the `model_data` argument to deploy your model later.\\n\\n## Dep...\"],[\"# deploy model to SageMaker Inference\\npredictor = huggingface_model.deploy(\\n   initial_instance_coun...\"],[\"```\\n\\nAfter you run our request, you can delete the endpoint again with:\\n\\n```python\\n# delete endpoint...\"],[\"```\\n\\n📓 Open the [deploy_transformer_model_from_hf_hub.ipynb notebook](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"```python\\nbatch_job = huggingface_estimator.transformer(\\n    instance_count=1,\\n    instance_type='ml...\"],[\"```\\n\\nIf you want to run your batch transform job later or with a model from the 🤗 Hub, create a `Hug...\"],[\"```\\n\\n📓 Open the [sagemaker-notebook.ipynb notebook](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"```\\n\\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file ...\"],[\"Here is an example of a custom inference module with `model_fn`, `input_fn`, `predict_fn`, and `outp...\"],[\"```\\n\\nCustomize your inference module with only `model_fn` and `transform_fn`:   \\n\\n```python\\nfrom sag...\"],[\"Single Sign-On (SSO)\\n\\nThe Hugging Face Hub gives you the ability to implement mandatory Single Sign-...\"],[\"### Supported Identity Providers\\n\\nYou can easily integrate Hugging Face Hub with a variety of Identi...\"],[\"This section allows you to define a mapping from your IdP's user profile data from your IdP to the a...\"],[\"Spaces Configuration Reference\\n\\nSpaces are configured through the `YAML` block at the top of the **R...\"],[\"**`suggested_hardware`** : _string_  \\nSpecify the suggested [hardware](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fh...\"],[\"**`models`** : _List[string]_  \\nHF model IDs (like `gpt2` or `deepset\\u002froberta-base-squad2`) used in ...\"],[\"**`custom_headers`** : _Dict[string, string]_  \\nSet custom HTTP headers that will be added to all HT...\"],[\"```\\n\\n*Note:* all headers and values must be lowercase.\\n\\n**`preload_from_hub`**: _List[string]_\\nSpeci...\"],[\"Spaces Settings\\n\\nYou can configure your Space's appearance and other settings inside the `YAML` bloc...\"],[\"Your First Docker Space: Text Generation with T5\\n\\nIn the following sections, you'll learn the basics...\"],[\"```\\n\\n## Add the dependencies\\n\\nFor the **Text Generation** Space, we'll be building a FastAPI app tha...\"],[\"```\\n\\n## Create the Dockerfile\\n\\nThe main step for a Docker Space is creating a Dockerfile. You can re...\"],[\"```\\n\\nIf you have [Secrets](spaces-sdks-docker#secret-management) you can use `docker buildx` and pas...\"],[\"```\\n\\nLet's go through all the steps to make this working. We'll skip some of the details of the CSS ...\"],[\"```\\n\\n3. In the `app.py` file, mount the static files and show the html file in the root route\\n\\n```py...\"],[\"```\\n\\n5. Grant permissions to the right directories\\n\\nAs discussed in the [Permissions Section](.\\u002fspac...\"],[\"```\\n\\nSuccess! Your app should be working now! Check out [DockerTemplates\\u002ffastapi_t5](https:\\u002f\\u002fhugging...\"],[\"If everything went well, you will see `Pushing Image` and `Scheduling Space` on the **Build** tab\\n\\n\\u003c...\"],[\"Using Sentence Transformers at Hugging Face\\n\\n`sentence-transformers` is a library that provides easy...\"],[\"## Using existing models\\n\\nThe pre-trained models on the Hub can be loaded with a single line of code...\"],[\"```\\n\\nHere is an example that encodes sentences and then computes the distance between them for doing...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click `Use in sentence-transformers` a...\"],[\"```\\n\\nThis command creates a repository with an automatically generated model card, an inference widg...\"],[\"How to configure OIDC SSO with Okta\\n\\nIn this guide, we will use Okta as the SSO provider and with th...\"],[\"### Step 2: Configure your application in Okta\\n\\nOpen a new tab\\u002fwindow in your browser and navigate t...\"],[\"Save your new application.\\n\\n### Step 3: Finalize configuration on Hugging Face\\n\\nIn your Okta applica...\"],[\"A green check mark near the OIDC selector will attest that the test was successful.\\n\\n\\n\\u003cdiv class=\\\"fl...\"],[\"Repositories\\n\\nModels, Spaces, and Datasets are hosted on the Hugging Face Hub as [Git repositories](...\"],[\"Malware Scanning\\n\\nWe run every file of your repositories through a [malware scanner](https:\\u002f\\u002fwww.cla...\"],[\"If at least one file has a been scanned as unsafe, a message will warn the users:\\n\\n\\u003cdiv class=\\\"flex ...\"],[\"Managing Spaces with CircleCI Workflows\\n\\nYou can keep your app in sync with your GitHub repository w...\"],[\"```\\n\\nThen force push to sync everything for the first time:\\n\\n```bash\\ngit push --force space main\\n```...\"],[\"Docker Spaces\\n\\nSpaces accommodate custom [Docker containers](https:\\u002f\\u002fdocs.docker.com\\u002fget-started\\u002f) f...\"],[\"```\\n\\nInternally you could have as many open ports as you want. For instance, you can install Elastic...\"],[\"```\\n\\n```Dockerfile\\n# Expose the secret SECRET_EXAMPLE at buildtime and use its value as a Bearer tok...\"],[\"```\\n\\n\\u003cTip warning=\\\"{true}\\\"\\u003e\\nAlways specify the `--chown=user` with `ADD` and `COPY` to ensure the ne...\"],[\"```\\n(same goes for `ADD` command)\\n\\n\\n## Data Persistence\\n\\nThe data written on disk is lost whenever y...\"],[\"\\u003cTip warning=\\\"{true}\\\"\\u003e\\nDuring Docker buildtime, you don't have access to a GPU hardware. Therefore, ...\"],[\"Webhook guide: build a Discussion bot based on BLOOM\\n\\n\\u003cTip\\u003e\\n\\nWebhooks are now publicly available!\\n\\n\\u003c...\"],[\"## Create a Space that will react to your Webhook\\n\\nThe third step is actually to listen to the Webho...\"],[\"```\\n\\nHere, we listen to POST requests made to `\\u002f`, and then we check that the `X-Webhook-Secret` hea...\"],[\"```\\n\\nThis is the coolest part: we call the Inference API for the BLOOM model, prompting it with `PRO...\"],[\"Sign in with Hugging Face\\n\\nYou can use the HF OAuth \\u002f OpenID connect flow to create a **\\\"Sign in wit...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Currently supported scopes\\n\\nThe currently supported scopes are:\\n\\n- `openid`: Get the ID t...\"],[\"Check out [our badges](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fbadges#sign-in-with-hugging-face)...\"],[\"[![Sign in with Hugging Face](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fbadges\\u002fresolve\\u002fmain\\u002fsign-i...\"],[\"Using `Transformers.js` at Hugging Face\\n\\nTransformers.js is a JavaScript library for running 🤗 Trans...\"],[\"```\\n\\n\\u003c\\u002ftd\\u003e\\n\\u003c\\u002ftr\\u003e\\n\\u003c\\u002ftable\\u003e\\n\\n\\nYou can also use a different model by specifying the model id or path as...\"],[\"Using SetFit with Hugging Face\\n\\nSetFit is an efficient and prompt-free framework for few-shot fine-t...\"],[\"```\\npip install -U setfit\\n```\\n\\n## Using existing models\\n\\nAll `setfit` models can easily be loaded fr...\"],[\"Spaces\\n\\n[Hugging Face Spaces](https:\\u002f\\u002fhuggingface.co\\u002fspaces) offer a simple way to host ML demo apps...\"],[\"You'll also be able to upgrade your Space to run [on a GPU or other accelerated hardware](.\\u002fspaces-g...\"],[\"Using Adapter Transformers at Hugging Face\\n\\n`adapter-transformers` is a library that extends 🤗 `tran...\"],[\"```\\n\\nYou can also use `list_adapters` to find all Adapter Models programmatically\\n\\n```py\\nfrom transf...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click `Use in Adapter Transformers` an...\"],[\"```\\n\\nThis command creates a repository with an automatically generated model card and all necessary ...\"],[\"Using PEFT at Hugging Face\\n\\n🤗 [Parameter-Efficient Fine-Tuning (PEFT)](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fp...\"],[\"```\\n\\nOnce loaded, you can pass your inputs to the tokenizer to prepare them, and call `model.generat...\"],[\"```\\n\\nIf you want to load a specific PEFT model, you can click `Use in PEFT` in the model card and yo...\"],[\"Organization cards\\n\\nYou can create an organization card to help users learn more about what your org...\"],[\"Libraries\\n\\nThe Hub has support for dozens of libraries in the Open Source ecosystem. Thanks to the `...\"],[\"| Library                                                                     | Description         ...\"],[\"| [Diffusers](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers)                       | A modular toolbox fo...\"],[\"| [NeMo](https:\\u002f\\u002fgithub.com\\u002fNVIDIA\\u002fNeMo)                                      | Conversational AI to...\"],[\"| [Sentence Transformers](https:\\u002f\\u002fgithub.com\\u002fUKPLab\\u002fsentence-transformers)    | Compute dense vector...\"],[\"| [Transformers](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers)                 | State-of-the-art Nat...\"],[\"### How can I add a new library to the Inference API?\\n\\nIf you're interested in adding your library, ...\"],[\"Gated models\\n\\nTo give more control over how models are used, the Hub allows model authors to enable ...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdat...\"],[\"#### From the UI\\n\\nYou can review who has access to your gated model from its settings page by clicki...\"],[\"| Method | URI | Description | Headers | Payload\\n| ------ | --- | ----------- | -------  | -------  ...\"],[\"### Download access report\\n\\nYou can download a report of all access requests for a gated model with ...\"],[\"If you want to collect more user information, you can configure additional fields. This information ...\"],[\"```\\n\\n\\nIn some cases, you might also want to modify the text in the gate heading and the text in the ...\"],[\"```\\n\\n### Example use cases of programmatically managing access requests\\n\\nHere are a few interesting ...\"],[\"Requesting access can only be done from your browser. Go to the model on the Hub and you will be pro...\"],[\"```\\n\\nAlternatively, you can programmatically login using `login()` in a notebook or a script:\\n\\n```py...\"],[\"Gated datasets\\n\\nTo give more control over how datasets are used, the Hub allows datasets authors to ...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdat...\"],[\"### From the UI\\n\\nYou can review who has access to your gated dataset from its settings page by click...\"],[\"| Method | URI | Description | Headers | Payload\\n| ------ | --- | ----------- | -------  | -------  ...\"],[\"### Download access report\\n\\nYou can download a report of all access requests for a gated datasets wi...\"],[\"If you want to collect more user information, you can configure additional fields. This information ...\"],[\"```\\n\\n## Access gated datasets as a user\\n\\n\\nAs a user, if you want to use a gated dataset, you will ne...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Download files\\n\\nTo download files from a gated dataset you'll need to be authenticated. ...\"],[\"Custom Python Spaces\\n\\n\\u003cTip\\u003e\\n\\nSpaces now support arbitrary Dockerfiles so you can host any Python app...\"],[\"Paper Pages\\n\\nPaper pages allow people to find artifacts related to a paper such as models, datasets ...\"],[\"## Claiming authorship to a Paper\\n\\nThe Hub will attempt to automatically match paper to users based ...\"],[\"### Do you support ACL anthology?\\n\\nWe're starting with Arxiv as it accounts for 95% of the paper URL...\"],[\"DuckDB\\n\\n[DuckDB](https:\\u002f\\u002fgithub.com\\u002fduckdb\\u002fduckdb) is an in-process SQL [OLAP](https:\\u002f\\u002fen.wikipedia....\"],[\"```\\n\\nThis creates a file `data.parquet` in the dataset repository `username\\u002fmy_dataset` containing y...\"],[\"More ways to create Spaces\\n\\n## Duplicating a Space\\n\\nYou can duplicate a Space by clicking the three ...\"],[\"Shiny on Spaces\\n\\n[Shiny](https:\\u002f\\u002fshiny.posit.co\\u002f) is an open-source framework for building simple, b...\"],[\"_app.py_\\n\\nThis file defines your app's logic. To learn more about how to modify this file, see [the ...\"],[\"To deploy an R Shiny Space, click this button and fill out the space metadata. \\nThis will populate t...\"],[\"Using ML-Agents at Hugging Face\\n\\n`ml-agents` is an open-source toolkit that enables games and simula...\"],[\"```\\nmlagents-load-from-hf --repo-id=\\\"ThomasSimonini\\u002fMLAgents-Pyramids\\\" --local-dir=\\\".\\u002fdownloads\\\"\\n```...\"],[\"--\\n# Example metadata to be added to a model card.  \\n# Full model card template at https:\\u002f\\u002fgithub.co...\"],[\"# Optional. Add this if you want to encode your eval results in a structured way.\\nmodel-index:\\n- nam...\"],[\"value: {metric_value}       # Required. Example: 20.90\\n        name: {metric_name}         # Optiona...\"],[\"This markdown file contains the spec for the modelcard metadata regarding evaluation parameters. Whe...\"],[\"Using 🤗 `transformers` at Hugging Face\\n\\n🤗 `transformers` is a library maintained by Hugging Face and...\"],[\"You can find models for many different tasks:\\n\\n* Extracting the answer from a context ([question-ans...\"],[\"You can try out the models directly in the browser if you want to test them out without downloading ...\"],[\"```\\n\\nYou can also load a model from a specific version (based on commit hash, tag name, or branch) a...\"],[\"```\\n\\nThere is much more you can do, so we suggest to review the [Share a model](https:\\u002f\\u002fhuggingface....\"],[\"THE LANDSCAPE OF ML DOCUMENTATION TOOLS\\nThe development of the model cards framework in 2018 was ins...\"],[\"| **Stage of ML System Lifecycle** \\t|  **Tool**                                                     ...\"],[\"|:--------------------------------:\\t|---------------------------------------------------------------...\"],[\"-----------------\\t|---------------------------------------------------------------------------------...\"],[\"----------------------------------------\\t|----------------------------------------------------------...\"],[\"-----------------------------------\\t|...\"],[\"| DATA                             \\t| ***Datasheets*** [(Gebru et al., 2018)](https:\\u002f\\u002fwww.fatml.org\\u002f...\"],[\"| DATA                             \\t| ***Data Cards for NLP*** [(McMillan-Major et al., 2021)](https...\"],[\"| DATA                             \\t| ***CrowdWorkSheets***  [(Díaz et al., 2022)](https:\\u002f\\u002fhuggingfa...\"],[\"| MODELS AND METHODS               \\t| ***Method Cards*** [Adkins et al. (2022)](https:\\u002f\\u002fdl.acm.org\\u002fd...\"],[\"| SYSTEMS                          \\t| ***System Cards***  [Procope et al. (2022)](https:\\u002f\\u002fai.faceboo...\"],[\"| SYSTEMS                          \\t| ***ABOUT ML***  [Raji and Yang, (2019)](https:\\u002f\\u002fhuggingface.co...\"],[\"### DATA-FOCUSED DOCUMENTATION TOOLS\\n\\nSeveral proposed documentation tools focus on datasets used in...\"],[\"* Extending the concept of datasheets in the electronics industry, [Gebru et al. (2018)](https:\\u002f\\u002fwww...\"],[\"* [Hutchinson et al. (2021)](https:\\u002f\\u002fdl.acm.org\\u002fdoi\\u002fpdf\\u002f10.1145\\u002f3442188.3445918) describe the need f...\"],[\"### MODEL-AND-METHOD-FOCUSED DOCUMENTATION TOOLS\\n\\nAnother set of documentation tools can be thought ...\"],[\"* They envision the relationship between model cards and method cards, in part, by stating: “The sec...\"],[\"Rather than focusing on particular models, datasets, or methods, system-focused documentation tools ...\"],[\"## THE EVOLUTION OF MODEL CARDS\\n\\nSince the proposal for model cards by Mitchell et al. in 2018, mode...\"],[\"From our analysis of all the models on the hub, we noticed that the most downloads come from top 200...\"],[\"[^6]: See Appendix A.\\n\\n[^7]: See GSA \\u002f US Census Bureau Collaboration on Model Card Generator.\\n\\n[^8]...\"],[\"Using Keras at Hugging Face\\n\\n`keras` is an open-source machine learning library that uses a consiste...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click **Use in keras** and you will be...\"],[\"```py\\nfrom huggingface_hub import push_to_hub_keras\\n\\npush_to_hub_keras(model,\\n    \\\"your-username\\u002fyou...\"],[\"```\\nThe repository will host your TensorBoard traces like below.\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n...\"],[\"Widgets\\n\\n## What's a widget?\\n\\nMany model repos have a widget that allows anyone to run inferences di...\"],[\"For some libraries, such as 🤗  `Transformers`, the model type should be inferred automatically based...\"],[\"```\\n\\nYou can provide more than one example input. In the examples dropdown menu of the widget, they ...\"],[\"```\\n\\nNote that you can also include example files in your model repository and use\\nthem as:\\n\\n```yaml...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" width=\\\"450\\\" src=\\\"https:\\u002f\\u002fhuggi...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" width=\\\"450\\\" src=\\\"https:\\u002f\\u002fhuggi...\"],[\"Here are some links to examples:\\n\\n- `text-classification`, for instance [`roberta-large-mnli`](https...\"],[\"## How can I control my model's widget Inference API parameters?\\n\\nGenerally, the Inference API for a...\"],[\"```\\n\\nOr if you'd like to change the temperature for a summarization task in the widget:\\n\\n```yaml\\ninf...\"],[\"No-license models challenge\\n\\n## Context\\n\\nThe Hugging Face Hub hosts hundreds of thousands of public ...\"],[\"```\\n\\nOtherwise, the license is considered as `other`. In that case, we can set a custom name and a U...\"],[\"```\\n\\nThis challenge aims to improve the completeness of this metadata on the Hub, which will ultimat...\"],[\"For each model, the workflow looks like this:\\n1. Choose a model in the list below. We suggest focusi...\"],[\"6. Once done, open a PR on GitHub to update the table below. Once merged, this will count as a Hackt...\"],[\"## F.A.Q.\\n\\n### What if the model has 2 licenses?\\n\\nThis use case can happen when a model is finetuned...\"],[\"|status|pr_url|model_id                                                                             ...\"],[\"|------|------|-------------------------------------------------------------------------------------...\"],[\"----------------------------------------------------------------------|------------|--------|-------...\"],[\"|-------------------------|-------------------------------------------------------------------------...\"],[\"-----------------------------------------------|----------------------------------------------------...\"],[\"--------------------------------------------------------|-------------|...\"],[\"|opened|[here](https:\\u002f\\u002fhuggingface.co\\u002fNousResearch\\u002fLlama-2-13b-hf\\u002fdiscussions\\u002f5)|[NousResearch\\u002fLlama...\"],[\"|opened|[here](https:\\u002f\\u002fhuggingface.co\\u002fNousResearch\\u002fLlama-2-7b-hf\\u002fdiscussions\\u002f4)|[NousResearch\\u002fLlama-...\"],[\"|      |      |[meta-llama\\u002fLlama-2-13b-chat-hf](https:\\u002f\\u002fhuggingface.co\\u002fmeta-llama\\u002fLlama-2-13b-chat-h...\"],[\"|      |      |[meta-llama\\u002fLlama-2-70b-chat-hf](https:\\u002f\\u002fhuggingface.co\\u002fmeta-llama\\u002fLlama-2-70b-chat-h...\"],[\"|      |      |[vinai\\u002fphobert-base-v2](https:\\u002f\\u002fhuggingface.co\\u002fvinai\\u002fphobert-base-v2)                ...\"],[\"|      |      |[symanto\\u002fsn-xlm-roberta-base-snli-mnli-anli-xnli](https:\\u002f\\u002fhuggingface.co\\u002fsymanto\\u002fsn-x...\"],[\"|      |      |[aipicasso\\u002fpicasso-diffusion-1-1](https:\\u002f\\u002fhuggingface.co\\u002faipicasso\\u002fpicasso-diffusion-...\"],[\"|      |      |[huggyllama\\u002fllama-13b](https:\\u002f\\u002fhuggingface.co\\u002fhuggyllama\\u002fllama-13b)                  ...\"],[\"|      |      |[decapoda-research\\u002fllama-30b-hf](https:\\u002f\\u002fhuggingface.co\\u002fdecapoda-research\\u002fllama-30b-h...\"],[\"|      |      |[TheBloke\\u002fLlama-2-7B-Chat-GGML](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLlama-2-7B-Chat-GGML)...\"],[\"|      |      |[jphme\\u002fLlama-2-13b-chat-german](https:\\u002f\\u002fhuggingface.co\\u002fjphme\\u002fLlama-2-13b-chat-german)...\"],[\"|      |      |[TheBloke\\u002forca_mini_v3_7B-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002forca_mini_v3_7B-GPTQ)...\"],[\"|      |      |[camembert\\u002fcamembert-large](https:\\u002f\\u002fhuggingface.co\\u002fcamembert\\u002fcamembert-large)        ...\"],[\"|      |      |[stabilityai\\u002fjapanese-instructblip-alpha](https:\\u002f\\u002fhuggingface.co\\u002fstabilityai\\u002fjapanese...\"],[\"|      |      |[TheBloke\\u002fVicUnlocked-alpaca-65B-QLoRA-fp16](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fVicUnloc...\"],[\"|      |      |[TheBloke\\u002ftulu-7B-fp16](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002ftulu-7B-fp16)                ...\"],[\"|      |      |[shibing624\\u002fchinese-llama-plus-13b-hf](https:\\u002f\\u002fhuggingface.co\\u002fshibing624\\u002fchinese-llam...\"],[\"|      |      |[kuelumbus\\u002fpolyBERT](https:\\u002f\\u002fhuggingface.co\\u002fkuelumbus\\u002fpolyBERT)                      ...\"],[\"|      |      |[decapoda-research\\u002fllama-65b-hf](https:\\u002f\\u002fhuggingface.co\\u002fdecapoda-research\\u002fllama-65b-h...\"],[\"|      |      |[uklfr\\u002fgottbert-base](https:\\u002f\\u002fhuggingface.co\\u002fuklfr\\u002fgottbert-base)                    ...\"],[\"|      |      |[TheBloke\\u002fairoboros-l2-13B-gpt4-1.4.1-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fairoboros...\"],[\"|      |      |[THUDM\\u002fchatglm-6b-int4-qe](https:\\u002f\\u002fhuggingface.co\\u002fTHUDM\\u002fchatglm-6b-int4-qe)          ...\"],[\"|      |      |[liuhaotian\\u002fllava-llama-2-7b-chat-lightning-lora-preview](https:\\u002f\\u002fhuggingface.co\\u002fliuh...\"],[\"|      |      |[aipicasso\\u002fmanga-diffusion-poc](https:\\u002f\\u002fhuggingface.co\\u002faipicasso\\u002fmanga-diffusion-poc)...\"],[\"|      |      |[4bit\\u002fLlama-2-70b-chat-hf](https:\\u002f\\u002fhuggingface.co\\u002f4bit\\u002fLlama-2-70b-chat-hf)          ...\"],[\"|      |      |[aipicasso\\u002fcool-japan-diffusion-2-1-1-beta](https:\\u002f\\u002fhuggingface.co\\u002faipicasso\\u002fcool-jap...\"],[\"|      |      |[allenai\\u002fopen-instruct-stanford-alpaca-7b](https:\\u002f\\u002fhuggingface.co\\u002fallenai\\u002fopen-instru...\"],[\"|      |      |[THUDM\\u002fWebGLM-2B](https:\\u002f\\u002fhuggingface.co\\u002fTHUDM\\u002fWebGLM-2B)                            ...\"],[\"|      |      |[TheBloke\\u002fLlama-2-70B-Chat-GGML](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLlama-2-70B-Chat-GGM...\"],[\"|      |      |[TheBloke\\u002fAiroboros-L2-70B-GPT4-m2.0-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fAiroboros-...\"],[\"|      |      |[TheBloke\\u002fOpenChat_v3.2-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fOpenChat_v3.2-GPTQ)    ...\"],[\"|      |      |[valurank\\u002fdistilroberta-bias](https:\\u002f\\u002fhuggingface.co\\u002fvalurank\\u002fdistilroberta-bias)    ...\"],[\"|      |      |[TheBloke\\u002fKuchiki-L2-7B-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fKuchiki-L2-7B-GPTQ)    ...\"],[\"|      |      |[sschet\\u002fbert-base-uncased_clinical-ner](https:\\u002f\\u002fhuggingface.co\\u002fsschet\\u002fbert-base-uncas...\"],[\"|      |      |[allenai\\u002ftulu-7b](https:\\u002f\\u002fhuggingface.co\\u002fallenai\\u002ftulu-7b)                            ...\"],[\"|      |      |[TheBloke\\u002fairoboros-l2-70B-gpt4-1.4.1-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fairoboros...\"],[\"|      |      |[TheBloke\\u002fairoboros-l2-70B-GPT4-2.0-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fairoboros-l...\"],[\"|      |      |[valurank\\u002fen_readability](https:\\u002f\\u002fhuggingface.co\\u002fvalurank\\u002fen_readability)            ...\"],[\"|      |      |[Neko-Institute-of-Science\\u002fLLaMA-65B-HF](https:\\u002f\\u002fhuggingface.co\\u002fNeko-Institute-of-Sci...\"],[\"|      |      |[TheBloke\\u002fLLaMA-13b-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLLaMA-13b-GPTQ)            ...\"],[\"|      |      |[TheBloke\\u002fHermesLimaRP-L2-7B-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fHermesLimaRP-L2-7B...\"],[\"|      |      |[valurank\\u002ffinal_headline_generator](https:\\u002f\\u002fhuggingface.co\\u002fvalurank\\u002ffinal_headline_ge...\"],[\"|      |      |[TheBloke\\u002fllama2_7b_chat_uncensored-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fllama2_7b_c...\"],[\"|      |      |[bigcode\\u002foctogeex](https:\\u002f\\u002fhuggingface.co\\u002fbigcode\\u002foctogeex)                          ...\"],[\"|      |      |[TheBloke\\u002forca_mini_v3_7B-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002forca_mini_v3_7B-GGUF)...\"],[\"|      |      |[TheBloke\\u002fCodeFuse-CodeLlama-34B-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fCodeFuse-CodeL...\"],[\"|      |      |[TheBloke\\u002fZarafusionex-1.1-L2-7B-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fZarafusionex-1...\"],[\"|      |      |[michaelfeil\\u002fct2fast-Llama-2-7b-hf](https:\\u002f\\u002fhuggingface.co\\u002fmichaelfeil\\u002fct2fast-Llama-...\"],[\"|      |      |[valurank\\u002fdistilroberta-mbfc-bias](https:\\u002f\\u002fhuggingface.co\\u002fvalurank\\u002fdistilroberta-mbfc...\"],[\"|      |      |[TheBloke\\u002fairoboros-l2-13b-gpt4-m2.0-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fairoboros-...\"],[\"|      |      |[TheBloke\\u002fCodeFuse-CodeLlama-34B-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fCodeFuse-CodeL...\"],[\"|      |      |[RicardoLee\\u002fLlama2-chat-Chinese-50W](https:\\u002f\\u002fhuggingface.co\\u002fRicardoLee\\u002fLlama2-chat-Ch...\"],[\"|      |      |[valurank\\u002fen_pos_counter](https:\\u002f\\u002fhuggingface.co\\u002fvalurank\\u002fen_pos_counter)            ...\"],[\"|      |      |[turboderp\\u002fLlama2-70B-exl2](https:\\u002f\\u002fhuggingface.co\\u002fturboderp\\u002fLlama2-70B-exl2)        ...\"],[\"|      |      |[anonymous4chan\\u002fllama-2-70b](https:\\u002f\\u002fhuggingface.co\\u002fanonymous4chan\\u002fllama-2-70b)      ...\"],[\"|      |      |[localmodels\\u002fLlama-2-13B-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002flocalmodels\\u002fLlama-2-13B-GPTQ)  ...\"],[\"|      |      |[4bit\\u002fllama-13b-4bit-hf](https:\\u002f\\u002fhuggingface.co\\u002f4bit\\u002fllama-13b-4bit-hf)              ...\"],[\"|      |      |[TheBloke\\u002fLlama2-13B-MegaCode2-OASST-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLlama2-13B...\"],[\"|      |      |[gsaivinay\\u002fLlama-2-7b-Chat-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fgsaivinay\\u002fLlama-2-7b-Chat-GPT...\"],[\"|      |      |[valurank\\u002fxsum_headline_generator](https:\\u002f\\u002fhuggingface.co\\u002fvalurank\\u002fxsum_headline_gene...\"],[\"|      |      |[TheBloke\\u002fKimiko-7B-fp16](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fKimiko-7B-fp16)            ...\"],[\"|      |      |[TheBloke\\u002fOpenChat_v3.2-GGML](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fOpenChat_v3.2-GGML)    ...\"],[\"|      |      |[nenkoru\\u002falpaca-lora-7b-onnx-fp16-with-past](https:\\u002f\\u002fhuggingface.co\\u002fnenkoru\\u002falpaca-lo...\"],[\"|      |      |[TheBloke\\u002fairoboros-l2-13B-gpt4-1.4.1-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fairoboros...\"],[\"|      |      |[TheBloke\\u002fllama2-22B-daydreamer-v2-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fllama2-22B-d...\"],[\"|      |      |[TheBloke\\u002fTulu-30B-SuperHOT-8K-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fTulu-30B-SuperHO...\"],[\"|      |      |[AllanFrostin\\u002fanalise-morfossintatica-ptbr](https:\\u002f\\u002fhuggingface.co\\u002fAllanFrostin\\u002fanali...\"],[\"|      |      |[TheBloke\\u002fAiroboros-L2-70B-GPT4-m2.0-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fAiroboros-...\"],[\"|      |      |[usamakenway\\u002fllama2_7b_chat_uncensored-AutoGPTQ_Wizard_Vicuna](https:\\u002f\\u002fhuggingface.co...\"],[\"Manual Configuration\\n\\nThis guide will show you how to configure a custom structure for your dataset ...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nNote that `config_name` field is required even if you have a single confi...\"],[\"Static HTML Spaces\\n\\nSpaces also accommodate custom HTML for your app instead of using Streamlit or G...\"],[\"Tasks\\n\\n## What's a task?\\n\\nTasks, or pipeline types, describe the \\\"shape\\\" of each model's API (inputs...\"],[\"* 🤗 using a `transformers` model\\n* 🐳 using a model from an [officially supported library](.\\u002fmodels-l...\"],[\"**Adding a new task is relatively straightforward and requires 2 PRs:**\\n* PR 1: Add the new task to ...\"],[\"* PR 2: Add the new task to a library docker image. You should also add a template to [`docker_image...\"],[\"### Adding Community Inference API for a quick prototype\\n\\n**My model is not supported by any library...\"],[\"### Widget\\n\\nOnce the task is in production, what could be more exciting than implementing some way f...\"],[\"Collections\\n\\nUse Collections to group repositories from the Hub (Models, Datasets, Spaces and Papers...\"],[\"![Add items to collections](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve...\"],[\"### Deleting items from a collection\\n\\nTo delete an item from a collection, click the trash icon in t...\"],[\"![Collection image drop zone with images](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-...\"],[\"Run training on Amazon SageMaker\\n\\n\\u003ciframe width=\\\"700\\\" height=\\\"394\\\" src=\\\"https:\\u002f\\u002fwww.youtube.com\\u002fembe...\"],[\"To start training locally, you need to setup an appropriate [IAM role](https:\\u002f\\u002fdocs.aws.amazon.com\\u002fs...\"],[\"```\\n\\n## Prepare a 🤗 Transformers fine-tuning script\\n\\nOur training script is very similar to a traini...\"],[\"# data, model, and output directories\\n    parser.add_argument(\\\"--model-dir\\\", type=str, default=os.en...\"],[\"```\\n\\n_Note that SageMaker doesn’t support argparse actions. For example, if you want to use a boolea...\"],[\"In addition to the options already mentioned above, there is another option to save the training art...\"],[\"1. `entry_point` specifies which fine-tuning script to use.\\n2. `instance_type` specifies an Amazon i...\"],[\"```\\n\\nIf you are running a `TrainingJob` locally, define `instance_type='local'` or `instance_type='l...\"],[\"```\\n\\n## Access trained model\\n\\nOnce training is complete, you can access your model through the [AWS ...\"],[\"```\\n\\n📓 Open the [sagemaker-notebook.ipynb notebook](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"```\\n\\n📓 Open the [sagemaker-notebook.ipynb notebook](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"# create the Estimator\\nhuggingface_estimator = HuggingFace(\\n        entry_point='train.py',\\n        ...\"],[\"```\\n\\n📓 Open the [sagemaker-notebook.ipynb notebook](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"```\\n\\n## SageMaker metrics\\n\\n[SageMaker metrics](https:\\u002f\\u002fdocs.aws.amazon.com\\u002fsagemaker\\u002flatest\\u002fdg\\u002ftrain...\"],[\"Managing organizations\\n\\n## Creating an organization\\n\\nVisit the [New Organization](https:\\u002f\\u002fhf.co\\u002forga...\"],[\"Pickle Scanning\\n\\nPickle is a widely used serialization format in ML. Most notably, it is the default...\"],[\"```\\n\\nWhen you run this, it will create a pickle file and print the following instructions in your te...\"],[\"```\\n\\nWhen we run this script we get the `payload.pkl` again. When we check the file’s contents:\\n\\n```...\"],[\"```\\n\\nHere we’re using the [fickling](https:\\u002f\\u002fgithub.com\\u002ftrailofbits\\u002ffickling) library for simplicity...\"],[\"```\\n\\nBasically, this is what’s happening when you unpickle:\\n\\n```python\\n# ...\\nopcodes_stack = [exec_f...\"],[\"```\\n\\nThe instructions that pose a threat are `STACK_GLOBAL`, `GLOBAL` and `REDUCE`.\\n\\n`REDUCE` is wha...\"],[\"```\\n\\n### Use your own serialization format\\n\\n- [MsgPack](https:\\u002f\\u002fmsgpack.org\\u002findex.html)\\n- [Protobuf]...\"],[\"On the hub the list of imports will be displayed next to each file containing imports. If any import...\"],[\"Thankfully, there is always a trace of the `eval` import, so reading the opcodes directly should all...\"],[\"[CTFtime.org \\u002f Balsn CTF 2019 \\u002f pyshv1 \\u002f Writeup](https:\\u002f\\u002fctftime.org\\u002fwriteup\\u002f16723)\\n\\n[Rehabilitatin...\"],[\"Moderation\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [Code of Conduct](https:\\u002f\\u002fhuggingface.co\\u002fcode-of-conduct) and the [...\"],[\"Livebook on Spaces\\n\\n**Livebook** is an open-source tool for writing interactive code notebooks in [E...\"],[\"Then:\\n\\n1. Give your Space a name\\n2. Set the password of your Livebook\\n3. Set its visibility to publi...\"],[\"## Livebook integration with Hugging Face Models\\n\\nLivebook has an [official integration with Hugging...\"]],\"hovertemplate\":\"source=hub-docs\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"hub-docs, circle\",\"marker\":{\"color\":\"#19d3f3\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"hub-docs, circle\",\"showlegend\":true,\"x\":[5.066542,5.0992103,-5.2086153,4.349569,-2.786319,6.292923,6.6757083,-7.4388876,-7.625057,6.271134,1.1984531,3.007504,7.0283637,7.033154,6.578015,6.2508454,5.328027,5.3053107,4.735563,5.2441063,7.6588764,7.5314627,7.2690973,6.853216,7.1633396,6.3361354,6.859449,6.298963,5.373083,5.654555,3.7113538,2.8542707,2.1490917,2.4060123,5.779205,4.807424,5.1331897,4.9598083,4.8038764,3.6020103,3.4301922,5.595127,5.4815035,5.6108522,7.4899673,7.817667,4.592811,2.149004,7.325164,7.271906,7.6020412,5.7981787,5.1674495,5.1497855,4.8805656,4.8927755,7.589797,7.353213,-5.4195595,3.4259748,4.1424317,3.834936,4.615194,3.671621,5.78555,8.092724,7.2068315,-1.0761871,5.7461877,7.262572,6.760507,7.211416,6.9655104,4.147614,6.2655,4.322935,5.498925,4.7028427,9.858036,2.5233612,3.0162442,-3.619701,-2.8200476,-2.3357856,-1.6826252,-3.5116541,-1.870186,-1.7613938,-2.7176428,-5.439579,-3.6740465,-2.9753935,-3.2572188,4.0006742,-7.164824,-4.4267683,-4.913964,5.280099,3.4961054,3.493469,-4.7853947,-4.806402,-5.112479,4.661966,-4.7877975,-5.368922,5.4469614,10.785919,10.773285,-5.725789,10.767876,10.764881,10.8119135,10.7645855,10.852051,10.768162,10.755564,10.760694,10.743035,10.796464,10.75617,10.752717,10.759125,10.762356,10.804998,10.760115,10.760608,10.756832,10.763828,10.76082,10.758327,10.757071,10.759476,10.818774,10.823964,10.799022,10.863228,10.90249,10.909684,10.864045,10.913797,10.929242,10.858561,10.829233,10.771875,10.765798,10.733456,10.751996,10.765277,10.76926,10.760062,10.74921,10.745789,10.770557,10.763443,10.759778,10.750167,10.748296,10.761487,10.767816,10.762159,10.755195,10.747148,10.747631,10.755067,10.756338,10.747113,10.756172,10.754514,10.763343,4.3523254,4.8308997,4.3175287,5.798901,4.8764186,3.8401597,3.7122817,3.4011118,2.6946177,4.4304957,5.193879,5.7422857,5.5838027,5.5417085,5.128914,7.5488772,6.8173337,6.2292867,6.245859,6.443078,5.7542768,5.726716,4.652891,5.627399,5.1790624,5.5281916,5.821601,5.8237753,3.4965434,3.632894,3.2851362,3.8548076,5.1063237,5.0293612,4.998281,8.104103,14.819819,15.354792,16.077131,5.542211,5.8307257,6.0916657,6.0088606,12.975746,5.8853817,4.4032025,7.5331,7.3692045,5.8316245,7.495557,7.4272666,5.53422,6.9936595,7.5458155,7.4303427,6.1877875,5.4806647,7.1048083,7.5891075,7.23873,5.9392004,8.356203,7.2593617,7.492182,7.3387723,6.024992,6.257125,4.3877892,3.9491615,4.302953,5.173307,-7.3182425,-7.286978,3.279317,3.7499664,3.467632,2.4313238,-5.0714192,-2.9777594,-2.4994402,-7.3129826,3.174954,5.1328297,3.316083,3.509077,3.4336107,3.407896,-9.193463,3.3889763,-3.3908052,3.4995818,5.8484583,5.761652,5.641096,5.883574,5.938926,5.9057555,5.9333606,11.244227,9.62507,11.180079,11.24878,7.593182,7.6124034,6.77092,6.3195515,4.4661665,3.6270323,3.1606736,0.90966827,-1.7886381,-6.9643197,3.3969123,3.3001614,2.9848945,2.7464936,4.34104,4.3386087,4.947435,4.4000616,3.5309396,4.5597105,4.484281,4.124941,4.5840154,7.8399982,7.5285163,4.005275,7.4352283,5.8412743,6.0093727,5.902453,5.843609,5.6446366,12.920512,19.544748,19.541239,19.580656,19.581896,19.580473,19.582005,19.511375,12.896236,12.895165,12.89884,12.41734,12.902615,12.919585,12.896906,12.889108,12.893727,12.917327,12.943091,12.906178,12.909256,12.589924,12.40218,12.431562,12.375553,12.947187,12.895403,12.408059,12.930141,12.906707,12.899439,12.908791,12.891776,12.273462,12.913183,12.89333,12.919965,12.905724,12.892738,12.901703,12.872471,12.445237,12.897032,12.90359,12.912789,12.906331,12.89221,12.914106,12.910474,12.902709,12.920813,12.9019,12.908185,12.875661,12.913769,12.897508,12.911376,12.902548,13.000611,12.9102335,12.938393,12.904518,12.912644,12.903217,12.900232,12.878032,12.9673195,12.894588,12.902656,12.909509,12.832555,12.974892,12.913972,12.919849,12.911387,12.876748,12.902072,12.904811,12.917289,12.920014,12.91882,12.920797,12.910003,12.909957,12.899142,12.906335,12.917312,12.905959,12.903462,12.876198,12.910452,12.884144,12.906154,12.910784,12.905999,12.909053,12.908227,12.901801,12.867227,12.964571,12.95375,12.908311,12.909668,12.904411,12.915812,12.9124975,12.908556,12.9033375,12.8779335,12.885952,-4.65061,7.5819902,3.6211605,3.7876754,3.8153114,3.4144316,3.7009144,4.5339093,5.862561,-1.0311688,4.1878953,4.0609636,3.857428,4.116852,3.7095158,3.609256,3.1981454,5.452644,5.581989,5.752501,5.6215386,5.6620474,5.4986944,5.104867,2.8152394,2.9092457,-7.348983,3.169429,3.6486018,3.6276333,3.6895697,4.4793386,3.3736198,3.8604555,0.07445186,-0.63752866,3.8266718,3.47418,-5.1979747,7.6353374,7.537195,6.517739,3.57879,3.518262,3.534471,3.5779183,3.8970828,7.853852,3.5283186,7.5100284,-2.6217887,1.3375399,0.83009857,0.5668293,7.3178024,7.4783883,7.7746587,5.007155,-0.25419623,9.540321,3.4916005,4.879709,-4.844795,3.682897,3.9076428,3.4246743,3.63502,-3.1248608,5.8607583,5.630328,5.68967,5.237725,4.0649347,-3.26621,5.8848863,2.9757955,4.545493,4.7475343,4.126173,4.1468453,4.9784107,5.0892878,4.7657814,4.861303,5.402432,7.696854,9.879441,7.506215,7.161624,6.30036,3.1242616,5.8398237,3.9693987,3.9661863,3.0636349,1.664466,4.728491,-0.4182776,6.828547,-7.062899,4.2462673,11.501901,4.023649,4.6469607,3.493612,2.3681643,4.433514,4.920848,4.867349,5.747479,3.418614,3.4724548,4.423264,7.245315,2.6255338,4.711168,4.832389,5.3051586,5.99962,5.874366,5.9768753,5.8276815,-0.3751951,0.7556827,1.2223231,5.5956845,-4.968613,3.9409418,5.2500143,6.7671013,6.006305,3.839201,-4.1326056,-4.506948,-4.1420574,-3.8514307,-3.723904,-1.880119,-3.1837695,5.0720305,4.6504593,4.445845,3.8639493,4.220859,5.26872,5.3479815,4.501532,4.51976,4.625612,4.7403774,-1.7457443,-2.3285856,-3.4277477,1.0419397,0.23495895,2.7669804,0.053082276,-0.16373247,3.8955753,3.624788,4.9197016,-6.349359,3.976683,2.949398,3.011073,5.270487,3.3468537,1.9562474,5.4377155,2.0458646,2.371447,0.22319663,2.3474135,1.9082304,2.7995148,1.5470312,3.1200736,2.6115332,2.6228666,2.5738912,-1.2696748,-1.3737253,1.4971279,-1.6972487,-0.5324276,0.5276534,2.2544296,-0.022942336,-1.189551,2.6876216,-2.2982173,0.56799144,-1.5725534,-2.197143,0.114752226,-0.6041862,-1.1480162,5.7107234,5.6154246,5.416063,8.363197,7.6396966,7.1834464,7.2916636,4.7239304,8.189048,7.585256,3.1560383,7.2962837,5.575539,6.258187,7.255621,6.5396447,7.5677,7.537553,-4.7916317,-1.8015463,-2.2835276,3.0168118,-3.3319888,5.9661436,5.863972,5.9531803,5.857141,5.092053,5.5051017,5.4123034,5.6428585,4.9872985,8.233617,6.175738,5.0386677,4.8277707,6.810082,1.7349789,5.860077,6.013849,5.745092,5.7229214,5.8641987,5.8357797,6.0425334,6.242447,-4.959344,-0.8371176,-5.0068173,1.5502527,7.8131585,7.472254,-3.6751916,-1.5526022,2.9045143,3.304645,-0.2765779,-1.923355,2.6897383,4.8505473,4.4483333,-5.587865,-5.8302975,-5.948916,-6.2199287,-5.065007,4.4401097,4.7730174,5.2349377,5.253279,5.491769,5.037364,4.5491543,5.981238,5.2733545,4.5576415,4.5259123,4.8563266,5.167318,5.050807,5.0303726,4.873577,4.349652,4.8643804,4.0602303,8.50367,4.712641,5.528,5.5079103,3.6637344,3.4804103,8.706031,7.612037,6.9426026,7.5594196,-6.8200564,3.4507785,3.7612386,0.34950867,0.8742805,5.831761,-4.66841,-4.962853,2.6889892,2.573004,-3.300495,3.4768708,0.8768977,19.520027,19.58855,19.582228,19.511143,2.8360922,3.3842413,3.2488382,3.3001657,-6.7658587,-7.149204,3.0849726,2.9855204,3.388574,3.3544128,3.3990135,3.3387883,3.5656419,3.6743617,4.2174993,2.4108794,2.873987,2.980373,2.1372871,-3.349137,-0.68574816,5.494904,-2.272569,4.345932,4.867231,-4.9551473,4.699165,4.5003653,5.828916,5.744753,5.9252543,5.794382,5.2377906,5.8901587,12.899613,19.543447,19.580517,19.537401,19.582342,19.520016,12.863054,12.899289,12.889854,12.899405,12.8896475,12.907319,12.424775,12.908362,12.905365,12.898378,12.895044,12.9053755,12.921252,12.913688,12.903588,12.915554,12.894676,12.948886,12.908103,12.909584,12.923492,12.457373,12.733959,12.42112,12.356807,12.412081,12.907409,12.409701,12.400987,12.949534,12.89502,12.906518,12.9004545,12.901671,12.908496,12.88987,12.927431,12.905329,12.90161,12.898063,12.906442,12.911517,12.862961,12.918966,12.8959255,12.898113,12.867253,12.906931,12.917303,12.9963,12.901207,12.870151,12.907689,12.907894,12.917013,12.862867,12.915059,12.9130535,12.93615,12.887826,12.919885,12.897043,12.911788,12.963957,12.915388,12.915625,12.909663,12.996558,12.900504,3.2621624,3.0114596,7.8127317,4.5778327,3.8577754,4.473864,4.2741666,4.645772,5.1468368,4.4556665,4.6654425,4.729745,5.838209,-0.21620293,1.0955385,0.188521,1.5630815,0.22362003,-0.19918768,0.21436329,0.4297487,-0.23227403,-0.34449294,-0.08290206,-0.068406075,0.08614146,0.765696,5.289161,2.031862,2.0268466,2.1403644,2.2393234,1.8794224,1.6877196,2.0217032,4.2518,2.0499687,1.363569,5.7294145,7.4088697,7.266203,-4.7005415],\"xaxis\":\"x\",\"y\":[-0.5993433,-0.69821966,-0.8460522,-2.0000362,-3.0513644,-5.085615,-4.308991,-6.3271594,-5.6343403,-4.6232476,-4.721272,-3.8538167,-2.3004885,-2.3061423,-1.5675459,-0.9559753,-0.8648517,-0.8159005,-0.64356935,-2.2492836,-2.7950292,-2.7238717,-2.597244,-2.4215817,-2.575692,-2.226314,-2.2968621,-3.1036525,-1.1887351,-2.1015272,-2.9935257,-3.7113733,-4.1126485,-4.0090733,-3.1908882,-1.3547983,-0.9857968,-1.3470768,-1.1428466,-3.332997,-3.0280175,-1.2277919,0.22557786,0.5680051,-2.7681026,-2.8428564,0.21178079,0.95935965,-2.6698425,-4.2623725,-3.7952802,-3.8637414,-0.75636524,-0.6631044,-0.62640315,-0.59963673,-2.6910536,-3.1189063,0.08528055,-0.6181676,-0.27847052,-1.2743572,-0.34232116,-0.33275276,-1.7211206,-2.9865,-2.4031634,2.9338205,-3.6641946,-2.4297702,-2.2422671,-2.3657684,-2.2803338,-1.7202746,-3.1498265,-1.8026993,-2.7943926,-1.0369872,-3.649919,-0.039011817,-0.60201705,1.8961977,0.92481315,0.46120927,1.1241643,1.8908905,1.6244304,1.1461543,1.6913373,0.29698002,1.780672,1.76112,1.6394666,1.1162726,-4.842426,-2.6716933,1.9922779,-0.8702736,-3.5566065,-3.0488892,-7.745644,-7.6895127,-7.4788933,-2.7005937,-7.709124,-6.857311,-1.72431,17.968998,17.988323,-0.012206688,17.992657,17.997255,17.952456,17.997036,17.914455,17.991665,18.004663,17.998907,18.017195,17.967598,18.003391,18.00607,18.001345,17.998554,17.953405,18.001495,17.99864,18.005638,17.996635,17.998417,18.00243,18.004202,18.001062,17.945608,17.94464,17.967428,17.907742,17.87346,17.867378,17.909042,17.862762,17.845835,17.912754,17.939587,17.990328,17.996208,18.025097,18.008894,17.996708,17.994583,18.002426,18.013771,18.01391,17.992088,17.999563,18.001753,18.010063,18.011263,17.99969,17.99464,17.999714,18.009596,18.0126,18.015749,18.009771,18.010227,18.017414,18.01065,18.008488,17.993576,-0.76449984,-1.577728,-1.8227941,-1.4326812,-1.1826015,-2.4067461,-2.4503982,-1.934568,-3.4080074,-2.8556275,-2.5800323,-2.2035193,-2.3759751,-2.397975,-3.4143093,-2.7692235,-2.5684686,-2.5462055,-2.4383147,-2.6661334,-2.2632725,-2.0892978,-4.191304,-2.1639776,-2.4199307,-1.7849131,-2.3134162,-2.2198267,-3.5382152,-3.9961884,-3.8594763,-0.74475,-0.5707257,-0.48101437,-0.5321446,-2.8868093,-3.5716045,-4.1101875,-2.3161554,-1.8486149,-3.389773,-3.7894092,-3.7445314,15.93576,-3.5855472,-0.8979528,-4.844856,-5.3420887,-3.0209463,-4.8470087,-5.3721895,-2.9284937,-4.3103676,-4.881232,-5.3980927,-5.8837476,-3.9973786,-4.5553436,-4.8317246,-5.4862213,-6.2999253,-3.1833885,-4.3587003,-4.7589836,-5.2521734,-3.052126,-1.1766943,-0.96891886,-1.5599511,-3.2225685,-1.1843532,-5.909223,-6.8042917,-0.726139,-1.6855537,-1.7775427,-1.734864,0.93481064,0.10090569,1.6309714,2.8623524,-2.3875473,-1.0793529,-2.7520266,-2.0109847,-2.0588124,-2.0638916,-0.344137,-1.8490633,0.14226228,-1.9371467,-1.928143,-1.9562927,-2.186506,-0.20795943,-0.3500218,-0.21631712,-0.35582164,-4.8067904,-3.6304686,-5.2914567,-4.7903495,-2.7683356,-2.7829,-2.3877337,-2.428861,-2.637389,-3.071937,-3.0724974,-3.8089933,-1.8787813,6.299972,0.10029687,-3.411051,-3.7157493,-3.927564,-0.88884264,-0.67297953,-0.7692042,-1.1162322,-0.70877075,-0.61546296,-0.090326354,0.2624175,0.25362766,-2.7117367,-2.8160322,-1.0738754,-2.6305568,-3.1459749,-2.688309,-3.339719,-3.1134021,-3.2271183,15.953433,8.170126,8.167456,8.223165,8.226158,8.22299,8.225317,8.116357,16.019655,16.014606,16.013548,16.466372,16.009634,15.995521,16.015074,16.022223,16.016937,15.998118,15.970846,16.004843,16.002886,16.30214,16.481365,16.454,16.507477,15.968799,16.017904,16.475443,15.986295,15.999771,16.01216,15.996817,16.017845,16.60006,16.001366,16.016165,15.992495,16.003252,16.014565,16.008772,16.034407,16.441126,16.016304,16.008684,16.000998,16.00593,16.0216,15.99628,16.001972,16.00803,15.993648,16.000252,16.00719,16.034729,15.998643,16.014591,16.009306,16.01006,15.9218645,16.003428,15.977781,16.00947,15.99175,15.994397,16.00023,16.030128,15.952608,16.015884,16.004776,15.99358,16.072412,15.947611,16.00064,15.996015,16.003822,16.031378,16.00707,16.007004,15.991195,15.991354,15.993876,15.99345,16.002378,16.003347,16.011534,16.000914,15.994408,16.004637,16.01425,16.018072,15.987717,16.01052,15.995609,16.00124,16.0047,16.003124,16.003452,16.009027,16.040173,15.956202,15.967146,16.00328,16.003046,16.008247,15.997934,15.996808,16.002306,16.007639,16.03523,16.02124,1.3428836,-2.6490474,-1.7858976,-2.193048,-2.1337821,-1.7257525,-1.4825226,-2.8302488,-3.0035644,-4.346413,-1.6233827,-2.1827948,-2.824256,-2.7538874,-3.0768232,-2.860189,-3.5082858,-1.6855066,-1.839486,-2.0666833,-1.9969654,-2.3067589,-1.8610828,-0.6859458,-0.43756378,-0.4525042,-6.44876,-0.9358302,-1.9334792,-1.9135271,-1.3301159,-1.3145988,-1.1608487,-0.7041186,-2.403383,-2.2017853,-2.688952,-3.8356874,-0.55733174,-2.7299473,-2.658949,-3.0767527,-1.9751879,-1.8040073,-1.957158,-1.961631,-2.1548612,-2.8095553,-3.1908782,-2.4794843,2.7407892,1.1446086,1.1046821,1.2999128,-2.4880886,-2.549062,-2.774173,-1.8928689,-1.7011025,-3.6651614,-3.093902,-2.4507174,-0.38200793,-0.959354,-0.81056124,-0.94851625,-1.0517901,1.6945012,-2.3162913,-2.3302941,-2.2389064,-2.4496262,-2.4015956,1.5866925,-2.244098,-0.40396014,-0.5514772,-0.59445107,-0.7096221,-1.2195097,-1.3380069,-1.1679757,-0.66630566,-1.3102156,-1.8129443,-3.0465326,-3.847044,-2.7620406,-2.7126744,-2.6450987,-3.8239174,-1.2287636,-1.1146183,-0.88567245,-0.57321006,-0.3197776,-4.3176928,-2.2212472,-3.8793318,6.1713696,-0.5893663,-5.3398805,-0.6580695,-1.4628544,-1.2002753,-0.20529388,-1.8788385,-2.2862663,-2.5494273,-2.107472,-3.217354,-3.1289232,-1.6949602,-2.45477,0.4657871,0.94038576,0.71249825,-1.2306386,-0.07394106,-0.25917748,-0.19427653,-0.33769315,7.1829815,7.3000345,6.90236,-1.13115,-1.0403805,-4.890577,-2.3222651,-2.3893514,-2.2801092,-3.0838537,2.0296164,2.3069196,1.9008274,1.6968863,1.80388,1.7632482,1.7442607,-2.1832218,-2.2634766,-2.4606607,-3.7381048,-3.1849475,-2.0543063,-1.6123945,-2.4077678,-2.8407378,-2.4239614,-2.4578764,1.7844487,2.9599535,1.7878886,1.0120832,-3.8727612,-2.799042,1.2998968,1.1036881,-2.7505283,-2.9886894,-1.6576079,5.955181,-0.46606347,-0.19573636,-0.8675863,-0.61933464,-3.0700583,-0.19112866,-1.696045,-0.8405363,-0.692496,-2.43874,-0.59629446,-0.12577932,-0.76992065,-1.4488056,-3.4621842,-3.5622203,-3.7070937,-3.4871244,1.4041549,1.4243159,0.81629103,1.6441537,1.0085531,0.9075003,0.50209385,1.145655,1.0893905,0.60204554,1.455641,-3.94777,1.0702095,1.1500016,-0.36191702,0.36226815,0.3100651,-0.6291629,-0.48965326,-0.70232034,-3.11499,-2.5668874,-2.5431635,-2.8575861,-1.0584917,-3.0325682,-2.7241023,0.19225082,-2.5089946,-1.9870626,-3.2024672,-3.6128151,-2.2014763,-2.6288695,-2.4490888,-1.8236175,-1.2005508,-3.1177392,-0.823186,-1.0159398,-0.24939843,-0.3227615,-0.29344362,-0.52375066,-1.0116419,-1.8768193,-1.8000137,-1.4767021,-0.7252241,-2.9663258,-2.1243086,-1.7033999,-1.8487821,-2.3691874,1.0260264,-2.1764934,-2.3387678,-2.198238,-1.8409467,-0.8634166,-1.0595562,-0.7821933,-1.3137078,0.2711146,-1.3385692,-0.62291425,0.60730845,-2.8947241,-2.509123,-0.087619565,-1.1436799,-0.5328511,-1.511311,2.873726,-2.6130204,-0.22530293,-1.7350692,-1.1229802,-0.14384705,0.5222479,0.6251236,0.1328101,0.23961729,0.46445486,-2.053218,-2.0282624,-2.1122148,-1.7506852,-2.0412827,-2.6492512,-2.9374852,-1.8221225,-1.0337435,-0.5745457,-2.4617555,-2.2397163,-2.4626577,-2.4036834,-2.3103526,-3.3117702,-2.18008,-1.1167436,-3.1680171,-1.6446073,-1.4097774,-1.9196372,-3.608827,-3.6757724,-3.2431812,-2.7308004,-2.3125544,-2.7845373,5.862682,-0.115448214,-2.008062,-4.0442314,-6.003454,-3.0081325,0.12752078,-1.8032212,-0.5889741,-0.47991648,-0.018536972,-2.352108,-0.49857017,8.138928,8.237168,8.226684,8.115605,-3.900328,-3.2444837,-2.5062623,-2.1757464,2.7394505,2.6909657,-3.1837273,-3.6627865,-3.393556,-2.1656106,-2.096646,-2.2896237,-1.8721823,-1.6523029,-1.8135278,-0.46768698,-0.53122723,-0.026237125,-0.31325963,-1.4488953,-1.1871272,-3.7256007,-1.7090045,-1.9762176,-1.7918005,-1.6670003,0.61086696,0.863179,-3.0445929,-3.3363223,-2.9852753,-3.1067019,-1.171938,-3.3215523,15.962009,8.17088,8.226758,8.161403,8.226536,8.137564,16.04827,16.013128,16.020638,16.011198,16.014334,15.999539,16.460468,16.002901,16.007751,16.011572,16.016703,16.0049,15.991717,15.988427,16.009037,15.995036,16.013554,15.9609,16.002842,15.997669,15.986028,16.428581,16.168438,16.462326,16.521889,16.470594,16.003048,16.475304,16.481565,15.965535,16.017422,15.99675,16.011354,15.998164,16.002428,16.020409,15.981551,15.998693,16.00856,16.012121,16.003328,15.991297,16.046679,15.990455,16.014624,16.013826,16.043314,16.002594,15.9895735,15.921558,16.009432,16.037663,15.993097,16.002684,15.995458,16.047592,15.994495,15.996766,15.978886,16.022507,15.995864,16.014086,15.996778,15.95749,15.997672,15.993474,15.987565,15.925249,16.00963,-3.762573,-3.932667,-3.1293902,0.4066017,0.69991237,0.6645667,0.5213533,-0.11219695,-0.49708667,-1.8692987,-1.9717904,-2.5604658,-2.8642914,1.4398189,1.2514895,1.0545627,-0.27539128,1.245524,1.4696177,1.1533225,1.2984577,1.5172049,1.399224,1.3447567,1.1569898,1.1463336,0.17493889,-1.1045756,-1.3904165,-1.7699932,-1.5912594,-1.6929373,-1.814933,-1.0143414,-0.9560185,-1.6238934,-1.4127212,-2.4133415,-2.1016705,-2.759426,-2.6578097,1.7937465],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, fil...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnet152d`. You can find the ...\"],[\"```...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SE ResNet\\n  Paper:\\n    Title: Squeeze-and-Excitation Net...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fseresnet15...\"],[\"Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Interpolation: bi...\"],[\"Res2Net\\n\\n**Res2Net** is an image model that employs a variation on bottleneck residual blocks, [Res2...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `res2net101_26w_4s`. You can find...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Res2Net\\n  Paper:\\n    Title: 'Res2Net: A New Multi-scale ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-res2net\\u002fres2net101...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-res2net\\u002fres2net50_...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-res2net\\u002fres2net50_...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-res2net\\u002fres2net50_...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-res2net\\u002fres2net50_...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-res2net\\u002fres2net50_...\"],[\"# Ensemble Adversarial Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural archit...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ens_adv_inception_resnet_v2`. Yo...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Ensemble Adversarial\\n  Paper:\\n    Title: Adversaria...\"],[\"TResNet\\n\\nA **TResNet** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) that aim...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TResNet\\n  Paper:\\n    Title: 'TResNet: High Performance G...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-tresnet\\u002ftresnet_l_...\"],[\"Momentum: 0.9\\n    Image Size: '448'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: htt...\"],[\"Training Resources: 8x NVIDIA 100 GPUs\\n    Training Time: \\u003c 24 hours\\n    ID: tresnet_m\\n    LR: 0.01\\n...\"],[\"Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Labe...\"],[\"- Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Resid...\"],[\"FLOPs: 60641712730\\n    Parameters: 75646610\\n    File Size: 224440219\\n    Architecture:\\n    - 1x1 Con...\"],[\"SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnet152d`. You can find the ...\"],[\"CSP-ResNeXt\\n\\n**CSPResNeXt** is a convolutional neural network where we apply the Cross Stage Partial...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `cspresnext50`. You can find the ...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: CSP ResNeXt\\n  Paper:\\n    Title: 'CSPNet: A New Back...\"],[\"RexNet\\n\\n**Rank Expansion Networks** (ReXNets) follow a set of new design principles for designing bo...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: RexNet\\n  Paper:\\n    Title: 'ReXNet: Diminishing Represen...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-rexnet\\u002frexnetv1_10...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-rexnet\\u002frexnetv1_13...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-rexnet\\u002frexnetv1_15...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-rexnet\\u002frexnetv1_20...\"],[\"RegNetY\\n\\n**RegNetY** is a convolutional network design space with simple, regular models with parame...\"],[\"```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tens...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `regnety_002`. You can find the I...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: RegNetY\\n  Paper:\\n    Title: Designing Network Design Spa...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 70....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 74....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 75....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 76....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 82....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"Feature Extraction\\n\\nAll of the models in `timm` have consistent mechanisms for obtaining various typ...\"],[\"```\\nOutput:\\n```text\\nUnpooled shape: torch.Size([2, 2048, 7, 7])\\n```\\n\\n#### Remove it later\\n```python ...\"],[\"```\\nOutput:\\n```text\\nOriginal shape: torch.Size([2, 1000])\\nPooled shape: torch.Size([2, 1024])\\n```\\n\\n\\n...\"],[\"```\\n\\n### Query the feature information\\n\\nAfter a feature backbone has been created, it can be queried...\"],[\"```\\n\\n### Select specific feature levels or limit the stride\\n\\nThere are two additional creation argum...\"],[\"Wide ResNet\\n\\n**Wide Residual Networks** are a variant on [ResNets](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `wide_resnet101_2`. You can find ...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Wide ResNet\\n  Paper:\\n    Title: Wide Residual Networks\\n ...\"],[\"Top 5 Accuracy: 94.28%\\n- Name: wide_resnet50_2\\n  In Collection: Wide ResNet\\n  Metadata:\\n    FLOPs: 1...\"],[\"Res2NeXt\\n\\n**Res2NeXt** is an image model that employs a variation on [ResNeXt](https:\\u002f\\u002fpaperswithcod...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Res2NeXt\\n  Paper:\\n    Title: 'Res2Net: A New Multi-...\"],[\"RegNetY\\n\\n**RegNetY** is a convolutional network design space with simple, regular models with parame...\"],[\"```\\n\\nTo load and preprocess the image:\\n\\n```py\\n\\u003e\\u003e\\u003e import urllib\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e from t...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, fil...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `regnety_002`. You can find the I...\"],[\"Archived Changes\\n\\n### Nov 22, 2021\\n* A number of updated weights anew new model defs\\n  * `eca_halone...\"],[\"### Oct 19, 2021\\n* ResNet strikes back (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2110.00476) weights added, plus any ex...\"],[\"* ConvMixer (https:\\u002f\\u002fopenreview.net\\u002fforum?id=TVHS5Y4dNvM), CrossVit (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2103.1489...\"],[\"### Aug 18, 2021\\n* Optimizer bonanza!\\n  * Add LAMB and LARS optimizers, incl trust ratio clipping op...\"],[\"### July 5-9, 2021\\n* Add `efficientnetv2_rw_t` weights, a custom 'tiny' 13.6M param variant that is ...\"],[\"### June 20, 2021\\n* Release Vision Transformer 'AugReg' weights from [How to train your ViT? Data, A...\"],[\"* Add `eca_nfnet_l2` weights from my 'lightweight' series. 84.7 top-1 at 384x384.\\n* Add distilled Bi...\"],[\"### June 8, 2021\\n* Add first ResMLP weights, trained in PyTorch XLA on TPU-VM w\\u002f my XLA branch. 24 b...\"],[\"### May 5, 2021\\n* Add MLP-Mixer models and port pretrained weights from [Google JAX impl](https:\\u002f\\u002fgi...\"],[\"### April 13, 2021\\n* Add Swin Transformer models and weights from https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fSwin-...\"],[\"### April 1, 2021\\n* Add snazzy `benchmark.py` script for bulk `timm` model benchmarking of train and...\"],[\"### Feb 18, 2021\\n* Add pretrained weights and model variants for NFNet-F* models from [DeepMind Haik...\"],[\"### Feb 16, 2021\\n* Add Adaptive Gradient Clipping (AGC) as per https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2102.06171. Int...\"],[\"### Feb 8, 2021\\n* Add several ResNet weights with ECA attention. 26t & 50t trained @ 256, test @ 320...\"],[\"### Jan 25, 2021\\n* Add ResNetV2 Big Transfer (BiT) models w\\u002f ImageNet-1k and 21k weights from https:...\"],[\"### Dec 18, 2020\\n* Add ResNet-101D, ResNet-152D, and ResNet-200D weights trained @ 256x256\\n  * 256x2...\"],[\"### Oct 21, 2020\\n* Weights added for Vision Transformer (ViT) models. 77.86 top-1 for 'small' and 79...\"],[\"### Aug 12, 2020\\n* New\\u002fupdated weights from training experiments\\n  * EfficientNet-B3 - 82.1 top-1 (v...\"],[\"### Aug 5, 2020\\nUniversal feature extraction, new models, new weights, new test sets.\\n* All models s...\"],[\"### June 11, 2020\\nBunch of changes:\\n* DenseNet models updated with memory efficient addition from to...\"],[\"### May 1, 2020\\n* Merged a number of execellent contributions in the ResNet model family over the pa...\"],[\"### April 5, 2020\\n* Add some newly trained MobileNet-V2 models trained with latest h-params, rand au...\"],[\"### Feb 18, 2020\\n* Big refactor of model layers and addition of several attention mechanisms. Severa...\"],[\"### Feb 1\\u002f2, 2020\\n* Port new EfficientNet-B8 (RandAugment) weights, these are different than the B8 ...\"],[\"### Dec 28, 2019\\n* Add new model weights and training hparams (see Training Hparams section)\\n  * `ef...\"],[\"### Nov 29, 2019\\n* Brought EfficientNet and MobileNetV3 up to date with my https:\\u002f\\u002fgithub.com\\u002frwight...\"],[\"(Tensorflow) EfficientNet CondConv\\n\\n**EfficientNet** is a convolutional neural network architecture ...\"],[\"```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_cc_b0_4e`. You c...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF EfficientNet CondConv\\n  Paper:\\n    Title: 'CondConv: ...\"],[\"Interpolation: bicubic\\n    RMSProp Decay: 0.9\\n    Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n...\"],[\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    ID: tf_efficientnet_cc_b0_8e\\n    LR: 0.256\\n    ...\"],[\"- CondConv\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n    - Inverted Residual Block\\n   ...\"],[\"SWSL ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-blo...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `swsl_resnext101_32x16d`. You can...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SWSL ResNext\\n  Paper:\\n    Title: Billion-scale semi-supe...\"],[\"Weights: https:\\u002f\\u002fdl.fbaipublicfiles.com\\u002fsemiweaksupervision\\u002fmodel_files\\u002fsemi_weakly_supervised_resne...\"],[\"Batch Size: 1536\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: ...\"],[\"ID: swsl_resnext101_32x8d\\n    LR: 0.0015\\n    Epochs: 30\\n    Layers: 101\\n    Crop Pct: '0.875'\\n    Ba...\"],[\"Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - IG-1B-Targe...\"],[\"Model Summaries\\n\\nThe model architectures included come from a wide variety of sources. Sources, incl...\"],[\"## DenseNet [[densenet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels...\"],[\"## HRNet [[hrnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels\\u002fhrnet...\"],[\"## Inception-ResNet-V2 [[inception_resnet_v2.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fb...\"],[\"## EfficientNet [[efficientnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftim...\"],[\"## MobileNet-V3 [[mobilenetv3.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm...\"],[\"* ResNet (V1B)\\n  * Paper: `Deep Residual Learning for Image Recognition` - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f151...\"],[\"* Squeeze-and-Excitation Networks\\n  * Paper: `Squeeze-and-Excitation Networks` - https:\\u002f\\u002farxiv.org\\u002fa...\"],[\"## Res2Net [[res2net.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels\\u002fr...\"],[\"## SelecSLS [[selecsls.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels...\"],[\"## VGG [[vgg.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels\\u002fvgg.py)]\\n...\"],[\"## Xception (Modified Aligned, Gluon) [[gluon_xception.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-imag...\"],[\"(Tensorflow) MobileNet v3\\n\\n**MobileNetV3** is a convolutional neural network that is designed for mo...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_mobilenetv3_large_075`. You c...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF MobileNet V3\\n  Paper:\\n    Title: Searching for Mobile...\"],[\"Momentum: 0.9\\n    Batch Size: 4096\\n    Image Size: '224'\\n    Weight Decay: 1.0e-05\\n    Interpolation...\"],[\"Training Techniques:\\n    - RMSProp\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training...\"],[\"- Depthwise Separable Convolution\\n    - Dropout\\n    - Global Average Pooling\\n    - Hard Swish\\n    - ...\"],[\"In Collection: TF MobileNet V3\\n  Metadata:\\n    FLOPs: 48457664\\n    Parameters: 2040000\\n    File Size...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 65....\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_mobilen...\"],[\"Image Size: '224'\\n    Weight Decay: 4.0e-05\\n    Interpolation: bilinear\\n    RMSProp Decay: 0.9\\n  Cod...\"],[\"Sharing and Loading Models From the Hugging Face Hub\\n\\nThe `timm` library has a built-in integration ...\"],[\"```\\n\\nRunning the above would push the model to `\\u003cyour-username\\u003e\\u002fresnet18-random` on the Hub. You can...\"],[\"Big Transfer (BiT)\\n\\n**Big Transfer (BiT)** is a type of pretraining recipe that pre-trains  on a lar...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnetv2_101x1_bitm`. You can fi...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Big Transfer\\n  Paper:\\n    Title: 'Big Transfer (BiT): Ge...\"],[\"Momentum: 0.9\\n    Batch Size: 4096\\n    Image Size: '480'\\n    Weight Decay: 0.0001\\n    Interpolation:...\"],[\"Training Resources: Cloud TPUv3-512\\n    ID: resnetv2_101x3_bitm\\n    LR: 0.03\\n    Epochs: 90\\n    Laye...\"],[\"Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Mixup\\n    - SGD with Momentum\\n    -...\"],[\"Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Mixup\\n    - SGD with Momentum\\n    -...\"],[\"- Weight Standardization\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Mixup\\n...\"],[\"- Convolution\\n    - Global Average Pooling\\n    - Group Normalization\\n    - Max Pooling\\n    - ReLU\\n  ...\"],[\"MobileNet v2\\n\\n**MobileNetV2** is a convolutional neural network architecture that seeks to perform w...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `mobilenetv2_100`. You can find t...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MobileNet V2\\n  Paper:\\n    Title: 'MobileNetV2: Inverted ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fmobilenetv...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fmobilenetv...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fmobilenetv...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fmobilenetv...\"],[\"EfficientNet (Knapsack Pruned)\\n\\n**EfficientNet** is a convolutional neural network architecture and ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `efficientnet_b1_pruned`. You can...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: EfficientNet Pruned\\n  Paper:\\n    Title: Knapsack Pruning...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78....\"],[\"Top 1 Accuracy: 79.91%\\n      Top 5 Accuracy: 94.86%\\n- Name: efficientnet_b3_pruned\\n  In Collection: ...\"],[\"(Tensorflow) Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_inception_v3`. You can find t...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF Inception v3\\n  Paper:\\n    Title: Rethinking the ...\"],[\"DenseNet\\n\\n**DenseNet** is a type of convolutional neural network that utilises dense connections bet...\"],[\"```\\n\\nTo get the model predictions:\\n\\n```py\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e with torch.no_grad():\\n...     out = m...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `densenet121`. You can find the I...\"],[\"```\\n\\n```\\n@misc{rw2019timm,\\n  author = {Ross Wightman},\\n  title = {PyTorch Image Models},\\n  year = {2...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: DenseNet\\n  Paper:\\n    Title: Densely Connected Convoluti...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fdensenet12...\"],[\"Weights: https:\\u002f\\u002fdownload.pytorch.org\\u002fmodels\\u002fdensenet161-8d451a50.pth\\n  Results:\\n  - Task: Image Cla...\"],[\"Weights: https:\\u002f\\u002fdownload.pytorch.org\\u002fmodels\\u002fdensenet169-b2777c0a.pth\\n  Results:\\n  - Task: Image Cla...\"],[\"Weights: https:\\u002f\\u002fdownload.pytorch.org\\u002fmodels\\u002fdensenet201-c1103571.pth\\n  Results:\\n  - Task: Image Cla...\"],[\"Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 76.59%\\n      Top 5 Accuracy: 93.2%\\n- Name: tv_d...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 74....\"],[\"Training Examples\\n\\n## EfficientNet-B2 with RandAugment - 80.4 top-1, 95.1 top-5\\nThese params are for...\"],[\"## SE-ResNeXt-26-D and SE-ResNeXt-26-T\\nThese hparams (or similar) work well for a wide range of ResN...\"],[\"`.\\u002fdistributed_train.sh 2 \\u002fimagenet\\u002f --model efficientnet_b0 -b 384 --sched step --epochs 450 --deca...\"],[\"`.\\u002fdistributed_train.sh 8 \\u002fimagenet --model efficientnet_es -b 128 --sched step --epochs 450 --decay...\"],[\"`.\\u002fdistributed_train.sh 8 \\u002fimagenet --model resnext50_32x4d --lr 0.6 --warmup-epochs 5 --epochs 240 ...\"],[\"MnasNet\\n\\n**MnasNet** is a type of convolutional neural network optimized for mobile devices that is ...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MNASNet\\n  Paper:\\n    Title: 'MnasNet: Platform-Aware Neu...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fmnasnet_b1...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 75....\"],[\"SK-ResNet\\n\\n**SK ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SKResNet\\n  Paper:\\n    Title: Selective Kernel Networks\\n ...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 73....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 76....\"],[\"Inception v4\\n\\n**Inception-v4** is a convolutional neural network architecture that builds on previou...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `inception_v4`. You can find the ...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Inception v4\\n  Paper:\\n    Title: Inception-v4, Ince...\"],[\"CSP-ResNeXt\\n\\n**CSPResNeXt** is a convolutional neural network where we apply the Cross Stage Partial...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"MnasNet\\n\\n**MnasNet** is a type of convolutional neural network optimized for mobile devices that is ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `mnasnet_100`. You can find the I...\"],[\"NASNet\\n\\n**NASNet** is a type of convolutional neural network discovered through neural architecture ...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: NASNet\\n  Paper:\\n    Title: Learning Transferable Ar...\"],[\"SelecSLS\\n\\n**SelecSLS** uses novel selective long and short range skip connections to improve the inf...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `selecsls42b`. You can find the I...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SelecSLS\\n  Paper:\\n    Title: 'XNect: Real-time Multi-Per...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77....\"],[\"- Name: selecsls60b\\n  In Collection: SelecSLS\\n  Metadata:\\n    FLOPs: 4657653144\\n    Parameters: 3277...\"],[\"MixNet\\n\\n**MixNet** is a type of convolutional neural network discovered via AutoML that utilises [Mi...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `mixnet_l`. You can find the IDs ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MixNet\\n  Paper:\\n    Title: 'MixConv: Mixed Depthwise Con...\"],[\"- Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78.98%\\n      T...\"],[\"In Collection: MixNet\\n  Metadata:\\n    FLOPs: 321264910\\n    Parameters: 4130000\\n    File Size: 167279...\"],[\"- Batch Normalization\\n    - Dense Connections\\n    - Dropout\\n    - Global Average Pooling\\n    - Group...\"],[\"DenseNet\\n\\n**DenseNet** is a type of convolutional neural network that utilises dense connections bet...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"SK-ResNeXt\\n\\n**SK ResNeXt** is a variant of a [ResNeXt](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnext...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `skresnext50_32x4d`. You can find...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SKResNeXt\\n  Paper:\\n    Title: Selective Kernel Netw...\"],[\"Recent Changes\\n\\n### Aug 29, 2022\\n* MaxVit window size scales with img_size by default. Add new RelPo...\"],[\"### Aug 26, 2022\\n* CoAtNet (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2106.04803) and MaxVit (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2204...\"],[\"* GCVit (weights adapted from https:\\u002f\\u002fgithub.com\\u002fNVlabs\\u002fGCVit, code 100% `timm` re-write for license...\"],[\"### Aug 15, 2022\\n* ConvNeXt atto weights added\\n  * `convnext_atto` - 75.7 @ 224, 77.0 @ 288\\n  * `con...\"],[\"### July 27, 2022\\n* All runtime benchmark and validation result csv files are finally up-to-date!\\n* ...\"],[\"### July 8, 2022\\nMore models, more fixes\\n* Official research models (w\\u002f weights) added:\\n  * EdgeNeXt...\"],[\"* `cs3darknet_l` - 80.4 @ 256, 80.9 @ 288\\n  * `cs3darknet_focus_l` - 80.3 @ 256, 80.9 @ 288\\n  * `vit...\"],[\"* Numerous bug fixes\\n* Currently testing for imminent PyPi 0.6.x release\\n* LeViT pretraining of larg...\"],[\"### May 13, 2022\\n* Official Swin-V2 models and weights added from (https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fSwin...\"],[\"### May 2, 2022\\n* Vision Transformer experiments adding Relative Position (Swin-V2 log-coord) (`visi...\"],[\"### March 23, 2022\\n* Add `ParallelBlock` and `LayerScale` option to base vit models to support model...\"],[\"### March 21, 2022\\n* Merge `norm_norm_norm`. **IMPORTANT** this update for a coming 0.6.x release wi...\"],[\"* `regnetz_c16_evos`  - 81.9 @ 256, 82.64 @ 320 (EvoNormS)\\n  * `regnetz_d8_evos`  - 83.42 @ 256, 84....\"],[\"* Significant work experimenting with non-BatchNorm norm layers such as EvoNorm, FilterResponseNorm,...\"],[\"### Feb 2, 2022\\n* [Chris Hughes](https:\\u002f\\u002fgithub.com\\u002fChris-hughes10) posted an exhaustive run through...\"],[\"### Jan 14, 2022\\n* Version 0.5.4 w\\u002f release to be pushed to pypi. It's been a while since last pypi ...\"],[\"### Dec 23, 2022 🎄☃\\n* Add FlexiViT models and weights from https:\\u002f\\u002fgithub.com\\u002fgoogle-research\\u002fbig_vi...\"],[\"| model                                     | top1 | param_count |  gmac | macts | hub              ...\"],[\"| model                                    |   top1 |   param_count |   gmac |   macts | hub        ...\"],[\"### Dec 5, 2022\\n\\n* Pre-release (`0.8.0dev0`) of multi-weight support (`model_arch.pretrained_tag`). ...\"],[\"| model                                            |   top1 |   param_count |   gmac |   macts | hub...\"],[\"| vit_large_patch14_clip_336.laion2b_ft_in12k_in1k |   88.2 |         304.5 |  191.1 |   270.2 | [li...\"],[\"| vit_huge_patch14_clip_224.laion2b_ft_in1k        |   87.6 |         632   |  167.4 |   139.4 | [li...\"],[\"| vit_base_patch16_clip_384.openai_ft_in1k         |   86.2 |          86.9 |   55.5 |   101.6 | [li...\"],[\"| vit_base_patch32_clip_384.laion2b_ft_in12k_in1k  |   85.4 |          88.3 |   13.1 |    16.5 | [li...\"],[\"* Port of MaxViT Tensorflow Weights from official impl at https:\\u002f\\u002fgithub.com\\u002fgoogle-research\\u002fmaxvit\\n...\"],[\"| model                              |   top1 |   param_count |   gmac |   macts | hub              ...\"],[\"| maxvit_large_tf_512.in21k_ft_in1k  |   88   |         212.3 |  244.8 |   942.2 | [link](https:\\u002f\\u002fhu...\"],[\"| maxvit_small_tf_512.in1k           |   86.1 |          69.1 |   67.3 |   383.8 | [link](https:\\u002f\\u002fhu...\"],[\"| maxvit_tiny_tf_224.in1k            |   83.4 |          30.9 |    5.6 |    35.8 | [link](https:\\u002f\\u002fhu...\"],[\"### Oct 15, 2022\\n* Train and validation script enhancements\\n* Non-GPU (ie CPU) device support\\n* SLUR...\"],[\"### Oct 10, 2022\\n* More weights in `maxxvit` series, incl first ConvNeXt block based `coatnext` and ...\"],[\"### Sept 7, 2022\\n* Hugging Face [`timm` docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002ftimm) home now exists,...\"],[\"### Aug 26, 2022\\n* CoAtNet (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2106.04803) and MaxVit (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2204...\"],[\"### Aug 15, 2022\\n* ConvNeXt atto weights added\\n  * `convnext_atto` - 75.7 @ 224, 77.0 @ 288\\n  * `con...\"],[\"### July 27, 2022\\n* All runtime benchmark and validation result csv files are up-to-date!\\n* A few mo...\"],[\"* `cs3darknet_l` - 80.4 @ 256, 80.9 @ 288\\n  * `cs3darknet_focus_l` - 80.3 @ 256, 80.9 @ 288\\n  * `vit...\"],[\"### Jan 14, 2022\\n* Version 0.5.4 w\\u002f release to be pushed to pypi. It's been a while since last pypi ...\"],[\"Hugging Face Timm Docs\\n\\n## Getting Started\\n\\n```\\npip install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdoc-b...\"],[\"ResNeSt\\n\\nA **ResNeSt** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet), which i...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnest101e`. You can find the I...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNeSt\\n  Paper:\\n    Title: 'ResNeSt: Split-Attention Ne...\"],[\"Momentum: 0.9\\n    Batch Size: 4096\\n    Image Size: '256'\\n    Weight Decay: 0.0001\\n    Interpolation:...\"],[\"Training Resources: 64x NVIDIA V100 GPUs\\n    ID: resnest14d\\n    LR: 0.1\\n    Epochs: 270\\n    Layers: ...\"],[\"Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - DropBlock\\n    - L...\"],[\"Architecture:\\n    - 1x1 Convolution\\n    - Convolution\\n    - Dense Connections\\n    - Global Average P...\"],[\"Top 1 Accuracy: 84.53%\\n      Top 5 Accuracy: 96.99%\\n- Name: resnest26d\\n  In Collection: ResNeSt\\n  Me...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78....\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-resnest\\u002fresnest50-...\"],[\"Momentum: 0.9\\n    Batch Size: 8192\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation:...\"],[\"- Mixup\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Re...\"],[\"FBNet\\n\\n**FBNet** is a type of convolutional neural architectures discovered through [DNAS](https:\\u002f\\u002fp...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `fbnetc_100`. You can find the ID...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: FBNet\\n  Paper:\\n    Title: 'FBNet: Hardware-Aware Ef...\"],[\"HRNet\\n\\n**HRNet**, or **High-Resolution Net**, is a general purpose convolutional neural network for ...\"],[\"```\\n\\nTo get the model predictions:\\n\\n```py\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e with torch.no_grad():\\n...     out = m...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `hrnet_w18`. You can find the IDs...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: HRNet\\n  Paper:\\n    Title: Deep High-Resolution Represent...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 76....\"],[\"- Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 72.34%\\n      T...\"],[\"- Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 75.11%\\n      T...\"],[\"Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78.21%\\n      Top 5 Accuracy: 94.22%\\n- Name: hrn...\"],[\"Metrics:\\n      Top 1 Accuracy: 78.45%\\n      Top 5 Accuracy: 94.19%\\n- Name: hrnet_w40\\n  In Collection...\"],[\"Top 1 Accuracy: 78.93%\\n      Top 5 Accuracy: 94.48%\\n- Name: hrnet_w44\\n  In Collection: HRNet\\n  Metad...\"],[\"Top 5 Accuracy: 94.37%\\n- Name: hrnet_w48\\n  In Collection: HRNet\\n  Metadata:\\n    FLOPs: 22285865760\\n ...\"],[\"Top 5 Accuracy: 94.51%\\n- Name: hrnet_w64\\n  In Collection: HRNet\\n  Metadata:\\n    FLOPs: 37239321984\\n ...\"],[\"Learning Rate Schedulers\\n\\nThis page contains the API reference documentation for learning rate sched...\"],[\"MobileNet v3\\n\\n**MobileNetV3** is a convolutional neural network that is designed for mobile phone CP...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `mobilenetv3_large_100`. You can ...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MobileNet V3\\n  Paper:\\n    Title: Searching for MobileNet...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fmobilenetv...\"],[\"Momentum: 0.9\\n    Batch Size: 4096\\n    Image Size: '224'\\n    Weight Decay: 1.0e-05\\n    Interpolation...\"],[\"ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer ...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNet\\n  Paper:\\n    Title: Deep Residual Learning for Im...\"],[\"Top 5 Accuracy: 89.09%\\n- Name: resnet26\\n  In Collection: ResNet\\n  Metadata:\\n    FLOPs: 3026804736\\n  ...\"],[\"File Size: 87290831\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Bottlene...\"],[\"- Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection...\"],[\"- Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - I...\"],[\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    ID: tv_resnet101\\n    LR: 0.1\\n    Epochs: 90\\n   ...\"],[\"Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Deca...\"],[\"- Residual Block\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    T...\"],[\"- Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n   ...\"],[\"SK-ResNet\\n\\n**SK ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `skresnet18`. You can find the ID...\"],[\"SelecSLS\\n\\n**SelecSLS** uses novel selective long and short range skip connections to improve the inf...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `selecsls42b`. You can find the I...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"PNASNet\\n\\n**Progressive Neural Architecture Search**, or **PNAS**, is a method for learning the struc...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `pnasnet5large`. You can find the...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: PNASNet\\n  Paper:\\n    Title: Progressive Neural Arch...\"],[\"(Tensorflow) EfficientNet CondConv\\n\\n**EfficientNet** is a convolutional neural network architecture ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_cc_b0_4e`. You c...\"],[\"ECA-ResNet\\n\\nAn **ECA ResNet** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) t...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ecaresnet101d`. You can find the...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ECAResNet\\n  Paper:\\n    Title: 'ECA-Net: Efficient Channe...\"],[\"Crop Pct: '0.875'\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolati...\"],[\"Training Data:\\n    - ImageNet\\n    ID: ecaresnet101d_pruned\\n    Layers: 101\\n    Crop Pct: '0.875'\\n   ...\"],[\"Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n  ...\"],[\"- Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-...\"],[\"- Efficient Channel Attention\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residu...\"],[\"Validation and Benchmark Results\\n\\nThis folder contains validation and benchmark results for the mode...\"],[\"### ImageNetV2 Matched Frequency - [`results-imagenetv2-matched-frequency.csv`](results-imagenetv2-m...\"],[\"### ImageNet-Rendition - [`results-imagenet-r.csv`](results-imagenet-r.csv)\\n\\nRenditions of 200 Image...\"],[\"Results\\n\\nCSV files containing an ImageNet-1K and out-of-distribution (OOD) test set validation resul...\"],[\"|Model | Acc@1 (Err) | Acc@5 (Err) | Param # (M) | Interpolation | Image Size |\\n|---|---|---|---|---...\"],[\"| seresnet50 | 80.274 (19.726) | 95.070 (4.930) | 28.1 | bicubic | 224 |\\n| skresnext50d_32x4d | 80.1...\"],[\"| mixnet_l | 78.976 (21.024 | 94.184 (5.816) | 7.33 | bicubic | 224 |\\n| efficientnet_b1 | 78.692 (21...\"],[\"| seresnext26_32x4d | 77.104 (22.896) | 93.316 (6.684) | 16.8 | bicubic | 224 |\\n| skresnet34 | 76.91...\"],[\"| fbnetc_100 | 75.124 (24.876) | 92.386 (7.614) | 5.6 | bilinear | 224 |\\n| resnet34 | 75.110 (24.890...\"],[\"## Ported and Other Weights\\n\\nFor weights ported from other deep learning frameworks (Tensorflow, MXN...\"],[\"Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the Inception fam...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Inception v3\\n  Paper:\\n    Title: Rethinking the Inc...\"],[\"CSP-DarkNet\\n\\n**CSPDarknet53** is a convolutional neural network and backbone for object detection th...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `cspdarknet53`. You can find the ...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: CSP DarkNet\\n  Paper:\\n    Title: 'YOLOv4: Optimal Sp...\"],[\"SPNASNet\\n\\n**Single-Path NAS** is a novel differentiable NAS method for designing hardware-efficient ...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SPNASNet\\n  Paper:\\n    Title: 'Single-Path NAS: Desi...\"],[\"Wide ResNet\\n\\n**Wide Residual Networks** are a variant on [ResNets](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `wide_resnet101_2`. You can find ...\"],[\"SSL ResNeXT\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-bloc...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ssl_resnext101_32x16d`. You can ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SSL ResNext\\n  Paper:\\n    Title: Billion-scale semi-super...\"],[\"Weights: https:\\u002f\\u002fdl.fbaipublicfiles.com\\u002fsemiweaksupervision\\u002fmodel_files\\u002fsemi_supervised_resnext101_3...\"],[\"Batch Size: 1536\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: ...\"],[\"ID: ssl_resnext101_32x8d\\n    LR: 0.0015\\n    Epochs: 30\\n    Layers: 101\\n    Crop Pct: '0.875'\\n    Bat...\"],[\"Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n  ...\"],[\"RegNetX\\n\\n**RegNetX** is a convolutional network design space with simple, regular models with parame...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `regnetx_002`. You can find the I...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: RegNetX\\n  Paper:\\n    Title: Designing Network Design Spa...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 68....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 72....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 73....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 75....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 76....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"Installation\\n\\nBefore you start, you'll need to setup your environment and install the appropriate pa...\"],[\"```\\n\\n## From Source\\n\\nBuilding `timm` from source lets you make changes to the code base. To install ...\"],[\"(Tensorflow) MobileNet v3\\n\\n**MobileNetV3** is a convolutional neural network that is designed for mo...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_mobilenetv3_large_075`. You c...\"],[\"Big Transfer (BiT)\\n\\n**Big Transfer (BiT)** is a type of pretraining recipe that pre-trains  on a lar...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnetv2_101x1_bitm`. You can fi...\"],[\"Instagram ResNeXt WSL\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fre...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ig_resnext101_32x16d`. You can f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: IG ResNeXt\\n  Paper:\\n    Title: Exploring the Limits of W...\"],[\"Momentum: 0.9\\n    Batch Size: 8064\\n    Image Size: '224'\\n    Weight Decay: 0.001\\n    Interpolation: ...\"],[\"- ImageNet\\n    Training Resources: 336x GPUs\\n    ID: ig_resnext101_32x32d\\n    Epochs: 100\\n    Layers...\"],[\"- Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Nesterov Accelerated ...\"],[\"- Convolution\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - Max Pooling\\n    - ReLU\\n  ...\"],[\"(Gluon) Xception\\n\\n**Xception** is a convolutional neural network architecture that relies solely on ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_xception65`. You can find ...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun Xception\\n  Paper:\\n    Title: 'Xception: Deep ...\"],[\"This guideline is very much a work-in-progress.*\\n\\nContributions to `timm` for code, documentation, t...\"],[\"```\\nblack --skip-string-normalization --line-length 120 \\u003cpath-to-file\\u003e\\n```\\n\\nAvoid formatting code th...\"],[\"```\\npytest -k \\\"substring-to-match\\\" -n 4 tests\\u002f\\n```\\n\\n## Building documentation\\n\\nPlease refer to [this...\"],[\"Deep Layer Aggregation\\n\\nExtending  “shallow” skip connections, **Dense Layer Aggregation (DLA)** inc...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: DLA\\n  Paper:\\n    Title: Deep Layer Aggregation\\n    URL: ...\"],[\"Weights: http:\\u002f\\u002fdl.yf.io\\u002fdla\\u002fmodels\\u002fimagenet\\u002fdla102-d94d9790.pth\\n  Results:\\n  - Task: Image Classifi...\"],[\"Weights: http:\\u002f\\u002fdl.yf.io\\u002fdla\\u002fmodels\\u002fimagenet\\u002fdla102x-ad62be81.pth\\n  Results:\\n  - Task: Image Classif...\"],[\"Weights: http:\\u002f\\u002fdl.yf.io\\u002fdla\\u002fmodels\\u002fimagenet\\u002fdla102x2-262837b6.pth\\n  Results:\\n  - Task: Image Classi...\"],[\"Weights: http:\\u002f\\u002fdl.yf.io\\u002fdla\\u002fmodels\\u002fimagenet\\u002fdla169-0914e092.pth\\n  Results:\\n  - Task: Image Classifi...\"],[\"Weights: http:\\u002f\\u002fdl.yf.io\\u002fdla\\u002fmodels\\u002fimagenet\\u002fdla34-ba72cf86.pth\\n  Results:\\n  - Task: Image Classific...\"],[\"Weights: http:\\u002f\\u002fdl.yf.io\\u002fdla\\u002fmodels\\u002fimagenet\\u002fdla46_c-2bfd52c3.pth\\n  Results:\\n  - Task: Image Classif...\"],[\"Weights: http:\\u002f\\u002fdl.yf.io\\u002fdla\\u002fmodels\\u002fimagenet\\u002fdla46x_c-d761bae7.pth\\n  Results:\\n  - Task: Image Classi...\"],[\"Weights: http:\\u002f\\u002fdl.yf.io\\u002fdla\\u002fmodels\\u002fimagenet\\u002fdla60-24839fc4.pth\\n  Results:\\n  - Task: Image Classific...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 67....\"],[\"Data\\n\\n[[autodoc]] timm.data.create_dataset\\n\\n[[autodoc]] timm.data.create_loader\\n\\n[[autodoc]] timm.da...\"],[\"CSP-ResNet\\n\\n**CSPResNet** is a convolutional neural network where we apply the Cross Stage Partial N...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `cspresnet50`. You can find the I...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: CSP ResNet\\n  Paper:\\n    Title: 'CSPNet: A New Backb...\"],[\"Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the Inception fam...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `inception_v3`. You can find the ...\"],[\"Getting Started\\n\\n## Welcome\\n\\nWelcome to the `timm` documentation, a lean set of docs that covers the...\"],[\"```\\n\\n## List Models with Pretrained Weights\\n```python\\nimport timm\\nfrom pprint import pprint\\nmodel_na...\"],[\"Model Summaries\\n\\nThe model architectures included come from a wide variety of sources. Sources, incl...\"],[\"## DenseNet\\n\\n* Implementation: [densenet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002f...\"],[\"## HRNet\\n\\n* Implementation: [hrnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster...\"],[\"## Inception-ResNet-V2\\n\\n* Implementation: [inception_resnet_v2.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpyto...\"],[\"## EfficientNet\\n\\n* Implementation: [efficientnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-mode...\"],[\"## MobileNet-V3\\n\\n* Implementation: [mobilenetv3.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-model...\"],[\"## Res2Net\\n\\n* Implementation: [res2net.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fma...\"],[\"## SelecSLS\\n\\n* Implementation: [selecsls.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002f...\"],[\"## VGG\\n\\n* Implementation: [vgg.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftim...\"],[\"## Xception\\n\\n* Implementation: [xception.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002f...\"],[\"Deep Layer Aggregation\\n\\nExtending  “shallow” skip connections, **Dense Layer Aggregation (DLA)** inc...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `dla102`. You can find the IDs in...\"],[\"Feature Extraction\\n\\nAll of the models in `timm` have consistent mechanisms for obtaining various typ...\"],[\"```\\n\\nOutput:\\n\\n```text\\nUnpooled shape: torch.Size([2, 2048, 7, 7])\\n```\\n\\n#### Remove it later\\n\\n```py\\n\\u003e...\"],[\"```\\n\\nOutput:\\n\\n```text\\nPooled shape: torch.Size([2, 2048])\\n```\\n\\n#### Remove it later\\n\\n```py\\n\\u003e\\u003e\\u003e impor...\"],[\"```\\n\\nOutput:\\n\\n```text\\ntorch.Size([2, 64, 112, 112])\\ntorch.Size([2, 256, 56, 56])\\ntorch.Size([2, 512,...\"],[\"```\\n\\n### Select specific feature levels or limit the stride\\n\\nThere are two additional creation argum...\"],[\"(Gluon) ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to th...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_resnet101_v1b`. You can fi...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun ResNet\\n  Paper:\\n    Title: Deep Residual Learning ...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"- Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.53%\\n      T...\"],[\"Metrics:\\n      Top 1 Accuracy: 80.4%\\n      Top 5 Accuracy: 95.02%\\n- Name: gluon_resnet101_v1s\\n  In C...\"],[\"Top 5 Accuracy: 95.16%\\n- Name: gluon_resnet152_v1b\\n  In Collection: Gloun ResNet\\n  Metadata:\\n    FLO...\"],[\"In Collection: Gloun ResNet\\n  Metadata:\\n    FLOPs: 15165680128\\n    Parameters: 60210000\\n    File Siz...\"],[\"Metadata:\\n    FLOPs: 15166131712\\n    Parameters: 60210000\\n    File Size: 241613584\\n    Architecture:...\"],[\"Parameters: 60320000\\n    File Size: 242032606\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch No...\"],[\"Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Bottleneck Residual Block\\n    - ...\"],[\"- Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - R...\"],[\"- Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection...\"],[\"- Residual Block\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    T...\"],[\"- Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: gluon_resn...\"],[\"- Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: gluon_resnet50_v1s\\n    Crop Pct: '0...\"],[\"Adversarial Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the I...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `adv_inception_v3`. You can find ...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Adversarial Inception v3\\n  Paper:\\n    Title: Advers...\"],[\"ResNet-D\\n\\n**ResNet-D** is a modification on the [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) a...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnet101d`. You can find the ID...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNet-D\\n  Paper:\\n    Title: Bag of Tricks for Image Cla...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 82....\"],[\"Top 5 Accuracy: 96.35%\\n- Name: resnet18d\\n  In Collection: ResNet-D\\n  Metadata:\\n    FLOPs: 2645205760...\"],[\"Parameters: 64690000\\n    File Size: 259662933\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch No...\"],[\"- Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - R...\"],[\"- Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - I...\"],[\"- ImageNet\\n    ID: resnet50d\\n    Crop Pct: '0.875'\\n    Image Size: '224'\\n    Interpolation: bicubic\\n...\"],[\"(Tensorflow) EfficientNet Lite\\n\\n**EfficientNet** is a convolutional neural network architecture and ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_lite0`. You can ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF EfficientNet Lite\\n  Paper:\\n    Title: 'EfficientNet: ...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 74....\"],[\"- Name: tf_efficientnet_lite2\\n  In Collection: TF EfficientNet Lite\\n  Metadata:\\n    FLOPs: 106849443...\"],[\"File Size: 33161413\\n    Architecture:\\n    - 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normal...\"],[\"- Dense Connections\\n    - Dropout\\n    - Inverted Residual Block\\n    - RELU6\\n    Tasks:\\n    - Image C...\"],[\"CSP-DarkNet\\n\\n**CSPDarknet53** is a convolutional neural network and backbone for object detection th...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"EfficientNet\\n\\n**EfficientNet** is a convolutional neural network architecture and scaling method tha...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `efficientnet_b0`. You can find t...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: EfficientNet\\n  Paper:\\n    Title: 'EfficientNet: Rethinki...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77....\"],[\"- Name: efficientnet_b2\\n  In Collection: EfficientNet\\n  Metadata:\\n    FLOPs: 1265324514\\n    Paramete...\"],[\"Architecture:\\n    - 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normalization\\n    - Convolutio...\"],[\"- Dropout\\n    - Inverted Residual Block\\n    - Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n  ...\"],[\"Training Data:\\n    - ImageNet\\n    ID: efficientnet_b3a\\n    Crop Pct: '1.0'\\n    Image Size: '320'\\n   ...\"],[\"- ImageNet\\n    ID: efficientnet_em\\n    Crop Pct: '0.882'\\n    Image Size: '240'\\n    Interpolation: bi...\"],[\"- ImageNet\\n    ID: efficientnet_es\\n    Crop Pct: '0.875'\\n    Image Size: '224'\\n    Interpolation: bi...\"],[\"- ImageNet\\n    ID: efficientnet_lite0\\n    Crop Pct: '0.875'\\n    Image Size: '224'\\n    Interpolation:...\"],[\"timm\\n\\n\\u003cimg class=\\\"float-left !m-0 !border-0 !dark:border-0 !shadow-none !max-w-lg w-[150px]\\\" src=\\\"ht...\"],[\"Read the [quick start guide](quickstart) to get up and running with the `timm` library. You will lea...\"],[\"Scripts\\nA train, validation, inference, and checkpoint cleaning script included in the github root f...\"],[\"`python validate.py \\u002fimagenet\\u002fvalidation\\u002f --model seresnext26_32x4d --pretrained`\\n\\nTo run inference ...\"],[\"(Legacy) SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNeXt](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmetho...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `legacy_seresnext101_32x4d`. You ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Legacy SE ResNeXt\\n  Paper:\\n    Title: Squeeze-and-Excita...\"],[\"Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Interpolation: bi...\"],[\"- ImageNet\\n    Training Resources: 8x NVIDIA Titan X GPUs\\n    ID: legacy_seresnext26_32x4d\\n    LR: 0...\"],[\"- ResNeXt Block\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks...\"],[\"RexNet\\n\\n**Rank Expansion Networks** (ReXNets) follow a set of new design principles for designing bo...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `rexnet_100`. You can find the ID...\"],[\"MixNet\\n\\n**MixNet** is a type of convolutional neural network discovered via AutoML that utilises [Mi...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"(Gluon) SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNext](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_seresnext101_32x4d`. You c...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun SEResNeXt\\n  Paper:\\n    Title: Squeeze-and-Excitati...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"RegNetX\\n\\n**RegNetX** is a convolutional network design space with simple, regular models with parame...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `regnetx_002`. You can find the I...\"],[\"(Tensorflow) EfficientNet\\n\\n**EfficientNet** is a convolutional neural network architecture and scali...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0`. You can fin...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF EfficientNet\\n  Paper:\\n    Title: 'EfficientNet: Rethi...\"],[\"Interpolation: bicubic\\n    RMSProp Decay: 0.9\\n    Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n...\"],[\"LR: 0.256\\n    Epochs: 350\\n    Crop Pct: '0.882'\\n    Momentum: 0.9\\n    Batch Size: 2048\\n    Image Siz...\"],[\"Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Label Smoothing\\n ...\"],[\"Architecture:\\n    - 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normalization\\n    - Convolutio...\"],[\"Top 1 Accuracy: 81.65%\\n      Top 5 Accuracy: 95.72%\\n- Name: tf_efficientnet_b4\\n  In Collection: TF E...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_efficie...\"],[\"Interpolation: bicubic\\n    RMSProp Decay: 0.9\\n    Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n...\"],[\"- ImageNet\\n    ID: tf_efficientnet_b6\\n    LR: 0.256\\n    Epochs: 350\\n    Crop Pct: '0.942'\\n    Moment...\"],[\"- Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n    - Image Classification\\n    Training Techni...\"],[\"Parameters: 87410000\\n    File Size: 351379853\\n    Architecture:\\n    - 1x1 Convolution\\n    - Average ...\"],[\"- Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 85.35%\\n      T...\"],[\"- Name: tf_efficientnet_em\\n  In Collection: TF EfficientNet\\n  Metadata:\\n    FLOPs: 3636607040\\n    Pa...\"],[\"File Size: 22008479\\n    Architecture:\\n    - 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normal...\"],[\"- Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n    - Inverted Residua...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 88....\"],[\"(Legacy) SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fr...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `legacy_seresnet101`. You can fin...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Legacy SE ResNet\\n  Paper:\\n    Title: Squeeze-and-Excitat...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-cadene\\u002fse_resnet10...\"],[\"Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Interpolation: bi...\"],[\"- ImageNet\\n    Training Resources: 8x NVIDIA Titan X GPUs\\n    ID: legacy_seresnet18\\n    LR: 0.6\\n    ...\"],[\"- Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - ...\"],[\"- Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - R...\"],[\"Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural architecture that builds on t...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `inception_resnet_v2`. You can fi...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Inception ResNet v2\\n  Paper:\\n    Title: Inception-v...\"],[\"(Gluon) Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the Incep...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_inception_v3`. You can fin...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun Inception v3\\n  Paper:\\n    Title: Rethinking t...\"],[\"(Legacy) SENet\\n\\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `legacy_senet154`. You can find t...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Legacy SENet\\n  Paper:\\n    Title: Squeeze-and-Excita...\"],[\"Quickstart\\n\\nThis quickstart is intended for developers who are ready to dive into the code and see a...\"],[\"```\\n\\nYou can also list models with a specific pattern in their name.\\n\\n```py\\n\\u003e\\u003e\\u003e import timm\\n\\u003e\\u003e\\u003e from...\"],[\"```\\n\\n## Image Augmentation\\n\\nTo transform images into valid inputs for a model, you can use [`timm.da...\"],[\"```\\n\\nWe can then resolve only the data related configuration by using [`timm.data.resolve_data_confi...\"],[\"```\\n\\n\\u003cTip\\u003e\\n    Note: Here, the pretrained model's config happens to be the same as the generic confi...\"],[\"```\\n\\nNow we can pass that image to the model to get the predictions. We use `unsqueeze(0)` in this c...\"],[\"```\\n\\nIf we check the imagenet labels for the top index, we can see what the model predicted...\\n\\n```p...\"],[\"Optimization\\n\\nThis page contains the API reference documentation for learning rate optimizers includ...\"],[\"SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNext](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresneXt...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnext26d_32x4d`. You can fin...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SEResNeXt\\n  Paper:\\n    Title: Squeeze-and-Excitation Net...\"],[\"Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Interpolation: bi...\"],[\"Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA Titan X GPUs\\n    ID: seresnext26t_32...\"],[\"- ReLU\\n    - ResNeXt Block\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Bloc...\"],[\"SWSL ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the l...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `swsl_resnet18`. You can find the...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SWSL ResNet\\n  Paper:\\n    Title: Billion-scale semi-super...\"],[\"Weights: https:\\u002f\\u002fdl.fbaipublicfiles.com\\u002fsemiweaksupervision\\u002fmodel_files\\u002fsemi_weakly_supervised_resne...\"],[\"Weights: https:\\u002f\\u002fdl.fbaipublicfiles.com\\u002fsemiweaksupervision\\u002fmodel_files\\u002fsemi_weakly_supervised_resne...\"],[\"FBNet\\n\\n**FBNet** is a type of convolutional neural architectures discovered through [DNAS](https:\\u002f\\u002fp...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `fbnetc_100`. You can find the ID...\"],[\"Dual Path Network (DPN)\\n\\nA **Dual Path Network (DPN)** is a convolutional neural network which prese...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: DPN\\n  Paper:\\n    Title: Dual Path Networks\\n    URL: http...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"- Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.83%\\n      T...\"],[\"Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 76.31%\\n      Top 5 Accuracy: 92.97%\\n- Name: dpn...\"],[\"Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.21%\\n      Top 5 Accuracy: 94.42%\\n- Name: dpn...\"],[\"Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.99%\\n      Top 5 Accuracy: 94.84%\\n- Name: dpn...\"],[\"(Tensorflow) MixNet\\n\\n**MixNet** is a type of convolutional neural network discovered via AutoML that...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_mixnet_l`. You can find the I...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF MixNet\\n  Paper:\\n    Title: 'MixConv: Mixed Depthwise ...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78....\"],[\"Top 5 Accuracy: 93.16%\\n- Name: tf_mixnet_s\\n  In Collection: TF MixNet\\n  Metadata:\\n    FLOPs: 3025876...\"],[\"Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural architecture that builds on t...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `inception_resnet_v2`. You can fi...\"],[\"(Gluon) SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNext](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_seresnext101_32x4d`. You c...\"],[\"(Tensorflow) Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_inception_v3`. You can find t...\"],[\"SK-ResNeXt\\n\\n**SK ResNeXt** is a variant of a [ResNeXt](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnext...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `skresnext50_32x4d`. You can find...\"],[\"HRNet\\n\\n**HRNet**, or **High-Resolution Net**, is a general purpose convolutional neural network for ...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"(Legacy) SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNeXt](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmetho...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `legacy_seresnext101_32x4d`. You ...\"],[\"SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNext](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresneXt...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnext26d_32x4d`. You can fin...\"],[\"(Legacy) SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fr...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `legacy_seresnet101`. You can fin...\"],[\"EfficientNet\\n\\n**EfficientNet** is a convolutional neural network architecture and scaling method tha...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"Res2NeXt\\n\\n**Res2NeXt** is an image model that employs a variation on [ResNeXt](https:\\u002f\\u002fpaperswithcod...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `res2next50`. You can find the ID...\"],[\"ECA-ResNet\\n\\nAn **ECA ResNet** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) t...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ecaresnet101d`. You can find the...\"],[\"Instagram ResNeXt WSL\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fre...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ig_resnext101_32x16d`. You can f...\"],[\"Res2Net\\n\\n**Res2Net** is an image model that employs a variation on bottleneck residual blocks, [Res2...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `res2net101_26w_4s`. You can find...\"],[\"Scripts\\n\\nA train, validation, inference, and checkpoint cleaning script included in the github root ...\"],[\"```\\n\\nTo run inference from a checkpoint:\\n\\n```bash\\npython inference.py \\u002fimagenet\\u002fvalidation\\u002f --model ...\"],[\"```\\n\\n### SE-ResNeXt-26-D and SE-ResNeXt-26-T\\n\\nThese hparams (or similar) work well for a wide range ...\"],[\"```\\n### EfficientNet-B3 with RandAugment - 81.5 top-1, 95.7 top-5\\n\\nThe training of this model starte...\"],[\"```\\n### ResNet50 with JSD loss and RandAugment (clean + 2x RA augs) - 79.04 top-1, 94.39 top-5\\n\\nTrai...\"],[\"```\\n### MobileNetV3-Large-100 - 75.766 top-1, 92,542 top-5\\n\\n```bash\\n.\\u002fdistributed_train.sh 2 \\u002fimagen...\"],[\"ResNeSt\\n\\nA **ResNeSt** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet), which i...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnest101e`. You can find the I...\"],[\"ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnet18`. You can find the IDs ...\"],[\"NASNet\\n\\n**NASNet** is a type of convolutional neural network discovered through neural architecture ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `nasnetalarge`. You can find the ...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: NASNet\\n  Paper:\\n    Title: Learning Transferable Ar...\"],[\"(Gluon) Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the Incep...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_inception_v3`. You can fin...\"],[\"(Gluon) SENet\\n\\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and-...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_senet154`. You can find th...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun SENet\\n  Paper:\\n    Title: Squeeze-and-Excitat...\"],[\"Vision Transformer (ViT)\\n\\nThe **Vision Transformer** is a model for image classification that employ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `vit_base_patch16_224`. You can f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Vision Transformer\\n  Paper:\\n    Title: 'An Image is Wort...\"],[\"Batch Size: 4096\\n    Image Size: '224'\\n    Warmup Steps: 10000\\n    Weight Decay: 0.03\\n    Interpolat...\"],[\"- JFT-300M\\n    Training Resources: TPUv3\\n    ID: vit_base_patch16_384\\n    Crop Pct: '1.0'\\n    Moment...\"],[\"Training Techniques:\\n    - Cosine Annealing\\n    - Gradient Clipping\\n    - SGD with Momentum\\n    Trai...\"],[\"- Layer Normalization\\n    - Multi-Head Attention\\n    - Scaled Dot-Product Attention\\n    - Tanh Activ...\"],[\"Architecture:\\n    - Attention Dropout\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n    - ...\"],[\"Metadata:\\n    FLOPs: 174702764032\\n    Parameters: 304720000\\n    File Size: 1218907013\\n    Architectu...\"],[\"Top 1 Accuracy: 85.17%\\n      Top 5 Accuracy: 97.36%\\n- Name: vit_small_patch16_224\\n  In Collection: V...\"],[\"ResNet-D\\n\\n**ResNet-D** is a modification on the [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) a...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"PNASNet\\n\\n**Progressive Neural Architecture Search**, or **PNAS**, is a method for learning the struc...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `pnasnet5large`. You can find the...\"],[\"AdvProp (EfficientNet)\\n\\n**AdvProp** is an adversarial training scheme which treats adversarial examp...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0_ap`. You can ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: AdvProp\\n  Paper:\\n    Title: Adversarial Examples Improve...\"],[\"Interpolation: bicubic\\n    RMSProp Decay: 0.9\\n    Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n...\"],[\"Training Data:\\n    - ImageNet\\n    ID: tf_efficientnet_b1_ap\\n    LR: 0.256\\n    Epochs: 350\\n    Crop P...\"],[\"- Inverted Residual Block\\n    - Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n    - Image Clas...\"],[\"Metadata:\\n    FLOPs: 2275247568\\n    Parameters: 12230000\\n    File Size: 49384538\\n    Architecture:\\n ...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 81....\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_efficie...\"],[\"Interpolation: bicubic\\n    RMSProp Decay: 0.9\\n    Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n...\"],[\"Training Data:\\n    - ImageNet\\n    ID: tf_efficientnet_b6_ap\\n    LR: 0.256\\n    Epochs: 350\\n    Crop P...\"],[\"- Dropout\\n    - Inverted Residual Block\\n    - Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n  ...\"],[\"In Collection: AdvProp\\n  Metadata:\\n    FLOPs: 80962956270\\n    Parameters: 87410000\\n    File Size: 35...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 85....\"],[\"MobileNet v2\\n\\n**MobileNetV2** is a convolutional neural network architecture that seeks to perform w...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `mobilenetv2_100`. You can find t...\"],[\"TResNet\\n\\nA **TResNet** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) that aim...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tresnet_l`. You can find the IDs...\"],[\"ESE-VoVNet\\n\\n**VoVNet** is a convolutional neural network that seeks to make [DenseNet](https:\\u002f\\u002fpaper...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ese_vovnet19b_dw`. You can find ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ESE VovNet\\n  Paper:\\n    Title: 'CenterMask : Real-Time A...\"],[\"- Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 76.82%\\n      T...\"],[\"CSP-ResNet\\n\\n**CSPResNet** is a convolutional neural network where we apply the Cross Stage Partial N...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"AdvProp (EfficientNet)\\n\\n**AdvProp** is an adversarial training scheme which treats adversarial examp...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0_ap`. You can ...\"],[\"(Legacy) SENet\\n\\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `legacy_senet154`. You can find t...\"],[\"(Gluon) SENet\\n\\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and-...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_senet154`. You can find th...\"],[\"SWSL ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-blo...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `swsl_resnext101_32x16d`. You can...\"],[\"ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-block) t...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnext101_32x8d`. You can find ...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNeXt\\n  Paper:\\n    Title: Aggregated Residual Transfor...\"],[\"- Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.3%\\n      To...\"],[\"Top 5 Accuracy: 94.61%\\n- Name: resnext50d_32x4d\\n  In Collection: ResNeXt\\n  Metadata:\\n    FLOPs: 5781...\"],[\"In Collection: ResNeXt\\n  Metadata:\\n    FLOPs: 5472648192\\n    Parameters: 25030000\\n    File Size: 100...\"],[\"PyTorch Image Models\\n- [What's New](#whats-new)\\n- [Introduction](#introduction)\\n- [Models](#models)\\n...\"],[\"❗Updates after Oct 10, 2022 are available in version \\u003e= 0.9❗\\n* Many changes since the last 0.6.x sta...\"],[\"* The Hugging Face Hub (https:\\u002f\\u002fhuggingface.co\\u002ftimm) is now the primary source for `timm` weights. M...\"],[\"### Nov 23, 2023\\n* Added EfficientViT-Large models, thanks [SeeFun](https:\\u002f\\u002fgithub.com\\u002fseefun)\\n* Fix...\"],[\"### Oct 20, 2023\\n* [SigLIP](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2303.15343) image tower weights supported ...\"],[\"### Aug 28, 2023\\n* Add dynamic img size support to models in `vision_transformer.py`, `vision_transf...\"],[\"### Aug 25, 2023\\n* Many new models since last release\\n  * FastViT - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2303.14189...\"],[\"### Aug 11, 2023\\n* Swin, MaxViT, CoAtNet, and BEiT models support resizing of image\\u002fwindow size on c...\"],[\"### May 11, 2023\\n* `timm` 0.9 released, transition from 0.8.xdev releases\\n\\n### May 10, 2023\\n* Huggin...\"],[\"### April 21, 2023\\n* Gradient accumulation support added to train script and tested (`--grad-accum-s...\"],[\"### April 5, 2023\\n* ALL ResNet models pushed to Hugging Face Hub with multi-weight support\\n  * All p...\"],[\"| model                                                                                             ...\"],[\"* Add EVA-02 MIM pretrained and fine-tuned weights, push to HF hub and update model cards for all EV...\"],[\"| model                                              |top1  |top5  |param_count|img_size|\\n|---------...\"],[\"| eva_giant_patch14_336.clip_ft_in1k                 |89.466|98.82 |1013.01    |336     |\\n| eva_larg...\"],[\"* Multi-weight and HF hub for DeiT and MLP-Mixer based models\\n\\n### March 22, 2023\\n* More weights pus...\"],[\"### Feb 26, 2023\\n* Add ConvNeXt-XXLarge CLIP pretrained image tower weights for fine-tune & features...\"],[\"### Feb 7, 2023\\n* New inference benchmark numbers added in [results](results\\u002f) folder.\\n* Add convnex...\"],[\"* Move ImageNet meta-data (synsets, indices) from `\\u002fresults` to [`timm\\u002fdata\\u002f_info`](timm\\u002fdata\\u002f_info\\u002f...\"],[\"### Jan 20, 2023\\n* Add two convnext 12k -\\u003e 1k fine-tunes at 384x384\\n  * `convnext_tiny.in12k_ft_in1k...\"],[\"|model                                                                                              ...\"],[\"|[maxvit_large_tf_512.in21k_ft_in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fmaxvit_large_tf_512.in21k_ft_in1k) ...\"],[\"|[coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fcoatnet_rmlp_2_rw_384.sw_in12k...\"],[\"|[maxvit_large_tf_512.in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fmaxvit_large_tf_512.in1k)                   ...\"],[\"|[maxvit_small_tf_384.in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fmaxvit_small_tf_384.in1k)                   ...\"],[\"|[coatnet_rmlp_2_rw_224.sw_in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fcoatnet_rmlp_2_rw_224.sw_in1k)         ...\"],[\"|[maxvit_tiny_tf_224.in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fmaxvit_tiny_tf_224.in1k)                     ...\"],[\"|[coatnet_bn_0_rw_224.sw_in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fcoatnet_bn_0_rw_224.sw_in1k)             ...\"],[\"### Jan 11, 2023\\n* Update ConvNeXt ImageNet-12k pretrain series w\\u002f two new fine-tuned weights (and p...\"],[\"| model                                     | top1 | param_count |  gmac | macts | hub              ...\"],[\"### Oct 15, 2022\\n* Train and validation script enhancements\\n* Non-GPU (ie CPU) device support\\n* SLUR...\"],[\"### Sept 23, 2022\\n* LAION-2B CLIP image towers supported as pretrained backbones for fine-tune or fe...\"],[\"## Models\\n\\nAll model architecture families include variants with pretrained weights. There are speci...\"],[\"* Aggregating Nested Transformers - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2105.12723\\n* BEiT - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f...\"],[\"* DPN (Dual-Path Network) - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1707.01629\\n* EdgeNeXt - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2206...\"],[\"* EfficientViT (MSRA) - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2305.07027\\n* EVA - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2211.07636\\n* ...\"],[\"* LeViT (Vision Transformer in ConvNet's Clothing) - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2104.01136\\n* MaxViT (Mult...\"],[\"* NF-RegNet \\u002f NF-ResNet - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2101.08692\\n* PNasNet - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1712.00...\"],[\"* Weakly-supervised (WSL) Instagram pretrained \\u002f ImageNet tuned ResNeXt101 - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1...\"],[\"* TResNet - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2003.13630\\n* Twins (Spatial Attention in Vision Transformers) - ht...\"],[\"## Features\\n\\nSeveral (less common) features that I often utilize in my projects are included. Many o...\"],[\"* All models have a common default configuration interface and API for\\n    * accessing\\u002fchanging the ...\"],[\"* PyTorch DistributedDataParallel w\\u002f multi-gpu, single process (AMP disabled as it crashes when enab...\"],[\"* `novograd` by [Masashi Kimura](https:\\u002f\\u002fgithub.com\\u002fconvergence-lab\\u002fnovograd) (https:\\u002f\\u002farxiv.org\\u002fabs...\"],[\"* CutMix (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1905.04899)\\n* AutoAugment (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1805.09501) and Ran...\"],[\"* Bottleneck Transformer - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2101.11605\\n    * CBAM - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1807....\"],[\"## Results\\n\\nModel validation results can be found in the [results tables](results\\u002fREADME.md)\\n\\n## Get...\"],[\"## Awesome PyTorch Resources\\n\\nOne of the greatest assets of PyTorch is the community and their contr...\"],[\"### Pretrained Weights\\nSo far all of the pretrained weights available here are pretrained on ImageNe...\"],[\"```\\n\\n### Latest DOI\\n\\n[![DOI](https:\\u002f\\u002fzenodo.org\\u002fbadge\\u002f168799526.svg)](https:\\u002f\\u002fzenodo.org\\u002fbadge\\u002flates...\"],[\"ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-block) t...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnext101_32x8d`. You can find ...\"],[\"Xception\\n\\n**Xception** is a convolutional neural network architecture that relies solely on [depthwi...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `xception`. You can find the IDs ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Xception\\n  Paper:\\n    Title: 'Xception: Deep Learning wi...\"],[\"Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.05%\\n      Top 5 Accuracy: 94.4%\\n- Name: xcep...\"],[\"Metadata:\\n    FLOPs: 17585702144\\n    Parameters: 39920000\\n    File Size: 160536780\\n    Architecture:...\"],[\"- Convolution\\n    - Dense Connections\\n    - Depthwise Separable Convolution\\n    - Global Average Poo...\"],[\"Dual Path Network (DPN)\\n\\nA **Dual Path Network (DPN)** is a convolutional neural network which prese...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `dpn107`. You can find the IDs in...\"],[\"(Gluon) ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_resnext101_32x4d`. You can...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun ResNeXt\\n  Paper:\\n    Title: Aggregated Residual Tr...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"- Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.63%\\n      T...\"],[\"# Ensemble Adversarial Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural archit...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ens_adv_inception_resnet_v2`. Yo...\"],[\"(Gluon) ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_resnext101_32x4d`. You can...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"SSL ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the la...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ssl_resnet18`. You can find the ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SSL ResNet\\n  Paper:\\n    Title: Billion-scale semi-superv...\"],[\"Weights: https:\\u002f\\u002fdl.fbaipublicfiles.com\\u002fsemiweaksupervision\\u002fmodel_files\\u002fsemi_supervised_resnet18-d92...\"],[\"Weights: https:\\u002f\\u002fdl.fbaipublicfiles.com\\u002fsemiweaksupervision\\u002fmodel_files\\u002fsemi_supervised_resnet50-083...\"],[\"Xception\\n\\n**Xception** is a convolutional neural network architecture that relies solely on [depthwi...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"(Gluon) ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to th...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_resnet101_v1b`. You can fi...\"],[\"(Tensorflow) EfficientNet Lite\\n\\n**EfficientNet** is a convolutional neural network architecture and ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_lite0`. You can ...\"],[\"Models\\n\\n[[autodoc]] timm.create_model\\n\\n[[autodoc]] timm.list_models...\"],[\"SPNASNet\\n\\n**Single-Path NAS** is a novel differentiable NAS method for designing hardware-efficient ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `spnasnet_100`. You can find the ...\"],[\"MobileNet v3\\n\\n**MobileNetV3** is a convolutional neural network that is designed for mobile phone CP...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `mobilenetv3_large_100`. You can ...\"],[\"EfficientNet (Knapsack Pruned)\\n\\n**EfficientNet** is a convolutional neural network architecture and ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `efficientnet_b1_pruned`. You can...\"],[\"```\\n\\n```\\n@misc{aflalo2020knapsack,\\n      title={Knapsack Pruning with Inner Distillation},\\n      aut...\"],[\"Inception v4\\n\\n**Inception-v4** is a convolutional neural network architecture that builds on previou...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"Noisy Student (EfficientNet)\\n\\n**Noisy Student Training** is a semi-supervised learning approach. It ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0_ns`. You can ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Noisy Student\\n  Paper:\\n    Title: Self-training with Noi...\"],[\"RMSProp Decay: 0.9\\n    Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n    Stochastic Depth Surviv...\"],[\"Training Data:\\n    - ImageNet\\n    - JFT-300M\\n    Training Resources: Cloud TPU v3 Pod\\n    ID: tf_eff...\"],[\"- Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 82....\"],[\"RMSProp Decay: 0.9\\n    Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n    Stochastic Depth Surviv...\"],[\"- RandAugment\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    - JFT-300M\\n    Training Resou...\"],[\"Architecture:\\n    - 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normalization\\n    - Convolutio...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 86....\"],[\"RMSProp Decay: 0.9\\n    Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n    Stochastic Depth Surviv...\"],[\"- RandAugment\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    - JFT-300M\\n    Training Resou...\"],[\"Architecture:\\n    - 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normalization\\n    - Convolutio...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 88....\"],[\"Noisy Student (EfficientNet)\\n\\n**Noisy Student Training** is a semi-supervised learning approach. It ...\"],[\"```\\n\\nTo load and preprocess the image:\\n\\n```py \\n\\u003e\\u003e\\u003e import urllib\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e from ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0_ns`. You can ...\"],[\"(Tensorflow) MixNet\\n\\n**MixNet** is a type of convolutional neural network discovered via AutoML that...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_mixnet_l`. You can find the I...\"],[\"SSL ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the la...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ssl_resnet18`. You can find the ...\"],[\"Adversarial Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the I...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `adv_inception_v3`. You can find ...\"],[\"SWSL ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the l...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `swsl_resnet18`. You can find the...\"],[\"(Tensorflow) EfficientNet\\n\\n**EfficientNet** is a convolutional neural network architecture and scali...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0`. You can fin...\"],[\"ESE-VoVNet\\n\\n**VoVNet** is a convolutional neural network that seeks to make [DenseNet](https:\\u002f\\u002fpaper...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ese_vovnet19b_dw`. You can find ...\"],[\"(Gluon) Xception\\n\\n**Xception** is a convolutional neural network architecture that relies solely on ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `gluon_xception65`. You can find ...\"]],\"hovertemplate\":\"source=pytorch-image-models\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"pytorch-image-models, circle\",\"marker\":{\"color\":\"#FF6692\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"pytorch-image-models, circle\",\"showlegend\":true,\"x\":[-14.13644,-0.097055115,-19.753466,5.2226577,-20.129223,-21.022594,-20.987053,-13.950478,-19.750443,-20.13555,-21.051985,-20.999424,-21.083729,-20.979156,-21.101841,-20.861996,-13.407159,-0.15547206,-19.756504,-22.6297,-20.053637,-13.892219,2.745048,-22.627949,-20.088945,-20.98616,-21.02728,-21.068266,-21.146362,-21.102493,-20.975992,-14.151508,-19.755129,-13.838982,-19.755243,-20.118778,-13.704526,2.744763,-22.630283,-20.142717,-20.985235,-21.00887,-21.039639,-20.972017,-13.57146,-0.0072028153,-19.755058,-20.130836,-19.953106,-19.975588,-19.957767,-19.984863,-19.973186,-19.976229,-19.974213,-19.967258,-19.989096,-19.961756,-19.962397,-20.350721,0.028467542,0.09244984,0.10424272,0.095365494,0.12501031,-14.035438,-19.759014,-22.631987,-20.17238,-20.525524,-14.038888,2.7446754,-22.628841,-20.092518,-12.910609,-13.68661,-0.11806476,-19.754389,-11.70567,-11.757424,-0.53806835,-11.689374,-11.55813,-11.546336,-11.630978,-11.641325,-11.663482,-11.413405,-11.624499,-11.501785,-11.651277,-11.695916,-11.578635,-11.654551,-11.666987,-11.699748,-11.630816,-11.696197,-11.620659,-11.631241,-11.618186,-11.689495,-11.720014,-11.76168,-12.14643,-13.684938,-19.757917,-22.629957,-20.13928,-21.21466,-21.106024,-21.172203,-14.184775,-19.761179,-22.629667,-20.25602,-20.593771,-20.954697,-20.716137,-21.047218,-11.66882,-12.590542,-12.380523,-12.387946,-12.580498,-12.521682,-12.150042,-12.22468,-12.440704,-12.462467,-11.905643,-12.922768,-13.944707,-19.757702,-22.628023,-20.176615,-21.104046,-21.143036,-21.126581,-20.553343,-20.514755,-21.04042,-21.06023,3.081228,2.059497,-13.725199,-19.753683,-20.16528,-21.050646,-21.03989,-21.007244,-21.048103,-21.099771,-21.08139,-13.993015,-19.757868,-22.630697,-20.171333,-21.011518,-20.995813,-20.995968,-20.886545,-12.100444,-19.756712,-20.037289,-20.342348,-20.724575,-13.427922,-19.75846,-22.628931,-20.004877,-13.218171,-0.08115264,-19.752789,1.0331486,-20.112225,-20.91199,-20.790579,-20.803244,-20.787113,-20.668741,-20.295412,-1.5615197,-11.777529,-11.734379,-11.780578,-11.706399,-13.993496,2.7446537,-22.629978,-20.084608,-20.927704,-20.30538,-14.014936,2.7447946,-22.628716,-20.115177,-20.14786,-20.350126,-13.450881,-19.75735,-20.0408,-13.881402,2.7447398,-22.62721,-14.011289,-19.754267,-13.725849,2.7450447,-22.63041,-20.102472,-13.794694,-19.755905,-20.057512,-20.234814,-20.302559,-13.815863,-19.754408,-20.168869,-20.438705,-20.551916,-21.004797,-13.551356,2.7449841,-22.63081,-14.033825,-19.754892,-20.108873,6.435394,-11.598966,-10.944604,-11.730622,-11.751996,-11.730593,-11.676908,-11.658313,-11.638494,-11.426064,-11.593123,-11.6796465,-11.578114,-11.387008,-11.399993,-11.685775,-11.588917,5.516231,5.5973067,-11.528061,5.6184826,5.724611,5.771784,5.7234674,5.735118,-11.634549,5.5697474,5.744056,5.7981334,5.7956414,-11.744641,-11.717047,-6.8106875,-11.611633,-11.894884,-11.702021,-11.665106,-11.7246475,5.213114,-14.067165,-19.755154,-20.126171,-21.047869,-20.982874,-21.09097,-21.112854,-20.860071,-20.391758,-21.007822,-21.045662,-21.08517,-13.827259,-19.754616,-20.116928,-13.488318,-0.07325814,-19.755684,-20.364319,-20.270048,-20.595488,-20.535448,-20.715666,-20.834587,-20.687344,-20.762121,-20.77854,-1.4589937,-13.970211,-19.757032,-22.630447,-20.141005,-20.96175,-21.090103,-14.050856,2.7449274,-22.631842,-20.15204,-20.6868,-20.656559,-20.780254,-20.858347,-20.995832,-21.185759,-20.945053,-21.05778,-13.96049,-19.753235,-13.711078,-19.75786,-22.62629,-13.499808,-19.754864,-20.124588,-12.103308,-19.758371,-14.047763,-19.754826,-20.0796,-20.988466,-21.036392,-21.104439,-21.06834,-21.0979,-20.48091,-20.27225,-11.3874655,-11.483181,5.5604196,5.750642,5.675022,5.7438483,5.5308604,-20.818645,-13.297056,2.744827,-19.878262,-13.724518,-19.752502,-20.116776,-13.557858,2.7447522,-22.62901,-20.027075,-14.013241,-19.754131,-14.094664,-19.759623,-20.265324,-20.645256,-20.985006,-20.862053,-21.095388,-13.767892,-19.754793,-20.10652,-19.93478,-19.95164,-19.934864,-19.965487,-19.951052,-19.931967,-19.934797,-19.959042,-19.929226,-19.947105,-19.964653,-20.359486,2.3319678,2.2769554,-13.869132,-19.757143,-13.657859,-19.75532,-14.052085,-19.757715,-20.216146,-21.018682,-21.031357,-21.047901,-21.072657,-13.592356,-19.757013,-20.17242,2.017175,2.1953714,1.502506,-13.646232,2.744645,-22.629564,-20.216719,-20.408031,-20.356276,-20.355614,-20.258276,-20.359194,-20.403503,-20.454971,-20.471361,-20.292381,-20.304222,-20.244678,-20.349007,2.423293,-13.762073,-19.754461,-20.144228,-13.372247,-19.75589,1.7439846,-0.7734103,-11.754621,-12.550319,-12.486377,-12.533176,-12.501419,-12.530302,-12.54613,-12.27736,-10.758638,-13.079503,-13.460864,-19.754747,0.23648533,-0.088036485,-0.14348517,-0.03114565,-0.057079144,-14.059133,-19.754225,-20.219315,-20.252157,-20.413483,-20.677597,-20.650997,-20.528984,-20.561577,-20.57134,-20.64213,-20.713734,-20.712904,-20.756456,-20.72278,-20.831032,-13.398664,-19.758692,-22.631027,-19.908026,-14.057656,-19.75415,-20.260265,-20.227884,-20.63466,-20.778095,-20.801685,-20.714727,-20.919485,-12.22606,-19.758009,-20.149042,-20.307344,-20.569393,-20.75671,-20.884117,-13.794828,2.744803,-22.630596,-11.979355,-19.758722,-20.15204,-20.274961,-20.597742,-20.9508,-21.055962,-20.877018,-20.890623,-20.854057,-20.95947,-8.91437,7.1013403,0.03980325,0.05963043,-14.187076,-19.751858,-20.095358,-21.04552,-21.129047,-21.161076,-13.80924,-19.754408,-13.751856,2.7449703,-22.628595,-14.174445,-19.753492,-20.14795,-20.21568,-20.197838,-20.265518,-13.374196,-19.755901,-12.162546,-19.759192,-20.176693,-21.275557,-21.138498,-21.195173,-21.143608,-20.698689,-21.057735,-21.234198,-21.082642,-21.19935,-21.155731,-20.484766,-20.537474,-20.710592,-21.234497,-20.349169,-14.1371,-19.753473,-20.080225,-21.02153,-21.077667,-21.08472,-21.183582,-21.124868,-13.5986395,-19.755451,-20.042604,-13.327,-19.754576,-20.020924,-13.837831,-19.752571,-20.111416,-0.61697626,-19.750881,0.5646241,-0.3390266,-13.700879,0.039804514,-0.40313876,-1.7220275,-14.175904,-19.753828,-20.135563,-21.03826,-21.15219,-21.137823,-14.019561,-19.759283,-22.628939,-20.227798,-20.749134,-20.620964,-13.821115,-19.754295,-13.434035,2.7448325,-22.630335,-20.156105,-20.201284,-20.445456,-20.710861,-20.677145,-20.612896,-13.795245,-19.755495,-20.196465,-20.393383,-20.67019,-13.41773,-19.756275,-14.194358,-19.754974,-13.424791,-19.757019,-14.070482,-19.753975,-13.573049,2.7450259,-22.629549,-14.154348,-19.751846,-14.158844,-19.75267,-14.123642,-19.75251,-12.27598,2.7449968,-22.628632,-14.071626,-19.754587,-14.063619,-19.755322,-14.005458,-19.756174,-13.951637,-19.758564,0.12333621,0.32944328,-11.666319,-11.691969,-11.680942,-11.719639,-13.860863,-19.75304,-14.026387,-19.75283,-13.741061,-19.754728,-20.095623,-13.392644,-19.756107,-13.780214,-19.755135,-20.112164,-13.713176,-19.756464,-9.776596,-21.108643,-21.296515,-21.152313,-21.194864,-21.252144,-21.281126,-21.033087,-14.036623,2.744931,-22.630793,-13.6242485,-19.754248,-13.75039,-19.759361,-20.182142,-21.363007,-21.099756,-21.283926,-21.114862,-20.316133,-21.00599,-21.216438,-21.208197,-21.174696,-20.556326,-20.305916,-13.988109,-19.754477,-13.846632,-19.754406,-13.843592,-19.754185,-20.154148,-20.453482,-13.77152,2.7450597,-13.749151,-19.757376,-13.830116,-19.7543,-13.852627,-19.75542,-14.149493,-19.761656,-14.111143,-19.758463,-22.63213,-20.228569,-20.479313,-20.664183,-20.536419,-11.131912,-11.771095,2.106061,-11.597242,-11.516863,-11.399452,-11.60657,-11.563261,-11.405524,-11.643757,-11.544654,5.588237,-2.266174,5.593581,5.668953,-11.575314,-11.632912,-11.605839,0.123907395,-11.539503,5.5500774,5.813997,5.7553945,5.7889433,5.788726,5.7623806,5.786044,5.7441554,-11.666778,5.506548,-11.668236,-11.2742,-2.555374,-10.127912,-11.46128,-10.822726,-10.473531,-11.087099,-11.144722,-10.028416,1.0122747,-3.716951,-1.1587887,-1.509581,-11.3039055,-10.532542,-9.918856,-11.161457,-11.028553,5.382953,-14.155115,-19.75515,-13.603453,-19.754927,-20.188223,-20.477903,-20.52498,-20.729189,-13.337796,-19.753496,-14.11567,-19.756334,-20.179129,-20.294247,-20.394352,-13.273661,-19.757875,-14.105142,-19.757793,-22.63124,-14.054691,-19.758911,-20.172354,-20.779058,-20.579336,-13.583818,2.7449002,-22.62919,-14.066722,-19.757952,-12.152095,-19.756542,-0.9842703,-13.559902,-19.752857,-14.103027,-19.756989,-12.213058,-19.757187,1.6217381,-13.379743,2.7450058,-22.630898,-3.2202,-19.760748,-20.092916,-21.21513,-21.064796,-21.220156,-20.457714,-21.19882,-21.141819,-21.18482,-20.415815,-21.30976,-21.175152,-21.178114,-20.282215,-3.0255065,-13.694206,-19.757498,-13.793932,-19.75669,-14.040932,-19.75915,-13.410005,-19.757875,-14.027216,-19.759398,-12.230803,-19.758684,-13.818317,-19.755396,-13.637173,-19.75515],\"xaxis\":\"x\",\"y\":[0.059002608,-2.6958454,-12.901744,-4.283797,7.1138334,7.7215834,7.5910234,0.053327855,-12.900306,7.1682034,7.7417502,7.8218017,7.798817,7.75807,7.754763,7.7461534,0.0845707,-2.79811,-12.903386,-6.2821035,7.12151,0.02573245,-23.084166,-6.2817254,7.1249585,7.7315674,7.6333547,7.604522,7.613268,7.60631,7.6258135,0.0588816,-12.901708,0.02292668,-12.900784,7.1590514,0.010774037,-23.084915,-6.281704,7.158538,7.739839,7.7769966,7.7602887,7.71332,0.0015572975,-2.4773674,-12.901414,7.162076,8.988524,8.957658,8.989403,8.93383,8.9604,8.9585495,8.959241,8.976746,8.925094,8.987293,8.987967,8.145413,-1.9579568,-2.0369964,-2.0505104,-2.0057814,-1.970371,0.06219225,-12.904239,-6.282027,7.209017,7.5361834,0.052875597,-23.084908,-6.282478,7.1373014,-0.07579226,0.03950571,-2.6827948,-12.901889,0.31718633,0.26683626,2.468348,0.35365543,0.25810316,0.21193452,0.33533597,0.29131064,0.29258162,0.15907358,0.33753955,0.38169807,0.23941594,0.31475446,0.26355502,0.30029783,0.27679366,0.34072298,0.2948511,0.3207002,0.30056018,0.3152736,0.31984997,0.34437013,0.33673364,0.19283356,-0.45686525,0.04088077,-12.903891,-6.2811522,7.1522875,7.610557,7.634416,7.61818,0.054684244,-12.906429,-6.281172,7.2586803,7.5210624,7.6130586,7.546084,7.598226,0.32753137,-0.02211813,0.00698366,0.0048393933,-0.038558744,-0.05953016,-0.013778826,-0.003305533,-0.035436213,-0.07020974,-0.10850122,0.031034458,0.012237231,-12.903317,-6.280992,7.1853933,7.6450377,7.615151,7.6438003,7.5172815,7.992513,7.7658825,7.651916,-0.19696195,-0.051416017,0.032846995,-12.900074,7.1856704,7.653351,7.576252,7.6175575,7.6250863,7.581802,7.596443,0.017278315,-12.901929,-6.282585,7.1645546,7.8018985,7.7479467,7.7489514,7.7427936,-0.40644765,-12.902859,7.093569,8.137669,7.696258,0.09634125,-12.904001,-6.2819834,7.0809712,-0.10352032,-2.6587906,-12.901372,-2.5978107,7.1429605,7.764286,7.747863,7.804692,7.7203927,7.7168546,8.234739,2.7124422,0.4548213,0.49257186,0.47417682,0.3823443,0.0066981395,-23.083015,-6.2818804,7.136534,7.6870713,8.18111,0.04612099,-23.084667,-6.2804193,7.153455,8.580872,8.145463,0.087945394,-12.903691,7.0996256,0.026652748,-23.083826,-6.281712,0.00084278145,-12.900847,-0.0073854723,-23.084642,-6.28194,7.109016,0.033705667,-12.900426,7.1390986,8.283277,7.40943,0.014784392,-12.90225,7.213268,7.725619,7.552024,7.6149135,-0.022878626,-23.084396,-6.2815194,0.044192757,-12.901618,7.144416,-4.9425235,0.32134968,-0.08810507,0.30573052,0.34479588,0.30122483,0.3281518,0.3899556,0.27877817,0.18245704,0.25776199,0.34276438,0.26089492,0.10335827,0.39668354,0.2840383,0.28194633,-7.166369,-7.1235805,0.3689893,-7.1209326,-6.8115506,-6.7786117,-6.700054,-6.757368,0.33353212,-7.1462536,-6.932048,-6.853459,-6.796861,0.44426593,0.27930352,1.2382268,0.31629056,0.2816874,0.38192946,0.3317213,0.32430097,-0.25260404,0.039635427,-12.900635,7.1498566,7.629746,7.575017,7.5991673,7.5813336,7.6773653,8.253202,7.694015,7.6299567,7.6004314,0.010942866,-12.9012985,7.155582,-0.02771849,-2.615504,-12.901934,7.291352,8.290491,7.785816,7.7849703,7.6840773,7.6764607,7.6749845,7.6959286,7.68995,2.9139035,0.010873865,-12.90299,-6.2812533,7.1615353,7.7463794,7.624674,0.0632417,-23.08435,-6.2823,7.1800704,7.547351,7.503918,7.421492,7.4476504,7.6061516,7.521634,7.555013,7.589636,0.040499285,-12.901832,0.012122043,-12.903385,-6.2817354,-0.01478679,-12.900028,7.1525493,-0.4636346,-12.903923,0.04760309,-12.902453,7.1137395,7.6032906,7.6112447,7.598338,7.5915465,7.6053987,7.811023,7.629216,0.38906416,0.5020005,-6.7977695,-6.4291563,-6.5246882,-6.5063186,-6.4380784,7.6860085,0.12656727,-23.084631,7.009896,0.0036019986,-12.900517,7.136685,0.006758654,-23.084494,-6.281862,7.091791,0.05536569,-12.90145,0.04877249,-12.904905,7.2794113,7.6953607,7.621598,7.662505,7.593084,0.014973488,-12.900966,7.1296864,9.039378,9.007486,9.052244,8.980204,9.00878,9.055131,9.036772,9.014171,9.042777,9.038498,8.9826145,8.126872,0.48080993,0.1277911,0.009668622,-12.901667,0.028813286,-12.899968,0.04409658,-12.902474,7.1652813,7.6229835,7.615426,7.5949764,7.59505,0.048876647,-12.903218,7.1870384,-2.6299603,-1.0478896,-3.1679204,0.0007716794,-23.084618,-6.2810545,7.321404,7.980261,8.007251,8.010279,8.061738,8.081312,8.018549,7.9776726,7.9778285,8.198888,8.241582,8.27882,8.144843,-3.732727,0.01766298,-12.901107,7.16286,0.09837599,-12.90311,-0.3534245,-0.79440135,0.25209248,-0.031119447,0.027925499,0.017696863,-0.031258855,-0.010989423,0.0041846237,-0.072714955,-0.028950969,0.079682305,-0.028284205,-12.900834,-1.8438113,-1.8418282,-1.9372413,-2.1812172,-1.8849365,0.060515217,-12.902437,7.208826,8.227107,7.805138,7.573058,7.6195507,7.56278,7.546377,7.446823,7.4036703,7.4415708,7.4720607,7.4007263,7.4631553,7.574451,0.098411836,-12.903801,-6.281383,7.0383964,0.05092003,-12.901274,7.2507095,8.299251,7.6306405,7.39965,7.4830217,7.449288,7.583136,-0.43959484,-12.902854,7.124643,8.188129,7.4566507,7.462525,7.499011,0.010823437,-23.082087,-6.281834,-0.44205454,-12.904261,7.170791,8.253521,7.5305834,7.4793816,7.543665,7.555368,7.5522227,7.539515,7.552583,0.600793,-4.6807685,3.1909003,3.528977,0.055101704,-12.90146,7.113426,7.6105814,7.5831304,7.545303,0.024899576,-12.9015665,-0.0022772425,-23.085615,-6.2810974,0.053864885,-12.902261,7.160364,8.283158,8.23371,8.214999,-0.011559029,-12.902019,-0.39087182,-12.904363,7.1830235,7.6223097,7.6146617,7.6096716,7.6191664,7.7202363,7.726134,7.5842223,7.6372304,7.62513,7.6370516,7.7731314,7.502303,7.5105724,7.632915,8.145394,0.05559696,-12.90096,7.1262317,7.727899,7.6123557,7.612462,7.6014285,7.588604,0.081502825,-12.903285,7.1040134,0.107379876,-12.90348,7.08476,0.019571984,-12.899203,7.1324015,-0.48992917,-12.898345,-2.294405,-1.9831791,0.029308816,-2.587981,-3.2150238,1.8013471,0.055640254,-12.902201,7.151041,7.602952,7.592697,7.5766773,0.04845189,-12.904012,-6.2807803,7.237551,7.754017,7.8595104,0.0102702,-12.900344,-0.018880803,-23.084621,-6.2806416,7.179846,8.409129,7.8768306,7.648563,7.674698,7.6903224,0.016482523,-12.903612,7.2227006,8.070568,7.5968184,0.08723841,-12.902721,0.0612643,-12.902536,0.095262595,-12.902849,0.046067514,-12.90012,-0.007753665,-23.084625,-6.2814226,0.054866664,-12.901764,0.055314958,-12.900863,0.058483943,-12.90291,-0.36698627,-23.084501,-6.2816515,0.05310163,-12.9012165,0.043325964,-12.9019375,0.038363416,-12.900467,0.059358347,-12.904143,3.0272026,3.808116,0.46595198,0.5565074,0.5106068,0.5848091,0.006318656,-12.900554,0.06557754,-12.901303,0.0071035116,-12.900543,7.099261,0.100287944,-12.903591,0.021456014,-12.901869,7.1330175,0.020449957,-12.900434,-0.31868786,7.6117997,7.5466776,7.601203,7.6236033,7.6074314,7.5753207,7.6142154,0.05267104,-23.085043,-6.2818933,-0.011395399,-12.900308,0.033854913,-12.905057,7.169812,7.5626664,7.583399,7.6145453,7.6070685,8.204742,7.7098694,7.610689,7.596235,7.611997,7.4990697,8.170904,0.01567227,-12.901109,0.018590273,-12.90099,0.018748606,-12.899618,7.218229,7.6643653,0.013246455,-23.084188,0.03029699,-12.905276,0.015980853,-12.900774,0.030433664,-12.902344,0.042854656,-12.905145,0.050760522,-12.904495,-6.2818356,7.2460785,7.7152114,7.6191845,7.5789347,0.2809842,0.36131904,-1.0865666,0.31152508,0.24974363,0.17346384,0.34504315,0.2710289,0.37800238,0.32162192,0.33803982,-7.0795574,1.2070549,-7.053624,-6.7282925,0.29022825,0.27937645,0.29350874,-2.6057243,0.20661443,-7.0230646,-6.8692193,-6.9048843,-6.9083705,-6.8674026,-6.8447304,-6.8720264,-6.8737626,0.28900582,-7.13912,0.3575698,0.29547808,-0.82866776,-0.49658957,-0.15520021,-0.3632201,-0.3737769,-0.057664905,-0.047296114,-0.6120424,-0.90141267,0.55474806,2.565842,2.587025,-0.16489884,-0.16681437,0.628257,0.24533142,0.10238418,-4.3557987,0.051096555,-12.901874,0.042288005,-12.902378,7.148758,7.479816,7.47606,7.5684657,-0.03193746,-12.901334,0.054191846,-12.903667,7.2018447,8.160505,7.802723,0.08752012,-12.903511,0.05119467,-12.903891,-6.2826505,0.060972,-12.903198,7.214939,7.7769866,7.892838,0.042365294,-23.085377,-6.2805686,0.06815944,-12.903813,-0.4081854,-12.90369,-0.9895207,0.0015961165,-12.901165,0.0012655134,-12.901535,-0.48656264,-12.903119,-4.5092535,0.09459671,-23.08435,-6.281893,-0.52246946,-12.905506,7.069854,7.6096473,7.649615,7.5914826,8.030038,7.608404,7.614217,7.6193666,8.089915,7.6244016,7.6034503,7.619528,8.218741,-0.2343033,0.03895286,-12.904148,0.019248523,-12.90311,0.064916365,-12.903865,0.09072252,-12.903811,0.061922666,-12.904715,-0.40865678,-12.904333,0.005699083,-12.900646,0.054867957,-12.902008],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"--\\ntitle: \\\"Large Language Models: A New Moore's Law?\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f33_large_language_mode...\"],[\"### Deep Learning, Deep Pockets?\\n\\nAs you would expect, training a 530-billion parameter model on hum...\"],[\"I'm left wondering what's the point of it all. Science for the sake of science? Good old marketing? ...\"],[\"Downsizing efforts are also under way in the Natural Language Processing community, using transfer l...\"],[\"You guessed it, that's another way to do transfer learning, and it'll help you save on everything!\\n ...\"],[\"However, the Machine Learning community is still struggling with this topic, and for good reason. Op...\"],[\"Instead of chasing trillion-parameter models (place your bets), wouldn't all be better off if we bui...\"],[\"--\\ntitle: \\\"Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too\\\"\\nthumbn...\"],[\"Now, you can reasonably argue that ECS was not the best approach to serving ML models, but it served...\"],[\"- Requester region: eu-east-1\\n- Requester instance size: t3-medium\\n- Inference endpoint region: eu-e...\"],[\"```\\nWhat we see from these results is pretty encouraging. The application that will consume these en...\"],[\"```\\n\\nWe can say a couple of things about this. Firstly, we want a managed solution to deployment, we...\"],[\"## Other considerations\\n\\n### Deployment Options\\n\\nCurrently you can deploy an Inference Endpoint from...\"],[\"```\\n\\nFor me, what’s lacking is a [custom terraform provider](https:\\u002f\\u002fwww.hashicorp.com\\u002fblog\\u002fwriting-...\"],[\"--\\ntitle: \\\"DuckDB: analyze 50,000+ datasets stored on the Hugging Face Hub\\\" \\nthumbnail: \\u002fblog\\u002fassets...\"],[\"We are happy to share that we recently added another feature to help you analyze datasets on the Hub...\"],[\"```\\n\\nCreate a connection to DuckDB and install and load the `httpfs` extension to allow reading and ...\"],[\"```\\n\\nTo learn more, check out the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets-server\\u002fparque...\"],[\"--\\ntitle: \\\"Building an AI WebTV\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f156_ai_webtv\\u002fthumbnail.gif\\nauthors:\\n- user:...\"],[\"The individual video sequences are purposely made to be short, meaning the WebTV should be seen as a...\"],[\"👉  You will need to use the same prompt for both the generation and upscaling.\\n\\n## Calling the video...\"],[\"export const generateVideo = async (prompt: string) =\\u003e {\\n  const api = await client(\\\"*** URL OF THE ...\"],[\"```\\n\\n\\n## Post-processing\\n\\nOnce an individual take (a video clip) is upscaled, it is then passed to F...\"],[\"let playlist = 'ffconcat version 1.0\\\\n'\\nallFilePaths.forEach(filePath =\\u003e {\\n  playlist += `file '${fi...\"],[\"```\\n\\nThis will generate the following playlist content:\\n\\n```bash\\nffconcat version 1.0\\nfile 'video1.m...\"],[\"```\\n\\nThere are many different configuration options for FFmpeg, for more information in the [officia...\"],[\"\\u003cfigure class=\\\"image flex flex-col items-center text-center m-0 w-full\\\"\\u003e\\n   \\u003cvideo\\n      alt=\\\"demo5....\"],[\"We've seen it with large language models and their ability to synthesize convincing content that mim...\"],[\"\\u003cfigure class=\\\"image flex flex-col items-center text-center m-0 w-full\\\"\\u003e\\n   \\u003cvideo\\n      alt=\\\"demo18...\"],[\"\\u003cfigure class=\\\"image flex flex-col items-center text-center m-0 w-full\\\"\\u003e\\n   \\u003cvideo\\n      alt=\\\"demo2....\"],[\"**Wrong direction:** the model sometimes has trouble with movement and direction. For instance, here...\"],[\"**Text or objects inserted into the image:** the model sometimes injects words from the prompt into ...\"],[\"## Maintaining consistency between scenes\\n\\nIf you plan to create sequences of multiple videos, you w...\"],[\"--\\ntitle: Fine tuning CLIP with Remote Sensing (Satellite) images and captions\\nthumbnail: \\u002fblog\\u002fasse...\"],[\"Over the next two weeks, teams participated in lectures from Hugging Face and Google, trained one or...\"],[\"The ability to search through large collections of images using text queries is an immensely powerfu...\"],[\"In addition, we used the [UCM Dataset](https:\\u002f\\u002fmega.nz\\u002ffolder\\u002fwCpSzSoS#RXzIlrv--TDt3ENZdKN8JA) and t...\"],[\"#### Data Augmentation\\n\\nIn order to regularize our dataset and prevent overfitting due to the size o...\"],[\"### Evaluation\\n\\n#### Metrics\\n\\nA subset of the RSICD test set was used for evaluation. We found 30 ca...\"],[\"| Model-name                               | k=1   | k=3   | k=5   | k=10  |\\n| ---------------------...\"],[\"| bs128x8-lr5e-5-imgaugs-textaugs-3\\u002fckpt-5 | 0.823 | 0.946 | 0.971 | 0.992 |\\n| bs128x8-lr5e-5-wd02\\u002fc...\"],[\"_1 - our best model, 2 - our second best model_\\n\\n\\n#### Demo\\n\\nYou can access the [CLIP-RSICD Demo](ht...\"],[\"--\\ntitle: \\\"Multivariate Probabilistic Time Series Forecasting with Informer\\\" \\nthumbnail: \\u002fblog\\u002fasset...\"],[\"## Introduction\\n\\nA few months ago we introduced the [Time Series Transformer](https:\\u002f\\u002fhuggingface.co...\"],[\"## Informer - Under The Hood\\n\\nBased on the vanilla Transformer ([Vaswani et al., 2017](https:\\u002f\\u002farxiv...\"],[\"### ProbSparse Attention\\n\\nThe main idea of ProbSparse is that the canonical self-attention scores fo...\"],[\"$$\\n\\\\textrm{Attention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{QK^T}{\\\\sqrt{d_k}} )V\\n$$\\n\\nWhere \\\\\\\\(Q\\\\in \\\\math...\"],[\"This is good! But how can we select the \\\\\\\\(u\\\\\\\\) \\\"active\\\" queries to create \\\\\\\\(Q_{reduce}\\\\\\\\)? Let's d...\"],[\"But how can we calculate the term \\\\\\\\(q_ik_j^T\\\\\\\\) in non-quadratic time? Recall that most of the dot-...\"],[\"# calculate u to find the Top-u queries under the sparsity measurement\\n    u = min(sampling_factor *...\"],[\"```\\nNote that in the implementation, \\\\\\\\(U_{part}\\\\\\\\) contain \\\\\\\\(L_Q\\\\\\\\) in the calculation, for stabil...\"],[\"Let's see this in code:\\n    \\n```python\\nfrom torch import nn\\n\\n# ConvLayer is a class with forward pas...\"],[\"```\\n    \\nBy reducing the input of each layer by two, we get a memory usage of \\\\\\\\(O(N\\\\cdot T \\\\log T)\\\\...\"],[\"```\\n\\n## Load Dataset\\n\\nIn this blog post, we'll use the `traffic_hourly` dataset, which is available ...\"],[\"```\\n\\nEach example contains a few keys, of which `start` and `target` are the most important ones. Le...\"],[\"```\\n\\nThe initial values are exactly the same as the corresponding training example. However, this ex...\"],[\"```\\n\\nWe now use `datasets`' [`set_transform`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fv2.7.0\\u002fen\\u002fpackage...\"],[\"```\\n\\n## Define the Model\\n\\nNext, let's instantiate a model. The model will be trained from scratch, h...\"],[\"```\\n\\nThis means that this would look back up to 721 hours (~30 days) for each time step, as addition...\"],[\"```\\n\\nIn this case, there are four additional features, namely \\\"hour of day\\\", \\\"day of week\\\", \\\"day of ...\"],[\"```\\n\\nNote that hours and days are encoded as values between `[-0.5, 0.5]` from GluonTS. For more inf...\"],[\"```\\n\\n## Define Transformations\\n\\nNext, we define the transformations for the data, in particular for ...\"],[\"```\\n\\nThe transformations below are annotated with comments, to explain what they do. At a high level...\"],[\"return Chain(\\n        # step 1: remove static\\u002fdynamic fields if not specified\\n        [RemoveFields(...\"],[\"),\\n            # step 4: add temporal features based on freq of the dataset\\n            # these serv...\"],[\"FieldName.TARGET: \\\"values\\\",\\n                    FieldName.OBSERVED_VALUES: \\\"observed_mask\\\",\\n        ...\"],[\"```\\n\\n## Define `InstanceSplitter`\\n\\nFor training\\u002fvalidation\\u002ftesting we next create an `InstanceSplitt...\"],[\"return InstanceSplitter(\\n        target_field=\\\"values\\\",\\n        is_pad_field=FieldName.IS_PAD,\\n     ...\"],[\"```\\n\\n## Create DataLoaders\\n\\nNext, it's time to create the DataLoaders, which allow us to have batche...\"],[\"# we initialize a Training instance\\n    instance_splitter = create_instance_splitter(config, \\\"train\\\"...\"],[\"```\\n\\n\\n```python\\ndef create_backtest_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n  ...\"],[\"transformation = create_transformation(freq, config)\\n    transformed_data = transformation.apply(dat...\"],[\"```\\n\\n\\n```python\\ntrain_dataloader = create_train_dataloader(\\n    config=config,\\n    freq=freq,\\n    da...\"],[\"```\\n\\nAs can be seen, we don't feed `input_ids` and `attention_mask` to the encoder (as would be the ...\"],[\"```\\n\\nNote that the model is returning a loss. This is possible as the decoder automatically shifts t...\"],[\"model, optimizer, train_dataloader = accelerator.prepare(\\n    model,\\n    optimizer,\\n    train_datalo...\"],[\"```\\n\\n```python\\n# view training\\nloss_history = np.array(loss_history).reshape(-1)\\nx = range(loss_hist...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002finfor...\"],[\"```\\n\\nThe model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`, `i...\"],[\"```\\n\\nWe can evaluate the resulting forecast with respect to the ground truth out of sample values pr...\"],[\"```\\n\\n\\n```python\\nplt.scatter(mase_metrics, smape_metrics, alpha=0.2)\\nplt.xlabel(\\\"MASE\\\")\\nplt.ylabel(\\\"s...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002finfor...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002finfor...\"],[\"So the vanilla Transformer still performs best here! In the future, we hope to better benchmark thes...\"],[\"--\\ntitle: \\\"Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\\\"...\"],[\"For example, the [IMDB dataset](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fimdb) specifies `en` in the YAML met...\"],[\"However, there is a major caveat to this. Most datasets (around 87%) do not specify the language use...\"],[\"### Predicting the Languages of Datasets Using Machine Learning\\n\\nWe’ve already seen that many of the...\"],[\"```\\n\\nHowever, for some of the datasets on the Hub, we might be keen not to download the whole datase...\"],[\"We pass 20 examples to the model representing rows from a dataset. This results in 20 individual lan...\"],[\"We discard the script information since this isn't currently captured consistently as metadata on th...\"],[\"#### Next Steps \\n\\nAs the number of datasets on the Hub grows, metadata becomes increasingly importan...\"],[\"--\\ntitle: \\\"Generating Human-level Text with Contrastive Search in Transformers 🤗\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"**[Remark]** For users who are not familiar with text generation, please refer more details to [this...\"],[\"```\\n\\n****\\n\\n\\u003cspan id='problems_of_decoding_methods'\\u002f\\u003e\\n\\n### 4. Problems of Existing Decoding Methods:\\n...\"],[\"```\\n\\n\\u003cdetails open\\u003e\\n\\u003csummary\\u003e\\u003cb\\u003eModel Output:\\u003c\\u002fb\\u003e\\u003c\\u002fsummary\\u003e\\n\\n```\\nOutput:\\n---------------------------...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n**[Remark]** From the result generated by greedy search, we can see obvious pattern ...\"],[\"```\\n\\n\\u003cdetails open\\u003e\\n\\u003csummary\\u003e\\u003cb\\u003eModel Output:\\u003c\\u002fb\\u003e\\u003c\\u002fsummary\\u003e\\n\\n```\\nOutput:\\n---------------------------...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n**[Remark]** While nucleus sampling can generate text free of repetitions, the seman...\"],[\"\\u003ccenter class=\\\"half\\\"\\u003e\\n    \\u003cimg src=\\\"assets\\u002f115_introducing_contrastive_search\\u002fformulation.png\\\" width...\"],[\"\\u003cspan id='contrastive_generation'\\u002f\\u003e\\n\\n#### 5.2. Generating Text with Contrastive Search:\\n\\nBelow, we u...\"],[\"```\\n\\nThe arguments are as follows:\\n* `--top_k`: The hyperparameter \\\\\\\\(k\\\\\\\\) in contrastive search.\\n* ...\"],[\"```\\nOutput:\\n----------------------------------------------------------------------------------------...\"],[\"\\\"The game of Go is a complex game in which players have to be very careful not to overextend their\\nt...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n**[Remark]** We see that the generated text is of exceptionally high quality. The en...\"],[\"****\\n\\n\\u003cspan id='more_examples'\\u002f\\u003e\\n\\n### 6. More Generated Examples:\\n\\nIn this section, we provide more ...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\u003cspan id='gpt2_greedy_example_one'\\u002f\\u003e\\n\\n##### 6.1.1. Generating Text with Greedy Searc...\"],[\"```\\nOutput:\\n----------------------------------------------------------------------------------------...\"],[\"The researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\nare...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\u003cspan id='gpt2_nucleus_example_one'\\u002f\\u003e\\n\\n##### 6.1.2. Generating Text with Nucleus Sam...\"],[\"```\\nOutput:\\n----------------------------------------------------------------------------------------...\"],[\"The scientists think that it could be ancient folklore that has survived and is no longer attributed...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\n\\u003cspan id='gpt2_contrastive_example_one'\\u002f\\u003e\\n\\n##### 6.1.3. Generating Text with Contra...\"],[\"```\\nOutput:\\n----------------------------------------------------------------------------------------...\"],[\"After analyzing the data, the team determined that the herd consisted of at least three species of u...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\n\\u003cspan id='opt_example_two'\\u002f\\u003e\\n\\n#### 6.2. Example Two - OPT:\\n\\nIn this part, we use th...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\u003cdetails\\u003e\\n\\u003csummary\\u003e\\u003cb\\u003eModel Output: [click to expand]\\u003c\\u002fb\\u003e\\u003c\\u002fsummary\\u003e\\n\\n```\\nOutput:\\n---...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\n\\u003cspan id='opt_greedy_example_two'\\u002f\\u003e\\n\\n##### 6.2.2. Generating Text with Nucleus Samp...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\n\\n\\u003cspan id='opt_contrastive_example_two'\\u002f\\u003e\\n\\n##### 6.2.3. Generating Text with Contra...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\u003cdetails open\\u003e\\n\\u003csummary\\u003e\\u003cb\\u003eModel Output:\\u003c\\u002fb\\u003e\\u003c\\u002fsummary\\u003e\\n\\n```\\nOutput:\\n----------------...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n****\\n\\n\\u003cspan id='resources'\\u002f\\u003e\\n\\n### 7. Resources:\\n\\nFor more details of contrastive sea...\"],[\"```\\n\\n\\n\\n****\\n\\n\\u003cspan id='references'\\u002f\\u003e\\n\\n## Reference:\\n\\u003e [1] Su et al., 2022 [\\\"A Contrastive Framework ...\"],[\"--\\ntitle: \\\"AMD + 🤗: Large Language Models Out-of-the-Box Acceleration with AMD GPU\\\"\\nthumbnail: \\u002fblog...\"],[\"```python\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\n\\nmodel_id = \\\"01-...\"],[\"```\\n\\nOne of the major aspects we have been working on is the ability to run Hugging Face Transformer...\"],[\"* Flash Attention v2 from AMD Open Source efforts in [ROCmSoftwarePlatform\\u002fflash-attention](https:\\u002f\\u002f...\"],[\"We are very excited to make these state of the art acceleration tools available and easy to use to H...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg alt=\\\"\\\" src=\\\"assets\\u002foptimum_amd\\u002ftrans...\"],[\"Performance-wise, we spent a lot of time benchmarking Text Generation Inference on AMD Instinct GPUs...\"],[\"Missing bars for A100 correspond to out of memory errors, as Llama 70B weights 138 GB in float16, an...\"],[\"Of course we'll soon be working on performance optimization for the MI300 lineup, ensuring that both...\"],[\"--\\ntitle: \\\"Machine Learning Experts - Lewis Tunstall\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f60_lewis_tunstall_inte...\"],[\"*Note: Transcription has been slightly modified\\u002freformatted to deliver the highest-quality reading e...\"],[\"This collaboration set the seeds for Leandro and I to eventually join Hugging Face. And I've been he...\"],[\"So for example, if you're trying to build a chatbot you need this model to be very fast and responsi...\"],[\"OpenAI actually provided in their blog posts some examples of the essays that this model had created...\"],[\"### You and other experts at Hugging Face have been working hard on the Hugging Face Course. How did...\"],[\"And he actually used that to apply to Hugging Face.\\n\\n### No way?!\\n\\n**Lewis:** He's joining the Big S...\"],[\"So this accelerates the whole field in a really powerful way. And I can imagine these applications u...\"],[\"### That is super interesting and powerful.\\n\\n**Lewis:** Maybe one thing to mention is that the whole...\"],[\"Although that may work, a lot of the time what happens is you introduce a lot of complexity into the...\"],[\"### If you could go back and do one thing differently at the beginning of your career in machine lea...\"],[\"### What are some of the industries you're most excited to see machine learning applied? \\n\\n**Lewis:*...\"],[\"I think there's hope that in my lifetime I will have a laundry-folding robot.\\n\\n### What have you bee...\"],[\"### What are some of your favorite Machine Learning papers?\\n\\n**Lewis:** Depends on how we measure th...\"],[\"But this example showed that you can actually be quite creative and help mathematicians find new ide...\"],[\"**Lewis:** So when O’Reilly is telling you “We're going to get our illustrator now to design the cov...\"],[\"### I love it. Well, it looks absolutely amazing. A lot of these types of books tend to be quite dry...\"],[\"**Lewis:** See ya, Britney. Bye.\\n\\nThank you for listening to Machine Learning Experts!\\n\\n\\u003ca href=\\\"htt...\"],[\"--\\ntitle: 'Welcome fastai to the Hugging Face Hub'\\nthumbnail: \\u002fblog\\u002fassets\\u002f64_fastai\\u002ffastai_hf_blog....\"],[\"Because of all this, and more (the writer of this post started his journey thanks to the fast.ai cou...\"],[\"![Fastai Models in the Hub](assets\\u002f64_fastai\\u002fhf_hub_fastai.png)\\n\\nIn addition to free model hosting a...\"],[\"```\\n\\n## Creating a fastai `Learner`\\n\\nHere we train the [first model in the fastbook](https:\\u002f\\u002fgithub....\"],[\"```\\n\\n3. Use the `token` argument of the `push_to_hub_fastai` function.\\n\\nYou can input `push_to_hub_f...\"],[\"```\\n\\nThe `Learner` is now in the Hub in the repo named [`espejelomar\\u002fidentify-my-cat`](https:\\u002f\\u002fhuggi...\"],[\"First, upload an image of a cat (or possibly a dog?). The [Colab notebook with this tutorial](https:...\"],[\"```\\nIt works 👇!\\n\\n```py\\n_,_,probs = learner.predict(img)\\nprint(f\\\"Probability it's a cat: {100*probs[1...\"],[\"```\\n\\n```python\\nimport torch\\nimport transformers\\nfrom fastai.text.all import *\\n\\nfrom blurr.text.data....\"],[\"```\\n\\nTry it with a couple sentences and review their sentiment (negative or positive) with `learner_...\"],[\"```\\n\\n\\n## What's next?\\n\\nTake the [fast.ai course](https:\\u002f\\u002fcourse.fast.ai\\u002f) (a new version is coming s...\"],[\"--\\ntitle: \\\"StackLLaMA: A hands-on guide to train LLaMA with RLHF\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f138_stack...\"],[\"By combining these approaches, we are releasing the StackLLaMA model. This model is available on the...\"],[\"## Stack Exchange dataset\\n\\nGathering human feedback is a complex and expensive endeavor. In order to...\"],[\"## Efficient training strategies\\n\\nEven training the smallest LLaMA model requires an enormous amount...\"],[\"In this scenario, a rule of thumb is to allocate ~1.2-1.4GB per billion parameters (depending on the...\"],[\"```bash\\naccelerate launch --multi_gpu --num_machines 1  --num_processes 8 my_accelerate_script.py\\nto...\"],[\"```\\n\\n## Supervised fine-tuning\\n\\nBefore we start training reward models and tuning our model with RL,...\"],[\"```python\\n# load model in 8bit\\nmodel = AutoModelForCausalLM.from_pretrained(\\n        args.model_path...\"],[\"```\\n\\nWe train the model for a few thousand steps with the causal language modeling objective and sav...\"],[\"A trick that works well instead of direct feedback is training a reward model on human annotations c...\"],[\"```python\\nclass RewardTrainer(Trainer):\\n    def compute_loss(self, model, inputs, return_outputs=Fal...\"],[\"```\\n\\nWe utilize a subset of a 100,000 pair of candidates and evaluate on a held-out set of 50,000. W...\"],[\"```\\n\\nThe same template was used for SFT, RM and RLHF stages.\\n\\nA common issue with training the langu...\"],[\"# Compute sentiment score\\n    texts = [q + r for q, r in zip(batch[\\\"query\\\"], batch[\\\"response\\\"])]\\n   ...\"],[\"```\\n\\nWe train for 20 hours on 3x8 A100-80GB GPUs, using the 🤗 research cluster, but you can also get...\"],[\"In general in RL, you want to achieve the highest reward. In RLHF we use a Reward Model, which is im...\"],[\"One needs to be careful when generating the responses and we suggest to always use a simple sampling...\"],[\"## Citation\\n\\n```bibtex\\n@misc {beeching2023stackllama,\\n    author       = { Edward Beeching and\\n     ...\"],[\"```\\n\\n## Acknowledgements\\n\\nWe thank Philipp Schmid for sharing his wonderful [demo](https:\\u002f\\u002fhuggingfa...\"],[\"--\\ntitle:  Deploy LLMs with Hugging Face Inference Endpoints\\nthumbnail: \\u002fblog\\u002fassets\\u002f155_inference_e...\"],[\"Before we start, let's refresh our knowledge about Inference Endpoints. \\n\\n## What is Hugging Face In...\"],[\"You can get started with Inference Endpoints at: [https:\\u002f\\u002fui.endpoints.huggingface.co\\u002f](https:\\u002f\\u002fui.e...\"],[\"![Select Instance Type](assets\\u002f155_inference_endpoints_llm\\u002finstance-selection.png \\\"Select Instance T...\"],[\"```\\n\\nYou can use different parameters to control the generation, defining them in the `parameters` a...\"],[\"## 3. Stream responses in Javascript and Python\\n\\nRequesting and generating text with LLMs can be a t...\"],[\"```\\n\\nWe can create a `InferenceClient` providing our endpoint URL and credential alongside the hyper...\"],[\"```\\n\\nWe can create a `HfInferenceEndpoint` providing our endpoint URL and credential alongside the h...\"],[\"```\\n\\nReplace the `process.stdout` call with the `yield` or with a function you want to stream the to...\"],[\"--\\ntitle: \\\"2D Asset Generation: AI for Game Development #4\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games...\"],[\"Requirements:\\n- Your preferred image-editing software, such as [Photoshop](https:\\u002f\\u002fwww.adobe.com\\u002fpro...\"],[\"In this section, I'll walk through how I generated a corn icon for the farming game. As a starting p...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"[Dreambooth](https:\\u002f\\u002fdreambooth.github.io\\u002f), [textual inversion](https:\\u002f\\u002ftextual-inversion.github.io...\"],[\"--\\ntitle: \\\"Supercharged Customer Service with Machine Learning\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f61_superchar...\"],[\"Assuming that a) messages of very unsatisfied customers represent only a fraction of all messages an...\"],[\"Let's take a look at all available Datasets on the [Hugging Face Hub](https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"Now we can inspect those datasets in more detail by reading through the dataset card, which ideally ...\"],[\"Let's quickly go over the dataset cards of the models above:\\n\\n-   *GLUE* is a collection of small da...\"],[\"As a final note, we recommend making use of Hub's dataset functionality even when working with priva...\"],[\"Let's take a look at all models that have been fine-tuned on Amazon Reviews Multi. You can find the ...\"],[\"However, both of the above resources are currently suboptimal. The model summary is not always kept ...\"],[\"## Training \\u002f Fine-tuning a model with 🤗 Transformers and 🤗 Datasets\\n\\nIn this section, we will jump ...\"],[\"```\\n\\nAlso, we install the 🤗 Transformers and 🤗 Datasets libraries to run this notebook. Since we wil...\"],[\"```\\n\\n\\n\\n### Preprocess the dataset\\n\\nBefore we can start training the model, we should bring the datas...\"],[\"```\\n\\n\\n\\nGreat, that was fast 🔥. Let's take a look at the structure of the dataset.\\n\\n\\n```python\\nprint(...\"],[\"```\\n\\n**Output:**\\n```\\n    Stars: 1\\n    Review: This product caused severe burning of my skin. I have ...\"],[\"```\\n\\n\\n\\n\\nAs mentioned before, we will use the `\\\"review_body\\\"` as the model's input and `\\\"stars\\\"` as t...\"],[\"```\\n\\n\\nTo apply this function to all data samples in our dataset, we use the [`map`](https:\\u002f\\u002fhuggingf...\"],[\"```\\n\\n**Output:**\\n```\\n    Input IDS: [1, 329, 714, 2044, 3567, 5127, 265, 312, 1158, 260, 273, 286, 4...\"],[\"```\\n\\n```\\n    Some weights of the model checkpoint at microsoft\\u002fdeberta-v3-base were not used when in...\"],[\"```\\n\\n\\n\\nNext, we load a data collator. A [data collator](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmai...\"],[\"```\\n\\nNext, we define the `compute_metrics` which will be applied to the predicted outputs of the mod...\"],[\"```\\n\\n\\nPutting it all together, we can finally instantiate the Trainer by passing all required compon...\"],[\"**Output:**\\n\\u003cdiv\\u003e\\n\\u003ctable\\u003e\\u003cp\\u003e\\n  \\u003ctbody\\u003e\\n \\u003ctr style=\\\"text-align: left;\\\"\\u003e\\n  \\u003ctd\\u003eStep\\u003c\\u002ftd\\u003e\\n  \\u003ctd\\u003eTrainin...\"],[\"\\u003ctd\\u003e0.910928\\u003c\\u002ftd\\u003e\\n    \\u003ctd\\u003e0.608400\\u003c\\u002ftd\\u003e\\n  \\u003c\\u002ftr\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd\\u003e30000\\u003c\\u002ftd\\u003e\\n    \\u003ctd\\u003e0.806700\\u003c\\u002ftd\\u003e\\n    ...\"],[\"**Output:**...\"],[\"```\\n    ***** Running Evaluation *****\\n      Num examples = 5000\\n      Batch size = 8\\n    Saving mod...\"],[\"```\\n\\n### Evaluate \\u002f Analyse the model\\n\\nNow that we have fine-tuned the model we need to be very care...\"],[\"```\\n    ***** Running Prediction *****\\n      Num examples = 5000\\n      Batch size = 8\\n```\\n\\n\\n**Output...\"],[\"```\\n\\n\\n\\nThe results are very similar to performance on the validation dataset, which is usually a goo...\"],[\"# Second let's compute how many satisfied messages we unnecessarily reply to\\n    satisfied_label_idx...\"],[\"```\\n\\n\\nWe again instantiate the `Trainer` to easily run the evaluation.\\n\\n\\n```python\\ntrainer = Trainer...\"],[\"```\\n\\n\\nand again upload everything on the Hub.\\n\\n\\n```python\\ntrainer.push_to_hub()\\n```\\n\\n**Output:**\\n```...\"],[\"```\\n\\n\\n\\nThe data is now saved [here](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten\\u002fdeberta_amazon_reviews_v...\"],[\"If you're looking for **highly optimized** solutions which don't require any technical knowledge, yo...\"],[\"--\\ntitle: \\\"Accelerating Document AI\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f112_document-ai\\u002fthumbnail.png\\nauthors:...\"],[\"There are at least six general use cases for building document AI solutions. These use cases differ ...\"],[\"![png](assets\\u002f112_document-ai\\u002focr.png)\\n\\nOCR is a backbone of Document AI use cases as it's essential...\"],[\"A basic approach is applying OCR on a document image, after which a [BERT](https:\\u002f\\u002fhuggingface.co\\u002fdo...\"],[\"\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv itemscope itemprop=\\\"mainEntity\\\" itemty...\"],[\"![png](assets\\u002f112_document-ai\\u002fDIT.png)\\n\\nDocument layout analysis with DiT.\\n\\nDocument layout analysis...\"],[\"The first version of LayoutLM (now known as LayoutLMv1) was released in 2020 and dramatically improv...\"],[\"![png](assets\\u002f112_document-ai\\u002flayoutlm.png)\\n\\nData scientists are finding document layout analysis an...\"],[\"The approach for table detection and structure recognition is similar to document layout analysis in...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv item...\"],[\"DocVQA is typically evaluated using the Average Normalized Levenshtein Similarity (ANLS) metric. For...\"],[\"\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv itemscope itemprop=\\\"mainEntity\\\" itemty...\"],[\"Data preparation for Document AI is critical and challenging. It's crucial to have properly annotate...\"],[\"The flexibility of building your models leads to many options for data scientists. Our strong recomm...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n\\n### Next Steps\\n\\nAre you seeing the possibilities of Document AI? ...\"],[\"A table of the currently available Transformers models achieving state-of-the-art performance on Doc...\"],[\"| model | paper | license | checkpoints |\\n| --- | --- | --- | --- |\\n| [Donut](https:\\u002f\\u002fhuggingface.co...\"],[\"| [LayoutLMv3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmodel_doc\\u002flayoutlmv3) | [arxiv](http...\"],[\"| [LiLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmodel_doc\\u002flilt) | [arxiv](https:\\u002f\\u002farxiv.or...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv item...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n \\u003c\\u002fhtml\\u003e...\"],[\"--\\ntitle: \\\"How we sped up transformer inference 100x for 🤗 API customers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f09...\"],[\"-| Naive version                                                                                    ...\"],[\"## Compilation FTW: the hard to get 10x\\nNow this is where it gets really tricky. In order to get the...\"],[\"To reach that bar, as Machine Learning Engineers at Hugging Face we certainly have an unfair advanta...\"],[\"--\\ntitle: \\\"Introducing HuggingFace blog for Chinese speakers: Fostering Collaboration with the Chine...\"],[\"In addition, the Chinese AI community has been actively engaged in creating trendy Spaces, such as [...\"],[\"We are excited to announce that we will continue to strengthen our ties with the Chinese AI communit...\"],[\"--\\ntitle: \\\"How to generate text: using different decoding methods for language generation with Trans...\"],[\"This blog post gives a brief overview of different decoding strategies\\nand more importantly shows ho...\"],[\"```\\n\\n``` python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\nimport torch\\n\\ntorch_dev...\"],[\"```\\n\\n\\n## Greedy Search\\n\\nGreedy search is the simplest decoding method.\\nIt selects the word with the ...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n\\nAlright\\\\! We have generated our first short text with GPT2 😊. The\\ngenerated words following t...\"],[\"\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f02_how-to-generate\\u002fbeam_search.png\\\" alt=\\\"beam search\\\" style=\\\"margin: auto; di...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\nAs can be seen, the five beam hypotheses are only marginally different\\nto each other - which s...\"],[\"$$ w_t \\\\sim P(w|w_{1:t-1}) $$\\n\\nTaking the example from above, the following graphic visualizes langu...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n\\nInteresting\\\\! The text seems alright - but when taking a closer look, it\\nis not very coherent...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n\\nOK. There are less weird n-grams and the output is a bit more coherent\\nnow\\\\! While applying t...\"],[\"Let's see how *Top-K* can be used in the library by setting `top_k=50`:\\n\\n\\n\\n``` python\\n# set seed to ...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n\\nNot bad at all\\\\! The text is arguably the most *human-sounding* text so\\nfar. One concern thou...\"],[\"\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f02_how-to-generate\\u002ftop_p_sampling.png\\\" alt=\\\"Top p sampling\\\" style=\\\"margin: au...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n```\\nOutput:\\n----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\nCool, now you should have all the tools to let your model write your\\nstories with `transformer...\"],[\"## Appendix\\n\\n`generate` has evolved into a highly composable method, with flags to manipulate the re...\"],[\"--\\ntitle: \\\"Accelerate your models with 🤗 Optimum Intel and OpenVINO\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f113_ope...\"],[\"​Let us show you how to get started in minutes!​\\n\\n## Quantizing a Vision Transformer with Optimum In...\"],[\"```\\n\\nNext, moving to a Python environment, we import the appropriate modules and download the origin...\"],[\"```\\n\\nAs usual with image datasets, we need to apply the same image transformations that were used at...\"],[\"```\\n\\nWe're now ready to quantize the model. The `OVQuantizer.quantize()` method quantizes the model ...\"],[\"```\\n\\nA minute or two later, the model has been quantized. We can then easily load it with our [`OVMo...\"],[\"```\\n\\n​To verify that quantization did not have a negative impact on accuracy, we applied an evaluati...\"],[\"```\\n\\nLooking at the quantized model, we see that its memory size decreased by **3.8x** from 344MB to...\"],[\"--\\ntitle: Guiding Text Generation with Constrained Beam Search in 🤗 Transformers\\nthumbnail: \\u002fblog\\u002fas...\"],[\"However, this is actually a very non-trivial problem. This is because the task requires us to force ...\"],[\"And what if you have multiple constraints with varying requirements? What if you want to force the p...\"],[\"```\\n!pip install -q git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers.git\\n```\\n\\n\\n```python\\nfrom transfo...\"],[\"```\\n\\n    Output:\\n    -------------------------------------------------------------------------------...\"],[\"```\\n\\n    Output:\\n    -------------------------------------------------------------------------------...\"],[\"force_word = \\\"scared\\\"\\nforce_flexible = [\\\"scream\\\", \\\"screams\\\", \\\"screaming\\\", \\\"screamed\\\"]\\n\\nforce_words_i...\"],[\"```\\n\\n    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\\n\\n\\n    Output:\\n    -...\"],[\"![Beam search step 1](https:\\u002f\\u002fraw.githubusercontent.com\\u002fhuggingface\\u002fblog\\u002fmain\\u002fassets\\u002f53_constrained_...\"],[\"Let's say that we're trying to force the phrase `\\\"is fast\\\"` in the generated output. \\n\\nIn the tradit...\"],[\"Banks solve this problem by creating a *balance* between fulfilling the constraints and creating sen...\"],[\"And finally notice how we ended up at a sensible output that contains our constraint phrase: `\\\"The d...\"],[\"print(\\\"Output:\\\\n\\\" + 100 * '-')\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))...\"],[\"```\\n\\n    Output:\\n    -------------------------------------------------------------------------------...\"],[\"```\\n\\nor:\\n```python\\nstarting_text = \\\"The woman\\\"\\ntemplate = [\\\"the\\\", \\\"\\\", \\\"\\\", \\\"University\\\", \\\"\\\", \\\"in\\\"]\\n\\np...\"],[\"Thanks to everybody that gave guidance for this feature contribution: Patrick von Platen for being i...\"],[\"--\\ntitle: \\\"Making a web app generator with open ML models\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f153_text_to_webap...\"],[\"Some of those techniques are now available as ready-to-use NPM libraries:\\n\\n- Using AI\\u002fML libraries s...\"],[\"## Architecture\\n\\nWe are going to use NodeJS to create our generative AI web server.\\n\\nThe model will ...\"],[\"```\\n\\nThen, we can install the Hugging Face Inference client:\\n\\n```html\\nnpm install @huggingface\\u002finfer...\"],[\"```\\n\\nYou can now tell the inference client to use our private endpoint and call our model:\\n\\n```javas...\"],[\"```\\n\\nStart your web server:\\n\\n```bash\\nnpm run start\\n```\\n\\nand open `https:\\u002f\\u002flocalhost:3000?prompt=some...\"],[\"```\\n\\n### Preventing hallucination\\n\\nIt can be difficult to reliably prevent hallucinations and failur...\"],[\"```\\n\\n## Adding support for images\\n\\nWe now have a system that can generate HTML, CSS and JS code, but...\"],[\"```\\n\\nYou can also try to be more specific, for example:\\n\\n```\\nOnly generate a few images and use desc...\"],[\"```\\n\\nTo make this work, you will have to make some changes:\\n\\n```javascript\\n...\\n\\n\\u002f\\u002f going to localhos...\"],[\"```\\n\\n## Going further\\n\\nThe final demo Space includes a [more complete example](https:\\u002f\\u002fhuggingface.c...\"],[\"--\\ntitle: 'Liftoff! How to get started with your first ML project 🚀'\\nthumbnail: \\u002fblog\\u002fassets\\u002f84_firs...\"],[\"\\u003e Compute dense vector representations for sentences, paragraphs, and images\\n\\nIn a nutshell, Sentenc...\"],[\"Comparing sentences by similarity means that if we have a collection of sentences or paragraphs, we ...\"],[\"Second, Sentence Transformers is an accessible entry-point to many important ML concepts that you ca...\"],[\"Third, embeddings are key for several industrial applications. Google searches use embeddings to [ma...\"],[\"1. **Do a brain dump of everything you know the tool’s capable of**: For Sentence Transformers this ...\"],[\"4. **Ideate:** Spend some time brainstorming on what different combination of the elements from the ...\"],[\"For my first Sentence Transformers project, I remembered that I had a little dataset of popular song...\"],[\"\\u003cdiv class=\\\"hidden xl:block\\\"\\u003e\\n\\u003cdiv style=\\\"display: flex; flex-direction: column; align-items: center...\"],[\"## What can you expect to learn from your first project?\\n\\nSince every project is unique, your learni...\"],[\"Further reading:\\n\\n- [Getting Started with Embeddings](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fgetting-started-wi...\"],[\"--\\ntitle: \\\"Fit More and Train Faster With ZeRO via DeepSpeed and FairScale\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"This blog post will describe how you can benefit from ZeRO regardless of whether you own just a sing...\"],[\"```\\nexport BS=16\\npython -m torch.distributed.launch --nproc_per_node=2 .\\u002ffinetune_trainer.py \\\\\\n--mod...\"],[\"```\\n\\nWe are just using the `DistributedDataParallel` (DDP) and nothing else to boost the performance...\"],[\"Let's look at the results of these six test runs:\\n\\n| Method                    | max BS |  train tim...\"],[\"If you would like to experiment with this benchmark yourself or want to know more details about the ...\"],[\"```\\nexport BS=1\\nCUDA_VISIBLE_DEVICES=0 .\\u002ffinetune_trainer.py \\\\\\n--model_name_or_path t5-3b --n_train ...\"],[\"```\\net voila! We get a batch size of 20 trained just fine. I could probably push it even further. Th...\"],[\"```\\nWe can't compare these to the baseline, since the baseline won't even start and immediately fail...\"],[\"This idea could be difficult to grasp, and you will find my attempt at an explanation [here](https:\\u002f...\"],[\"```\\nRuntimeError: CUDA out of memory. Tried to allocate 1.48 GiB (GPU 0; 23.65 GiB total capacity;\\n1...\"],[\"```\\nThe program wants to allocate ~1.5GB and the GPU still has some 6-7GBs of unused memory, but it ...\"],[\"You can, of course, modify your own trainer to integrate DeepSpeed and FairScale, based on each proj...\"],[\"* Paper: [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https:\\u002f\\u002farxiv.org\\u002fab...\"],[\"We were quite astonished at the amazing level of support we received from the FairScale and DeepSpee...\"],[\"--\\ntitle: \\\"Ethics and Society Newsletter #1\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f103_ethics-soc-1\\u002fthumbnail.png...\"],[\"To this end, we share some of our recent thinking and work in the new Hugging Face _Ethics and Socie...\"],[\"- We ground the creation of these tools and artifacts in _responsibility_ for the impacts of what we...\"],[\"Building from these basics, we are taking an approach to operationalizing values that center the con...\"],[\"In the coming months, we will be putting together several other pieces on values, tensions, and ethi...\"],[\"--\\ntitle: \\\"Open LLM Leaderboard: DROP deep dive\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fevaluating-mmlu-leaderboard...\"],[\"We added it to the Open LLM Leaderboard three weeks ago, and observed that the f1-scores of pretrain...\"],[\"Normalization happens in several steps, both for generation and gold:\\n1) **Split on separators** `|`...\"],[\"## Diving into the results\\nExtending our investigations, our friends at [Zeno](https:\\u002f\\u002fzenoml.com) j...\"],[\"We hypothesized that both these problems could be fixed by using `\\\\n` instead of `.` as an end of ge...\"],[\"In 10% of the cases, the gold answer is a floating number (for example `12.25`) and model prediction...\"],[\"Thanks to the many community members who pointed out issues on DROP scores, and many thanks to the E...\"],[\"--\\ntitle: \\\"Evaluating Language Model Bias with 🤗 Evaluate\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f112_evaluating-ll...\"],[\"The workflow has two main steps:\\n- Prompting the language model with a predefined set of prompts (ho...\"],[\"```python\\n\\u003e\\u003e\\u003e male_prompts = [\\n'The janitor reprimanded the accountant because he',\\n'The carpenter a...\"],[\"```\\n\\nAlthough we define these prompts directly for the sake of example here, more can be extracted d...\"],[\"```\\nAs you can see above, a simple difference in pronoun can result in a higher toxicity ratio for f...\"],[\"```\\n\\nAnd as before, we use GPT-2 to generate completions:\\n```python\\n\\u003e\\u003e\\u003e profession1_completions = [\\\"...\"],[\"```\\nBased on the Regard scores above, the completions for profession 1 (truck drivers) have a more n...\"],[\"```\\n\\nHigher HONEST scores mean more hurtful completions. Based on the model completions above, we ha...\"],[\"*- Written by Sasha Luccioni and Meg Mitchell, drawing on work from the Evaluate crew and the Societ...\"],[\"--\\ntitle: \\\"Can foundation models label data like humans?\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fllm-leaderboard\\u002fle...\"],[\"![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fllm-leaderboa...\"],[\"## Evaluating preferences of open-source models\\n\\nAny point in a training process where humans are ne...\"],[\"To do this, we curated a held-out set of instruction prompts and completions from a popular set of o...\"],[\"With these completions, we set off to evaluate the quality of the models with Scale AI and GPT-4. \\nT...\"],[\"****************Elo rankings without ties (bootstrapped from 1000 rounds of sampling games)*********...\"],[\"****************Elo rankings w\\u002f ties (bootstrapped from 1000 rounds of sampling games)**************...\"],[\"**Elo rankings w\\u002f ties (bootstrapped from 1000 rounds of sampling games)**\\n\\n*Reminder, in the Likert...\"],[\"```\\n### Question\\n{question}\\n\\n### The Start of Assistant 1's Answer\\n{answer_1}\\n### The End of Assista...\"],[\"```\\n\\nThe histogram of responses from GPT-4 starts to show a clear issue with LLM based evaluation: *...\"],[\"## Related work\\n\\nWe are not the only ones to share the GPT-4 may not be a perfect tool for training ...\"],[\"Below we’ve included a couple examples of what the evaluations look like to give you a sense why and...\"],[\"I am excited about what lies ahead and can't wait to join the team at [Company Name]. Thank you agai...\"],[\"This answer only takes up 34 characters compared to longer explanations like sunlight reaching earth...\"],[\"### Ablations\\n\\n**GPT-4 Elo with score rather than ranking**\\n\\nOther evaluation benchmarks use a ranki...\"],[\"```\\n\\nThis resulted in the histogram of rankings below, which flipped the bias from before (but did n...\"],[\"## Takeaways and discussion\\n\\nThere is a lot here, but the most important insights in our experiments...\"],[\"Continuing with this, it is worth noting that ChatGPT (a slightly less high performance model) actua...\"],[\"- **Correct generation parameters**: in the early stages of our experiments, we had to spend substan...\"],[\"### Resources and citation\\n\\n- More information on our labeling instructions can be found [here](http...\"],[\"```\\n@article{rajani2023llm_labels,\\n  author = {Rajani, Nazneen, and Lambert, Nathan and Han, Sheon a...\"],[\"--\\ntitle: \\\"Student Ambassador Program’s call for applications is open!\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f67_a...\"],[\"🎎 Network of peers with whom ambassadors can collaborate. \\n\\n🧑🏻‍💻 Workshops and support from the Hugg...\"],[\"--\\ntitle: \\\"Training a language model with 🤗 Transformers using TensorFlow and TPUs\\\"\\nthumbnail: \\u002fblog...\"],[\"Unlike our Colab example, however, this example is designed to be **scalable** and much closer to a ...\"],[\"## What to expect\\n\\nWe’re going to train a [RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_d...\"],[\"## Getting the data and training a tokenizer\\n\\nAs mentioned, we used the [WikiText dataset (v1)](http...\"],[\"## Tokenizing the data and creating TFRecords\\n\\nOnce the tokenizer is trained, we can use it on all t...\"],[\"## Training a model on data in GCS\\n\\nIf you’re familiar with using 🤗 Transformers, then you already k...\"],[\"```\\n\\nBut since we’re in the TPU territory, we need to perform this initialization under a strategy s...\"],[\"```\\n\\nSimilarly, the optimizer also needs to be initialized under the same strategy scope with which ...\"],[\"```\\n\\nIf `args.dataset` contains the `gs:\\u002f\\u002f` identifier, TensorFlow will understand that it needs to ...\"],[\"[{'score': 0.1003185287117958,\\n  'token': 52,\\n  'token_str': 'be',\\n  'sequence': 'Goal of my life is...\"],[\"```\\n\\n## Conclusion\\n\\nIf there’s one thing we want to emphasize with this example, it’s that TPU train...\"],[\"--\\ntitle: \\\"A Dive into Vision-Language Models\\\"\\nthumbnail: \\u002fblog\\u002f\\u002fassets\\u002f128_vision_language_pretrain...\"],[\"## Table of contents\\n\\n1. [Introduction](#introduction)\\n2. [Learning Strategies](#learning-strategies...\"],[\"To predict something like that, the model needs to understand both the input image and the text prom...\"],[\"We’ll cover the following themes in the pre-training objectives: \\n- **Contrastive Learning:** Aligni...\"],[\"Contrastive learning is a commonly used pre-training objective for vision models and has proven to b...\"],[\"### 2) PrefixLM\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocume...\"],[\"Models that leverage a unified multi-modal architecture to fuse visual information into a language m...\"],[\"Models such as [Frozen](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2106.13884) and [ClipCap](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2111.0...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"### 4) Masked-Language Modeling \\u002f Image-Text Matching\\n\\nAnother line of vision-language models uses a...\"],[\"For the ITM objective, given an image and caption pair, the task is to predict whether the caption m...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"### Pre-training datasets\\n\\nVision-language models are typically pre-trained on large multi-modal dat...\"],[\"Even image-text datasets consisting solely of human-generated captions, such as Flickr30K, are inher...\"],[\"Models fine-tuned on the question-answering downstream task, such as [ViLT](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f21...\"],[\"Note that vision-language models are used for various classical NLP and computer vision tasks such a...\"],[\"* [CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclip)\\n* [FLAVA](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"* [TrOCR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmodel_doc\\u002ftrocr) (an instance of the `Vis...\"],[\"While models such as CLIP, FLAVA, BridgeTower, BLIP, LiT and `VisionEncoderDecoder` models provide j...\"],[\"Let’s go ahead and experiment with some of these models. We will use [ViLT](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"```\\n\\nNext, we will download a random image of two cats and preprocess both the image and our  query ...\"],[\"```\\n\\nStraight-forward, right? Let’s do another demonstration with CLIPSeg and see how we can perform...\"],[\"```\\n\\nSimilar to ViLT, it’s important to refer to the [original work](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2112.1000...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"We also see a massive surge of works that leverage joint vision-language representations for image m...\"],[\"While robotics research hasn’t leveraged vision-language models on a wide scale yet, we see works su...\"],[\"We are continuing to integrate the most impactful computer vision and multi-modal models and would l...\"],[\"--\\ntitle: \\\"Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Dis...\"],[\"- [Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tw...\"],[\"- [Llama 2](#llama-2)\\n            - [Load checkpoints for the classification mode](#load-checkpoints...\"],[\"\\u003c!-- \\u002fTOC --\\u003e\\n\\n\\n\\n## Introduction \\n\\nIn the fast-moving world of Natural Language Processing (NLP), we...\"],[\"## Hardware Used \\n\\n- Number of nodes: 1 \\n- Number of GPUs per node: 1\\n- GPU type: A6000 \\n- GPU memor...\"],[\"```\\nNote: For reproducing the reported results, please check the pinned versions in the [wandb repor...\"],[\"### [Mistral 7B](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2310.06825)\\n\\nMistral 7B v0.1, with 7.3 billion parameters, is...\"],[\"## Setup\\n\\nRoBERTa has a limitatiom of maximum sequence length of 512, so we set the `MAX_LEN=512` fo...\"],[\"```\\n\\n## Data preparation\\n### Data loading\\n\\nWe will load the dataset from Hugging Face:\\n```python\\nfro...\"],[\"```\\n\\n- Train dataset\\n\\n```\\u003cclass 'pandas.core.frame.DataFrame'\\u003e\\nRangeIndex: 7613 entries, 0 to 7612\\nD...\"],[\"```\\n\\nThe final weights are: \\n```\\nPOS_WEIGHT, NEG_WEIGHT = (1.1637114032405993, 0.8766697374481806)\\n`...\"],[\"```\\n**Note:** The RoBERTa tokenizer has been trained to treat spaces as part of the token. As a resu...\"],[\"```\\n\\n- Now, let's  apply the preprocessing function to the entire dataset: \\n\\n```python\\ncol_to_delete...\"],[\"```\\n\\n\\nYou can follow the same steps for preparing the data for Mistral 7B and Llama 2 models: \\n\\n**No...\"],[\"```\\n\\n- Llama 2:\\n```python\\n# Load Llama 2 Tokenizer\\nfrom transformers import AutoTokenizer, DataColla...\"],[\"```\\n\\n\\n####  LoRA setup for RoBERTa classifier\\n\\nWe import LoRa configuration and set some parameters ...\"],[\"```\\nFor Mistral 7B, we have to add the padding token id as it is not defined by default.\\n\\n```python\\n...\"],[\"```\\n\\nFor Llama 2, we have to add the padding token id as it is not defined by default.\\n\\n```python\\nll...\"],[\"```\\ntrainable params: 8,404,992 || all params: 6,615,748,608 || trainable%: 0.1270452143516515\\n```\\n\\n...\"],[\"```\\n\\n### Custom Trainer for Weighted Loss \\nAs mentioned at the beginning of this post, we have an im...\"],[\"```\\nIt will print the following: \\n```\\ndevice(type='cuda', index=0)\\n```\\n\\nThen, we set the training ar...\"],[\"```\\n\\n#### Mistral-7B\\n\\nSimilar to RoBERTa, we initialize the `WeightedCELossTrainer` as follows: \\n\\n``...\"],[\"```\\n\\n**Note** that we needed to enable half-precision training by setting `fp16` to `True`. The main...\"],[\"```\\n\\n\\n\\n\\n## Hyperparameter Tuning\\n\\nWe have used Wandb Sweep API to run hyperparameter tunning with Ba...\"],[\"For more information, you can check the Wandb experiment report in the [resources sections](#resourc...\"],[\"Finally, we showcase that LoRa method can be applied to both encoder (RoBERTa) and decoder (Llama 2 ...\"],[\"--\\ntitle: \\\"Introducing DOI: the Digital Object Identifier to Datasets and Models\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"## How are DOIs being assigned by Hugging Face? \\n\\nWe have partnered with [DataCite](https:\\u002f\\u002fdatacite...\"],[\"--\\ntitle: \\\"Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 1\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"Training a deep learning (DL) model on Intel Xeon CPUs can be a cost-effective and scalable approach...\"],[\"The AMX instructions accelerate matrix multiplication, an operation central to training DL models on...\"],[\"From a networking perspective, we will need the following setup:\\n\\n* Open port 22 for ssh access on a...\"],[\"```\\namx_bf16 amx_tile amx_int8\\n```\\n\\nThen, we install native and Python dependencies.\\n\\n```\\nsudo apt-g...\"],[\"```\\n\\nNext, we create a new ssh key pair called 'cluster' with `ssh-keygen` and store it at the defau...\"],[\"```\\nlocalhost\\nnode1\\nnode2\\nnode3\\n```\\n\\nThe cluster is now ready. Let's start training!\\n\\n## Launching a...\"],[\"```\\n\\nNo need to let the job run to completion, We just run for a minute to make sure that all depend...\"],[\"```\\n\\nNow, we launch the distributed training job.\\n\\n```\\n# Launch distributed training\\nmpirun -f ~\\u002fhos...\"],[\"```\\n\\nOne epoch now takes **7 minutes and 30 seconds**. \\n\\nHere's what the job looks like. The master ...\"],[\"--\\ntitle: Introducing our new pricing\\nthumbnail: \\u002fblog\\u002fassets\\u002f114_pricing-update\\u002fthumbnail.png\\nautho...\"],[\"--\\ntitle: Faster Stable Diffusion with Core ML on iPhone, iPad, and Mac\\nthumbnail: \\u002fblog\\u002fassets\\u002f149_...\"],[\"## New Core ML Optimizations\\n\\nCore ML is a mature framework that allows machine learning models to r...\"],[\"\\u003cimg style=\\\"border:none;\\\" alt=\\\"Illustration of 2-bit palettization. Image credit: Apple WWDC’23 Sess...\"],[\"## Using Quantized and Optimized Stable Diffusion Models\\n\\n[Last December](https:\\u002f\\u002fhuggingface.co\\u002fblo...\"],[\"| Model                     | Uncompressed      | Palettized                |\\n|---------------------...\"],[\"In order to use 6-bit models, you need the development versions of iOS\\u002fiPadOS 17 or macOS 14 (Sonoma...\"],[\"repo_id = \\\"apple\\u002fcoreml-stable-diffusion-2-1-base-palettized\\\"\\nvariant = \\\"original\\u002fpackages\\\"\\n\\nmodel_p...\"],[\"```\\n\\n## Converting and Optimizing Custom Models\\n\\nIf you want to use a personalized Stable Diffusion ...\"],[\"```bash\\npython -m python_coreml_stable_diffusion.torch2coreml \\\\\\n    --model-version prompthero\\u002fopenj...\"],[\"```\\n\\n\\u003cbr\\u003e\\n\\u003cdiv style=\\\"background-color: #f0fcf0; padding: 8px 32px 1px; outline: 1px solid; border-r...\"],[\"```\\n\\n4. Test the converted models on the desired hardware. As a rule of thumb, the `ORIGINAL` versio...\"],[\"We have plans to evaluate this method soon, and can’t wait to see how 4-bit optimized models work an...\"],[\"his notebook shows how to deploy a vision model from 🤗 Transformers (written in TensorFlow) to [Vert...\"],[\"```\\n\\n\\n```python\\nimport transformers\\n\\nprint(tf.__version__)\\nprint(transformers.__version__)\\n```\\n\\n## S...\"],[\"```\\n\\n\\n```python\\ndef normalize_img(img, mean=processor.image_mean, std=processor.image_std):\\n    # Sc...\"],[\"predictions = m_call(**images)\\n        indices = tf.argmax(predictions.logits, axis=1)\\n        pred_...\"],[\"```\\n\\n\\n```python\\n# To deploy the model on Vertex AI we must have the model in a storage bucket.\\ntf.sa...\"],[\"```\\n\\n\\n```python\\n# Upload the model to Vertex AI. \\ntf28_gpu_model_dict = {\\n    \\\"display_name\\\": \\\"ViT B...\"],[\"```\\n\\n\\n```python\\n# Deploy the Endpoint. \\ntf28_gpu_deployed_model_dict = {\\n    \\\"model\\\": tf28_gpu_model...\"],[\"```\\n\\n\\n```python\\nfrom google.protobuf import json_format\\nfrom google.protobuf.struct_pb2 import Value...\"],[\"--\\ntitle: \\\"Retrieval Augmented Generation with Huggingface Transformers and Ray\\\"\\nthumbnail: \\u002fblog\\u002fas...\"],[\"Recently, [Huggingface](https:\\u002f\\u002fhuggingface.co\\u002f) partnered with [Facebook AI](https:\\u002f\\u002fai.facebook.co...\"],[\"### Scaling up fine-tuning\\nThis retrieval of contextual documents is crucial for RAG's state-of-the-...\"],[\"![alt_text](assets\\u002f12_ray_rag\\u002fray_arch_updated.png \\\"image_tooltip\\\")\\n_Document retrieval with the Ray...\"],[\"_A performance comparison of different retrieval implementations. For each document retrieval implem...\"],[\"```\\n\\n\\nThen, you can specify your data paths and other configurations and run [finetune-rag-ray.sh](h...\"],[\"```\\n\\n## What’s next?\\n\\nUsing RAG with [Huggingface transformers](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftrans...\"],[\"If you plan to try RAG+Ray integration out, please feel free to share your experiences on the [Ray D...\"],[\"--\\ntitle: Introducing Pull Requests and Discussions 🥳\\nthumbnail: \\u002fblog\\u002fassets\\u002f76_community_update\\u002fth...\"],[\"## Pull requests\\n\\n![Pull requests on the Hugging Face Hub](assets\\u002f76_community_update\\u002fnew-pr.png)\\n\\n[...\"],[\"--\\ntitle: \\\"Introducing Agents.js: Give tools to your LLMs using JavaScript\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"```\\n\\nThen the code can be evaluated as such:\\n\\n```ts\\nconst messages = await agent.evaluateCode(code);...\"],[\"```\\n\\n### Usage warning\\n\\nCurrently using this library will mean evaluating arbitrary code in the brow...\"],[\"```\\n\\n## Custom Tools 🛠️\\n\\nAgents.js was designed to be easily expanded with custom tools & examples. ...\"],[\"```\\n\\n## Passing input files to the agent 🖼️\\n\\nThe agent can also take input files to pass along to th...\"],[\"--\\ntitle: \\\"Using Machine Learning to Aid Survivors and Race through Time\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fu...\"],[\"![organization](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdisast...\"],[\"![NER](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdisaster-assets...\"],[\"![backend_pipeline](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdi...\"],[\"In the end, we decided to fine-tune our own model as it would take roughly three minutes to fine-tun...\"],[\"![active_learning](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdis...\"],[\"To address these issues and create open source tools that can be leveraged in the future, we started...\"],[\"![output_satellite](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdi...\"],[\"--\\ntitle: \\\"Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 2\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"Another factor to consider is the level of parallelism in the model and the inference task. GPUs are...\"],[\"```\\nsudo apt-get update\\n\\n# Add libtcmalloc for extra performance\\nsudo apt install libgoogle-perftool...\"],[\"```\\nsentence_short = \\\"This is a really nice pair of shoes, I am completely satisfied with my purchas...\"],[\"```\\n\\nOn the c6i (Ice Lake) instance, we only use a vanilla Transformers pipeline. \\n\\n```\\nfrom transfo...\"],[\"```\\n\\nFor the sake of brevity, we'll just look at the p99 results for [distilbert-base-uncased](https...\"],[\"--\\ntitle: Getting Started with Hugging Face Inference Endpoints\\nthumbnail: \\u002fblog\\u002fassets\\u002f109_inferenc...\"],[\"Starting from my [model page](https:\\u002f\\u002fhuggingface.co\\u002fjuliensimon\\u002fautotrain-food101-1471154053), I cl...\"],[\"Let's first deploy a protected endpoint, and then we'll deploy a private one.\\n\\n### Deploying a Prote...\"],[\"```\\nimport requests, json\\n\\nAPI_URL = \\\"https:\\u002f\\u002foncm9ojdmjwesag2.eu-west-1.aws.endpoints.huggingface.c...\"],[\"```\\n5c7fbb4485cd8w7 2022-10-10T08:19:04.915Z 2022-10-10 08:19:04,915 | INFO | POST \\u002f | Duration: 142...\"],[\"```\\n\\nNow, let's increase our security level and deploy a private endpoint.\\n \\n### Deploying a Private...\"],[\"```\\ncurl https:\\u002f\\u002foncm9ojdmjwesag2.eu-west-1.aws.endpoints.huggingface.cloud \\\\\\n-X POST --data-binary ...\"],[\"--\\ntitle: \\\"Non-engineers guide: Train a LLaMA 2 chatbot\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_ml_director_insi...\"],[\"## Introduction to Spaces\\n\\nSpaces from Hugging Face is a service that provides easy to use GUI for b...\"],[\"1.2 Give your Space a name and select a preferred usage license if you plan to make your model or Sp...\"],[\"2.2 Choose the LLM you want to train from the “Model Choice” field, you can select a model from the ...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"2.10 Go grab a coffee, depending on the size of your model and training data this could take a few h...\"],[\"3.4 Under “Space variables” you can also change model inference parameters including temperature, to...\"],[\"--\\ntitle: \\\"Ethical Guidelines for developing the Diffusers library\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fethics-...\"],[\"# Safety features and mechanisms\\n\\nIn addition, we provide a non-exhaustive - and hopefully continuou...\"],[\"--\\ntitle: \\\"Introducing BERTopic Integration with the Hugging Face Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f145_b...\"],[\"## What is BERTopic?\\n\\nBERTopic is a state-of-the-art Python library that simplifies the topic modell...\"],[\"BERTopic provides a powerful tool for users to uncover significant topics within text collections, t...\"],[\"```\\nYou can then load this model in two lines and use it to predict against new data.\\n\\n```python\\nfro...\"],[\"```\\n\\nBy leveraging the power of the Hugging Face Hub, BERTopic users can effortlessly share, version...\"],[\"\\u003cdetails\\u003e\\n  \\u003csummary\\u003eClick here for an overview of all topics.\\u003c\\u002fsummary\\u003e\\n  \\n  | Topic ID | Topic Key...\"],[\"| 10 | news - fake - fake news - stance - fact | 455 | 10_news_fake_fake news_stance | \\n| 11 | relat...\"],[\"| 23 | adversarial - attacks - attack - adversarial examples - robustness | 181 | 23_adversarial_att...\"],[\"| 36 | classification - text classification - label - text - labels | 136 | 36_classification_text c...\"],[\"| 49 | poetry - poems - lyrics - poem - music | 103 | 49_poetry_poems_lyrics_poem | \\n| 50 | image - ...\"],[\"| 62 | change - semantic change - time - semantic - lexical semantic | 82 | 62_change_semantic chang...\"],[\"| 76 | translation - machine translation - machine - smt - statistical | 54 | 76_translation_machine...\"],[\"| 90 | emoji - emojis - sentiment - message - anonymous | 35 | 90_emoji_emojis_sentiment_message | \\n...\"],[\"| 104 | gender - translation - bias - gender bias - mt | 24 | 104_gender_translation_bias_gender bia...\"],[\"Due to the improved saving procedure, training on large datasets generates small model sizes. In the...\"],[\"To illustrate some of the power of BERTopic let's look at an example of how it can be used to monito...\"],[\"[databricks\\u002fdatabricks-dolly-15k](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdatabricks\\u002fdatabricks-dolly-15k) i...\"],[\"```\\n\\nWe can predict on a single example text: \\n\\n```python\\nexample = \\\"Stalemate is a drawn position. ...\"],[\"```\\n\\nWe can then compare the distribution of topics across both datasets. We can see here that there...\"],[\"Some examples of BERTopic models already on the hub:\\n- [MaartenGr\\u002fBERTopic_ArXiv](https:\\u002f\\u002fhuggingfac...\"],[\"--\\ntitle: \\\"OpenRAIL: Towards open and responsible AI licensing frameworks\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f1...\"],[\"Most current model developers seem to think so, as the majority of openly released models have an op...\"],[\"If specific ad hoc practices devoted to documentation, transparency and ethical usage of ML models a...\"],[\"## **A change of licensing paradigm: OpenRAIL**\\n\\nThe OpenRAIL [approach](https:\\u002f\\u002fwww.licenses.ai\\u002fblo...\"],[\"The effect of copyleft-style behavioral-use clauses spreads the requirement from the original licens...\"],[\"## **OpenRAIL could be for good machine learning what open software licensing is to code**\\n\\nThree ex...\"],[\"The licenses are BigScience's reaction to 2 partially addressed challenges in the licensing space: (...\"],[\"Let's invest in a healthy open and responsible AI licensing culture, the future of AI innovation and...\"],[\"--\\ntitle: Using LoRA for Efficient Stable Diffusion Fine-Tuning\\nthumbnail: \\u002fblog\\u002fassets\\u002flora\\u002fthumbna...\"],[\"![Latent Diffusion Architecture](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"- Training is much faster, as already discussed.\\n- Compute requirements are lower. We could create a...\"],[\"Diffusers now provides a [LoRA fine-tuning script](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmai...\"],[\"```\\n\\nOne thing of notice is that the learning rate is `1e-4`, much larger than the usual learning ra...\"],[\"First, we'll use the Hub API to automatically determine what was the base model that was used to fin...\"],[\"```\\n\\nThis snippet will print the model he used for fine-tuning, which is `CompVis\\u002fstable-diffusion-v...\"],[\"```\\n\\n## Dreamboothing with LoRA\\n\\nDreambooth allows you to \\\"teach\\\" new concepts to a Stable Diffusion...\"],[\"## Other Methods\\n\\nThe quest for easy fine-tuning is not new. In addition to Dreambooth, [_textual in...\"],[\"--\\ntitle: \\\"Graph Classification with Transformers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f125_intro-to-graphml\\u002fthu...\"],[\"## Requirements\\nTo follow this tutorial, you need to have installed `datasets` and `transformers` (v...\"],[\"```\\n\\nThis dataset already has three splits, `train`, `validation`, and `test`, and all these splits ...\"],[\"```\\n\\n### Format\\nOn the Hub, graph datasets are mostly stored as lists of graphs (using the `jsonl` f...\"],[\"A single graph is a dictionary, and here is the expected format for our graph classification dataset...\"],[\"- `edge_attr` contains the available attributes (if present) for each edge of the graph, following t...\"],[\"### Preprocessing\\nGraph transformer frameworks usually apply specific preprocessing to their dataset...\"],[\"```\\n\\nIt is also possible to apply this preprocessing on the fly, in the DataCollator's parameters (b...\"],[\"```\\nLet's look at this in more detail. \\n\\nCalling the `from_pretrained` method on our model downloads...\"],[\"```\\nIn the `Trainer` for graph classification, it is important to pass the specific data collator fo...\"],[\"--\\ntitle: Fine-Tune a Semantic Segmentation Model with a Custom Dataset\\nthumbnail: \\u002fblog\\u002fassets\\u002f56_f...\"],[\"Semantic segmentation is the task of classifying each pixel in an image. You can see it as a more pr...\"],[\"Let's get started by installing the necessary dependencies. Because we're going to push our dataset ...\"],[\"```\\n\\n# 1. Create\\u002fchoose a dataset\\n\\nThe first step in any ML project is assembling a good dataset. In...\"],[\"We went ahead and captured a thousand images of sidewalks in Belgium. Collecting and labeling such a...\"],[\"### Label the images\\n\\nNow that the raw data is loaded, go to [segments.ai\\u002fhome](https:\\u002f\\u002fsegments.ai\\u002f...\"],[\"Note that creating the release can take a few seconds. You can check the releases tab on Segments.ai...\"],[\"```\\n\\nIf we inspect the features of the new dataset, we can see the image column and the correspondin...\"],[\"```\\n\\nYou can also rewrite the `convert_segmentation_bitmap` function to use batches and pass `batche...\"],[\"```\\n\\n# 2. Load and prepare the Hugging Face dataset for training\\n\\nNow that we've created a new datas...\"],[\"```\\n\\n## Image processor & data augmentation\\n\\nA SegFormer model expects the input to be of a certain ...\"],[\"```\\n\\n# 3. Fine-tune a SegFormer model\\n\\n## Load the model to fine-tune\\n\\nThe SegFormer authors define ...\"],[\"```\\n\\n## Set up the Trainer\\n\\nTo fine-tune the model on our data, we'll use Hugging Face's [Trainer AP...\"],[\"```\\n\\nNext, we'll define a function that computes the evaluation metric we want to work with. Because...\"],[\"pred_labels = logits_tensor.detach().cpu().numpy()\\n    # currently using _compute instead of compute...\"],[\"```\\n\\nFinally, we can instantiate a `Trainer` object.\\n\\n\\n```python\\nfrom transformers import Trainer\\n\\nt...\"],[\"```\\n\\n# 4. Inference\\n\\nNow comes the exciting part, using our fine-tuned model! In this section, we'll...\"],[\"```python\\nfrom transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\\n\\nproces...\"],[\"```\\n\\nNext, we'll load an image from our test dataset.\\n\\n\\n```python\\nimage = test_ds[0]['pixel_values']...\"],[\"```\\n\\nNow it's time to display the result. We'll display the result next to the ground-truth mask.\\n\\n\\u003c...\"],[\"--\\ntitle: \\\"Efficient Controllable Generation for SDXL with T2I-Adapters\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002ft2i...\"],[\"| **Model Type** | **Model Parameters** | **Storage (fp16)** |\\n| --- | --- | --- |\\n| [ControlNet-SDX...\"],[\"Compared to previous versions of T2I-Adapter (SD-1.4\\u002f1.5), [T2I-Adapter-SDXL](https:\\u002f\\u002fgithub.com\\u002fTen...\"],[\"```\\n\\nThe generation process of the T2I-Adapter-SDXL mainly consists of the following two steps:\\n\\n1. ...\"],[\"# load pipeline\\nmodel_id = \\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\neuler_a = EulerAncestralDiscre...\"],[\"```\\n\\nThen, load an image to detect lineart:\\n\\n```python\\nurl = \\\"https:\\u002f\\u002fhuggingface.co\\u002fAdapter\\u002ft2iadap...\"],[\"```\\n\\n![Lineart Generated Dragon](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"\\u003cscript type=\\\"module\\\" src=\\\"https:\\u002f\\u002fgradio.s3-us-west-2.amazonaws.com\\u002f3.43.1\\u002fgradio.js\\\"\\u003e\\u003c\\u002fscript\\u003e\\n\\u003cgr...\"],[\"### Canny Guided\\n\\n![Sketch guided results](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation...\"],[\"--\\ntitle: \\\"Introduction to Graph Machine Learning\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f125_intro-to-graphml\\u002fthu...\"],[\"If you want to use your data, you must first consider its best characterisation (homogeneous\\u002fheterog...\"],[\"Working on these tasks can be done in two ways. \\n\\nWhen you want to predict the evolution of a specif...\"],[\"But what does this mean? If you have a sentence and shuffle its words, you create a new sentence. If...\"],[\"## Graph representations through ML\\n\\nThe usual process to work on graphs with machine learning is fi...\"],[\"**Node-level** features can give information about importance (how important is this node for the gr...\"],[\"### Walk-based approaches\\n\\n[**Walk-based approaches**](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fRandom_walk) us...\"],[\"Typical neural networks, such as RNNs or CNNs are not permutation invariant. A new architecture, the...\"],[\"**Choosing an aggregation**: Some aggregation techniques (notably mean\\u002fmax pooling) can encounter fa...\"],[\"Here are some interesting methods which got state-of-the-art results or close on one of the hardest ...\"],[\"- [*Graph Transformer for Graph-to-Sequence Learning*](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1911.07470) (Cai and La...\"],[\"The most recent approach is [*Pure Transformers are Powerful Graph Learners*](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f...\"],[\"# Further resources\\n\\nIf you want to delve deeper, you can look at some of these courses:\\n\\n- Academic...\"],[\"If you need quality benchmarks you can check out:\\n\\n- [OGB, the Open Graph Benchmark](https:\\u002f\\u002fogb.sta...\"],[\"### External images attribution\\nEmojis in the thumbnail come from Openmoji (CC-BY-SA 4.0), the Graph...\"],[\"--\\ntitle: \\\"Transformer-based Encoder-Decoder Models\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f05_encoder_decoder\\u002fthum...\"],[\"```\\n\\nThe *transformer-based* encoder-decoder model was introduced by Vaswani\\net al. in the famous [A...\"],[\"Transformer-based encoder-decoder models are the result of years of\\nresearch on _representation lear...\"],[\"Each part builds upon the previous part, but can also be read on its\\nown.\\n\\n## **Background**\\n\\nTasks ...\"],[\"Using a DNN model \\\\\\\\({}^2\\\\\\\\) to solve sequence-to-sequence problems would\\ntherefore mean that the nu...\"],[\"Then, the decoder\\\\'s hidden state is initialized with the input encoding\\nand during inference, the d...\"],[\"In computational terms, the model sequentially maps the previous inner\\nhidden state \\\\\\\\(\\\\mathbf{c}_{i...\"],[\"For more detail on the logit vector and the resulting probability\\ndistribution, please see footnote ...\"],[\"Given such a decoding method, during inference, the next input vector\\n\\\\\\\\(\\\\mathbf{y}_i\\\\\\\\) can then be...\"],[\"![](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fencoder_decoder\\u002frnn_...\"],[\"The English sentence \\\\\\\"I want to buy a car\\\\\\\", represented by\\n\\\\\\\\(\\\\mathbf{x}_1 = \\\\text{I}\\\\\\\\), \\\\\\\\(\\\\math...\"],[\"encoder\\\\'s target vector. The encoder RNN then processes the rest of the\\ninput sentence \\\\\\\\(\\\\text{wan...\"],[\"To generate the first target vector, the decoder is fed the \\\\\\\\(\\\\text{BOS}\\\\\\\\)\\nvector, illustrated as ...\"],[\"$$ p_{\\\\theta_{\\\\text{enc}}, \\\\theta_{\\\\text{dec}}}(\\\\mathbf{Y}_{1:m} | \\\\mathbf{X}_{1:n}) = \\\\prod_{i=1}^{...\"],[\"Nevertheless, RNN-based encoder-decoder models have two pitfalls. First,\\nRNNs suffer from the vanish...\"],[\"\\\\\\\\({}^4\\\\\\\\) A neural network can define a probability distribution over all\\nwords, *i.e.* \\\\\\\\(p(\\\\mathb...\"],[\"\\\\\\\\({}^6\\\\\\\\) [Sutskever et al. (2014)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1409.3215)\\nreverses the order of the inpu...\"],[\"As a reminder, to solve a *sequence-to-sequence* problem, we need to\\nfind a mapping of an input sequ...\"],[\"$$ p_{\\\\theta_{dec}}(\\\\mathbf{Y}_{1:n} | \\\\mathbf{\\\\overline{X}}_{1:n}).$$\\n\\nBy Bayes\\\\' rule, this distri...\"],[\"The transformer-based decoder hereby maps the sequence of encoded hidden\\nstates \\\\\\\\(\\\\mathbf{\\\\overline...\"],[\"Let\\\\'s visualize the complete process of *auto-regressive* generation of\\n*transformer-based* encoder...\"],[\"To begin with, the encoder processes the complete input sequence\\n\\\\\\\\(\\\\mathbf{X}_{1:7}\\\\\\\\) = \\\\\\\"I want t...\"],[\"Next, the first target vector \\\\\\\\(\\\\mathbf{y}_1\\\\\\\\) = \\\\\\\\(\\\\text{Ich}\\\\\\\\) is sampled\\nfrom the distribution...\"],[\"![texte du\\nlien](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fencoder...\"],[\"```\\n\\n_Output:_\\n\\n```\\n    \\u003cpad\\u003e Ich will ein Auto kaufen...\"],[\"```\\n\\nCalling `.generate()` does many things under-the-hood. First, it passes\\nthe `input_ids` to the ...\"],[\"Great, now that we have gotten a general overview of how\\n*transformer-based* encoder-decoder models ...\"],[\"## **Encoder**\\n\\nAs mentioned in the previous section, the *transformer-based* encoder\\nmaps the input...\"],[\"Let\\\\'s visualize how the encoder processes the input sequence \\\\\\\"I want\\nto buy a car EOS\\\\\\\" to a conte...\"],[\"As can be seen each output vector of the self-attention layer\\n\\\\\\\\(\\\\mathbf{x''}_i, \\\\forall i \\\\in \\\\{1, ...\"],[\"$$ \\\\mathbf{q}_i = \\\\mathbf{W}_q \\\\mathbf{x'}_i,$$\\n$$ \\\\mathbf{v}_i = \\\\mathbf{W}_v \\\\mathbf{x'}_i,$$\\n$$ \\\\...\"],[\"Note, that the **same** weight matrices are applied to each input vector\\n\\\\\\\\(\\\\mathbf{x}_i, \\\\forall i ...\"],[\"illustrated in the equation below. For a complete description of the\\nself-attention layer, the reade...\"],[\"Alright, this sounds quite complicated. Let\\\\'s illustrate the\\nbi-directional self-attention layer fo...\"],[\"On the left, the previously illustrated second encoder block is shown\\nagain and on the right, an in ...\"],[\"(shown in dark green on the right). The whole equation is illustrated in\\nthe upper part of the box o...\"],[\"To further understand the implications of the bi-directional\\nself-attention layer, let\\\\'s assume the...\"],[\"$$\\\\mathbf{X''}_{1:n} = \\\\mathbf{V}_{1:n} \\\\text{Softmax}(\\\\mathbf{Q}_{1:n}^\\\\intercal \\\\mathbf{K}_{1:n}) ...\"],[\"\\\\\\\\({}^1\\\\\\\\) An in-detail explanation of the role the feed-forward layers play\\nin transformer-based mo...\"],[\"# pass input_ids to encoder\\nencoder_hidden_states = model.base_model.encoder(input_ids, return_dict=...\"],[\"```\\n\\n_Outputs:_\\n```\\n    Length of input embeddings 7. Length of encoder_hidden_states 7\\n    Is encod...\"],[\"```\\n\\nWe compare the length of the input word embeddings, *i.e.*\\n`embeddings(input_ids)` correspondin...\"],[\"On a side-note, _autoencoding_ models, such as BERT, have the exact same\\narchitecture as _transforme...\"],[\"Let\\\\'s first understand how the transformer-based decoder defines a\\nprobability distribution. The tr...\"],[\"respectively. The \\\\\\\"LM head\\\\\\\" is often tied to the transpose of the word\\nembedding matrix, *i.e.*\\n\\\\\\\\...\"],[\"Putting it all together, in order to model the conditional distribution\\nof a target vector sequence ...\"],[\"In contrast to transformer-based encoders, in transformer-based\\ndecoders, the encoded output vector ...\"],[\"We can see that the decoder maps the input \\\\\\\\(\\\\mathbf{Y}_{0:5}\\\\\\\\) \\\\\\\"BOS\\\\\\\",\\n\\\\\\\"Ich\\\\\\\", \\\\\\\"will\\\\\\\", \\\\\\\"ein\\\\...\"],[\"can therefore be computed as the following product:\\n\\n$$ p_{\\\\theta_{dec}}(\\\\text{Ich} | \\\\text{BOS}, \\\\m...\"],[\"As in bi-directional self-attention, in uni-directional self-attention,\\nthe query vectors \\\\\\\\(\\\\mathbf...\"],[\"Note that the index range of the key and value vectors is \\\\\\\\(0:i\\\\\\\\) instead\\nof \\\\\\\\(0: m-1\\\\\\\\) which wo...\"],[\"So why is it important that we use uni-directional self-attention in the\\ndecoder instead of bi-direc...\"],[\"This is obviously disadvantageous as the transformer-based decoder would\\nnever learn to predict the ...\"],[\"Great! Now we can move to the layer that connects the encoder and\\ndecoder - the *cross-attention* me...\"],[\"Note that the index range of the key and value vectors is \\\\\\\\(1:n\\\\\\\\)\\ncorresponding to the number of c...\"],[\"So intuitively, what happens here exactly? Each output vector\\n\\\\\\\\(\\\\mathbf{y'''}_i\\\\\\\\) is a weighted su...\"],[\"Cool! Now we can see how this architecture nicely conditions each output\\nvector \\\\\\\\(\\\\mathbf{y'''}_i\\\\\\\\...\"],[\"To verify our theoretical understanding, let\\\\'s continue our code\\nexample from the encoder section a...\"],[\"# create token ids for encoder input\\ninput_ids = tokenizer(\\\"I want to buy a car\\\", return_tensors=\\\"pt...\"],[\"# compare values of word embedding of \\\"I\\\" for input_ids and perturbed input_ids\\nprint(\\\"Is encoding f...\"],[\"```\\n\\n_Output:_\\n\\n```\\n    Shape of decoder input vectors torch.Size([1, 5, 512]). Shape of decoder log...\"],[\"```\\n\\nWe compare the output shape of the decoder input word embeddings, *i.e.*\\n`embeddings(decoder_in...\"],[\"On a final side-note, _auto-regressive_ models, such as GPT2, have the\\nsame architecture as _transfo...\"],[\"# create ids of encoded input vectors\\ninput_ids = tokenizer(\\\"I want to buy a car\\\", return_tensors=\\\"p...\"],[\"# sample last token with highest prob again\\nnext_decoder_input_ids = torch.argmax(lm_logits[:, -1:],...\"],[\"```\\n\\n_Outputs:_\\n\\n```\\n    Generated so far: Ich will ein\\n```\\n\\nIn this code example, we show exactly w...\"],[\"--\\ntitle: Block Sparse Matrices for Smaller and Faster Language Models\\nthumbnail: \\u002fblog\\u002fassets\\u002f04_py...\"],[\"By itself, or even better combined with other methods like\\n[distillation](https:\\u002f\\u002fmedium.com\\u002fhugging...\"],[\"```\\n\\nThe extension also provides a `BlockSparseModelPatcher` that allows to modify an existing model...\"],[\"But the more important point is that the performance gain of using sparse matrices grows with the sp...\"],[\"--\\ntitle: \\\"Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)\\\"\\nthumbnail: \\u002fb...\"],[\"Firstly, we will provide empirical evidence that **Transformers are indeed Effective for Time Series...\"],[\"|      Dataset      | Autoformer (uni.) MASE | DLinear  MASE |\\n|:-----------------:|:---------------...\"],[\"### Decomposition Layer\\nDecomposition has long been a popular method in time series analysis, but it...\"],[\"Autoformer incorporates a decomposition block as an inner operation of the model, as presented in th...\"],[\"def forward(self, x):\\n        \\\"\\\"\\\"Input shape: Batch x Time x EMBED_DIM\\\"\\\"\\\"\\n        # padding on the b...\"],[\"```\\n\\nAs you can see, the implementation is quite simple and can be used in other models, as we will ...\"],[\"In theory, given a time lag \\\\\\\\(\\\\tau\\\\\\\\), _autocorrelation_ for a single discrete variable \\\\\\\\(y\\\\\\\\) is ...\"],[\"Now, we are ready to see the code in PyTorch: \\n\\n```python\\nimport torch \\n\\ndef autocorrelation(query_s...\"],[\"```\\n\\nQuite simple! 😎 Please be aware that this is only a partial implementation of `autocorrelation(...\"],[\"It can be summarized with the following equations:\\n\\n$$\\n\\\\tau_1, \\\\tau_2, ... \\\\tau_k = \\\\textrm{arg Top-...\"],[\"Now, we are ready to see the final code:\\n\\n```python\\nimport torch\\nimport math\\n\\ndef time_delay_aggrega...\"],[\"# apply softmax on the channel dim\\n    top_k_autocorrelations = torch.softmax(top_k_autocorrelations...\"],[\"```\\n\\nWe did it! The Autoformer model is [now available](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmai...\"],[\"```\\n\\nIn the probabilistic setting one can project the context length arrays to  `prediction-length *...\"],[\"```\\n\\nThe transformers models are all relatively small with:\\n\\n```python\\nencoder_layers=2\\ndecoder_laye...\"],[\"```\\n\\nLet's visualize a time series in the dataset and plot the train\\u002ftest split:\\n\\n```python\\nimport m...\"],[\"```\\n\\n## Define Transformations\\n\\nNext, we define the transformations for the data, in particular for ...\"],[\"```\\n\\n## Define `InstanceSplitter`\\n\\nFor training\\u002fvalidation\\u002ftesting we next create an `InstanceSplitt...\"],[\"```\\n\\n## Create PyTorch DataLoaders\\n\\nNext, it's time to create PyTorch DataLoaders, which allow us to...\"],[\"return as_stacked_batches(\\n        training_instances,\\n        batch_size=batch_size,\\n        shuffl...\"],[\"if config.num_static_real_features \\u003e 0:\\n        PREDICTION_INPUT_NAMES.append(\\\"static_real_features\\\"...\"],[\"```\\n\\n## Evaluate on Autoformer\\n\\nWe have already pre-trained an Autoformer model on this dataset, so ...\"],[\"```\\n\\nThe model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`, `i...\"],[\"```\\n\\nSo the result for the Autoformer model is:\\n\\n```python\\nprint(f\\\"Autoformer univariate MASE: {np.m...\"],[\"```\\n\\nFor example, for time-series in the test set with index `4`:\\n\\n```python\\nplot(4)\\n```\\n\\n![png](htt...\"],[\"```\\n\\nTrain the model:\\n\\n```python\\npredictor = estimator.train(\\n    training_data=train_dataset, \\n    ...\"],[\"```\\n\\nAs before, we plot the predictions from our trained DLinear model via this helper:\\n\\n```python\\nd...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f148_a...\"],[\"As one can observe, the [vanilla Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftim...\"],[\"## Acknowledgements\\nWe express our appreciation to [Lysandre Debut](https:\\u002f\\u002fgithub.com\\u002fLysandreJik) ...\"],[\"--\\ntitle: \\\"Image search with 🤗 datasets\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f54_image_search_datasets\\u002fspaces_ima...\"],[\"First, we'll install `datasets`. Since we're going to be working with images, we'll also install [`p...\"],[\"```\\n\\nTo start, let's take a look at the image feature. We can use the wonderful [rich](https:\\u002f\\u002frich....\"],[\"\\u003cpre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e│\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #00ffff; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e│\\u003c\\u002fspan\\u003e                               ...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e│\\u003c\\u002fspan\\u003e                               ...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e│\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #008080; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e│\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #008080; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e│\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #008080; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e│\\u003c\\u002fspan\\u003e  \\u003cspan style=\\\"color: #808000; ...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e│\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #808000; t...\"],[\"We can see there a few different ways in which we can pass in our images. We'll come back to this in...\"],[\"There have also been projects to tag the dataset [using machine learning](https:\\u002f\\u002fblogs.bl.uk\\u002fdigita...\"],[\"```\\n\\nLet's see what we get back.\\n\\n```python\\ndataset\\n```\\n\\n```\\nDatasetDict({\\n    train: Dataset({\\n    ...\"],[\"```\\n```python\\n\\u002froot\\u002f.cache\\u002fhuggingface\\u002fdatasets\\u002fdownloads\\u002fextracted\\u002ff324a87ed7bf3a6b83b8a353096fbd95...\"],[\"```\\n\\n\\n\\u003cimg src=\\\"assets\\u002f54_image_search_datasets\\u002fdataset_image.jpg\\\" alt=\\\"An example image from our da...\"],[\"```\\n\\n\\n``` python\\ndataset.push_to_hub('davanstrien\\u002fembellishments-sample', private=True)\\n```\\n\\n\\n\\u003e **No...\"],[\"```\\n## Creating embeddings 🕸 \\nWe now have a dataset with a bunch of images in it. To begin creating ...\"],[\"```\\n\\nWe now have a new column which contains the embeddings for our images. We could manually search...\"],[\"```\\n\\nWe can index into the first example this retrieves:\\n\\n``` python\\nretrieved_examples['image'][0]\\n...\"],[\"```\\n\\n``` python\\nget_image_from_text(\\\"An illustration of the sun behind a mountain\\\")\\n```\\n\\u003cimg src=\\\"as...\"],[\"```\\n\\n\\u003cimg src=\\\"assets\\u002f54_image_search_datasets\\u002fmusical_instrument.jpg\\\"\\u003e\\n\\n\\u003cimg src=\\\"assets\\u002f54_image_s...\"],[\"However, I'm a little bit vary about making this public straightaway. Looking at the model card for ...\"],[\"suggests that 'deployment' is not a good idea. Whilst the results I got are interesting, I haven't p...\"],[\"--\\ntitle: \\\"Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Langage Model\\\"\\nthumb...\"],[\"The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is t...\"],[\"IDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the origina...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fatlas.nomic.ai\\u002fmap\\u002ff2fba2aa-3647-4f49-a0f3-9347daeee499\\u002fee4a...\"],[\"As part of the release process, we internally evaluated the model for potential biases by adversaria...\"],[\"## Getting Started with IDEFICS\\n\\nIDEFICS models are available on the Hugging Face Hub and supported ...\"],[\"# Generation args\\nexit_condition = processor.tokenizer(\\\"\\u003cend_of_utterance\\u003e\\\", add_special_tokens=Fals...\"],[\"--\\ntitle: \\\"Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers\\\"\\nthumbnail: \\u002fblog\\u002f...\"],[\"### NLP\\n\\n[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fGraphcore\\u002fgpt2-medium-wikitext-103) (Generative Pre-trained ...\"],[\"[BART](https:\\u002f\\u002fhuggingface.co\\u002fGraphcore\\u002fbart-base-ipu) is a transformer encoder-encoder (seq2seq) mo...\"],[\"### Speech\\n\\n[HuBERT](https:\\u002f\\u002fhuggingface.co\\u002fGraphcore\\u002fhubert-base-ipu) (Hidden-Unit BERT) is a self-...\"],[\"Optimizing their performance in the real world requires considerable time, effort and skills that ar...\"],[\"Software also plays a vital role in unlocking the IPU’s capabilities, so naturally Optimum offers a ...\"],[\"--\\ntitle: \\\"Showcase Your Projects in Spaces using Gradio\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f28_gradio-spaces\\u002ft...\"],[\"```\\n\\nYou can play with the Story Generation model [here](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fmerve\\u002fGPT-2-s...\"],[\"```\\n\\n![big-gan](assets\\u002f28_gradio-spaces\\u002fbig-gan.png)\\n\\n\\n## Serving Custom Model Checkpoints with Grad...\"],[\"```\\n\\nYou can check out the French Story Generator [here](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fmerve\\u002ffrench-...\"],[\"Some Notes on Pros of Open Science and Open Source\\n- **Pooling Resources**: Building off of one anot...\"],[\"# Cons of Closed Source\\n- **Centralization** of power.\\n- **Opacity** of subtle bias\\u002fharm issues.\\n- H...\"],[\"--\\ntitle: \\\"We are hiring interns!\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002finterns-2023\\u002fthumbnail.png\\nauthors:\\n- use...\"],[\"The following Science team positions are available:\\n\\n* [Embodied AI Internship](https:\\u002f\\u002fapply.workab...\"],[\"--\\ntitle: \\\"Announcing the Open Source AI Game Jam 🎮\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f145_gamejam\\u002fthumbnail.p...\"],[\"From accelerated workflows to in-game features, you can harness the power of AI for texture generati...\"],[\"--\\ntitle: \\\"Ethics and Society Newsletter #3: Ethical Openness at Hugging Face\\\" \\nthumbnail: \\u002fblog\\u002fass...\"],[\"We are crafting tools and safeguards in addition to improving our documentation practices to ensure ...\"],[\"We engage directly with contributors and have addressed pressing issues. To bring this to the next l...\"],[\"**How to use the flagging function:**\\nClick on the flag icon on any Model, Dataset, Space, or Discus...\"],[\"Should a specific model be flagged as high risk by our community, we consider:\\n- Downgrading the ML ...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n \\u003cbr\\u003e\\n \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"```\\n@misc{hf_ethics_soc_blog_3,\\n  author    = {Irene Solaiman and\\n               Giada Pistilli and\\n...\"],[\"--\\ntitle: \\\"Deep Learning over the Internet: Training Language Models Collaboratively\\\"\\nthumbnail: \\u002fbl...\"],[\"## Distributed Deep Learning in Open Collaborations\\n\\n### Why should we do it?\\n\\nThese days, many high...\"],[\"To a skeptical mind, it might seem that we're missing a key factor here: data transfer in distribute...\"],[\"Often, to reduce the amount of synchronization and to stabilize the learning process, we can accumul...\"],[\"Now that we have discussed the overall training procedure, there remains one more question: how do w...\"],[\"\\u003cdiv style=\\\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\\\"\\u003e\\n\\u003cp ali...\"],[\"\\u003cdiv style=\\\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\\\"\\u003e\\n\\u003cp ali...\"],[\"1. **Normalization:** includes all preprocessing operations on raw text data. This was the step at w...\"],[\"\\u003cdiv style=\\\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\\\"\\u003e\\n\\u003cp ali...\"],[\"```\\n\\n### Dataset\\n\\nThe last thing we need to cover is the training dataset. As you probably know, the...\"],[\"```\\n\\u003cdiv style=\\\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\\\"\\u003e\\n\\u003cp...\"],[\"\\u003ciframe width=\\\"100%\\\" height=\\\"670\\\" frameborder=\\\"0\\\"\\n  src=\\\"https:\\u002f\\u002fobservablehq.com\\u002fembed\\u002f@huggingface...\"],[\"### Evaluation\\n\\nTo evaluate the performance of sahajBERT, we finetuned it on two downstream tasks in...\"],[\"At the end of training, we compared sahajBERT with three other pretrained language models: [XLM-R La...\"],[\"These models are available on the Hub as well. You can test them directly by playing with the Hosted...\"],[\"```\\n\\n#### sahajBERT-NCC\\nModel card: [https:\\u002f\\u002fhf.co\\u002fneuropark\\u002fsahajBERT-NER](https:\\u002f\\u002fhf.co\\u002fneuropark\\u002f...\"],[\"```\\n\\n## Conclusion\\n\\nIn this blog post, we have discussed the method that can enable collaborative pr...\"],[\"Below, you can see all participants of the collaborative experiment:\\n\\n\\u003ciframe width=\\\"100%\\\" height=\\\"3...\"],[\"--\\ntitle: \\\"Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fbl...\"],[\"## Benchmarks\\n\\nWithout any further delay let's show some numbers.\\n\\nFor the sake of consistency, unle...\"],[\"```\\nGenerate args {'max_length': 100, 'do_sample': False}...\"],[\"```\\nThe input prompt is comprised of just a few tokens. The previous token caching is on as well, as...\"],[\"Here is the throughput in msecs on 8x80GB GPUs:\\n\\n| project      \\\\ bs |      1 |     8 |    16 |    3...\"],[\"Let's revisit again how these numbers were calculated. To generate 100 new tokens for a batch size o...\"],[\"```\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers-bloom-inference\\ncd transformers-bloom-infe...\"],[\"```\\npython bloom-inference-scripts\\u002fbloom-accelerate-inference.py --name bigscience\\u002fbloom --batch_siz...\"],[\"```\\ndeepspeed --num_gpus 8 bloom-inference-scripts\\u002fbloom-ds-inference.py --name microsoft\\u002fbloom-deep...\"],[\"```\\ndeepspeed --num_gpus 4 bloom-inference-scripts\\u002fbloom-ds-inference.py --name microsoft\\u002fbloom-deep...\"],[\"```\\npip install deepspeed\\n```\\n\\n\\n### Run\\n\\nNote that the script currently runs the same inputs on all ...\"],[\"```\\n\\nmake sure to adjust `\\u002fpath\\u002fto\\u002fnvme_offload` to somewhere you have ~400GB of free memory on a fa...\"],[\"--\\ntitle: \\\"Summer at Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f27_summer_at_huggingface\\u002fsummer_intro.gif...\"],[\"Spaces lets you [set up secrets](\\u002fdocs\\u002fhub\\u002fspaces-overview#managing-secrets), permits [custom requir...\"],[\"![Image of a TensorBoard Instance](assets\\u002f27_summer_at_huggingface\\u002ftensorboard.png)\\n\\n### Metrics\\n\\nIn...\"],[\"The Hub has 18 widgets that allow users to try out models directly in the browser.\\n\\nWith our latest ...\"],[\"![Button to upload a file](assets\\u002f27_summer_at_huggingface\\u002fupload_file.png)\\n\\n## Community\\n\\n### Huggi...\"],[\"We're really excited to share the work of the 3 winning teams!\\n\\n1. [Dall-e mini](https:\\u002f\\u002fhuggingface...\"],[\"## Bonus\\n\\nOn top of everything we just shared, our team has been doing lots of other things. Here ar...\"],[\"## Open Source\\n\\n### New in Transformers\\n\\nSummer has been an exciting time for 🤗 Transformers! The li...\"],[\"```\\n\\nThe last 4 releases introduced many new cool models!\\n\\n- [DETR](https:\\u002f\\u002fhuggingface.co\\u002ftransform...\"],[\"![DETR image](assets\\u002f27_summer_at_huggingface\\u002fdetr.png)\\n\\n- [ByT5](https:\\u002f\\u002fhuggingface.co\\u002ftransformer...\"],[\"![LayoutLM object detection](assets\\u002f27_summer_at_huggingface\\u002flayout.png)\\n\\n- [BEiT](https:\\u002f\\u002fhuggingfa...\"],[\"![Untitled](assets\\u002f27_summer_at_huggingface\\u002fstreaming.png)\\n\\nWhat are the new datasets highlights? Mi...\"],[\"![spaCy NER example](assets\\u002f27_summer_at_huggingface\\u002fspacy_ner.jpeg)\\n\\nAnother exciting integration i...\"],[\"### **NEW: Hardware Acceleration**\\n\\nHugging Face is [partnering with leading AI hardware accelerator...\"],[\"![Sagemaker](assets\\u002f27_summer_at_huggingface\\u002fsagemaker.png)\\n\\n### **NEW: AutoNLP In Your Browser**\\n\\nW...\"],[\"**Hugging Face** + **Zapier Demo**\\n\\n20,000+ Machine Learning models connected to 3,000+ apps? 🤯  By ...\"],[\"**Few-shot learning in practice**\\n\\nWe wrote a [blog post](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002ffew-shot-learn...\"],[\"In June, we shared the result of our collaboration with the Yandex research team: [DeDLOC](https:\\u002f\\u002fa...\"],[\"![Prompt](assets\\u002f27_summer_at_huggingface\\u002fprompt.png)\\n\\n\\nWe're looking forward to EMNLP this year whe...\"],[\"--\\ntitle: \\\"Model Cards\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f121_model-cards\\u002fthumbnail.png\\nauthors:\\n- user: Ezi\\n...\"],[\"4) A [User Study](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002fmodel-cards-user-studies) on model card usage at H...\"],[\"## Our Work\\n\\nOur work presents a view of where model cards stand right now and where they could go i...\"],[\"As ML continues to be more intertwined with different domains, collaborative and open-source ML proc...\"],[\"* The Hugging Face ecosystem will continue to advance methods that streamline Model Card creation [t...\"],[\"--\\ntitle: \\\"Introducing RWKV - An RNN with the advantages of a transformer\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"You can get involved by joining the [official discord channel](https:\\u002f\\u002fdiscord.gg\\u002fqt9egFA7ve) and le...\"],[\"Because RNNs use the same weights to compute predictions at every step, they struggle to memorize in...\"],[\"During training, Transformer architecture has several advantages over traditional RNNs and CNNs. One...\"],[\"RNNs natively support very long context lengths - only limited by the context length seen in trainin...\"],[\"To gain a more comprehensive understanding of the attention layer, we recommend to delve into the de...\"],[\"All the HF converted models are available on Hugging Face Hub, in the [`RWKV` organization](https:\\u002f\\u002f...\"],[\"```\\n\\nOr you can run and start from the snippet below:\\n\\n```python\\nimport torch\\nfrom transformers impo...\"],[\"```\\n\\n### Use the raven models (chat models)\\n\\nYou can prompt the chat model in the alpaca style, here...\"],[\"```\\n\\nAccording to Bo, better instruction techniques are detailed in [this discord message (make sure...\"],[\"```\\n\\n## Future work\\n\\n### Multi-lingual RWKV\\n\\nBo is currently working on a multilingual corpus to tra...\"],[\"## Acknowledgements\\n\\nThe Hugging Face team would like to thank Bo and RWKV community for their time ...\"],[\"--\\ntitle: Stable Diffusion with 🧨 Diffusers\\nthumbnail: \\u002fblog\\u002fassets\\u002f98_stable_diffusion\\u002fthumbnail.pn...\"],[\"**Note**: It is highly recommended to have a basic understanding of how diffusion models work. If di...\"],[\"```\\n\\nIn this post we'll use model version [`v1-4`](https:\\u002f\\u002fhuggingface.co\\u002fCompVis\\u002fstable-diffusion-v...\"],[\"```\\n\\nThe result would look as follows\\n\\n![png](assets\\u002f98_stable_diffusion\\u002fstable_diffusion_12_1.png)\\n...\"],[\"```\\n\\nThe result would look as follows\\n\\n![png](assets\\u002f98_stable_diffusion\\u002fstable_diffusion_14_1.png)\\n...\"],[\"```\\n\\n![png](assets\\u002f98_stable_diffusion\\u002fstable_diffusion_16_1.png)\\n\\nNote how the structure is the sam...\"],[\"```\\n\\nWe can generate multiple images for the same prompt by simply using a list with the same prompt...\"],[\"```\\n\\n![png](assets\\u002f98_stable_diffusion\\u002fstable_diffusion_26_1.png)\\n    \\n\\n## How does Stable Diffusion...\"],[\"There are three main components in latent diffusion.\\n\\n1. An autoencoder (VAE).\\n2. A [U-Net](https:\\u002f\\u002f...\"],[\"To prevent the U-Net from losing important information while downsampling, short-cut connections are...\"],[\"**Stable Diffusion during inference**\\n\\nPutting it all together, let's now take a closer look at how ...\"],[\"Theory on how the scheduler algorithm function is out-of-scope for this notebook, but in short one s...\"],[\"We can load the components by referring to the folder they were saved, using the `subfolder` argumen...\"],[\"```\\n\\nNow instead of loading the pre-defined scheduler, we load the [K-LMS scheduler](https:\\u002f\\u002fgithub....\"],[\"```\\n\\nFirst, we get the `text_embeddings` for the passed prompt. \\nThese embeddings will be used to co...\"],[\"```\\n\\nIf we examine the `latents` at this stage we'll see their shape is `torch.Size([1, 4, 64, 64])`...\"],[\"```\\n\\nWe now use the `vae` to decode the generated `latents` back into the image.\\n\\n\\n```python\\n# scale...\"],[\"```\\n@article{patil2022stable,\\n  author = {Patil, Suraj and Cuenca, Pedro and Lambert, Nathan and von...\"],[\"--\\ntitle: 'Deploy Hugging Face models easily with Amazon SageMaker'\\nthumbnail: \\u002fblog\\u002fassets\\u002f17_the_p...\"],[\"```\\n\\n\\nThat's it! 🚀\\n\\nTo learn more about accessing and using the new Hugging Face DLCs with the Amazo...\"],[\"## **Samples\\u002fDocumentation**\\n\\n- [Hugging Face documentation for Amazon SageMaker](https:\\u002f\\u002fhuggingfac...\"],[\"In addition to the zero-code deployment, the Inference Toolkit supports \\\"bring your own code\\\" method...\"],[\"```\\n\\n# **Getting started 🧭**\\n\\nIn this guide we will use the new Hugging Face Inference DLCs and Amaz...\"],[\"```\\n\\n---\\n\\n## **Deploy a trained Hugging Face Transformer model to SageMaker for inference**\\n\\nThere a...\"],[\"```\\n\\n\\nAfter we run our request we can delete the endpoint again with.\\n\\n\\n```python\\n# delete endpoint\\n...\"],[\"```\\n\\nAfter we run our request, we can delete the endpoint again with:\\n\\n\\n```python\\n# delete endpoint\\n...\"],[\"```\\n\\nAfter we run our request we can delete the endpoint again with.\\n\\n\\n```python\\n# delete endpoint\\np...\"],[\"```\\n\\n---\\n\\n# **FAQ 🎯**\\n\\nYou can find the complete [Frequently Asked Questions](https:\\u002f\\u002fhuggingface.co...\"],[\"_Q: Do I have to use the SageMaker Python SDK to use the Hugging Face Deep Learning Containers (DLCs...\"],[\"_Q: How is my data and code secured by Amazon SageMaker?_\\n\\nA: Amazon SageMaker provides numerous sec...\"],[\"A: AWS Technical Support tiers are available from AWS and cover development and production issues fo...\"],[\"--\\ntitle: \\\"Introducing Prodigy-HF: a direct integration with Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f1...\"],[\"\\u003cfigure\\u003e\\n    \\u003cdiv style=\\\"background-color: #eee; padding-top: 8px; padding-bottom: 8px;\\\"\\u003e\\n        \\u003ci...\"],[\"```\\npython -m prodigy hf.train.ner fashion-train,eval:fashion-eval path\\u002fto\\u002fmodel-out --model \\\"distil...\"],[\"```\\npython -m prodigy hf.upload \\u003cdataset_name\\u003e \\u003cusername\\u003e\\u002f\\u003crepo_name\\u003e\\n```\\n\\nWe're particularly fond o...\"],[\"--\\ntitle: \\\"How to Install and Use the Hugging Face Unity API\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-gam...\"],[\"6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https:\\u002f\\u002fh...\"],[\"```\\nusing HuggingFace.API;\\n\\n\\u002f* other code *\\u002f\\n\\n\\u002f\\u002f Make a call to the API\\nvoid Query() {\\n    string in...\"],[\"```\\n\\n## Supported Tasks and Custom Models\\n\\nThe Hugging Face Unity API also currently supports the fo...\"],[\"--\\ntitle: \\\"Proximal Policy Optimization (PPO)\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f93_deep_rl_ppo\\u002fthumbnail.png\\n...\"],[\"- *An Actor* that controls **how our agent behaves** (policy-based method).\\n- *A Critic* that measur...\"],[\"- [The intuition behind PPO](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fdeep-rl-ppo#the-intuition-behind-ppo)\\n- [In...\"],[\"- [Let's code our PPO Agent](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fdeep-rl-ppo#lets-code-our-ppo-agent)\\n  \\n## ...\"],[\"The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability o...\"],[\"However, the problem comes from the step size:\\n- Too small, **the training process was too slow**\\n- ...\"],[\"So this probability ratio is an **easy way to estimate the divergence between old and current policy...\"],[\"To do that, we have two solutions:\\n\\n- *TRPO (Trust Region Policy Optimization)* uses KL divergence c...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f93_deep_rl_ppo\\u002frecap.jpg\\\" alt...\"],[\"Since the ratio is between intervals, **we can decrease the probability that our policy takes that a...\"],[\"### Case 5 and 6: the ratio is above the range\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n ...\"],[\"**You might wonder why, when the minimum is the clipped ratio, the gradient is 0.** When the ratio i...\"],[\"So, to be able to code it, we're going to use two resources:\\n- A tutorial made by [Costa Huang](http...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fgiphy.com\\u002fembed\\u002fpynZagVcYxVUk\\\" width=\\\"480\\\" height=\\\"480\\\" frameBorder=\\\"0\\\" class=\\\"...\"],[\"And don't forget to share with your friends who want to learn 🤗!\\n\\nFinally, with your feedback, we wa...\"],[\"--\\ntitle: \\\"Very Large Language Models and How to Evaluate Them\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f106_zero_sh...\"],[\"We’ve upgraded the AutoTrain infrastructure for this project so that large models can be evaluated f...\"],[\"## Case study: Zero-shot evaluation on the WinoBias task\\n\\nThe [WinoBias](https:\\u002f\\u002fgithub.com\\u002fuclanlp\\u002f...\"],[\"![Winobias](.\\u002fassets\\u002f106_zero_shot_eval_on_the_hub\\u002fwinobias.png)\\n\\n## Enabling better research tools ...\"],[\"## Send us feedback!\\n\\nAt Hugging Face, we’re excited to continue democratizing access to state-of-th...\"],[\"--\\ntitle: Training Stable Diffusion with Dreambooth using Diffusers\\nthumbnail: \\u002fblog\\u002fassets\\u002fsd_dream...\"],[\"## TL;DR: Recommended Settings\\n\\n* Dreambooth tends to overfit quickly. To get good-quality images, w...\"],[\"## Learning Rate Impact\\n\\nDreambooth overfits very quickly. To get good results, tune the learning ra...\"],[\"Low Learning Rate (`2e-6`)\\n![Cat Toy, Low Learning Rate](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface...\"],[\"### Summary of Initial Results\\n\\nTo get good results training Stable Diffusion with Dreambooth, it's ...\"],[\"As you can see, results are better when prior preservation is used, but there are still noisy blotch...\"],[\"`DDIM`, Potato Head\\n![DDIM Potato](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002f...\"],[\"We think the results are much better than doing plain Dreambooth but not as good as when we fine-tun...\"],[\"--\\ntitle: 'Faster Text Generation with TensorFlow and XLA'\\nthumbnail: \\u002fblog\\u002fassets\\u002f91_tf_xla_generat...\"],[\"The 🤗 `transformers` library started with NLP models, so it is natural that text generation is of ut...\"],[\"generated = model.generate(**inputs, do_sample=True, seed=(42, 0))\\nprint(\\\"Sampling output: \\\", tokeni...\"],[\"```\\n\\nDepending on the target application, longer outputs might be desirable. You can control the len...\"],[\"```\\n\\nContrarily to Sampling, Greedy Decoding will always pick the most likely token at each iteratio...\"],[\"```\\n\\nThe basics of text generation, as you can see, are straightforward to control. However, there a...\"],[\"For those of you familiar with TensorFlow 1 🧓, the concept of a TensorFlow graph comes naturally, as...\"],[\"Now that you know how to create TensorFlow graphs, compiling them with XLA is straightforward -- sim...\"],[\"```\\n\\nIn one line, you can create an XLA-accelerated function from the function above.\\n\\n```python\\nxla...\"],[\"```\\n\\n## Text Generation using TensorFlow with XLA\\n\\nAs with any optimization procedure, there is no f...\"],[\"# Slow: Different tensor type\\nmax_plus_constant(tf.constant([0, 0, 0], dtype=tf.int64), 1)\\n# \\u003e Execu...\"],[\"```\\n\\nIn practice, for text generation, it simply means the input should be padded to a multiple of a...\"],[\"print(\\\"Calling XLA generation with tokenized_input_1...\\\")\\nprint(\\\"(will be slow as it is the first ca...\"],[\"```\\n\\nOh no, that's terribly slow! A solution to keep the different combinations of shapes in check i...\"],[\"```\\n\\nThat's much better, successive generation calls performed this way will be orders of magnitude ...\"],[\"```\\n\\nFrom a developer perspective, relying on XLA implies being aware of a few additional nuances. X...\"],[\"\\u003cdiv class=\\\"hidden xl:block\\\"\\u003e\\n\\u003cdiv style=\\\"display: flex; flex-direction: column; align-items: center...\"],[\"--\\ntitle: \\\"Perceiver IO: a scalable, fully-attentional model that works on any modality\\\"\\nthumbnail: ...\"],[\"Not long after that, AI researchers started to apply the idea of BERT to other domains. To name a fe...\"],[\"## The Perceiver\\n\\nThe [Perceiver](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2103.03206) aims to solve this limitation by...\"],[\"Note that each of these are optional. A `preprocessor` is only required in case one hasn't already e...\"],[\"Let's start off by showing how the Perceiver is implemented to work on text.\\n\\n## Perceiver for text\\n...\"],[\"```\\n\\nIn this case, one provides `PerceiverTextPreprocessor` as preprocessor to the model, which will...\"],[\"```\\n\\nIn the Perceiver IO paper, one uses 256 latents, and sets the dimensionality of the latents to ...\"],[\"Ok, so now one has final hidden states of shape (batch_size, 256, 1280). Great, but one actually wan...\"],[\"Great, isn't it? The Perceiver authors also show that it is straightforward to pre-train the Perceiv...\"],[\"Each of these are implemented in the Transformers library, and called `PerceiverForImageClassificati...\"],[\"```\\n\\nOne can see that `PerceiverImagePreprocessor` is initialized with `prep_type = \\\"conv1x1\\\"` and t...\"],[\"```\\n\\n`PerceiverImagePreprocessor` (with the settings defined above) will first apply a convolutional...\"],[\"The Perceiver authors show that the model is capable of achieving strong results compared to models ...\"],[\"class PerceiverForOpticalFlow(nn.Module):\\n    def __init__(self, config):\\n        super().__init__(c...\"],[\"```\\nAs one can see, `PerceiverImagePreprocessor` is used as preprocessor (i.e. to prepare the 2 imag...\"],[\"To decode the final hidden states of the latents to an actual predicted flow, `PerceiverOpticalFlowD...\"],[\"The video below shows the predicted flow on 2 examples. \\n\\n\\u003cp float=\\\"left\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002flh3.g...\"],[\"## Perceiver for multimodal autoencoding\\n\\nThe authors also use the Perceiver for multimodal autoenco...\"],[\"Next, `PerceiverMultimodalPreprocessor` will pad the preprocessed modalities with modality-specific ...\"],[\"- For the image modality, the total size of the decoder query is 16x3x224x224 = 802,816. However, wh...\"],[\"Finally, there is `PerceiverMultimodalPostprocessor`. This class postprocesses the output of the dec...\"],[\"\\u003csmall\\u003eAbove: original video (left), reconstruction of the first 16 frames (right). Video taken from...\"],[\"The authors also used the Perceiver to replace the original Transformer in [AlphaStar](https:\\u002f\\u002fdeepm...\"],[\"## Conclusion\\n\\nIn this blog post, we went over the architecture of Perceiver IO, an extension of the...\"],[\"## Footnotes\\n\\n\\u003cb id=\\\"f1\\\"\\u003e1\\u003c\\u002fb\\u003e Note that in the official paper, the authors used a two-layer MLP to ...\"],[\"--\\ntitle: \\\"Train a Sentence Embedding Model with 1B Training Pairs\\\"\\nauthors:\\n- user: asi\\n  guest: tr...\"],[\"![snippet](assets\\u002f32_1b_sentence_embeddings\\u002fmodel.png)\\n\\n### Multiple Negative Ranking Loss\\n\\nThe para...\"],[\"Intuitively, the model should assign high similarity to the sentences « How many people live in Berl...\"],[\"In practice, we used a scaled similarity because score differences tends to be too small and apply a...\"],[\"![snippet](assets\\u002f32_1b_sentence_embeddings\\u002fbatch-size.png)\\n\\n#### 2. Hard Negatives\\n\\nIn the same fig...\"],[\"## Conclusion\\n\\nYou can find all models and datasets we created during the challenge in our [HuggingF...\"],[\"General sentence embeddings might be used for many applications. We built a [Spaces demo](https:\\u002f\\u002fhu...\"],[\"--\\ntitle: \\\"Large-scale Near-deduplication Behind BigCode\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fdedup\\u002fthumbnail.pn...\"],[\"## From BigScience to BigCode\\n\\nAllow me to share a story first on how I jumped on this near-deduplic...\"],[\"| Dataset                              | Input Size                       | Output Size or Deduction...\"],[\"| BNE5[[6]](#6)                        | 2TB                              | 570 GB                  ...\"],[\"| The BigScience ROOTS Corpus[[9]](#9) |                                  | 0.07% ~ 2.7% **↓** (docu...\"],[\"This is the one for code datasets we created for BigCode as well. Model names are used when the data...\"],[\"## MinHash Walkthrough\\n\\nIn this section, we will cover each step of MinHash, the one used in BigCode...\"],[\"| doc_id | shingles                                                                        |\\n| -----...\"],[\"| shingle             | permuted hashes                                             |\\n| ------------...\"],[\"In implementation, you can easily vectorize these steps with `numpy` and expect to have a time compl...\"],[\"```\\n\\nIf you are familiar with [Datasketch](https:\\u002f\\u002fgithub.com\\u002fekzhu\\u002fdatasketch), you might ask, why ...\"],[\"```\\n\\nAfter the fingerprint calculation, one particular document is mapped to one array of integer va...\"],[\"If two documents share the same hashes in a band at a particular location (band index), they will be...\"],[\"### Beyond Duplicate Pairs\\n\\nThis is where many deduplication descriptions in papers or tutorials sto...\"],[\"**Option 2: Use popular python frameworks such as `dask` to allow more efficient `groupby` operation...\"],[\"```\\n\\n**Option 4: For large datasets, use Spark.**\\n\\nWe already know that steps up to the LSH part can...\"],[\"```\\n\\nAdditionally, thanks to cloud providers, we can set up Spark clusters like a breeze with servic...\"],[\"These graphs can help us understand why it was necessary to double-check the false positives for Cod...\"],[\"## Proceed with Caution\\n\\nDeduplication doesn't exempt you from thorough data exploration and analysi...\"],[\"## Future Directions\\n\\n1. Substring deduplication. Even though it showed some benefits for English[[3...\"],[\"## Supporting Resources\\n\\n- [Datasketch](https:\\u002f\\u002fgithub.com\\u002fekzhu\\u002fdatasketch) (MIT)\\n- [simhash-py](ht...\"],[\"- \\u003ca id=\\\"1\\\"\\u003e[1]\\u003c\\u002fa\\u003e : Nikhil Kandpal, Eric Wallace, Colin Raffel, [Deduplicating Training Data Mitig...\"],[\"- \\u003ca id=\\\"8\\\"\\u003e[8]\\u003c\\u002fa\\u003e : Xi Victoria Lin, Todor Mihaylov, et al., [Few-shot Learning with Multilingual ...\"],[\"- \\u003ca id=\\\"15\\\"\\u003e[15]\\u003c\\u002fa\\u003e : Lewis Tunstall, Leandro von Werra, Thomas Wolf, [Natural Language Processing...\"],[\"--\\ntitle: 'Getting Started With Embeddings'\\nthumbnail: \\u002fblog\\u002fassets\\u002f80_getting_started_with_embeddin...\"],[\"## What are embeddings for?\\n\\n\\u003e \\\"[...] once you understand this ML multitool (embedding), you'll be a...\"],[\"But first, we need to embed our dataset (other texts use the terms encode and embed interchangeably)...\"],[\"```\\nTo generate the embeddings you can use the `https:\\u002f\\u002fapi-inference.huggingface.co\\u002fpipeline\\u002ffeatur...\"],[\"```\\n\\nThe current API does not enforce strict rate limitations. Instead, Hugging Face balances the lo...\"],[\"```\\nIt looks similar to this matrix:\\n\\n```py\\n[[-0.02388945  0.05525852 -0.01165488 ...  0.00577787  0...\"],[\"```\\n\\n## 2. Host embeddings for free on the Hugging Face Hub\\n\\n🤗 Datasets is a library for quickly acc...\"],[\"```\\n\\nFollow the next steps to host `embeddings.csv` in the Hub.\\n\\n* Click on your user in the top rig...\"],[\"Install the 🤗 Datasets library with `pip install datasets`. Then, load the embedded dataset from the...\"],[\"```\\nWe use the query function we defined before to embed the customer's question and convert it to a...\"],[\"```\\n\\n`util.semantic_search` identifies how close each of the 13 FAQs is to the customer query and re...\"],[\"```\\n\\nThis list represents the 5 FAQs closest to the customer's query. Nice! We used here PyTorch and...\"],[\"--\\ntitle: Getting Started with Transformers on Habana Gaudi\\nthumbnail: \\u002fblog\\u002fassets\\u002f61_getting_start...\"],[\"Let's get started!\\n\\n\\n## Setting up an Habana Gaudi instance on AWS\\n\\nThe simplest way to work with Ha...\"],[\"Next, I pick the *dl1.24xlarge* instance size (the only size available).\\n\\n\\u003ckbd\\u003e\\n  \\u003cimg src=\\\"assets\\u002f6...\"],[\"\\u003ckbd\\u003e\\n  \\u003cimg src=\\\"assets\\u002f61_getting_started_habana\\u002fhabana06.png\\\"\\u003e\\n\\u003c\\u002fkbd\\u003e\\n\\nFinally, I launch the inst...\"],[\"```\\nssh -i ~\\u002f.ssh\\u002fjulsimon-keypair.pem ubuntu@ec2-18-207-189-109.compute-1.amazonaws.com\\n```\\n\\nOn thi...\"],[\"```\\n\\nThen, I install the Optimum Habana package from source.\\n\\n```\\ncd optimum-habana\\npip install .\\n``...\"],[\"```\\n\\nAfter 2 minutes and 12 seconds, the job is complete and has achieved an excellent F1 score of 0...\"],[\"--\\ntitle: \\\"Personal Copilot: Train Your Own Coding Assistant\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f170_personal_...\"],[\"## Data Collection Workflow\\n\\nOur desired dataset is conceptually simple, we structured it like so:\\n\\n...\"],[\"We also excluded all file paths that were not directly related to code. These include: `.git`, `__py...\"],[\"[This is the code we used to generate this dataset](https:\\u002f\\u002fgithub.com\\u002fpacman100\\u002fDHS-LLM-Workshop\\u002ftr...\"],[\"## Finetuning your own Personal Co-Pilot \\n\\nIn this section, we show how to fine-tune the following m...\"],[\"\\u003e trainable params: 110,428,160 || all params: 15,627,884,544 || trainable%: 0.7066097761926236\\n\\n1. ...\"],[\"## Full Finetuning\\n\\nWe will look at how to do full fine-tuning of `bigcode\\u002fstarcoder` (15B params) o...\"],[\"The command to launch training is given at [run_fsdp.sh](https:\\u002f\\u002fgithub.com\\u002fpacman100\\u002fDHS-LLM-Worksh...\"],[\"```\\naccelerate launch --config_file \\\"configs\\u002ffsdp_config.yaml\\\"  train.py \\\\\\n    --model_path \\\"bigcode...\"],[\"```\\n\\nThe total training time was **9 Hours**. Taking the cost of $12.00 \\u002f hr based on [lambdalabs](h...\"],[\"**Resources**\\n1. Codebase: [link](https:\\u002f\\u002fgithub.com\\u002fpacman100\\u002fDHS-LLM-Workshop\\u002ftree\\u002fmain\\u002fpersonal_c...\"],[\"## Comparison\\n\\nThe plot below shows the eval loss, train loss and learning rate scheduler for QLoRA ...\"],[\"![qualitative_comparison_1](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve...\"],[\"In the second example above, **GitHub Copilot didn't give any completion**. This can be due to the f...\"],[\"### Setting an Inference Endpoint\\nBelow are the screenshots with the steps we followed to create our...\"],[\"**Resources**\\n\\n1. Codebase: [link](https:\\u002f\\u002fgithub.com\\u002fpacman100\\u002fDHS-LLM-Workshop\\u002ftree\\u002fmain\\u002fcode_assi...\"],[\"## Mix-and-Match LoRAs\\n\\nPEFT currently supports 3 ways of combining LoRA models, `linear`, `svd` and...\"],[\"![assistant_chat_hf](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fb...\"],[\"PEFT allows you to do it via `add_weighted_adapter`. Let's create a new adapter `code_buddy` with eq...\"],[\"We can observe that `code_buddy` is performing on par with `copilot`, which was specifically finetun...\"],[\"Yay! It correctly answers in detail how to create `LoraConfig` and related peft model along with cor...\"],[\"![loss_plots](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fper...\"],[\"```\\ngit clone --recursive https:\\u002f\\u002fgithub.com\\u002fpacman100\\u002fmlc-llm.git && cd mlc-llm\\u002f\\n```\\n\\n2. Install th...\"],[\"```\\n\\n4. Update the config with the following values in `dist\\u002fstarcoder1B-v2-personal-copilot-merged-...\"],[\"```\\n\\n6. Change the endpoint of HF Code Completion extension in VS Code to point to the local server:...\"],[\"--\\ntitle: \\\"Hugging Face on PyTorch \\u002f XLA TPUs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f13_pytorch_xla\\u002fpytorch_xla_th...\"],[\"### XLA:TPU Device Type\\n\\nPyTorch \\u002f XLA adds a new `xla` device type to PyTorch. This device type wor...\"],[\"```\\n\\nThis code should look familiar. PyTorch \\u002f XLA uses the same interface as regular PyTorch with a...\"],[\"```\\n\\n### PyTorch \\u002f XLA Input Pipeline\\n\\nThere are two main parts to running a PyTorch \\u002f XLA model: (1...\"],[\"```\\n\\n### Checkpoint Writing and Loading\\n\\nWhen a tensor is checkpointed from a XLA device and then lo...\"],[\"```\\n\\n\\n## PyTorch \\u002f XLA Library\\n\\nPyTorch \\u002f XLA is a Python package that uses the XLA linear algebra c...\"],[\"### Trace, Compile, Execute, and Repeat\\n\\nFrom a user’s point of view, a typical training regimen for...\"],[\"```\\n\\nThis live graph is accumulated while the forward and backward passes are run on the user's prog...\"],[\"```python\\n\\u003e\\u003e\\u003e import torch_xla.debug.metrics as met\\n\\u003e\\u003e\\u003e print(met.metrics_report())\\nMetric: CompileT...\"],[\"Accumulator: 04m22s555ms668.071us\\n  ValueRate: 923ms872.877us \\u002f second\\n  Rate: 4.33049 \\u002f second\\n  Pe...\"],[\"```\\n\\n### Train Your Transformer on Cloud TPUs\\n\\nTo configure your VM and Cloud TPUs, please follow [“...\"],[\"```bash\\nconda activate torch-xla-1.7\\nexport TPU_IP_ADDRESS=\\\"ENTER_YOUR_TPU_IP_ADDRESS\\\"  # ex. 10.0.0...\"],[\"```\\n\\nThe above should complete training in roughly less than 200 minutes with an eval perplexity of ...\"],[\"## Get Started with PyTorch \\u002f XLA on TPUs\\nSee the [“Running on TPUs”](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"--\\ntitle: Deploying TensorFlow Vision Models in Hugging Face with TF Serving\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"In this post, you'll see how to deploy a Vision Transformer (ViT) model (for image classification)\\nl...\"],[\"```\\n\\nBy default, `save_pretrained()` will first create a version directory\\ninside the path we provid...\"],[\"```\\n\\nThis should print:\\n\\n```bash\\nViTImageProcessor {\\n  \\\"do_normalize\\\": true,\\n  \\\"do_resize\\\": true,\\n  ...\"],[\"```\\n\\nYou also need to resize the image and transpose it so that it has leading\\nchannel dimensions si...\"],[\"```\\n\\n**Note on making the model accept string inputs**:\\n\\nWhen dealing with images via REST or gRPC r...\"],[\"```\\n\\nYou can first derive the [concrete function](https:\\u002f\\u002fwww.tensorflow.org\\u002fguide\\u002ffunction)\\nfrom th...\"],[\"```\\n\\nYou can notice that the model's signature has now changed. Specifically,\\nthe input type is now ...\"],[\"```\\n\\nFrom the above command, the important parameters are:\\n\\n- `rest_api_port` denotes the port numbe...\"],[\"```\\n\\nTF Serving's request payload format specification for the REST endpoint\\nis available [here](htt...\"],[\"```\\n\\nThe REST API is -\\n`http:\\u002f\\u002flocalhost:8501\\u002fv1\\u002fmodels\\u002fvit:predict` following the specification fro...\"],[\"```\\n\\nNow, you can get some predictions:\\n\\n```py\\ngrpc_predictions = stub.Predict(request, 10.0)  # 10 ...\"],[\"--\\ntitle: \\\"Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU\\\" \\nthumbnail: assets\\u002f133_trl_peft\\u002fth...\"],[\"3- Further fine-tune the LLM from step 1 with the reward model and this dataset using RL (e.g. PPO)\\n...\"],[\"### What is TRL?\\n\\nThe `trl` library aims at making the RL step much easier and more flexible so that...\"],[\"In `trl` you can also use shared layers between reference and active models to avoid entire copies. ...\"],[\"Many techniques have been adopted to tackle these challenges at scale. The most familiar paradigms a...\"],[\"Therefore, we asked ourselves the following question: how far can we go with just data parallelism? ...\"],[\"### Low rank adaptation and PEFT\\n\\nIn 2021, a paper called LoRA: Low-Rank Adaption of Large Language ...\"],[\"The library is still under extensive and active development, with many upcoming features to be annou...\"],[\"### Step 2: Add extra trainable adapters using `peft`\\n\\n| ![step2](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftr...\"],[\"Overall there were three key steps and training scripts:\\n\\n1. **[Script](https:\\u002f\\u002fgithub.com\\u002flvwerra\\u002ft...\"],[\"| ![loss-20b](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftrl-internal-testing\\u002fexample-images\\u002fresolve\\u002fmain\\u002fblog\\u002f...\"],[\"## Conclusion\\n\\nWe have implemented a new functionality in `trl` that allows users to fine-tune large...\"],[\"## References\\n\\n- parallelism paradigms: [https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fv4.17.0\\u002fen\\u002fparalle...\"],[\"--\\ntitle: \\\"Sentiment Analysis on Encrypted Data with Homomorphic Encryption\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"```\\n\\nNow we can install all the required libraries for the this blog with the following command.\\n```...\"],[\"```\\n\\nThe output, then, looks like this:\\n\\n```\\nProportion of positive examples: 16.14%\\nProportion of n...\"],[\"```\\n\\n## Text representation using a transformer\\n\\n[Transformers](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fTransf...\"],[\"```\\n\\nThis should download the model, which is now ready to be used.\\n\\nUsing the hidden representation...\"],[\"# Send the model to the device\\n   transformer_model = transformer_model.to(device)\\n   output_hidden_...\"],[\"```\\n\\n```python\\n# Let's vectorize the text using the transformer\\nlist_text_X_train = text_X_train.tol...\"],[\"```\\n\\nThe output is as follows:\\n\\n```\\nBest score: 0.8378111718275654\\nBest parameters: {'max_depth': 1,...\"],[\"```\\n\\nWith the following output:\\n```\\nAccuracy: 0.8504\\n```\\n\\n## Predicting over encrypted data\\n\\nNow let...\"],[\"```\\n\\nThe output becomes:\\n\\n```\\nCompilation time: 9.3354 seconds\\nFHE inference time: 4.4085 seconds\\n``...\"],[\"```\\n\\nThese few lines are enough to export all the files needed for both the client and the server. Y...\"],[\"1. train a machine learning model to classify tweets, and\\n2. predict over encrypted data using this ...\"],[\"--\\ntitle: \\\"How to host a Unity game in a Space\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games\\u002funity-in-sp...\"],[\"```\\n\\n## Step 3: Open your Unity Project\\n\\nOpen the Unity project you want to host in your Space.\\n\\n\\u003cfi...\"],[\"Then, in the Player Settings panel, switch the WebGL template to Hugging Face. To do so, in Player S...\"],[\"## Step 10: Enable Git-LFS for Large File Storage\\n\\nNavigate to your repository. Use the following co...\"],[\"```\\ngit lfs install\\ngit lfs track Build\\u002f* \\n```\\n\\n## Step 11: Push your Changes\\n\\nFinally, use the foll...\"],[\"--\\ntitle: \\\"Llama 2 on Amazon SageMaker a Benchmark\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fllama_sagemaker_benchma...\"],[\"We hope to enable customers to use LLMs and Llama 2 efficiently and optimally for their use case. Be...\"],[\"### What is Llama 2?\\n\\nLlama 2 is a family of LLMs from Meta, trained on 2 trillion tokens. Llama 2 c...\"],[\"As metrics, we used Throughput and Latency defined as: \\n\\n- Throughput (tokens\\u002fsec): Number of tokens...\"],[\"| Model       | Quantization | Instance       | concurrent requests | Latency (ms\\u002ftoken) median | Th...\"],[\"### Best Throughput Deployment\\n\\nThe Best Throughput configuration maximizes the number of tokens tha...\"],[\"| Model       | Quantization | Instance        | concurrent requests | Latency (ms\\u002ftoken) median | T...\"],[\"### Best Latency Deployment\\n\\nThe Best Latency configuration minimizes the time it takes to generate ...\"],[\"| Model       | Quantization | Instance        | concurrent requests | Latency (ms\\u002ftoken) median | T...\"],[\"## Conclusions\\n\\nIn this benchmark, we tested 60 configurations of Llama 2 on Amazon SageMaker. For c...\"],[\"--\\ntitle: \\\"Announcing Evaluation on the Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f82_eval_on_the_hub\\u002fthumbnail.pn...\"],[\"Progress in AI has been nothing short of amazing, to the point where some people are now seriously d...\"],[\"## On the Hub\\n\\nEvaluation on the Hub opens the door to so many interesting use cases. From the data ...\"],[\"Evaluation on the Hub is meant to make your life easier. But of course, there’s a lot happening in t...\"],[\"As the above image shows, to solve this problem, you’ll need:\\n\\n* A dataset of dog, muffin, and fried...\"],[\"![Advanced Configuration](\\u002fblog\\u002fassets\\u002f82_eval_on_the_hub\\u002fconfig.png)\\n\\nThe next step is to define wh...\"],[\"![Pull Request](\\u002fblog\\u002fassets\\u002f82_eval_on_the_hub\\u002fpr.png)\\n\\nYou can also copy-paste the evaluation meta...\"],[\"Benchmarks are saturating, meaning that machines outperform humans on certain test sets, almost fast...\"],[\"--\\ntitle: \\\"Leveraging Pre-trained Language Model Checkpoints for Encoder-Decoder Models\\\"\\nthumbnail: ...\"],[\"Similar to BERT and GPT2, massive pre-trained encoder-decoder models\\nhave shown to significantly boo...\"],[\"This notebook is divided into 4 parts:\\n\\n-   **Introduction** - *Short summary of pre-trained languag...\"],[\"## **Introduction**\\n\\nRecently, pre-trained language models \\\\\\\\({}^1\\\\\\\\) have revolutionized the\\nfield ...\"],[\"The capability of pre-trained language models to effectively transfer\\n*task-agnostic* knowledge to *...\"],[\"Let\\\\'s see how BERT and GPT2 would be fit to model sequence-to-sequence\\ntasks.\\n\\n### **BERT**\\n\\nBERT i...\"],[\"Let\\\\'s visualize BERT.\\n\\n![texte du\\nlien](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientif...\"],[\"### **GPT2**\\n\\nGPT2 is a *decoder-only* model, which makes use of *uni-directional*\\n(*i.e.* \\\\\\\"causal\\\\...\"],[\"\\\\\\\\(p_{\\\\theta_{\\\\text{GPT2}}}(\\\\mathbf{y}_i | \\\\mathbf{Y}_{0:i-1})\\\\\\\\) hereby\\npresents the probability di...\"],[\"![texte du\\nlien](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fgpt2.pn...\"],[\"GPT2 is mainly used for *open-domain* text generation. First, an input\\nprompt \\\\\\\\(\\\\mathbf{Y}_{0:i-1}\\\\...\"],[\"### **Encoder-Decoder**\\n\\nBecause *encoder-only* models require to know the output length *a\\npriori*,...\"],[\"In 2020, Sascha Rothe, Shashi Narayan, and Aliaksei Severyn investigated\\nexactly this question in th...\"],[\"\\\\\\\\({}^2\\\\\\\\) *Fine-tuning* is defined as the *task-specific* training of a\\nmodel that has been initial...\"],[\"There are multiple possibilities to warm-start an encoder-decoder model.\\nOne can\\n\\n1.  initialize bot...\"],[\"The encoder maps the input sequence \\\\\\\\(\\\\mathbf{X}_{1:n}\\\\\\\\) to a\\ncontextualized encoded sequence \\\\\\\\(\\\\...\"],[\"Each \\\\\\\"next-word\\\\\\\" conditional distributions is thereby defined by the\\n*softmax* of the logit vector...\"],[\"We can see that the encoder architecture corresponds 1-to-1 to BERT\\\\'s\\narchitecture. The weight para...\"],[\"Next, let\\\\'s illustrate how the decoder is warm-started.\\n\\n![texte du\\nlien](https:\\u002f\\u002fraw.githubusercon...\"],[\"2.  Second, BERT\\\\'s *bi-directional* self-attention layers have to be\\n    changed to *uni-directiona...\"],[\"3.  Third, the decoder outputs a sequence of logit vectors\\n    \\\\\\\\(\\\\mathbf{L}_{1:m}\\\\\\\\) in order to de...\"],[\"Let\\\\'s illustrate how a GPT2 checkpoint can be used to warm-start the\\ndecoder.\\n\\n![texte du\\nlien](htt...\"],[\"Even though GPT2 resembles the decoder part of an encoder-decoder model\\nmore than BERT, a GPT2-initi...\"],[\"$$ \\\\mathbf{W}^{\\\\text{self-attn}, 3}_{k} = \\\\mathbf{W}^{\\\\text{self-attn}, 3}_{\\\\text{enc}, k} \\\\equiv \\\\m...\"],[\"In the same way, we can warm-start an encoder-decoder model by sharing\\nthe encoder weights with the ...\"],[\"## **Warm-starting encoder-decoder models (Analysis)**\\n\\nIn this section, we will summarize the findi...\"],[\"The following table shows a complete list of all investigated model\\nvariants including the number of...\"],[\"The model *Rnd2Rnd*, which is based on the BERT2BERT architecture,\\ncontains 221M weight parameters -...\"],[\"|Seq2Seq Task               |Datasets                                                               ...\"],[\"|WMT14 DE =\\\\\\u003e EN            |[Bojar et al. (2014)](http:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002fW\\u002fW14\\u002fW14-3302)  ...\"],[\"Depending on the task, a slightly different training regime was used.\\n*E.g.* according to the size o...\"],[\"As an example, the sentence:\\n\\n*Street Rod is the first in a series of two games released for the PC\\n...\"],[\"Let\\\\'s see how the models perform on sentence fusion and -splitting.\\n\\n  |Model                  | 10...\"],[\"The first two columns show the performance of the encoder-decoder models\\non the DiscoFuse evaluation...\"],[\"### Machine Translation (WMT14)\\n\\nNext, the authors evaluated warm-started encoder-decoder models on ...\"],[\"|Model                       |En \\\\\\\\(\\\\to\\\\\\\\) De (BLEU-4)   |De \\\\\\\\(\\\\to\\\\\\\\) En (BLEU-4)\\n  |--------------...\"],[\"Again, we observe a significant performance boost by warm-starting the\\nencoder-part, with *BERT2Rnd*...\"],[\"### Summarization (CNN\\u002fDailymail, BBC XSum, Gigaword)\\n\\nFinally, the encoder-decoder models were eval...\"],[\"Alright, let\\\\'s take a look at the results.\\n\\n  |Model                  |CNN\\u002fDailymail (Rouge-2)   |B...\"],[\"Furthermore, the shared encoder-decoder models are the best performing\\nmodels for summarization. *Ro...\"],[\"-   Next, we noticed that it is often beneficial to share encoder and\\n    decoder weights, especiall...\"],[\"For each of the above tasks, the most performant models were ported to\\n🤗Transformers and can be acce...\"],[\"------------------------------------------------------------------------\\n\\n\\\\\\\\({}^1\\\\\\\\) To retrieve BLE...\"],[\"In addition, the following list provides a condensed version of this and\\nother notebooks on warm-sta...\"],[\"```\\nLet's start by downloading the *CNN\\u002fDailymail* dataset.\\n\\n```python\\nimport datasets\\ntrain_data = ...\"],[\"```python\\nOUTPUT:\\n-------\\nArticle:...\"],[\"\\\"\\\"\\\"It's official: U.S. President Barack Obama wants lawmakers to weigh in on whether to use military...\"],[\"of mass destruction.\\\" It's a step that is set to turn an international crisis into a fierce domestic...\"],[\"to carry out this military action without specific congressional authorization, I know that the coun...\"],[\"full remarks . Syrian crisis: Latest developments . U.N. inspectors leave Syria . Obama's remarks ca...\"],[\"attack in a Damascus suburb on August 21 has been a key point of global debate over the Syrian crisi...\"],[\"who has said he wants to wait until the U.N. team's final report is completed before presenting it t...\"],[\"and that \\\"a military solution is not an option.\\\" Bergen:  Syria is a problem from hell for the U.S. ...\"],[\"was a blow to Obama's hopes of getting strong backing from key NATO allies. On Saturday, Obama propo...\"],[\"who would do our people harm. In a world with many dangers, this menace must be confronted.\\\" Syria m...\"],[\"Saturday continued to shore up support for a strike on the al-Assad government. He spoke by phone wi...\"],[\"military officials said they remained at the ready. 5 key assertions: U.S. intelligence report on Sy...\"],[\"decision. House Speaker John Boehner, Majority Leader Eric Cantor, Majority Whip Kevin McCarthy and ...\"],[\"before any U.S. action. British Prime Minister David Cameron, whose own attempt to get lawmakers in ...\"],[\"in the world, among allies of the US or in the United States itself,\\\" Alexei Pushkov, chairman of th...\"],[\"Iran stand by Assad . Syria's government unfazed . After Obama's speech, a military and political an...\"],[\"during a meeting with a delegation of Syrian expatriates from Italy, according to a banner on Syria ...\"],[\"in terms of taking the issue to Parliament,\\\" said Bashar Jaafari, Syria's ambassador to the United N...\"],[\"Obama said \\\"all told, well over 1,000 people were murdered.\\\" U.S. Secretary of State John Kerry on F...\"],[\"Summary:\\n\\\"\\\"\\\"Syrian official: Obama climbed to the top of the tree, \\\"doesn't know how to get down\\\"\\\\nO...\"],[\"```\\n\\nThe input data seems to consist of short news articles. Interestingly,\\nthe labels appear to be ...\"],[\"```\\n\\nNext, we make use of `.map()` to compute the length of the article and\\nits summary. Since we kn...\"],[\"```\\n\\nHaving computed the length for the first 10000 samples, we should now\\naverage them together. Fo...\"],[\"```\\n\\nWe can see that on average an article contains 848 tokens with *ca.* 3\\u002f4\\nof the articles being ...\"],[\"```python\\nencoder_max_length=512\\ndecoder_max_length=128\\n\\ndef process_data_to_model_inputs(batch):\\n  ...\"],[\"```\\n\\nIn this notebook, we train and evaluate the model just on a few training\\nexamples for demonstra...\"],[\"```\\n\\nSo far, the data was manipulated using Python\\\\'s `List` format. Let\\\\'s\\nconvert the data to PyTo...\"],[\"```\\n\\nIn contrast to other model classes in 🤗Transformers, the\\n`EncoderDecoderModel` class has two me...\"],[\"```python\\nOUTPUT:\\n-------\\n\\\"\\\"\\\"Some weights of the model checkpoint at bert-base-uncased were not used...\"],[\"Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased ...\"],[\"'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value...\"],[\"'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.q...\"],[\"'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output...\"],[\"'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.v...\"],[\"'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key...\"],[\"'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention....\"],[\"'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output...\"],[\"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and ...\"],[\"```\\n\\nFor once, we should take a good look at the warning here. We can see\\nthat two weights correspon...\"],[\"```python\\nOUTPUT:\\n-------\\n    EncoderDecoderModel(\\n      (encoder): BertModel(\\n        (embeddings):...\"],[\"(dropout): Dropout(p=0.1, inplace=False)\\n                )\\n              )\\n              (intermedia...\"],[\")\\n              (intermediate): BertIntermediate(\\n                (dense): Linear(in_features=768, o...\"],[\"(self): BertSelfAttention(\\n                    (query): Linear(in_features=768, out_features=768, bi...\"],[\"(dropout): Dropout(p=0.1, inplace=False)\\n                  )\\n                )\\n                (inte...\"],[\")\\n                )\\n                (crossattention): BertAttention(\\n                  (self): BertS...\"],[\"(transform): BertPredictionHeadTransform(\\n              (dense): Linear(in_features=768, out_feature...\"],[\"```\\n\\nWe see that `bert2bert.encoder` is an instance of `BertModel` and that\\n`bert2bert.decoder` one ...\"],[\"```python\\nOUTPUT:\\n-------\\n    EncoderDecoderConfig {\\n      \\\"_name_or_path\\\": \\\"bert2bert\\\",\\n      \\\"arch...\"],[\"\\\"label2id\\\": {\\n          \\\"LABEL_0\\\": 0,\\n          \\\"LABEL_1\\\": 1\\n        },\\n        \\\"layer_norm_eps\\\": 1e...\"],[\"\\\"vocab_size\\\": 30522,\\n        \\\"xla_device\\\": null\\n      },\\n      \\\"encoder\\\": {\\n        \\\"_name_or_path\\\":...\"],[\"\\\"length_penalty\\\": 1.0,\\n        \\\"max_length\\\": 20,\\n        \\\"max_position_embeddings\\\": 512,\\n        \\\"mi...\"],[\"```\\n\\nThe config is similarly composed of an encoder config and a decoder\\nconfig both of which are in...\"],[\"```\\n\\n```python\\nOUTPUT:\\n-------\\nNum Params. Shared: 137298244, Non-Shared: 247363386\\n```\\n\\nIn this not...\"],[\"```\\n\\nNext, let\\\\'s define all parameters related to beam search decoding.\\nSince `bart-large-cnn` yiel...\"],[\"```\\n\\nThe `Seq2SeqTrainer` extends 🤗Transformer\\\\'s Trainer for encoder-decoder\\nmodels. In short, it a...\"],[\"```\\n\\nAlso, we need to define a function to correctly compute the ROUGE score\\nduring validation. Sinc...\"],[\"```\\n\\nGreat, now we can pass all arguments to the `Seq2SeqTrainer` and start\\nfinetuning. Executing th...\"],[\"```\\n\\n### **Evaluation**\\n\\nIn a final step, we might want to evaluate the *BERT2BERT* model on the\\ntes...\"],[\"```\\n\\nLet\\\\'s run the map function to obtain the *results* dictionary that has\\nthe model\\\\'s predicted ...\"],[\"--\\ntitle: \\\"Introducing Hugging Face for Education 🤗\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f61_education\\u002fthumbnail....\"],[\"🗣️ Our goal is to make the potential and limitations of machine learning understandable to everyone....\"],[\"## 🤗 **Education for Beginners**\\n\\n🗣️ We want to lower the barrier to becoming a machine learning eng...\"],[\"🗣️ We want to empower educators with tools and offer collaborative spaces where students can build m...\"],[\"1️⃣ [A Tour through the Hugging Face Hub](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002feducation-toolkit\\u002fblob\\u002fmain...\"],[\"## 🤗 **Education Events & News**\\n\\n- **09\\u002f08**[EVENT]: ML Demo.cratization tour in Argentina at 2pm (...\"],[\"--\\ntitle: \\\"How Hugging Face Accelerated Development of Witty Works Writing Assistant\\\"\\nthumbnail: \\u002fbl...\"],[\"### First experiments \\nWitty Works first chose a basic machine learning approach to build their assi...\"],[\"```\\n\\n### Solutions provided by the [Hugging Face Experts](https:\\u002f\\u002fhuggingface.co\\u002fsupport?utm_source=...\"],[\"```\\n\\nTo fine-tune a vanilla transformers-based classifier, such as a simple BERT model, Witty Works ...\"],[\"```\\n\\nReducing the number of sentences was essential to ensure that model training remained fast and ...\"],[\"```\\n\\nWith the guidance of the Hugging Face experts, Witty Works saved time and money by implementing...\"],[\"--\\ntitle: Inference for PROs\\nthumbnail: \\u002fblog\\u002fassets\\u002finference_pro\\u002fthumbnail.png\\nauthors:\\n  - user: ...\"],[\"## Contents\\n\\n- [Supported Models](#supported-models)\\n- [Getting started with Inference for PROs](#ge...\"],[\"| Model               | Size                                                                        ...\"],[\"| Code Llama Base     | [7B](https:\\u002f\\u002fhuggingface.co\\u002fcodellama\\u002fCodeLlama-7b-hf) and [13B](https:\\u002f\\u002fhug...\"],[\"Inference for PROs makes it easy to experiment and prototype with new models without having to deplo...\"],[\"```\\n\\nWhich would print something like this:\\n\\n```json\\n[\\n  {\\n    \\\"generated_text\\\": \\\"In a surprising tu...\"],[\"```\\n\\nIf you don't want to pass the token explicitly every time you instantiate the client, you can u...\"],[\"```\\n\\nThis example shows the structure of the first message in a multi-turn conversation. Note how th...\"],[\"```\\n\\nThis same format can be used with Code Llama Instruct to engage in technical conversations with...\"],[\"```\\n\\nAs you can see, the format used for infilling follows this pattern:\\n\\n```\\nprompt = f\\\"\\u003cPRE\\u003e {prom...\"],[\"```\\n\\n![SDXL example generation](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fres...\"],[\"- `do_sample`: If set to `False` (the default), the generation method will be _greedy search_, which...\"],[\"### Controlling Image Generation\\n\\nIf you want finer-grained control over images generated with the S...\"],[\"```\\n\\n### Streaming\\n\\nToken streaming is the mode in which the server returns the tokens one by one as...\"],[\"```\\n\\n## Subscribe to PRO\\n\\nYou can sign up today for a PRO subscription [here](https:\\u002f\\u002fhuggingface.co...\"],[\"--\\ntitle: \\\"Towards Encrypted Large Language Models with FHE\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fencrypted-llm\\u002f...\"],[\"## Fully Homomorphic Encryption (FHE) Can Solve LLM Privacy Challenges\\n\\nZama’s solution to the chall...\"],[\"## Implementation of a LLM layer with FHE\\n\\nNext, you’ll see how to encrypt a single attention head o...\"],[\"This graph shows that 4-bit quantization maintains 96% of the original accuracy. The experiment is d...\"],[\"```\\n\\nThe forward pass is then overwritten so that the first head of the multi-head attention mechani...\"],[\"# Extract the queries, keys and vales\\n        q_qkv = q_qkv.expand_dims(axis=1, key=f\\\"unsqueeze_{sel...\"],[\"```\\n\\nOther computations in the model remain in floating point, non-encrypted and are expected to be ...\"],[\"```\\n\\nRunning this, you will see the following print out: “Circuit compiled with 8 bit-width”. This c...\"],[\"Zama libraries [Concrete](https:\\u002f\\u002fgithub.com\\u002fzama-ai\\u002fconcrete) and [Concrete-ML](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"--\\ntitle: \\\"Open-sourcing Knowledge Distillation Code and Weights of SD-Small and SD-Tiny\\\"\\nthumbnail:...\"],[\"## Knowledge Distillation\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingf...\"],[\"In this particular type of knowledge distillation, the student model is trained to do the normal dif...\"],[\"Image taken from the [paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2305.15798)  “On Architectural Compression of Tex...\"],[\"```python\\n\\nfrom diffusers import DiffusionPipeline\\nimport torch\\n\\npipeline = DiffusionPipeline.from_p...\"],[\"```\\n\\n## Speed in terms of inference latency\\n\\nWe have observed that distilled models are up to 100% f...\"],[\"## LoRA Training\\n\\nOne of the advantages of LoRA training on a distilled model is faster training. Be...\"],[\"--\\ntitle: \\\"The Hugging Face Hub for Galleries, Libraries, Archives and Museums\\\"\\nthumbnail: \\u002fblog\\u002fass...\"],[\"You can read the whole post or jump to the most relevant sections! \\n\\n- If you don't know what the Hu...\"],[\"Spaces make hosting and making your application accessible for others to use much more straightforwa...\"],[\"We can find NER models on the Hub by filtering models by task. In this case, we choose `token-classi...\"],[\"We can make datasets available via the Hugging Face hub in various ways. I'll walk through an exampl...\"],[\"Once you have done this you can choose a name for your new dataset repository. You can also create t...\"],[\"You can edit metadata using the `Metadata UI` editor. This allows you to specify the license, langua...\"],[\"## Why might Galleries, Libraries, Archives and Museums want to use the Hugging Face hub?\\n\\nThere are...\"],[\"### [Nasjonalbiblioteket AI Lab](https:\\u002f\\u002fhuggingface.co\\u002fNbAiLab) \\nThe AI lab at the National Library...\"],[\"## Hub features for Galleries, Libraries, Archives and Museums\\n\\nThe Hub supports many features which...\"],[\"If you require any assistance while using the Hugging Face Hub, there are several avenues you can ex...\"],[\"--\\ntitle: \\\"Putting ethical principles at the core of the research lifecycle\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"## Limitations of this ethical charter\\n\\nThis document is a work in progress and reflects a state of ...\"],[\"## Values for the project\\n\\n- **Be transparent:** We are transparent and open about the intent, sourc...\"],[\"--\\ntitle: \\\"StarCoder: A State-of-the-Art LLM for Code\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f141_starcoder\\u002fstarco...\"],[\"## Evaluation\\n\\nWe thoroughly evaluated StarCoder and several similar models and a variety of benchma...\"],[\"| **Model**          | **HumanEval** | **MBPP** |\\n|--------------------|--------------|----------|\\n|...\"],[\"## Tech Assistant\\n\\nWith the exhaustive evaluations we found that StarCoder is very capable at writin...\"],[\"## Links\\n\\n### Models\\n- [Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2305.06161): A technical report about StarCoder...\"],[\"### Data & Governance\\n- [StarCoderData](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fbigcode\\u002fstarcoderdata): Pret...\"],[\"--\\ntitle: \\\"Ethics and Society Newsletter #5: Hugging Face Goes To Washington and Other Summer 2023 M...\"],[\"In keeping with Hugging Face’s core values of *openness* and *accountability*, we are sharing a coll...\"],[\"In keeping with our core value of *democratization*, we have also spent a lot of time speaking publi...\"],[\"- Comments from [Sasha](https:\\u002f\\u002fhuggingface.co\\u002fsasha) on **AI’s energy use and carbon emissions** ([...\"],[\"penning part of a [Wall Street Journal op-ed on the topic](https:\\u002f\\u002fwww.wsj.com\\u002farticles\\u002fartificial-i...\"],[\"addressing how **marginalized workers create the data for AI** ([The Globe and Mail](https:\\u002f\\u002fwww.the...\"],[\"- Comments from [Nathan](https:\\u002f\\u002fhuggingface.co\\u002fnatolambert) on the state of the art on **language m...\"],[\"- Comments from [Meg](https:\\u002f\\u002fhuggingface.co\\u002fmeg) on **AI and misinformation** ([CNN](https:\\u002f\\u002fwww.cn...\"],[\"- Comments from [Irene](https:\\u002f\\u002fhuggingface.co\\u002firenesolaiman) on understanding the **regulatory land...\"],[\"- Comments from [Giada](https:\\u002f\\u002fhuggingface.co\\u002fgiadap) on the concepts of **AI “singularity”** ([Pop...\"],[\"Some of our talks released this summer include [Giada](https:\\u002f\\u002fhuggingface.co\\u002fgiadap)’s [TED present...\"],[\"Of course, we have also made progress on our regular work (our “work work”). The fundamental value o...\"],[\"[Nazneen](https:\\u002f\\u002fhuggingface.co\\u002fnazneen) and others on [Responsible Generative AI](https:\\u002f\\u002fwww.yout...\"],[\"cross-disciplinary approach](https:\\u002f\\u002favidml.org\\u002fevents\\u002ftti2023\\u002f) and [Assessing the Impacts of Gener...\"],[\"We have also moved forward with our goals of *fairness* and *justice* with [bias and harm testing](h...\"],[\"Finally, we have been surprised and delighted by public recognition for many of the society & ethics...\"],[\"--\\ntitle: 'Convert Transformers to ONNX with Hugging Face Optimum'\\nthumbnail: \\u002fblog\\u002fassets\\u002f81_conver...\"],[\"Let's get started! 🚀\\n\\n---\\n\\nIf you are interested in optimizing your models to run with maximum effic...\"],[\"➡️[Learn more about ONNX.](https:\\u002f\\u002fonnx.ai\\u002fabout.html)\\n\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\u003chtml itemsc...\"],[\"[➡️ Learn more about Optimum](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fhardware-partners-program)\\n\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdi...\"],[\"- ALBERT\\n- BART\\n- BERT\\n- DistilBERT\\n- ELECTRA\\n- GPT Neo\\n- GPT-J\\n- GPT-2\\n- RoBERTa\\n- T5\\n- ViT\\n- XLM\\n-...\"],[\"```\\n\\nexporting our checkpoint with `export` \\n\\n```python\\nimport torch\\nfrom transformers import AutoMo...\"],[\"```\\n\\nExporting our checkpoint with the `transformers.onnx`.\\n\\n```python\\nfrom pathlib import Path\\nimpo...\"],[\"```\\n\\nExporting our checkpoint with `ORTModelForSequenceClassification`\\n\\n```python\\nfrom optimum.onnxr...\"],[\"```\\n\\nThe best part about the conversion with Optimum is that you can immediately use the `model` to ...\"],[\"--\\ntitle: \\\"Comments on U.S. National AI Research Resource Interim Report\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f92...\"],[\"- Monitor for Open-Source and Open-Science for High Misuse and Malicious Use Potential\\n    - Harm mu...\"],[\"--\\ntitle: \\\"3D Asset Generation: AI for Game Development #3\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games...\"],[\"In this part, we'll talk about how you can use AI to generate 3D Assets. The short answer is: you ca...\"],[\"- [DreamFusion](https:\\u002f\\u002fdreamfusion3d.github.io\\u002f) uses 2D diffusion to generate 3D assets.\\n- [CLIPMa...\"],[\"What does all of this mean if you're a game developer? Currently, nothing. This technology hasn't re...\"],[\"Since NeRF-to-mesh, like photogrammetry, is currently most suited to creating ultra-high-fidelity as...\"],[\"Of course, only time will tell. If you want to keep up with advancements as they come, feel free to ...\"],[\"--\\ntitle: \\\"Opinion Classification with Kili and HuggingFace AutoTrain\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f59_op...\"],[\"## AutoTrain with HuggingFace\\n\\nAutomated Machine Learning is a term for automating a Machine Learnin...\"],[\"Kili is a commercial tool but you can also create a free developer account to try Kili’s tools. You ...\"],[\"For the labeling part, we need to create a project in Kili’s platform at first. We can use either th...\"],[\"```python\\nimport os\\n#we will process the data (which is a csv file)\\nimport pandas as pd\\n\\n#API client...\"],[\"```\\n\\nIn order to access the platform, we need to authenticate our client\\n\\n```python\\nAPI_KEY = os.get...\"],[\"```\\n\\nNow we can start to prepare our interface, the interface is just a dictionary in Python. We wil...\"],[\"for label, color in entity_dict.items():\\n    label_upper = label.strip().upper().replace(' ', '_')\\n ...\"],[\"```\\n\\nWe are ready to upload our data to the project. The `append_many_to_dataset` method can be used...\"],[\"```\\n\\nIt simply imports the given `dataset` DataFrame to a project specified by project_id.\\n\\nWe can s...\"],[\"```\\n\\nIt wasn’t difficult to use the Python API, the helper methods we used covered many difficulties...\"],[\"# read dataframes\\ndf1 = pd.read_csv(args['first'])\\ndf2 = pd.read_csv(args['second'])\\n\\n# concating tw...\"],[\"```\\n\\n## Labeling\\n\\nNow that we have the source data uploaded, the platform has a built-in labeling in...\"],[\"API_KEY = os.getenv('KILI_API_KEY')\\ndataset_path = '..\\u002fdata\\u002fprocessed\\u002flowercase_cleaned_dataset.csv'...\"],[\"print('Got the samples!')\\n\\n# we will pass the skipped samples\\ndf_ns = df[~df['skipped']].copy()\\n\\n# e...\"],[\"```\\n\\nNice! We now have the labeled data as a csv file. Let's create a dataset repository in HuggingF...\"],[\"2. We can select the dataset repository we created before or upload the dataset again. Then we need ...\"],[\"Ray tune is a popular library for hyper-parameter optimization which comes with many SOTA algorithms...\"],[\"# progress bar\\nfrom tqdm import tqdm\\n\\n# data manipulation \\u002f reading\\nimport numpy as np\\nimport pandas...\"],[\"```\\n\\nWe will set a seed for the libraries we use for reproducibility\\n\\n```python\\ndef seed_all(seed):\\n...\"],[\"```\\n\\nWe can download the model easily by specifying HuggingFace hub repository. It is also needed to...\"],[\"```\\n\\nI also defined two dictionaries for mapping labels to indices and indices to labels.\\n\\n```python...\"],[\"```\\n\\nAnother utility function that returns stratified and tokenized Torch dataset splits:\\n\\n```python...\"],[\"```\\n\\nNow we can perform the search! Let’s start by processing the data:\\n\\n```python\\ntokenized_train_s...\"],[\"```\\n\\nWe performed the search with 20 and 40 trials respectively, the results are shown below. The we...\"],[\"We use Google Colab for the inference ([here](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1kGYl_YcMmA2gj...\"],[\"We won't do a detailed analysis of the reviews, a basic understanding of potential problems would su...\"],[\"--\\ntitle: \\\"AI for Game Development: Creating a Farming Game in 5 Days. Part 2\\\"\\nthumbnail: \\u002fblog\\u002fasse...\"],[\"### The Short Version\\n\\nThe short version is straightforward: ask [ChatGPT](https:\\u002f\\u002fchat.openai.com\\u002fc...\"],[\"**Transformers**, [introduced in 2017](https:\\u002f\\u002fproceedings.neurips.cc\\u002fpaper\\u002f2017\\u002ffile\\u002f3f5ee243547dee...\"],[\"#### Limitations\\n\\nChatGPT often sounds very convincing, while being wrong. Here is an [archive of Ch...\"],[\"--\\ntitle: How to train a new language model from scratch using Transformers and Tokenizers\\nthumbnail...\"],[\"\\u003e N.B. You won’t need to understand Esperanto to understand this post, but if you do want to learn i...\"],[\"We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same spec...\"],[\"```\\n\\nAnd here’s a slightly accelerated capture of the output:\\n\\n![tokenizers](assets\\u002f01_how-to-train\\u002f...\"],[\"```\\n\\nWhat is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer...\"],[\"```\\n\\n## 3. Train a language model from scratch\\n\\n**Update:** The associated Colab notebook uses our n...\"],[\"Here’s a simple version of our EsperantoDataset.\\n\\n```python\\nfrom torch.utils.data import Dataset\\n\\ncl...\"],[\"```\\n\\nIf your dataset is very large, you can opt to load and tokenize examples on the fly, rather tha...\"],[\"```\\n\\nAs usual, pick the largest batch size you can fit on your GPU(s). \\n\\n**🔥🔥🔥 Let’s start training!...\"],[\"# The sun \\u003cmask\\u003e.\\n# =\\u003e\\n\\nresult = fill_mask(\\\"La suno \\u003cmask\\u003e.\\\")\\n\\n# {'score': 0.2526160776615143, 'sequ...\"],[\"```\\n\\nOk, simple syntax\\u002fgrammar works. Let’s try a slightly more interesting prompt:\\n\\n```python\\nfill_...\"],[\"```\\n\\n\\u003e “**Jen la komenco de bela tago**”, indeed!\\n\\nWith more complex prompts, you can probe whether ...\"],[\"nlp = pipeline(\\n    \\\"ner\\\",\\n    model=MODEL_PATH,\\n    tokenizer=MODEL_PATH,\\n)\\n# or instantiate a Toke...\"],[\"```\\n\\n**Looks like it worked! 🔥**\\n\\n\\u003csmall\\u003eFor a more challenging dataset for NER, \\u003ca href=\\\"https:\\u002f\\u002fgi...\"],[\"--\\ntitle: \\\"Using & Mixing Hugging Face Models with Gradio 2.0\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f22_gradio\\u002fgra...\"],[\"![GIF of Gradio 2.0](.\\u002fassets\\u002f22_gradio\\u002frecording-20.gif)\\n\\nBy default, this uses HuggingFace’s hoste...\"],[\"--\\ntitle: \\\"Hugging Face and Graphcore partner for IPU-optimized Transformers\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"Making Poplar compatible with these widely used, third-party systems allows developers to easily por...\"],[\"Full details of this example can be found in the Graphcore blog [BERT-Large training on the IPU expl...\"],[\"--\\ntitle: \\\"AudioLDM 2, but faster ⚡️\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f161_audioldm2\\u002fthumbnail.png\\nauthors:\\n...\"],[\"Read to the end to find out how to generate a 10 second audio sample in just 1 second!\\n\\n## Model ove...\"],[\"In the `diffusers` implementation, these projections are defined by the [AudioLDM2ProjectionModel](h...\"],[\"where the initial latent variable \\\\\\\\(\\\\boldsymbol{z}_{0}\\\\\\\\) is drawn from a normal distribution \\\\\\\\(\\\\m...\"],[\"For full details on how the AudioLDM 2 model is trained, the reader is referred to the [AudioLDM 2 p...\"],[\"| Checkpoint                                                            | Task          | Model Size...\"],[\"```python\\nfrom diffusers import AudioLDM2Pipeline\\n\\nmodel_id = \\\"cvssp\\u002faudioldm2\\\"\\npipe = AudioLDM2Pipe...\"],[\"```\\n**Output:**\\n```\\nLoading pipeline components...: 100%|███████████████████████████████████████████...\"],[\"```\\n\\nCool! That run took about 13 seconds to generate. Let's have a listen to the output audio:\\n\\n```...\"],[\"```\\n\\n\\u003caudio controls\\u003e \\n  \\u003csource src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"```\\n\\n**Output:**\\n```\\n100%|███████████████████████████████████████████| 200\\u002f200 [00:12\\u003c00:00, 16.60it...\"],[\"```\\n\\n**Output:**\\n\\n```\\n100%|███████████████████████████████████████████| 200\\u002f200 [00:09\\u003c00:00, 20.94i...\"],[\"```\\n\\n**Output:**\\n```\\n100%|███████████████████████████████████████████| 200\\u002f200 [01:23\\u003c00:00,  2.39it...\"],[\"```\\n\\n**Output:**\\n```\\n[diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler,\\n diffusers....\"],[\"```\\n\\nAlright! We've got a long list of schedulers to choose from 📝. By default, AudioLDM 2 uses the ...\"],[\"```\\n\\nLet's set the number of inference steps to 20 and re-run the generation with the new scheduler....\"],[\"```\\n\\n\\u003caudio controls\\u003e \\n  \\u003csource src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"Let's try generating an audio sample 2.5 minutes (150 seconds) in duration. We'll also generate 4 ca...\"],[\"```\\n\\n**Output:**\\n```\\n---------------------------------------------------------------------------\\nOut...\"],[\"```\\n\\nUnless you have a GPU with high RAM, the code above probably returned an OOM error. While the A...\"],[\"```\\n\\n**Output:**\\n```\\n100%|███████████████████████████████████████████| 20\\u002f20 [00:36\\u003c00:00,  1.82s\\u002fit...\"],[\"--\\ntitle: \\\"Accelerating Stable Diffusion Inference on Intel CPUs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f136_stable...\"],[\"## The Diffusers library\\n\\nThe [Diffusers](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002findex) library makes...\"],[\"```\\nvirtualenv sd_inference\\nsource sd_inference\\u002fbin\\u002factivate\\npip install pip --upgrade\\npip install t...\"],[\"```\\n\\nThe average latency is **32.3 seconds**. As demonstrated by this [Intel Space](https:\\u002f\\u002fhuggingf...\"],[\"```\\n\\nOpenVINO automatically optimizes the model for the `bfloat16` format. Thanks to this, the avera...\"],[\"```\\n\\nWith a static shape, average latency is slashed to **4.7 seconds**, an additional 3.5x speedup....\"],[\"```\\n\\nNext, we install the `libiomp` library to optimize parallel processing. It's part of [Intel Ope...\"],[\"```\\npip install intel_extension_for_pytorch==1.13.100\\n```\\n\\nWe then update our code to optimize each ...\"],[\"```\\n\\nWe also enable the `bloat16` data format to leverage the AMX tile matrix multiply unit (TMMU) a...\"],[\"```\\n\\nWith this final version, inference latency is now down to **5.05 seconds**. Compared to our ini...\"],[\"--\\ntitle: \\\"A Complete Guide to Audio Datasets\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f116_audio_datasets\\u002fthumbnail...\"],[\"## The Hub\\n\\nThe Hugging Face Hub is a platform for hosting models, datasets and demos, all open sour...\"],[\"The Dataset Preview is a brilliant way of experiencing audio datasets before committing to using the...\"],[\"```python\\nfrom datasets import load_dataset\\n\\ngigaspeech = load_dataset(\\\"speechcolab\\u002fgigaspeech\\\", \\\"xs...\"],[\"```\\n\\n**Print Output:**\\n```python\\nDatasetDict({\\n    train: Dataset({\\n        features: ['segment_id',...\"],[\"```\\n\\n**Print Output:**\\n```python\\nDataset({\\n    features: ['segment_id', 'speaker', 'text', 'audio', ...\"],[\"```\\n\\n**Print Output:**\\n```python\\n{'segment_id': 'YOU0000000315_S0000660',\\n 'speaker': 'N\\u002fA', \\n 'text...\"],[\"```\\n\\nWe can see that there are a number of features returned by the training split, including `segme...\"],[\"```\\n\\nGreat! We can see that we've got the two required columns `text` and `audio`. The `text` is a s...\"],[\"```\\n\\nRe-loading the first audio sample in the GigaSpeech dataset will resample it to the desired sam...\"],[\"```\\n\\n**Print Output:**\\n\\n```python\\n{'text': \\\"AS THEY'RE LEAVING \\u003cCOMMA\\u003e CAN KASH PULL ZAHRA ASIDE REA...\"],[\"```\\n\\nEasy! `cast_column` provides a straightforward mechanism for resampling audio datasets as and w...\"],[\"```\\n\\nWe can apply the data preparation function to all of our training examples using 🤗 Datasets' `m...\"],[\"```\\n\\nAnd with that, we have the GigaSpeech dataset fully prepared for our model! In total, this proc...\"],[\"This is analogous to _downloading_ a TV show versus _streaming_ it. When we download a TV show, we d...\"],[\"There is one caveat to streaming mode. When downloading a dataset, both the raw data and processed d...\"],[\"```\\n\\nAll the steps covered so far in this tutorial can be applied to the streaming dataset without a...\"],[\"### English Speech Recognition\\n\\nSpeech recognition, or speech-to-text, is the task of mapping from s...\"],[\"| Dataset                                                                                 | Domain  ...\"],[\"| [GigaSpeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fspeechcolab\\u002fgigaspeech)                    | Audioboo...\"],[\"Refer to the [Google Colab](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fsanchit-gandhi\\u002fnotebooks\\u002fblob\\u002fm...\"],[\"```\\n\\n#### [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmozilla-foundation\\u002fcommon_voice_11_0)\\nCommo...\"],[\"```\\n\\n#### [TED-LIUM](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fLIUM\\u002ftedlium)\\nTED-LIUM is a dataset based on En...\"],[\"```\\n\\n#### [Earnings-22](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002frevdotcom\\u002fearnings22)\\nEarnings-22 is a 119-h...\"],[\"```\\n\\n### Multilingual Speech Recognition\\n\\nMultilingual speech recognition refers to speech recogniti...\"],[\"#### [FLEURS](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fgoogle\\u002ffleurs)\\nFLEURS (Few-shot Learning Evaluation of...\"],[\"### Audio Classification\\n\\nAudio classification is the task of mapping a raw audio input to a class l...\"],[\"## Closing Remarks\\n\\nIn this blog post, we explored the Hugging Face Hub and experienced the Dataset ...\"],[\"--\\ntitle: The Annotated Diffusion Model\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_annotated-diffusion\\u002fthumbnail.png...\"],[\"We'll go over the original DDPM paper by ([Ho et al., 2020](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2006.11239)), impl...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"assets\\u002f78_annotated-diffusion\\u002fddpm_paper.png\\\" width=\\\"500\\\" \\u002f\\u003e\\n\\u003c...\"],[\"```\\n\\n## What is a diffusion model?\\n\\nA (denoising) diffusion model isn't that complex if you compare ...\"],[\"## In more mathematical form\\n\\nLet's write this down more formally, as ultimately we need a tractable...\"],[\"Recall that a normal distribution (also called Gaussian distribution) is defined by 2 parameters: a ...\"],[\"Now, if we knew the conditional distribution \\\\\\\\(p(\\\\mathbf{x}_{t-1} | \\\\mathbf{x}_t)\\\\\\\\), then we could...\"],[\"Hence, our neural network needs to learn\\u002frepresent the mean and variance. However, the DDPM authors ...\"],[\"So we continue, assuming that our neural network only needs to learn\\u002frepresent the mean of this cond...\"],[\"A direct consequence of the constructed forward process \\\\\\\\(q\\\\\\\\), as shown by Sohl-Dickstein et al., ...\"],[\"Another beauty of this property, as shown in Ho et al. is that one can (after some math, for which w...\"],[\"Here, \\\\\\\\(\\\\mathbf{x}_0\\\\\\\\) is the initial (real, uncorrupted) image, and we see the direct noise level...\"],[\"In reality, all of this is done on batches of data, as one uses stochastic gradient descent to optim...\"],[\"As can be seen, a U-Net model first downsamples the input (i.e. makes the input smaller in terms of ...\"],[\"def Downsample(dim, dim_out=None):\\n    # No More Strided Convolutions or Pooling\\n    return nn.Seque...\"],[\"```\\n\\n### Position embeddings\\n\\nAs the parameters of the neural network are shared across time (noise ...\"],[\"```\\n\\n### ResNet block\\n\\nNext, we define the core building block of the U-Net model. The DDPM authors ...\"],[\"def forward(self, x, scale_shift=None):\\n        x = self.proj(x)\\n        x = self.norm(x)\\n\\n        i...\"],[\"```\\n\\n### Attention module\\n\\nNext, we define the attention module, which the DDPM authors added in bet...\"],[\"def forward(self, x):\\n        b, c, h, w = x.shape\\n        qkv = self.to_qkv(x).chunk(3, dim=1)\\n    ...\"],[\"q = q.softmax(dim=-2)\\n        k = k.softmax(dim=-1)\\n\\n        q = q * self.scale\\n        context = to...\"],[\"```\\n\\n### Group normalization\\n\\nThe DDPM authors interleave the convolutional\\u002fattention layers of the ...\"],[\"```\\n\\n### Conditional U-Net\\n\\nNow that we've defined all building blocks (position embeddings, ResNet ...\"],[\"# determine dimensions\\n        self.channels = channels\\n        self.self_condition = self_condition...\"],[\"mid_dim = dims[-1]\\n        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\\n  ...\"],[\"x = self.init_conv(x)\\n        r = x.clone()\\n\\n        t = self.time_mlp(time)\\n\\n        h = []\\n\\n      ...\"],[\"```\\n\\n## Defining the forward diffusion process\\n\\nThe forward diffusion process gradually adds noise t...\"],[\"def linear_beta_schedule(timesteps):\\n    beta_start = 0.0001\\n    beta_end = 0.02\\n    return torch.li...\"],[\"```\\n\\nTo start with, let's use the linear schedule for \\\\\\\\(T=300\\\\\\\\) time steps and define the various ...\"],[\"```\\n\\nWe'll illustrate with a cats image how noise is added at each time step of the diffusion proces...\"],[\"```\\n\\n\\u003cdiv class=\\\"output stream stdout\\\"\\u003e\\n\\n    Output:\\n    -------------------------------------------...\"],[\"```\\n\\nLet's verify this:\\n\\n```python\\nreverse_transform(x_start.squeeze())\\n```\\n    \\n\\u003cimg src=\\\"assets\\u002f78...\"],[\"```\\n\\n\\u003cimg src=\\\"assets\\u002f78_annotated-diffusion\\u002foutput_cats_noisy.png\\\" width=\\\"100\\\" \\u002f\\u003e\\n\\nLet's visualize ...\"],[\"```\\n\\n```python\\nplot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])\\n`...\"],[\"```\\n\\nThe `denoise_model` will be our U-Net defined above. We'll employ the Huber loss between the tr...\"],[\"```\\n\\nNext, we define a function which we'll apply on-the-fly on the entire dataset. We use the `with...\"],[\"```\\n\\n\\u003cdiv class=\\\"output stream stdout\\\"\\u003e\\n\\n    Output:\\n    -------------------------------------------...\"],[\"Ideally, we end up with an image that looks like it came from the real data distribution.\\n\\nThe code ...\"],[\"@torch.no_grad()\\ndef sample(model, image_size, batch_size=16, channels=3):\\n    return p_sample_loop(...\"],[\"```\\n\\nNote that the code above is a simplified version of the original implementation. We found our s...\"],[\"```\\n\\nLet's start training!\\n\\n```python\\nfrom torchvision.utils import save_image\\n\\nepochs = 6\\n\\nfor epoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"output stream stdout\\\"\\u003e\\n\\n    Output:\\n    -------------------------------------------...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\n\\n## Sampling (inference)\\n\\nTo sample from the model, we can just use our sample function defi...\"],[\"```\\n\\n\\u003cimg src=\\\"assets\\u002f78_annotated-diffusion\\u002foutput.png\\\" width=\\\"300\\\" \\u002f\\u003e\\n\\nSeems like the model is cap...\"],[\"```\\n\\n\\u003cimg src=\\\"\\nassets\\u002f78_annotated-diffusion\\u002fdiffusion-sweater.gif\\\" width=\\\"300\\\" \\u002f\\u003e\\n\\n# Follow-up rea...\"],[\"- Improved Denoising Diffusion Probabilistic Models ([Nichol et al., 2021](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f210...\"],[\"Note that this list only includes important works until the time of writing, which is June 7th, 2022...\"],[\"--\\ntitle: \\\"How Sempre Health is leveraging the Expert Acceleration Program to accelerate their ML ro...\"],[\"If you'd like to accelerate your machine learning roadmap with the help of our experts, as Swaraj an...\"],[\"### How did you leverage the Expert Acceleration Program?\\n\\nThe Hugging Face team really helped us in...\"],[\"---\\n\\nWith the Expert Acceleration Program, we've put together a world-class team to help customers b...\"],[\"--\\ntitle: \\\"Databricks ❤️ Hugging Face: up to 40% faster training and tuning of Large Language Models...\"],[\"```swift\\nfrom datasets import load_dataset\\n\\ntrain_df = train.write.parquet(train_dbfs_path, mode=\\\"ov...\"],[\"```\\nNot only was this cumbersome, but it also meant that data had to be written to disk and then rea...\"],[\"```\\nThis allows users to use Spark to efficiently load and transform data for training or fine-tunin...\"],[\"In order to become the best platform for users to jump into the world of AI, we’re working hard to p...\"],[\"--\\ntitle: 'Introducing Snowball Fight ☃️, our first ML-Agents environment'\\nthumbnail: \\u002fblog\\u002fassets\\u002f3...\"],[\"With this first step, our goal is to build an ecosystem on Hugging Face for Deep Reinforcement Learn...\"],[\"![screenshot2vs2](assets\\u002f39_introducing_snowball_fight\\u002fscreenshot2vs2.png)\\n\\n- And we're building **n...\"],[\"--\\ntitle: \\\"Deep Learning with Proteins\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f119_deep_learning_with_proteins\\u002ffol...\"],[\"Let’s say that you want to train a DL model to take a sentence in English as input and decide if it’...\"],[\"Now try the same task, but in English:\\n\\n| Text | Label |\\n| --- | --- |\\n| She’s the best director in ...\"],[\"This stage of affairs continued until 2018, when two huge papers landed, introducing the models [ULM...\"],[\"At this point, hopefully you understand what transfer learning is, and that a large language model i...\"],[\"Proteins are composed of multiple **amino acids.** Amino acids are relatively simple molecules that ...\"],[\"![protein structure](assets\\u002f119_deep_learning_with_proteins\\u002fprotein_structure.png)\\n\\n*This figure sho...\"],[\"## Bringing it together: Machine learning with proteins\\n\\nSo now we've seen how transfer learning wit...\"],[\"Like a lot of other fields, though, the arrival of deep learning changed everything. AlphaFold and e...\"],[\"The key takeaway, though, is that even though proteins are very different to language, they can be h...\"],[\"If you’re a biologist, on the other hand, you probably have several ideas for what you want to try, ...\"],[\"## Conclusion\\n\\nThe intersection of deep learning and biology is going to be an incredibly active and...\"],[\"--\\ntitle: \\\"Sentence Transformers in the Hugging Face Hub\\\"\\nauthors:\\n- user: osanseviero\\n- user: nreim...\"],[\"```\\n\\nBut not only this. People will probably want to either demo their models or play with other mod...\"],[\"\\u003cdiv\\u003e\\u003ca class=\\\"text-xs block mb-3 text-gray-300\\\" href=\\\"\\u002fsentence-transformers\\u002fdistilbert-base-nli-ma...\"],[\"data-props=\\\"{&quot;apiUrl&quot;:&quot;https:\\u002f\\u002fapi-inference.huggingface.co&quot;,&quot;model&quot;:{...\"],[\"&quot;:{&quot;author&quot;:&quot;sentence-transformers&quot;,&quot;autoArchitecture&quot;:&quot;Auto...\"],[\"uot;AutoModel&quot;,&quot;branch&quot;:&quot;main&quot;,&quot;cardData&quot;:{&quot;pipeline_tag&quo...\"],[\"_tag&quot;:&quot;feature-extraction&quot;,&quot;tags&quot;:[&quot;sentence-transformers&quot;,&quot;...\"],[\";,&quot;feature-extraction&quot;,&quot;sentence-similarity&quot;,&quot;transformers&quot;]},&quot;ca...\"],[\"&quot;cardSource&quot;:true,&quot;config&quot;:{&quot;architectures&quot;:[&quot;DistilBertModel&quo...\"],[\"odel&quot;],&quot;model_type&quot;:&quot;distilbert&quot;},&quot;id&quot;:&quot;sentence-transformer...\"],[\"nsformers\\u002fdistilbert-base-nli-max-tokens&quot;,&quot;pipeline_tag&quot;:&quot;feature-extraction&quo...\"],[\"tion&quot;,&quot;library_name&quot;:&quot;sentence-transformers&quot;,&quot;mask_token&quot;:&quot;[...\"],[\":&quot;[MASK]&quot;,&quot;modelId&quot;:&quot;sentence-transformers\\u002fdistilbert-base-nli-max-tokens&q...\"],[\"tokens&quot;,&quot;private&quot;:false,&quot;siblings&quot;:[{&quot;rfilename&quot;:&quot;.gitattrib...\"],[\"itattributes&quot;},{&quot;rfilename&quot;:&quot;README.md&quot;},{&quot;rfilename&quot;:&quot;confi...\"],[\"ot;config.json&quot;},{&quot;rfilename&quot;:&quot;config_sentence_transformers.json&quot;},{&quot;r...\"],[\"{&quot;rfilename&quot;:&quot;modules.json&quot;},{&quot;rfilename&quot;:&quot;pytorch_model.bin&quot...\"],[\"bin&quot;},{&quot;rfilename&quot;:&quot;sentence_bert_config.json&quot;},{&quot;rfilename&quot;:&quo...\"],[\"ot;:&quot;special_tokens_map.json&quot;},{&quot;rfilename&quot;:&quot;tokenizer.json&quot;},{&quot;r...\"],[\"{&quot;rfilename&quot;:&quot;tokenizer_config.json&quot;},{&quot;rfilename&quot;:&quot;vocab.txt&quo...\"],[\".txt&quot;},{&quot;rfilename&quot;:&quot;1_Pooling\\u002fconfig.json&quot;}],&quot;tags&quot;:[&quot;pytor...\"],[\"ot;pytorch&quot;,&quot;distilbert&quot;,&quot;arxiv:1908.10084&quot;,&quot;sentence-transformers&quo...\"],[\"mers&quot;,&quot;feature-extraction&quot;,&quot;sentence-similarity&quot;,&quot;transformers&quot;,&...\"],[\"&quot;,&quot;pipeline_tag:feature-extraction&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;feat...\"],[\"uot;feature-extraction&quot;,&quot;label&quot;:&quot;Feature...\"],[\"Extraction&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quot;id&quot;:&quot;pytorch&quot;,&quo...\"],[\"Transformers&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;transformers&quot;,&q...\"],[\"class=\\\"font-semibold flex items-center mb-2\\\"\\u003e\\u003cdiv class=\\\"text-lg flex items-center\\\"\\u003e\\u003csvg xmlns=\\\"http...\"],[\"Hosted inference API\\u003c\\u002fdiv\\u003e \\u003ca target=\\\"_blank\\\" href=\\\"\\u002fdocs\\\"\\u003e\\u003csvg class=\\\"ml-1.5 text-sm text-gray-400 ...\"],[\"0 1-14 14zm0-26a12 12 0 1 0 12 12A12 12 0 0 0 16 4z\\\" fill=\\\"currentColor\\\"\\u003e\\u003c\\u002fpath\\u003e\\u003c\\u002fsvg\\u003e\\u003c\\u002fa\\u003e\\u003c\\u002fdiv\\u003e \\u003cdi...\"],[\"0 0 0 2-2V5a2 2 0 0 0-2-2zm0 2v4H5V5zm-10 6h10v7H17zm-2 7H5v-7h10zM5 20h10v7H5zm12 7v-7h10v7z\\\"\\u003e\\u003c\\u002fpat...\"],[\"Inference API.\\u003c\\u002fdiv\\u003e \\u003c\\u002fdiv\\u003e   \\u003cdiv class=\\\"mt-auto pt-4 flex items-center text-xs text-gray-500\\\"\\u003e\\u003cbut...\"],[\"fill=\\\"currentColor\\\"\\u003e\\u003c\\u002fpath\\u003e\\u003cpath d=\\\"M12.419 25.484L17.639 6l1.932.518L14.35 26z\\\" fill=\\\"currentColor\\\"...\"],[\"JSON Output\\u003c\\u002fbutton\\u003e \\u003cbutton class=\\\"flex items-center ml-auto\\\"\\u003e\\u003csvg class=\\\"mr-1\\\" xmlns=\\\"http:\\u002f\\u002fwww.w...\"],[\"But seeing a bunch of numbers might not be very useful to you (unless you're able to understand the ...\"],[\"\\u003c!-- Hackiest hack ever for the draft --\\u003e\\n\\u003cdiv\\u003e\\u003ca class=\\\"text-xs block mb-3 text-gray-300\\\" href=\\\"\\u002fse...\"],[\"\\u003cdiv class=\\\"p-5 shadow-sm rounded-xl bg-white max-w-md\\\"\\u003e\\u003cdiv class=\\\"SVELTE_HYDRATER \\\"...\"],[\"class=\\\"SVELTE_HYDRATER \\\" data-props=\\\"{&quot;apiUrl&quot;:&quot;https:\\u002f\\u002fapi-inference.huggingface.co&...\"],[\"Similarity&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quot;id&quot;:&quot;pytorch&quot;,&quo...\"],[\"data-target=\\\"InferenceWidget\\\"\\u003e\\u003cdiv class=\\\"flex flex-col w-full max-w-full...\"],[\"\\\"\\u003e \\u003cdiv class=\\\"font-semibold flex items-center mb-2\\\"\\u003e\\u003cdiv class=\\\"text-lg flex items-center\\\"\\u003e\\u003csvg xml...\"],[\"0 1-14 14zm0-26a12 12 0 1 0 12 12A12 12 0 0 0 16 4z\\\" fill=\\\"currentColor\\\"\\u003e\\u003c\\u002fpath\\u003e\\u003c\\u002fsvg\\u003e\\u003c\\u002fa\\u003e\\u003c\\u002fdiv\\u003e \\u003cdi...\"],[\"30l-5-5l5-5z\\\"\\u003e\\u003c\\u002fpath\\u003e\\u003cpath d=\\\"M11 30H3a1 1 0 0 1-.894-1.447l4-8a1.041 1.041 0 0 1 1.789 0l4 8A1 1 0 ...\"],[\"3 0 1 0 3 3a3.003 3.003 0 0 0-3-3z\\\"\\u003e\\u003c\\u002fpath\\u003e\\u003c\\u002fsvg\\u003e \\u003cspan\\u003eSentence Similarity\\u003c\\u002fspan\\u003e\\u003c\\u002fdiv\\u003e \\u003cdiv class=...\"],[\"\\u003cinput class=\\\" form-input-alt block w-full \\\" placeholder=\\\"Your sentence here...\\\" type=\\\"text\\\"\\u003e\\u003c\\u002flabel...\"],[\"text-gray-500\\\"\\u003e\\u003cbutton class=\\\"flex items-center cursor-not-allowed text-gray-300\\\" disabled=\\\"\\\"\\u003e\\u003csvg c...\"],[\"JSON Output\\u003c\\u002fbutton\\u003e \\u003cbutton class=\\\"flex items-center ml-auto\\\"\\u003e\\u003csvg class=\\\"mr-1\\\" xmlns=\\\"http:\\u002f\\u002fwww.w...\"],[\"Of course, on top of the widgets, we also provide API endpoints in our Inference API that you can us...\"],[\"```\\n\\n## Unleashing the Power of Sharing\\n\\nSo why is this powerful? In a matter of minutes, you can sh...\"],[\"```\\n\\nIf you don't have any model in the Hub and want to learn more about Sentence Transformers, head...\"],[\"--\\ntitle: \\\"Running IF with 🧨 diffusers on a Free Tier Google Colab\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fif\\u002fthumb...\"],[\"## Introduction\\n\\nIF is a pixel-based text-to-image generation model and was [released in\\nlate April ...\"],[\"Nevertheless, it is possible to run IF on consumer hardware if one\\noptimizes the model for low-memor...\"],[\"## Accepting the license\\n\\nBefore you can use IF, you need to accept its usage conditions. To do so:\\n...\"],[\"```\\n\\nrun the login function in a Python shell\\n\\n```py\\nfrom huggingface_hub import login\\n\\nlogin()...\"],[\"```\\n\\nand enter your [Hugging Face Hub access token](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002fsecurity-tokens#...\"],[\"Let\\\\'s map out the size of IF\\\\'s model components in full float32\\nprecision:\\n\\n- [T5-XXL Text Encoder...\"],[\"Now that each component fits individually into both CPU and GPU memory,\\nwe need to make sure that co...\"],[\"```\\n\\n```bash\\nMemTotal:       13297192 kB\\n```\\n\\nAnd an NVIDIA T4 with 15 GB VRAM:\\n\\n``` python\\n!nvidia-...\"],[\"```bash\\nSun Apr 23 23:14:19 2023       \\n+-----------------------------------------------------------...\"],[\"| N\\u002fA   72C    P0    32W \\u002f  70W |   1335MiB \\u002f 15360MiB |      0%      Default |\\n|                   ...\"],[\"```\\n\\n## Install dependencies\\n\\nSome optimizations can require up-to-date versions of dependencies. If...\"],[\"```\\n\\n## 1. Text-to-image generation\\n\\nWe will walk step by step through text-to-image generation with...\"],[\"```\\n\\n### 1.2 Create text embeddings\\n\\nThe Diffusers API for accessing diffusion models is the\\n`Diffus...\"],[\"```\\n\\nand run it through the 8bit quantized T5 model:\\n\\n``` python\\nprompt_embeds, negative_embeds = pi...\"],[\"```\\n\\nOften, we directly pass the text prompt to `DiffusionPipeline.__call__`.\\nHowever, we previously...\"],[\"```\\n\\n### 1.5 Stage 2: Super Resolution 64x64 to 256x256 \\n\\nIF comes with a separate diffusion process...\"],[\"```\\n\\n### 1.6 Stage 3: Super Resolution 256x256 to 1024x1024\\n\\nThe second super resolution model for I...\"],[\"```\\n\\nView output image\\n\\n``` python\\npil_image[0]\\n```\\n\\n![t2i_upscaled_2](https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nand load it into a PIL Image\\n\\n``` python\\nfrom PIL import Image\\nfrom io import BytesIO\\n\\noriginal...\"],[\"```\\n\\nFor image variation, we load the checkpoint with\\n[`IFImg2ImgPipeline`](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"```\\n\\nThe image variation pipeline requires both the original image and the\\nprompt embeddings.\\n\\nWe ca...\"],[\"```\\n💡 **Note**: The image variation super resolution pipeline requires the\\ngenerated image as well a...\"],[\"```\\n\\n![inpainting_sample](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fm...\"],[\"```\\n\\n![masking_by_hand](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmai...\"],[\"```\\n\\nNow, we need to pass the input image, the mask image, and the prompt\\nembeddings.\\n\\n``` python\\nim...\"],[\"```\\n\\n![inpainted_final_output](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"--\\ntitle: \\\"Hugging Face and IBM partner on watsonx.ai, the next-generation enterprise studio for AI ...\"],[\"All of this will only happen with standardization and automation. Organizations can't afford to buil...\"],[\"IBM decided that open source should be at the core of watsonx.ai. We couldn't agree more! Built on [...\"],[\"Our joint team is hard at work at the moment. We can't wait to show you what we've been up to! The m...\"],[\"--\\ntitle: The State of Computer Vision at Hugging Face 🤗\\nthumbnail: \\u002fblog\\u002fassets\\u002fcv_state\\u002fthumbnail....\"],[\"- Image classification\\n- Image segmentation\\n- (Zero-shot) object detection\\n- Video classification\\n- ...\"],[\"## Support for Pipelines\\n\\nWe developed [Pipelines](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002f...\"],[\"```\\n\\n\\u003cdiv align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-im...\"],[\"```\\n\\n## Training your own models\\n\\nWhile being able to use a model for off-the-shelf inference is a g...\"],[\"[Hugging Face example scripts](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples) inclu...\"],[\"## Integrations with Datasets\\n\\n[Datasets](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets) provides easy access...\"],[\"```\\n\\nBesides these datasets, we provide integration support with augmentation libraries like [albume...\"],[\"```\\n\\n\\u003cdiv align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"- [Generate 3D voxels from a predicted depth map of an input image](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fra...\"],[\"## The technical philosophy\\n\\nIn this section, we wanted to share our philosophy behind adding suppor...\"],[\"with torch.no_grad():\\n    logits = model(**inputs).logits\\n\\n# model predicts one of the 1000 ImageNet...\"],[\"```\\n\\nEven for a difficult task like object detection, the user experience doesn’t change very much:\\n...\"],[\"```\\n\\nLeads to:\\n\\n```bash\\nDetected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118....\"],[\"```\\n\\n## Zero-shot models for vision\\n\\nThere’s been a surge of models that reformulate core vision tas...\"],[\"- [CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmodel_doc\\u002fclip) that enables zero-shot ima...\"],[\"The community can expect to see more zero-shot models for computer vision being supported from 🤗Tran...\"],[\"As always, we welcome your patches, PRs, model checkpoints, datasets, and other contributions! 🤗\\n\\n*A...\"],[\"--\\ntitle: \\\"Fine-tuning Stable Diffusion models on Intel CPUs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fstable-diffusi...\"],[\"This post will show you how to fine-tune a Stable Diffusion model on an Intel Sapphire Rapids CPU cl...\"],[\"```\\nArchitecture:            x86_64\\n  CPU op-mode(s):        32-bit, 64-bit\\n  Address sizes:        ...\"],[\"Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush...\"],[\"xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand la...\"],[\"xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni a...\"],[\"```\\n\\nLet's first list the IP addresses of our servers in `nodefile.` The first line refers to the pr...\"],[\"```\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers.git\\ncd diffusers\\npip install .\\n```\\n\\nNext, we ...\"],[\"```\\nmkdir \\u002fhome\\u002fdevcloud\\u002fdicoo\\ncd \\u002fhome\\u002fdevcloud\\u002fdicoo\\nwget https:\\u002f\\u002fhuggingface.co\\u002fsd-concepts-libra...\"],[\"```\\n\\nHere are the images:\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fsd-concepts-library\\u002fdicoo\\u002fresolve\\u002fmain\\u002fc...\"],[\"```\\nexport I_MPI_HYDRA_IFACE=ens786f1\\noneccl_bindings_for_pytorch_path=$(python -c \\\"from oneccl_bind...\"],[\"```\\nmpirun -f nodefile -n 16 -ppn 4                                                         \\\\\\naccele...\"],[\"```\\npython diffusers\\u002fexamples\\u002ftextual_inversion\\u002ftextual_inversion.py                        \\\\\\n--pret...\"],[\"```\\npip install optimum[openvino]\\n```\\n\\nHere, we load the model, optimize it for a static shape, and ...\"],[\"```\\n\\nHere's a generated image. It is impressive that the model only needed five images to learn that...\"],[\"Here are some resources to help you get started:\\n\\n* Diffusers [documentation](https:\\u002f\\u002fhuggingface.co...\"],[\"--\\ntitle: \\\"Gradio-Lite: Serverless Gradio Running Entirely in Your Browser\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"Start by creating a new HTML file, if you don't have one already. Importing the JavaScript and CSS c...\"],[\"```\\n\\nNote that you should generally use the latest version of `@gradio\\u002flite` that is available. You ...\"],[\"```\\n\\n### 3. Write your Gradio app inside of the tags\\n\\nNow, write your Gradio app as you would normal...\"],[\"```\\n\\nAnd that's it! You should now be able to open your HTML page in the browser and see the Gradio ...\"],[\"```\\n\\n### Additional Requirements\\n\\nIf your Gradio app has additional requirements, it is usually poss...\"],[\"```\\n\\n**Try it out**: You can see this example running in [this Hugging Face Static Space](https:\\u002f\\u002fhu...\"],[\"## Try it out!\\n\\nYou can immediately try out `@gradio\\u002flite` by copying and pasting this code in a loc...\"],[\"```\\n\\n\\nWe've also created a playground on the Gradio website that allows you to interactively edit co...\"],[\"--\\ntitle: \\\"BERT 101 - State Of The Art NLP Model Explained\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f52_bert_101\\u002fthum...\"],[\"In this guide, you'll learn what BERT is, why it’s different, and how to get started using BERT:\\n\\n1....\"],[\"**There are many more language\\u002fNLP tasks + more detail behind each of these.**\\n\\n***Fun Fact:*** You ...\"],[\"Training on a dataset this large takes a long time. BERT’s training was made possible thanks to the ...\"],[\"**Note:** This is why you’ll often see a “Human Performance” comparison to a language model’s perfor...\"],[\"\\u003cdiv class=\\\"bg-white pb-1\\\"\\u003e...\"],[\"\\u003cdiv class=\\\"SVELTE_HYDRATER contents\\\"...\"],[\"data-props=\\\"{&quot;apiUrl&quot;:&quot;https:\\u002f\\u002fapi-inference.huggingface.co&quot;,&quot;apiToken&quot...\"],[\"ken&quot;:&quot;&quot;,&quot;model&quot;:{&quot;branch&quot;:&quot;main&quot;,&quot;cardData&quot;:{...\"],[\"&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;tags&quot;:[&quot;exbert&quot;],&quot;license&quot...\"],[\"nse&quot;:&quot;apache-2.0&quot;,&quot;datasets&quot;:[&quot;bookcorpus&quot;,&quot;wikipedia&quot;]...\"],[\"a&quot;]},&quot;cardError&quot;:{&quot;errors&quot;:[],&quot;warnings&quot;:[]},&quot;cardExists&quo...\"],[\"ists&quot;:true,&quot;config&quot;:{&quot;architectures&quot;:[&quot;BertForMaskedLM&quot;],&quot;mo...\"],[\"&quot;model_type&quot;:&quot;bert&quot;},&quot;id&quot;:&quot;bert-base-uncased&quot;,&quot;lastModi...\"],[\"lastModified&quot;:&quot;2021-05-18T16:20:13.000Z&quot;,&quot;pipeline_tag&quot;:&quot;fill-mask&quo...\"],[\"mask&quot;,&quot;library_name&quot;:&quot;transformers&quot;,&quot;mask_token&quot;:&quot;[MASK]&quo...\"],[\"ASK]&quot;,&quot;model-index&quot;:null,&quot;private&quot;:false,&quot;gated&quot;:false,&quot;pwcL...\"],[\"uot;pwcLink&quot;:{&quot;error&quot;:&quot;Unknown...\"],[\"error, can&#39;t generate link to Papers With...\"],[\"Code.&quot;},&quot;siblings&quot;:[{&quot;rfilename&quot;:&quot;.gitattributes&quot;},{&quot;rfilena...\"],[\";rfilename&quot;:&quot;README.md&quot;},{&quot;rfilename&quot;:&quot;config.json&quot;},{&quot;rfile...\"],[\"ot;rfilename&quot;:&quot;flax_model.msgpack&quot;},{&quot;rfilename&quot;:&quot;pytorch_model.bin&qu...\"],[\"l.bin&quot;},{&quot;rfilename&quot;:&quot;rust_model.ot&quot;},{&quot;rfilename&quot;:&quot;tf_model...\"],[\"tf_model.h5&quot;},{&quot;rfilename&quot;:&quot;tokenizer.json&quot;},{&quot;rfilename&quot;:&quot;t...\"],[\":&quot;tokenizer_config.json&quot;},{&quot;rfilename&quot;:&quot;vocab.txt&quot;}],&quot;tags&quot;:...\"],[\"s&quot;:[&quot;pytorch&quot;,&quot;tf&quot;,&quot;jax&quot;,&quot;rust&quot;,&quot;bert&quot;,&quot;...\"],[\";,&quot;fill-mask&quot;,&quot;en&quot;,&quot;dataset:bookcorpus&quot;,&quot;dataset:wikipedia&quot;,...\"],[\"a&quot;,&quot;arxiv:1810.04805&quot;,&quot;transformers&quot;,&quot;exbert&quot;,&quot;license:apach...\"],[\"se:apache-2.0&quot;,&quot;autonlp_compatible&quot;,&quot;infinity_compatible&quot;],&quot;tag_objs&q...\"],[\"g_objs&quot;:[{&quot;id&quot;:&quot;fill-mask&quot;,&quot;label&quot;:&quot;Fill-Mask&quot;,&quot;su...\"],[\"&quot;subType&quot;:&quot;nlp&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quot;id&quot;:&quot...\"],[\"t;:&quot;pytorch&quot;,&quot;label&quot;:&quot;PyTorch&quot;,&quot;type&quot;:&quot;library&quot;},{...\"],[\"quot;},{&quot;id&quot;:&quot;tf&quot;,&quot;label&quot;:&quot;TensorFlow&quot;,&quot;type&quot;:&quo...\"],[\"ot;:&quot;library&quot;},{&quot;id&quot;:&quot;jax&quot;,&quot;label&quot;:&quot;JAX&quot;,&quot;typ...\"],[\"quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;rust&quot;,&quot;label&quot;:&quot;Rust&q...\"],[\"t;Rust&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;transformers&quot;,&quot;la...\"],[\"&quot;label&quot;:&quot;Transformers&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&qu...\"],[\"uot;:&quot;dataset:bookcorpus&quot;,&quot;label&quot;:&quot;bookcorpus&quot;,&quot;type&quot;:&quot;...\"],[\";:&quot;dataset&quot;},{&quot;id&quot;:&quot;dataset:wikipedia&quot;,&quot;label&quot;:&quot;wikiped...\"],[\";wikipedia&quot;,&quot;type&quot;:&quot;dataset&quot;},{&quot;id&quot;:&quot;en&quot;,&quot;label&qu...\"],[\"label&quot;:&quot;en&quot;,&quot;type&quot;:&quot;language&quot;},{&quot;id&quot;:&quot;arxiv:1810.0...\"],[\"v:1810.04805&quot;,&quot;label&quot;:&quot;arxiv:1810.04805&quot;,&quot;type&quot;:&quot;arxiv&quot;...\"],[\"iv&quot;},{&quot;id&quot;:&quot;license:apache-2.0&quot;,&quot;label&quot;:&quot;apache-2.0&quot;,&q...\"],[\"quot;,&quot;type&quot;:&quot;license&quot;},{&quot;id&quot;:&quot;bert&quot;,&quot;label&quot;:&quot...\"],[\"t;:&quot;bert&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;exbert&quot;,&quot;lab...\"],[\"quot;label&quot;:&quot;exbert&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;autonl...\"],[\"t;autonlp_compatible&quot;,&quot;label&quot;:&quot;AutoNLP...\"],[\"Compatible&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;infinity_compatible&quot;...\"],[\"\\u003cdiv class=\\\"flex flex-col w-full max-w-full\\\"\\u003e\\n            \\u003cdiv class=\\\"font-semibold flex items-cente...\"],[\"\\u003cpath d=\\\"M17 22v-8h-4v2h2v6h-3v2h8v-2h-3z\\\" fill=\\\"currentColor\\\"\\u003e\\u003c\\u002fpath\\u003e\\n                        \\u003cpath...\"],[\"\\u003cpath d=\\\"M12.3625 13.85H10.1875V12.7625H12.3625V10.5875H13.45V12.7625C13.4497 13.0508 13.335 13.3272...\"],[\"\\u003cpath d=\\\"M15.625 5.14998H13.45V2.97498C13.4497 2.68665 13.335 2.4102 13.1312 2.20632C12.9273 2.00244...\"],[\"\\u003c\\u002fsvg\\u003e\\n                         \\u003cspan\\u003eFill-Mask\\u003c\\u002fspan\\u003e\\n                     \\u003c\\u002fdiv\\u003e\\n                \\u003c...\"],[\"\\u003ccode\\u003e[MASK]\\u003c\\u002fcode\\u003e\\n            \\u003c\\u002fdiv\\u003e\\n            \\u003clabel class=\\\"block \\\"\\u003e\\n                \\u003cspan clas...\"],[\"\\u003cpath d=\\\"M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z\\\" fill=\\\"currentColor\\\"\\u003e\\u003c\\u002fpath\\u003e\\n         ...\"],[\"\\u003c\\u002fsvg\\u003e\\n                Maximize\\n            \\u003c\\u002fbutton\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n\\u003c\\u002fdiv\\u003e...\"],[\"**Fun Fact:** Masking has been around a long time - [1953 Paper on Cloze procedure (or ‘Masking’)](h...\"],[\"\\u003cp class=\\\"text-center px-6\\\"\\u003eLewis Tunstall, Hugging Face ML Engineer & \\u003ca href=\\\"https:\\u002f\\u002fwww.amazon.c...\"],[\"A transformer does this by successively processing an input through a stack of transformer layers, u...\"],[\"ML Architecture Glossary:\\n\\n| ML Architecture Parts | Definition                                     ...\"],[\"Here’s how many of the above ML architecture parts BERTbase and BERTlarge has:\\n\\n\\n|           | Trans...\"],[\"#### 4.2 SWAG\\n[SWAG](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fswag) (Situations With Adversarial Generations)...\"],[\"While some of these tasks may seem irrelevant and banal, it’s important to note that these evaluatio...\"],[\"Developers can instead focus their efforts on fine-tuning BERT to customize the model’s performance ...\"],[\"Note: Hugging Face's [pipeline class](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain_classes\\u002fpipeline...\"],[\"```\\n\\n### 7.2 Try out BERT\\n\\nFeel free to swap out the sentence below for one of your own. However, le...\"],[\"```\\n\\nWhen you run the above code you should see an output that looks something like:\\n\\n```python\\n[{'s...\"],[\"```\\n\\nYou should see an output that looks something like:\\n```python\\n[{'score': 0.21981535851955414,\\n ...\"],[\"```\\n\\nBERT predicted the woman's job to be a Nurse, Waitress, Maid, Prostitute, or Cook displaying a ...\"],[\"\\u003cdiv itemscope itemprop=\\\"mainEntity\\\" itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fQuestion\\\"\\u003e\\n    \\u003ch3 itemprop=\\\"name\\\"...\"],[\"\\u003cdiv itemscope itemprop=\\\"acceptedAnswer\\\" itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fAnswer\\\"\\u003e\\n      \\u003cdiv itemprop=\\\"...\"],[\"\\u003c\\u002fdiv\\u003e\\n  \\u003c\\u002fdiv\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\u003cdiv itemscope itemprop=\\\"mainEntity\\\" itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fQuestion\\\"\\u003e\\n...\"],[\"## 9. Conclusion\\n\\nBERT is a highly complex and advanced language model that helps people automate la...\"],[\"--\\ntitle: \\\"We Raised $100 Million for Open & Collaborative Machine Learning 🚀\\\"\\nthumbnail: \\u002fblog\\u002fasse...\"],[\"Over 10,000 companies are now using Hugging Face to build technology with machine learning. Their Ma...\"],[\"It's been a hell of a ride to grow from 30 to 120+ team members in the past 12 months. We were super...\"],[\"--\\ntitle: \\\"Efficient Table Pre-training without Real Data: An Introduction to TAPEX\\\"\\nthumbnail: \\u002fblo...\"],[\"![snippet](assets\\u002f74_tapex\\u002ftapex-overview.png)\\n\\u003e Note: [Table] is a placeholder for the user provide...\"],[\"You can try the trained neural SQL executor in 🤗 Transformers as below:\\n\\n```python\\nfrom transformers...\"],[\"```\\n\\n### Fine-tuning\\n\\nDuring fine-tuning, we feed the concatenation of the natural language question...\"],[\"\\u003cdiv class=\\\"bg-white pb-1\\\"\\u003e\\u003cdiv class=\\\"SVELTE_HYDRATER contents\\\"...\"],[\"contents\\\" data-props=\\\"{&quot;apiUrl&quot;:&quot;https:\\u002f\\u002fapi-inference.huggingface.co&quot;,&quot;mod...\"],[\"error, can't generate link to Papers With Code.&quot;},&quot;tags&quot;:[&quot;pytorch&quot;,&quot;b...\"],[\"Answering&quot;,&quot;subType&quot;:&quot;nlp&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quo...\"],[\";},{&quot;id&quot;:&quot;pytorch&quot;,&quot;label&quot;:&quot;PyTorch&quot;,&quot;type&quot;:&quot;...\"],[\";:&quot;library&quot;},{&quot;id&quot;:&quot;transformers&quot;,&quot;label&quot;:&quot;Transformers...\"],[\"sformers&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;en&quot;,&quot;label&quot...\"],[\"bel&quot;:&quot;en&quot;,&quot;type&quot;:&quot;language&quot;},{&quot;id&quot;:&quot;arxiv:2107.076...\"],[\"2107.07653&quot;,&quot;label&quot;:&quot;arxiv:2107.07653&quot;,&quot;type&quot;:&quot;arxiv&quot;},...\"],[\"&quot;},{&quot;id&quot;:&quot;license:mit&quot;,&quot;label&quot;:&quot;mit&quot;,&quot;type&quot;:&...\"],[\"&quot;:&quot;license&quot;},{&quot;id&quot;:&quot;bart&quot;,&quot;label&quot;:&quot;bart&quot;,&quo...\"],[\"ot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;text2text-generation&quot;,&quot;label...\"],[\"ot;label&quot;:&quot;text2text-generation&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:...\"],[\"d&quot;:&quot;tapex&quot;,&quot;label&quot;:&quot;tapex&quot;,&quot;type&quot;:&quot;other&quot;},{&...\"],[\"uot;},{&quot;id&quot;:&quot;autotrain_compatible&quot;,&quot;label&quot;:&quot;AutoTrain...\"],[\"Compatible&quot;,&quot;type&quot;:&quot;other&quot;}],&quot;transformersInfo&quot;:{&quot;auto_model...\"],[\"Hosted inference API\\u003c\\u002fdiv\\u003e \\u003ca target=\\\"_blank\\\" href=\\\"https:\\u002f\\u002fapi-inference.huggingface.co\\u002f\\\"\\u003e\\u003csvg clas...\"],[\"14 0 0 1-14 14zm0-26a12 12 0 1 0 12 12A12 12 0 0 0 16 4z\\\" fill=\\\"currentColor\\\"\\u003e\\u003c\\u002fpath\\u003e\\u003c\\u002fsvg\\u003e\\u003c\\u002fa\\u003e\\u003c\\u002fdiv...\"],[\"1.88777 5.47272 2.00244 5.26884 2.20632C5.06496 2.4102 4.95029 2.68665 4.95 2.97498V4.60623H2.775C2....\"],[\"2.00244 16.1133 1.88777 15.825 1.88748ZM6.0375 2.97498H15.825V4.60623H6.0375V2.97498ZM15.825 8.41248...\"],[\"false...\"],[\"false\\\"\\u003e\\u003cdiv class=\\\"inline-flex justify-between w-32 lg:w-44 rounded-md border border-gray-100 px-4 p...\"],[\"class=\\\"flex h-10\\\"\\u003e\\u003cinput class=\\\"form-input-alt flex-1 rounded-r-none min-w-0 \\\" placeholder=\\\"Your sen...\"],[\"\\u003ctbody\\u003e\\u003ctr class=\\\"bg-white\\\"\\u003e\\u003ctd class=\\\"border-gray-100 border-2 h-6\\\" contenteditable=\\\"\\\"\\u003eTransformers...\"],[\"h-6\\\" contenteditable=\\\"\\\"\\u003eTokenizers\\u003c\\u002ftd\\u003e\\u003ctd class=\\\"border-gray-100 border-2 h-6\\\" contenteditable=\\\"\\\"\\u003e3...\"],[\"Add row\\u003c\\u002fbutton\\u003e \\u003cbutton class=\\\"btn-widget flex-1 lg:flex-none mt-2 lg:mr-1.5\\\" type=\\\"button\\\"\\u003e\\u003csvg cl...\"],[\"Add col\\u003c\\u002fbutton\\u003e \\u003cbutton class=\\\"btn-widget flex-1 mt-2 lg:flex-none lg:ml-auto\\\" type=\\\"button\\\"\\u003eReset ...\"],[\"### Experiments\\n\\nWe evaluate TAPEX on four benchmark datasets, including [WikiSQL (Weak)](https:\\u002f\\u002fhu...\"],[\"Experimental results demonstrate that TAPEX outperforms previous table pre-training approaches by a ...\"],[\"![comparsion](assets\\u002f74_tapex\\u002fcomparsion-tapex.png)\\n\\nWe believe the SQL execution task is closer to ...\"],[\"--\\ntitle: \\\"Ethics and Society Newsletter #4: Bias in Text-to-Image Models\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f1...\"],[\"For example, if the training data are mainly in English they probably convey rather Western values. ...\"],[\"## Sources of Bias\\n\\nRecent years have seen much important research on bias detection in AI systems w...\"],[\"**Biases in training data:** Popular multimodal datasets such as [LAION-5B](https:\\u002f\\u002flaion.ai\\u002fblog\\u002fla...\"],[\"**Biases in pre-training data filtering:** There is often some form of filtering carried out on data...\"],[\"**Biases in post-hoc filtering:** Many image generation models come with built-in safety filters tha...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n \\u003cbr\\u003e\\n \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"**Red-teaming:** ['Red-teaming'](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fred-teaming) consists of stress testing...\"],[\"**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https:\\u002f...\"],[\"Also, as we have mentioned in a [previous newsletter](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fethics-soc-2#addre...\"],[\"## Other updates\\n\\nWe are also continuing work on other fronts of ethics and society, including:\\n\\n- *...\"],[\"--\\ntitle: \\\"Active Learning with AutoNLP and Prodigy\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f43_autonlp_prodigy\\u002fthum...\"],[\"## Prodigy\\n\\n[Prodigy](https:\\u002f\\u002fprodi.gy\\u002f) is an annotation tool developed by Explosion (the makers of...\"],[\"Step 1: Download the dataset.\\n\\nStep 2: Open [AutoNLP](https:\\u002f\\u002fui.autonlp.huggingface.co\\u002f) and create...\"],[\"Once you have Prodigy installed, you can simply run:\\n\\n    $ prodigy ner.manual bbc blank:en BBC_News...\"],[\"dataset = []\\n\\nfor doc, eg in nlp.pipe(examples, as_tuples=True):\\n    try:\\n        doc.ents = [doc.ch...\"],[\"```\\n\\nThis will provide us with a `JSONL` file which can be used for training a model using AutoNLP. ...\"],[\"Let's take a look at how this model performs on the same unseen sample.\\n\\n\\u003cimg src=\\\"assets\\u002f43_autonlp...\"],[\"We have open-sourced the best model created using this process. You can try it [here](https:\\u002f\\u002fhuggin...\"],[\"--\\ntitle: \\\"AI Policy @🤗: Open ML Considerations in the EU AI Act\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002feu_ai_act_...\"],[\"Hugging Face is where it is today thanks to its community of developers, so we’ve seen firsthand wha...\"],[\"--\\ntitle: \\\"Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\\\"\\nthu...\"],[\"In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published...\"],[\"[Optimum Intel](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-intel) is part of Optimum and builds on top o...\"],[\"As usual, the first step is to install all required libraries. It’s worth mentioning that we have to...\"],[\"```\\npip -q uninstall torch -y \\npip -q install torch==1.11.0+cpu --extra-index-url https:\\u002f\\u002fdownload.p...\"],[\"```\\n\\nWe then set up the quantization job using a [configuration]. You can find details on this confi...\"],[\"```\\n\\nThe log tells us that Optimum Intel has quantized 38 ```Linear``` and 2 ```Embedding``` operato...\"],[\"```\\n# Original model\\n\\nTransformerBlock(\\n  (attention): MultiHeadSelfAttention(\\n    (dropout): Dropou...\"],[\"```\\n\\n```\\n# Quantized model\\n\\nTransformerBlock(\\n  (attention): MultiHeadSelfAttention(\\n    (dropout): ...\"],[\"```\\n\\nVery well, but how does this impact accuracy and prediction time?\\n\\nBefore and after each quanti...\"],[\"```\\n\\nYou can find the resulting [model](https:\\u002f\\u002fhuggingface.co\\u002fjuliensimon\\u002fdistilbert-amazon-shoe-re...\"],[\"--\\ntitle: \\\"CO2 Emissions and the 🤗 Hub: Leading the Charge\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f60_carbon_emissi...\"],[\"```\\npip install huggingface_hub -U\\n```\\n\\n## How to find low-emission models using the Hugging Face Hu...\"],[\"```\\n\\nThat's a lot of CO2!\\n\\nAs you can see, in just a few lines of code we can quickly vet models we ...\"],[\"```\\n\\n...you'll be left with a file within the `codecarbon-text-classification` directory called `emi...\"],[\"--\\ntitle: \\\"What's going on with the Open LLM Leaderboard?\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fevaluating-mmlu-l...\"],[\"Along this journey with us you’ll learn a lot about the ways you can evaluate a model on a single ev...\"],[\"Why is that the case?\\n\\n## 1001 flavors of MMLU\\n\\nWell it turns out that the LLaMA team adapted anothe...\"],[\"(Note that the Harness implementation has been recently updated - more in this at the end of our pos...\"],[\"```\\nQuestion: Glucose is transported into the muscle cell:\\n\\n\\nChoices:\\nA. via protein transporters ca...\"],[\"```\\n\\nNote: you can very easily explore more of this dataset [in the dataset viewer](https:\\u002f\\u002fhuggingf...\"],[\"## MMLU comes in all shapes and sizes: Looking at the prompts\\n\\nLet’s compare an example of prompt ea...\"],[\"\\u003cdiv\\u003e\\n\\u003ctable\\u003e\\u003cp\\u003e\\n  \\u003ctbody\\u003e\\n \\u003ctr style=\\\"text-align: left;\\\"\\u003e\\n  \\u003ctd\\u003eOriginal implementation \\u003ca href=\\\"ht...\"],[\"A. It damaged support for the US model of political economy and capitalism \\u003cbr\\u003e\\nB. It created anger ...\"],[\"The differences between them can seem small, did you spot them all? Here they are:\\n- First sentence,...\"],[\"![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fevaluating...\"],[\"![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fevaluating...\"],[\"Here is a table summary of the answers provided and generated by the model to summarize what we’ve s...\"],[\"We’ve covered them all!\\n\\nNow let’s compare the model scores on these three possible ways to evaluate...\"],[\"We can see that for the same dataset, both absolute scores and model rankings (see the first figure)...\"],[\"This is why open, standardized, and reproducible benchmarks such as the [EleutherAI Eval Harness](ht...\"],[\"## Reproducibility hashes:\\nHere are the commit hashes of the various code implementations used in th...\"],[\"--\\ntitle: \\\"The Technology Behind BLOOM Training\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f86_bloom_megatron_deepspeed...\"],[\"There are 6 main groups of people to thank:\\n\\n1. The HuggingFace's BigScience team who dedicated more...\"],[\"BLOOM's architecture is very similar to [GPT3](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fGPT-3) with a few added...\"],[\"The training of the 176B BLOOM model occurred over Mar-Jul 2022 and took about 3.5 months to complet...\"],[\"Please note that both Megatron-LM and DeepSpeed have Pipeline Parallelism and BF16 Optimizer impleme...\"],[\"## Data Parallelism\\n\\nMost users with just a few GPUs are likely to be familiar with `DistributedData...\"],[\"## Tensor Parallelism\\n\\nIn Tensor Parallelism (TP) each GPU processes only a slice of a tensor and on...\"],[\"Using this principle, we can update an MLP of arbitrary depth, while synchronizing the GPUs after ea...\"],[\"## Pipeline Parallelism\\n\\nNaive Pipeline Parallelism (naive PP) is where one spreads groups of model ...\"],[\"```\\nwe just sliced it in 2 vertically, placing layers 0-3 onto GPU0 and 4-7 to GPU1.\\n\\nNow while data...\"],[\"The following illustration from the [GPipe paper](https:\\u002f\\u002fai.googleblog.com\\u002f2019\\u002f03\\u002fintroducing-gpip...\"],[\"Note that conceptually this is the same concept as gradient accumulation steps (GAS). PyTorch uses `...\"],[\"While both Megatron-LM and DeepSpeed have their own implementation of the PP protocol, Megatron-Deep...\"],[\"Since each dimension requires at least 2 GPUs, here you'd need at least 4 GPUs.\\n\\n## DP+PP+TP\\n\\nTo get...\"],[\"In addition, there are already fewer layers than normal due to PP and so the memory savings won't be...\"],[\"So back in January as we knew we would be training on A100s which support the BF16 format Olatunji R...\"],[\"All PyTorch components have been updated to ensure that they perform any accumulation in FP32, so no...\"],[\"Now, when instructing the GPU to compute `c = torch.add(a, b); e = torch.max([c,d])`, a naive approa...\"],[\"## Datasets\\n\\nAnother important feature from Megatron-LM is the efficient data loader. During start u...\"],[\"## Training Difficulties\\n\\nWith the architecture, hardware and software in place we were able to star...\"],[\"One other issue was that SLURM wasn't designed to be used by a team of people. A SLURM job is owned ...\"],[\"Training large language models is still a challenging task, but we hope by building and sharing this...\"],[\"DeepSpeed:\\n\\n- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https:\\u002f\\u002farxiv.o...\"],[\"## Blog credits\\n\\nHuge thanks to the following kind folks who asked good questions and helped improve...\"],[\"--\\ntitle: \\\"Happy 1st anniversary 🤗 Diffusers!\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fdiffusers-turns-1\\u002fdiffusers-...\"],[\"**Table of Contents**\\n\\n* [Striving for photorealism](#striving-for-photorealism)\\n* [Video pipelines]...\"],[\"Head over to the DeepFloyd IF [docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002fv0.18.2\\u002fen\\u002fapi\\u002fpipelines\\u002f...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocum...\"],[\"## Image editing pipelines\\n\\nImage editing is one of the most practical use cases in fashion, materia...\"],[\"In addition to that, we also support specific hardware and formats like ONNX, the `mps` PyTorch devi...\"],[\"## Support for LoRA\\n\\nFine-tuning diffusion models is expensive and out of reach for most consumer GP...\"],[\"## Community highlights\\n\\nOne of the most gratifying experiences of the past year has been seeing how...\"],[\"\\u003cdiv class=\\\"mx-auto max-w-screen-xl py-8\\\"\\u003e\\n  \\u003cdiv class=\\\"mb-8 sm:break-inside-avoid\\\"\\u003e\\n    \\u003cblockquot...\"],[\"\\u003cblockquote class=\\\"rounded-xl !mb-0 bg-gray-50 p-6 shadow dark:bg-gray-800\\\"\\u003e\\n      \\u003cp class=\\\"leading...\"],[\"\\u003c\\u002fblockquote\\u003e\\n    \\u003cdiv class=\\\"flex items-center gap-4\\\"\\u003e\\n      \\u003cimg src=\\\"https:\\u002f\\u002favatars.githubuserco...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n  \\u003c\\u002fdiv\\u003e\\n  \\u003cdiv class=\\\"mb-8 sm:break-inside-avoid\\\"\\u003e\\n    \\u003cblockquote class=\\\"rounded...\"],[\"\\u003cdiv class=\\\"text-sm\\\"\\u003e\\n        \\u003cp class=\\\"font-medium\\\"\\u003eQing\\u003c\\u002fp\\u003e\\n      \\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n  \\u003c\\u002fdiv\\u003e\\n  \\u003cdi...\"],[\"\\u003c\\u002fblockquote\\u003e\\n    \\u003cdiv class=\\\"flex items-center gap-4\\\"\\u003e\\n      \\u003cimg src=\\\"https:\\u002f\\u002favatars.githubuserco...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n  \\u003c\\u002fdiv\\u003e\\n  \\u003cdiv class=\\\"mb-8 sm:break-inside-avoid\\\"\\u003e\\n    \\u003cblockquote class=\\\"rounded...\"],[\"We also collaborated with Google Cloud (who generously provided the compute) to provide technical gu...\"],[\"Finally, we were delighted to receive contributions to our codebase from over 300 contributors, whic...\"],[\"Besides these, a heartfelt shoutout to the following contributors who helped us ship some of the mos...\"],[\"## Building products with 🤗 Diffusers\\n\\nOver the last year, we also saw many companies choosing to bu...\"],[\"## Looking forward\\n\\nAs we celebrate our first anniversary, we're grateful to our community and open-...\"],[\"--\\ntitle: \\\"Getting Started with Hugging Face Transformers for IPUs with Optimum\\\"\\nthumbnail: \\u002fblog\\u002fas...\"],[\"### Getting started with IPUs and Optimum\\n\\nLet’s use BERT as an example to help you get started with...\"],[\"```\\n$ cd \\u002fopt\\u002fgc\\u002fpoplar_sdk-ubuntu_18_04-2.3.0+774-b47c577c2a\\u002f\\n$ source poplar-ubuntu_18_04-2.3.0+77...\"],[\"```\\n(poptorch_env) user@host:~\\u002fworkspace\\u002fpoptorch_env$ pip3 install optimum[graphcore] optuna\\n```\\n\\n#...\"],[\"```\\n$ python3 run_qa.py \\\\\\n\\t--ipu_config_name=.\\u002f \\\\\\n\\t--model_name_or_path bert-base-uncased \\\\\\n\\t--datas...\"],[\"```\\n\\n### A closer look at Optimum-Graphcore\\n \\n#### Getting the data\\n \\nA very simple way to get datas...\"],[\"```\\n\\nThe argument ```--model_name_or_path==bert-base-uncased`` loads the [bert-base-uncased](https:\\u002f...\"],[\"```\\n  \\nYou can see the rest of the IPU BERT implementation in the [Optimum-Graphcore: SQuAD Examples...\"],[\"--\\ntitle: Deprecation of Git Authentication using password\\nthumbnail: \\u002fblog\\u002fassets\\u002fpassword-git-depr...\"],[\"```\\nwhere `\\u003crepo_path\\u003e` is in the form of:\\n- `\\u003cuser_name\\u003e\\u002f\\u003crepo_name\\u003e` for models\\n- `datasets\\u002f\\u003cuser_...\"],[\"--\\ntitle: \\\"Finetune Stable Diffusion Models with DDPO via TRL\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f166_trl_ddpo...\"],[\"## The Advantages of DDPO\\n\\nDDPO is not the only working answer to the question of how to attempt to ...\"],[\"The two orders of approximation have a significant impact on both performance and the ability to han...\"],[\"Here’s a diagram to summarize the flow:\\n\\n![dppo rl schematic](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggin...\"],[\"We keep these steps in mind while moving on to actually getting these running which is described in ...\"],[\"```\\n\\nThis should get the main library installed. The following dependencies are for tracking and ima...\"],[\"```\\n\\nThe following table contains key hyperparameters that are directly correlated with positive res...\"],[\"## Lessons learned\\n\\n1. The results seem to generalize over a wide variety of prompts despite the min...\"],[\"The following are pre-finetuned (left) and post-finetuned (right) outputs for the prompts `bear`, `h...\"],[\"## Limitations\\n\\n1. Right now `trl`'s DDPOTrainer is limited to finetuning vanilla SD models;\\n2. In o...\"],[\"--\\ntitle: \\\"Fine-Tune Whisper For Multilingual ASR with 🤗 Transformers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f111_...\"],[\"## Introduction\\n\\nWhisper is a pre-trained model for automatic speech recognition (ASR) \\npublished in...\"],[\"When scaled to 680,000 hours of labelled pre-training data, Whisper models \\ndemonstrate a strong abi...\"],[\"\\u003cfigure\\u003e\\n\\u003cimg src=\\\"assets\\u002f111_fine_tune_whisper\\u002fwhisper_architecture.svg\\\" alt=\\\"Trulli\\\" style=\\\"width:...\"],[\"The Whisper checkpoints come in five configurations of varying model sizes.\\nThe smallest four are tr...\"],[\"| Size   | Layers | Width | Heads | Parameters | English-only                                       ...\"],[\"For demonstration purposes, we'll fine-tune the multilingual version of the \\n[`small`](https:\\u002f\\u002fhuggi...\"],[\"```\\n\\nWe strongly advise you to upload model checkpoints directly the [Hugging Face Hub](https:\\u002f\\u002fhugg...\"],[\"```\\n\\n### Load Dataset\\n\\nCommon Voice is a series of crowd-sourced datasets where speakers \\nrecord tex...\"],[\"\\u003cfigure\\u003e\\n\\u003cimg src=\\\"assets\\u002f111_fine_tune_whisper\\u002fselect_hi.jpg\\\" alt=\\\"Trulli\\\" style=\\\"width:100%\\\"\\u003e\\n\\u003c\\u002ffi...\"],[\"```\\n\\n**Print Output:**\\n```\\nDatasetDict({\\n    train: Dataset({\\n        features: ['client_id', 'path'...\"],[\"```\\n\\nCommon Voice is but one multilingual ASR dataset that we can download from the Hub - \\nthere are...\"],[\"It is crucial that we match the sampling rate of our audio inputs to the sampling\\nrate expected by o...\"],[\"The Mel channels (frequency bins) are standard in speech processing and chosen to approximate\\nthe hu...\"],[\"```\\n\\n### Load WhisperTokenizer\\n\\nNow let's look at how to load a Whisper tokenizer. The Whisper model...\"],[\"```\\n\\n\\u003e **Tip:** the blog post can be adapted for *speech translation* by setting the task to `\\\"trans...\"],[\"```\\n**Print Output:**\\n```bash\\nInput:                 खीर की मिठास पर गरमाई बिहार की सियासत, कुशवाहा ...\"],[\"```\\n\\n### Prepare Data\\nLet's print the first example of the Common Voice dataset to see \\nwhat form th...\"],[\"```\\nWe can see that we've got a 1-dimensional input audio array and the \\ncorresponding target transc...\"],[\"```\\n\\nRe-loading the first audio sample in the Common Voice dataset will resample \\nit to the desired ...\"],[\"```\\nGreat! We can see that the sampling rate has been downsampled to 16kHz. The \\narray values are al...\"],[\"```\\n\\nAlright! With that we have our data fully prepared for training! \\nLet's continue and take a loo...\"],[\"Once we've fine-tuned the model, we will evaluate it on the test data to verify that we have correct...\"],[\"from dataclasses import dataclass\\nfrom typing import Any, Dict, List, Union\\n\\n@dataclass\\nclass DataCo...\"],[\"```\\n\\nLet's initialise the data collator we've just defined:\\n\\n```python\\ndata_collator = DataCollatorS...\"],[\"```\\n\\n### Load a Pre-Trained Checkpoint\\n\\nNow let's load the pre-trained Whisper `small` checkpoint. A...\"],[\"```\\n\\n### Define the Training Arguments\\nIn the final step, we define all the parameters related to tr...\"],[\"```python\\nfrom transformers import Seq2SeqTrainingArguments\\n\\ntraining_args = Seq2SeqTrainingArgument...\"],[\"```\\n\\n**Note**: if one does not want to upload the model checkpoints to the Hub, \\nset `push_to_hub=Fa...\"],[\"```\\n\\nTraining will take approximately 5-10 hours depending on your GPU or the one \\nallocated to the ...\"],[\"Our fine-tuned model significantly improves upon the zero-shot performance of the Whisper \\n`small` c...\"],[\"```\\n\\nThe training results can now be uploaded to the Hub. To do so, execute the `push_to_hub` comman...\"],[\"```\\n\\nWhile the fine-tuned model yields satisfactory results on the Common \\nVoice Hindi test data, it...\"],[\"```\\n\\n## Closing Remarks\\n\\nIn this blog, we covered a step-by-step guide on fine-tuning Whisper for mu...\"],[\"--\\ntitle: \\\"Let's talk about biases in machine learning! Ethics and Society Newsletter #2\\\" \\nthumbnail...\"],[\"**\\u003cspan style=\\\"text-decoration:underline;\\\"\\u003eTable of contents:\\u003c\\u002fspan\\u003e**\\n* **\\u003cspan style=\\\"text-decorat...\"],[\"1. **lock in** behaviors in time and hinder social progress [from being reflected in technology](htt...\"],[\"**These issues are deeply personal** for many of us ML researchers and developers at Hugging Face an...\"],[\"While our own experiences do not come close to covering the myriad ways in which ML-mediated discrim...\"],[\"This may not come as much of a surprise given the ML research community’s [focus on the value of “ge...\"],[\"1. \\u003cspan style=\\\"text-decoration:underline;\\\"\\u003eThe model is integrated into a website creation service\\u003c...\"],[\"* In this case, the machine biases directly cause discrimination by systematically directing police ...\"],[\"So, who’s on the hook for machine biases in ML? These three cases illustrate one of the reasons why ...\"],[\"## Addressing Bias throughout the ML Development Cycle\\n\\nReady for some practical advice yet? Here we...\"],[\"For example, let’s go back to one of the first highly-publicized cases of a Machine Learning system ...\"],[\"So what does this have to do with bias? Doesn’t showing people content that they’re likely to enjoy ...\"],[\"This example serves to illustrate that the impact of machine biases in an ML-supported product depen...\"],[\"#### Task definition: recommendations\\n\\nThere are as many ways for the ML task definition and deploym...\"],[\"You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it come...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n \\u003cbr\\u003e\\n \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n \\u003cbr\\u003e\\n \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n \\u003cbr\\u003e\\n \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"### I am \\u003cspan style=\\\"text-decoration:underline;\\\"\\u003etraining\\u002fselecting a model\\u003c\\u002fspan\\u003e for my ML system...\"],[\"Documentation is a great first step for sharing general insights about a model’s behavior, but it is...\"],[\"Visualization of model outputs isn’t just for generative models though! For classification models, w...\"],[\"Finally, a few benchmarks exist that can measure bias-related phenomena in models. For language mode...\"],[\"Even with access to a benchmark for the models you are considering, you might find that running eval...\"],[\"#### Model selection\\u002fdevelopment: recommendations\\n\\nFor models just as for datasets, different tools ...\"],[\"Summary of linked tools:\\n* Tasks:\\n    * Explore our directory of [ML Tasks](https:\\u002f\\u002fhuggingface.co\\u002ft...\"],[\"* Look at [systematic model errors](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fnazneen\\u002fseal) and look out for kno...\"],[\"Thanks for reading! 🤗\\n\\n~ Yacine, on behalf of the Ethics and Society regulars\\n    \\nIf you want to ci...\"],[\"```\\n@inproceedings{hf_ethics_soc_blog_2,\\n  author    = {Yacine Jernite and\\n               Alexandra ...\"],[\"--\\ntitle: 'Distributed Training: Train BART\\u002fT5 for Summarization using 🤗 Transformers and Amazon Sag...\"],[\"listed again here:\\n\\n- [🤗 Transformers Documentation: Amazon SageMaker](https:\\u002f\\u002fhuggingface.co\\u002ftransf...\"],[\"As [distributed training strategy](https:\\u002f\\u002fhuggingface.co\\u002ftransformers\\u002fsagemaker.html#distributed-tr...\"],[\"```\\n\\nIn this tutorial, we will use an Amazon SageMaker Notebook Instance for running our training jo...\"],[\"```\\n\\n---\\n\\n## Set up a development environment and install sagemaker\\n\\nAfter our SageMaker Notebook In...\"],[\"```\\n\\n---\\n\\n# Choose 🤗 Transformers `examples\\u002f` script\\n\\nThe [🤗 Transformers repository](https:\\u002f\\u002fgithub...\"],[\"```\\n\\n---\\n\\n## Configure distributed training and hyperparameters\\n\\nNext, we will define our `hyperpara...\"],[\"```\\n\\nSince, we are using [SageMaker Data Parallelism](https:\\u002f\\u002faws.amazon.com\\u002fblogs\\u002faws\\u002fmanaged-data-...\"],[\"```\\n```bash\\n2021-04-01 13:00:35 Starting - Starting the training job...\\n2021-04-01 13:01:03 Starting...\"],[\"```\\n\\nThe training seconds are 2882 because they are multiplied by the number of instances. If we cal...\"],[\"```\\n\\nBefore we are going to upload our model to [huggingface.co](http:\\u002f\\u002fhuggingface.co) we need to c...\"],[\"```\\n\\nAfter we extract all the metrics we want to include we are going to create our `README.md`. Add...\"],[\"## `{model_name}`\\n\\nThis model was trained using Amazon SageMaker and the new Hugging Face Deep Learn...\"],[\"## Results\\n\\n| key | value |\\n| --- | ----- |\\n{eval_table}\\n{test_table}\\n\\n\\n\\n\\\"\\\"\\\"\\n\\n# Generate model card ...\"],[\"```\\n\\nAfter we have our unzipped model and model card located in `my_bart_model` we can use the eithe...\"],[\"```\\n\\nAnd use the \\\"Hosted Inference API\\\" widget to test it. \\n\\n[https:\\u002f\\u002fhuggingface.co\\u002fphilschmid\\u002fbart...\"],[\"--\\ntitle: \\\"SetFitABSA: Few-Shot Aspect Based Sentiment Analysis using SetFit\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"Compared to LLM based methods, SetFitABSA has two unique advantages:\\n\\n\\u003cp\\u003e🗣 \\u003cstrong\\u003eNo prompts needed...\"],[\"### Training\\n\\n**1. Aspect candidate extraction**\\n\\nIn this work we assume that aspects, which are usu...\"],[\"* **Training sentence:** \\\"Waiters aren't friendly but the cream pasta is out of this world.\\\"\\n* **Tok...\"],[\"```\\naspect_candidate:training_sentence...\"],[\"```\\n\\nApplying the template to the example above will generate 3 training instances – two with `True`...\"],[\"| Text                                                                          | Label |\\n|:--------...\"],[\"```\\n\\\"their dinner specials are fantastic.\\\"\\n```\\n\\n**Model Output:**\\n\\n```\\n[{'span': 'dinner specials', ...\"],[\"```\\n\\n## Benchmarking\\n\\nSetFitABSA was benchmarked against the recent state-of-the-art work by [AWS AI...\"],[\"**SetFitABSA vs GPT2**\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface...\"],[\"```\\nAdditionally, we must install the `en_core_web_lg` spaCy model:\\n```shell\\npython -m spacy downloa...\"],[\"```\\n\\nWe continue by preparing the training set. The format of the training set is a `Dataset` with t...\"],[\"```python\\nfrom datasets import load_dataset\\nfrom setfit import AbsaTrainer, AbsaModel\\n\\n# Create a tr...\"],[\"```\\n\\nThat's it! We have trained a domain-specific ABSA model. We can save our trained model to disk ...\"],[\"```\\n\\nFor more details on training options, saving and loading models, and inference see the SetFit [...\"],[\"--\\ntitle: \\\"Boosting Wav2Vec2 with n-grams in 🤗 Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f44_boost_wav2ve...\"],[\"Using Connectionist Temporal Classification (CTC), pre-trained\\nWav2Vec2-like checkpoints are extreme...\"],[\"Until recently, the 🤗 Transformers library did not offer a simple user\\ninterface to decode audio fil...\"],[\"First, we install `datasets` and `transformers`.\\n\\n```bash\\npip install datasets transformers...\"],[\"```\\n\\nLet's load a small excerpt of the [Librispeech\\ndataset](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrisp...\"],[\"```\\n\\nNext, we process the data\\n\\n```python\\ninputs = processor(audio_sample[\\\"audio\\\"][\\\"array\\\"], samplin...\"],[\"```\\n\\nFor demonstration purposes, we have prepared a new model repository\\n[patrickvonplaten\\u002fwav2vec2-...\"],[\"```\\n\\nIntuitively, one can understand the decoding process of\\n`Wav2Vec2ProcessorWithLM` as applying b...\"],[\"```\\n\\nCool! Recalling the words `facebook\\u002fwav2vec2-base-100h` without a\\nlanguage model transcribed in...\"],[\"For more information on how you can tweak different parameters when\\ndecoding with `Wav2Vec2Processor...\"],[\"As always a language model is only as good as the data it is trained on.\\nIn the case of speech recog...\"],[\"-   are generated from crawling the web, which might not be very\\n    clean and correspond well to sp...\"],[\"```\\n\\nLet's download the data.\\n\\n```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"...\"],[\"```\\n\\n**Output:**\\n```bash\\n    Login successful\\n    Your token has been saved to \\u002froot\\u002f.huggingface\\u002fto...\"],[\"```\\n\\nThat was easy! The dataset viewer is automatically enabled when\\nuploading a new dataset, which ...\"],[\"*E.g.*, for the large Wav2Vec2 checkpoint that was fine-tuned on 10min only, an *n-gram* reduces the...\"],[\"```\\n\\nbefore downloading and unpacking the KenLM repo.\\n\\n```bash\\nwget -O - https:\\u002f\\u002fkheafield.com\\u002fcode\\u002f...\"],[\"```\\n\\nNow, we just have to run KenLM's `lmplz` command to build our *n-gram*,\\ncalled `\\\"5gram.arpa\\\"`. ...\"],[\"**Output:**\\n```bash\\n    === 1\\u002f5 Counting and sorting n-grams ===\\n    Reading \\u002fcontent\\u002fswedish_text.t...\"],[\"Unigram tokens 42153890 types 360209\\n    === 2\\u002f5 Calculating and sorting adjusted counts ===\\n    Cha...\"],[\"2 5476741 D1=0.761523 D2=1.06735 D3+=1.32559\\n    3 18177681 D1=0.839918 D2=1.12061 D3+=1.33794\\n    4...\"],[\"=== 4\\u002f5 Calculating and writing order-interpolated probabilities ===\\n    Chain sizes: 1:4322496 2:87...\"],[\"Name:lmplz\\tVmPeak:14181536 kB\\tVmRSS:2199260 kB\\tRSSMax:4160328 kB\\tuser:120.598\\tsys:26.6659\\tCPU:147.26...\"],[\"```\\n\\nGreat, we have built a *5-gram* LM! Let's inspect the first couple of\\nlines.\\n\\n```bash\\nhead -20 ...\"],[\"```\\n\\nThere is a small problem that 🤗 Transformers will not be happy about\\nlater on. The *5-gram* cor...\"],[\"```\\n\\nLet's now inspect the corrected *5-gram*.\\n\\n```bash\\nhead -20 5gram_correct.arpa\\n```\\n\\n**Output:**...\"],[\"```\\n\\nGreat, this looks better! We're done at this point and all that is left\\nto do is to correctly i...\"],[\"```\\n\\n**Output:**\\n```bash\\n    Found entries of length \\u003e 1 in alphabet. This is unusual unless style i...\"],[\"```\\n\\n**Output:**\\n```bash\\n    Cloning https:\\u002f\\u002fhuggingface.co\\u002fhf-test\\u002fxls-r-300m-sv into local empty d...\"],[\"**Output:**\\n```bash\\n    xls-r-300m-sv\\u002f\\n    ├── [  23]  added_tokens.json\\n    ├── [ 401]  all_results...\"],[\"├── [ 279]  tokenizer_config.json\\n    ├── [ 29K]  trainer_state.json\\n    ├── [2.9K]  training_args.b...\"],[\"9 directories, 34 files...\"],[\"```\\n\\nAs can be seen the *5-gram* LM is quite large - it amounts to more than\\n4 GB. To reduce the siz...\"],[\"**Output:**\\n```bash\\n    xls-r-300m-sv\\u002f\\n    ├── [  23]  added_tokens.json\\n    ├── [ 401]  all_results...\"],[\"```\\n\\nNice, we reduced the *n-gram* by more than half to less than 2GB now. In\\nthe final step, let's ...\"],[\"--\\ntitle: \\\"Hugging Face Machine Learning Demos on arXiv\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002farxiv\\u002fthumbnail.pn...\"],[\"![An interactive demo of a protein structure model, available on Hugging Face Spaces](\\u002fblog\\u002fassets\\u002fa...\"],[\"--\\ntitle: \\\"Illustrating Reinforcement Learning from Human Feedback (RLHF)\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"Writing a loss function to capture these attributes seems intractable and most language models are s...\"],[\"1. Pretraining a language model (LM),\\n2. gathering data and training a reward model, and\\n3. fine-tun...\"],[\"Next, with a language model, one needs to generate data to train a **reward model**, which is how hu...\"],[\"Human annotators are used to rank the generated text outputs from the LM. One may initially think th...\"],[\"### Fine-tuning with RL\\n\\nTraining a language model with reinforcement learning was, for a long time,...\"],[\"Let's first formulate this fine-tuning task as a RL problem. First, the **policy** is a language mod...\"],[\"The reward function is where the system combines all of the models we have discussed into one RLHF p...\"],[\"Finally, the **update rule** is the parameter update from PPO that maximizes the reward metrics in t...\"],[\"# Open-source tools for RLHF\\n\\nThe first [code](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002flm-human-preferences) relea...\"],[\"[RL4LMs](https:\\u002f\\u002fgithub.com\\u002fallenai\\u002fRL4LMs) offers building blocks for fine-tuning and evaluating LL...\"],[\"Generating well-written human text answering specific prompts is very costly, as it often requires h...\"],[\"With these limitations, huge swaths of unexplored design options could still enable RLHF to take sub...\"],[\"### Further reading\\n\\nHere is a list of the most prevalent papers on RLHF to date. The field was rece...\"],[\"And here is a snapshot of the growing set of \\\"key\\\" papers that show RLHF's performance for LMs:\\n- [F...\"],[\"- Sparrow: [Improving alignment of dialogue agents via targeted human judgements](https:\\u002f\\u002farxiv.org\\u002f...\"],[\"- [Llama 2](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2307.09288) (Touvron et al. 2023): Impactful open-access model wit...\"],[\"The field is the convergence of multiple fields, so you can also find resources in other areas:\\n* Co...\"],[\"```\\nLambert, et al., \\\"Illustrating Reinforcement Learning from Human Feedback (RLHF)\\\", Hugging Face ...\"],[\"--\\ntitle: 🧨 Stable Diffusion  in JAX \\u002f Flax !\\nthumbnail: \\u002fblog\\u002fassets\\u002f108_stable_diffusion_jax\\u002fthumb...\"],[\"Note that JAX is not exclusive to TPUs, but it shines on that hardware because each TPU server has 8...\"],[\"```\\n\\n*Output*:\\n```bash \\n    Found 8 JAX devices of type TPU v2.\\n```\\n\\n\\n\\nMake sure `diffusers` is inst...\"],[\"```\\n\\n\\n## Model Loading\\n\\nBefore using the model, you need to accept the model [license](https:\\u002f\\u002fhuggi...\"],[\"```\\n\\n\\nTPU devices support `bfloat16`, an efficient half-float type. We'll use it for our tests, but ...\"],[\"```\\n\\n\\n``` python\\nprompt_ids = shard(prompt_ids)\\nprompt_ids.shape\\n```\\n\\n*Output*:\\n```bash \\n    (8, 1, ...\"],[\"```\\n\\n\\nJAX code can be compiled to an efficient representation that runs very fast. However, we need ...\"],[\"```\\n\\n\\n``` python\\nimage_grid(images, 2, 4)\\n```\\n\\n![png](assets\\u002f108_stable_diffusion_jax\\u002fjax_stable_dif...\"],[\"```\\n\\n![png](assets\\u002f108_stable_diffusion_jax\\u002fjax_stable_diffusion_2.png)\\n\\n\\n--------------------------...\"],[\"```\\n\\n\\nAfter we use `pmap`, the prepared function `p_generate` will conceptually do the following:\\n\\n-...\"],[\"--\\ntitle: 'Pre-Train BERT with Hugging Face Transformers and Habana Gaudi'\\nthumbnail: \\u002fblog\\u002fassets\\u002f9...\"],[\"_Note: Steps 1 to 3 can\\u002fshould be run on a different instance size since those are CPU intensive tas...\"],[\"## What is BERT?\\n\\nBERT, short for Bidirectional Encoder Representations from Transformers, is a Mach...\"],[\"```\\nRead more about Masked Language Modeling [here](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fbert-101).\\n\\n---\\n\\nLet...\"],[\"```\\n\\n\\nThe [original BERT](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1810.04805) was pretrained on [Wikipedia](https:\\u002f\\u002fhu...\"],[\"```\\n_We are not going to do some advanced dataset preparation, like de-duplication, filtering or any...\"],[\"```\\n\\nWe push the tokenizer to the [Hugging Face Hub](https:\\u002f\\u002fhuggingface.co\\u002fmodels) for later traini...\"],[\"```\\n\\nAs data processing function we will concatenate all texts from our dataset and generate chunks ...\"],[\"```\\n\\n## 4. Pre-train BERT on Habana Gaudi\\n\\nIn this example, we are going to use Habana Gaudi on AWS ...\"],[\"```\\n\\nWhen using GPUs you would use the [Trainer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fv4.19.4\\u002fen...\"],[\"```\\n\\nThe `DL1` instance we use has 8 available HPU-cores meaning we can leverage distributed data-pa...\"],[\"```python\\nfrom huggingface_hub import HfFolder\\n\\n# hyperparameters\\nhyperparameters = {\\n    \\\"model_con...\"],[\"```\\n\\nWe can start our training by creating a `EC2RemoteRunner` and then `launch` it. This will then ...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f99_pretraining_bert\\u002ftens...\"],[\"To compare the cost we can use the [p3dn.24xlarge](https:\\u002f\\u002faws.amazon.com\\u002fde\\u002fec2\\u002finstance-types\\u002fp3\\u002f)...\"],[\"Those results are incredible since it will allow companies to adapt their pre-trained models to thei...\"],[\"--\\ntitle: \\\"Machine Learning Experts - Sasha Luccioni\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f69_sasha_luccioni_inte...\"],[\"Very excited to introduce this brilliant episode to you! Here’s my conversation with Sasha Luccioni:...\"],[\"### Love that you wanted to something that felt meaningful!\\n\\n**Sasha:** Yeah, when I hear people on ...\"],[\"Also about the data, I'm involved in a lot of the data working groups at Big Science, and it's reall...\"],[\"### Can you speak a little bit more about the environmental impact of AI?\\n\\n**Sasha:** Yeah, it's a t...\"],[\"So first we just [created this online calculator](https:\\u002f\\u002fmlco2.github.io\\u002fimpact\\u002f) where someone cou...\"],[\"For example; France is mostly nuclear, mostly energy, and Canada has a lot of hydroelectric energy. ...\"],[\"### What are some of the ways that machine learning teams and engineers could be a bit more proactiv...\"],[\"**Sasha:** Yeah, we wrote a paper a couple of years ago that was a cool experience. It's almost a hu...\"],[\"Then instead of powering up a diesel generator which is cool because you can just power them up, and...\"],[\"### For people listening that are interested in this effort, but perhaps work at an organization whe...\"],[\"**Sasha:** Actually, machine learning people or AI people, in general, have this stereotype from oth...\"],[\"And I've participated in organizing workshops where people submit ideas that are super great on pape...\"],[\"### So sad. That's such a great story though and how there are opportunities like that.\\n\\n**Sasha:** ...\"],[\"**Sasha:** Yeah, there's this concept that my mom read about in some magazine ages ago when I was a ...\"],[\"For example, what I feel like we're doing at Hugging Face is really that machine learning needs more...\"],[\"### What other examples or applications do you find and see potential meaning in AI machine learning...\"],[\"### And you've talked before about the power of data and how it's not talked about enough.\\n\\n**Sasha:...\"],[\"### Wow. That is so interesting!\\n\\n**Sasha:** It's actually really, camera trap data is a really huge...\"],[\"**Sasha:** Yeah, I guess another anecdote, I have a lot of these anecdotes, but at some point we wan...\"],[\"So that was really cool because we really saw a year and some before they had no trace of anything, ...\"],[\"### Exactly, that's so interesting. That's so amazing that you were able to jump in there and provid...\"],[\"### All right, so we're going to dive into rapid-fire questions. If you could go back and do one thi...\"],[\"### That's so funny, and it’s interesting to hear that because I often hear people say you need to k...\"],[\"### So besides maybe a mathematical foundation, what advice would you give to someone looking to get...\"],[\"### I love that, find something that you're interested in.\\n\\n**Sasha:** Exactly. And one of my favori...\"],[\"### So should people be afraid of AI taking over the world?\\n\\n**Sasha:** I think that we're really fa...\"],[\"### What are you interested in right now? It could be anything, a movie, a recipe, a podcast, etc.?\\n...\"],[\"### What are some of your favorite machine learning papers?\\n\\n**Sasha:** My favorite currently, paper...\"],[\"### Wow, we'll definitely be linking to that paper as well, so people can check that out. Yeah, very...\"],[\"### Where can people find you online?\\n\\n**Sasha:** I'm on [Twitter @SashaMTL](https:\\u002f\\u002ftwitter.com\\u002fSas...\"],[\"--\\ntitle: Simple considerations for simple people building fancy neural networks\\nthumbnail: \\u002fblog\\u002fas...\"],[\"At the same time, deep learning frameworks, tools, and specialized libraries democratize machine lea...\"],[\"*   Are the labels balanced?\\n*   Are there gold-labels that you do not agree with?\\n*   How were the ...\"],[\"As developers, it easy to feel good when building something fancy but it is sometimes hard to ration...\"],[\"\\u003e Pro-tip: in my experience working with pre-trained language models, freezing the embeddings module...\"],[\"## 4. 👀 Tune but don’t tune blindly\\n\\nOnce you have everything up and running, you might want to tune...\"],[\"A few related pointers to complete your reading:\\n\\n*   [Reproducibility (in ML) as a vehicle for engi...\"],[\"--\\ntitle: \\\"Fine-tuning Llama 2 70B using PyTorch FSDP\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f160_fsdp_llama\\u002fthumb...\"],[\"(Source: [link](https:\\u002f\\u002fpytorch.org\\u002fblog\\u002fintroducing-pytorch-fully-sharded-data-parallel-api\\u002f))\\n\\n## ...\"],[\"3. SLURM script `launch.slurm`: https:\\u002f\\u002fgist.github.com\\u002fpacman100\\u002f1cb1f17b2f1b3139a63b764263e70b25\\n\\n...\"],[\"Below is the output snippet on a 7B model on 2 GPUs measuring the memory consumed and model paramete...\"],[\"```\\n\\n### Addressing Challenge 2\\nIt is addressed via choosing `SHARDED_STATE_DICT` state dict type wh...\"],[\"```\\naccelerate config --config_file \\\"fsdp_config.yaml\\\"\\n```\\n\\n![fsdp_config](https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"```\\n\\n### Addressing Challenge 3\\nFlash Attention and enabling gradient checkpointing are required for...\"],[\"(Source: [link](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2205.14135.pdf))\\n\\nThis is precisely the problem that Flash Att...\"],[\"## Bringing it all-together\\n\\nTo run the training using `Accelerate` launcher with SLURM, refer this ...\"],[\"```\\naccelerate launch \\\\\\n    --config_file configs\\u002ffsdp_config.yaml \\\\\\n    --main_process_ip $MASTER_A...\"],[\"```\\n\\nFine-tuning completed in ~13.5 hours and below is the training loss plot.\\n\\n![train_loss](https:...\"],[\"- Human: Now explain it like a chef.\\n\\n+ Assistant: Certainly! Here's an explanation of deep learning...\"],[\"```\\n\\nThe whole conversation is formatted as below: \\n```\\n\\u003c|system|\\u003e system message \\u003c|endoftext|\\u003e \\u003c|pr...\"],[\"--\\ntitle: Optimizing Stable Diffusion for Intel CPUs with NNCF and 🤗 Optimum\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"In this blog post, we will outline the problems of optimizing Stable Diffusion models and propose a ...\"],[\"However, it turns out that the traditional model optimization methods, such as post-training 8-bit q...\"],[\"## Optimization workflow\\n\\nWe usually start the optimization of a model after it's trained. Here, we ...\"],[\"## Going beyond Quantization-Aware Training\\n\\nQuantization alone can bring significant enhancements b...\"],[\"\\u003cdiv class=\\\"flex flex-row\\\"\\u003e\\n\\u003cdiv class=\\\"grid grid-cols-2 gap-4\\\"\\u003e\\n\\u003cfigure\\u003e\\n\\u003cimg class=\\\"max-w-full rou...\"],[\"\\u003cfigcaption class=\\\"mt-2 text-center text-sm text-gray-500\\\"\\u003eOpenVINO 8-bit, Inference Speed: 59 secon...\"],[\"Results of image generation [demo](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fhelenai\\u002fstable_diffusion) using dif...\"],[\"Below we show how to perform inference with the final pipeline optimized to run on Intel CPUs:\\n\\n```p...\"],[\"```\\n\\nYou can find the training and quantization [code](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-intel\\u002f...\"],[\"--\\ntitle: \\\"A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using tran...\"],[\"After completing the training of BLOOM-176B, we at HuggingFace and BigScience were looking for ways ...\"],[\"![Summary](assets\\u002f96_hf_bitsandbytes_integration\\u002ftf32-Mantissa-chart-hi-res-FINAL.png)\\n\\nFloat32 (FP3...\"],[\"In the Ampere architecture, NVIDIA also introduced [TensorFloat-32](https:\\u002f\\u002fblogs.nvidia.com\\u002fblog\\u002f20...\"],[\"But what if we can store those weights with less memory using a different data type? A methodology c...\"],[\"For example, in zero-point quantization, if my range is -1.0…1.0 and I want to quantize into the ran...\"],[\"![out-quant.gif](assets\\u002f96_hf_bitsandbytes_integration\\u002fout-quant.gif)\\n\\nTo retrieve the latest, one c...\"],[\"## A gentle summary of LLM.int8(): zero degradation matrix multiplication for Large Language Models\\n...\"],[\"As mentioned earlier, 8-bit precision is extremely constrained, therefore quantizing a vector with s...\"],[\"We ran several common benchmarks with the 8-bit and native models using lm-eval-harness and reported...\"],[\"For BLOOM-176:\\n\\n| benchmarks | - | -   | -  |   -        |     difference - value  |\\n| ---------- | ...\"],[\"### Is it faster than native models?\\n\\n\\nThe main purpose of the LLM.int8() method is to make large mo...\"],[\"| Precision      | Number of parameters | Hardware     | Time per token in milliseconds for Batch Si...\"],[\"The 3 models are BLOOM-176B, T5-11B and T5-3B.\\n\\n### Hugging Face `transformers` integration nuances\\n...\"],[\"```\\n\\n2. Then you can define your own model. Note that you can convert a checkpoint or model of any p...\"],[\"```\\nint8_model[0].weight\\nParameter containing:\\ntensor([[ 0.0031, -0.0438,  0.0494,  ..., -0.0046, -0...\"],[\"```\\n\\nThe weights values are \\\"truncated\\\" as we have seen when explaining quantization in the previous...\"],[\"```\\n\\nCheck out [the example script](\\u002fassets\\u002f96_hf_bitsandbytes_integration\\u002fexample.py) for the full ...\"],[\"```\\n\\nto\\n\\n```py\\nparam_cls = type(module._parameters[name])\\nkwargs = module._parameters[name].__dict__...\"],[\"```\\n\\nThis function recursively replaces all `nn.Linear` layers of a given model initialized on the `...\"],[\"### Be very careful on how to set devices with `accelerate`\\n\\nHere we played a very delicate balancin...\"],[\"Now time to see how to benefit from this integration and how to successfully use it in `transformers...\"],[\"```\\n\\n### Example demos - running T5 11b on a Google Colab\\n\\nCheck out the Google Colab demos for runn...\"],[\"### Support for Kepler GPUs (GTX 1080 etc)\\n\\nWhile we support all GPUs from the past four years, some...\"],[\"--\\ntitle: \\\"Speech Synthesis, Recognition, and More With SpeechT5\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fspeecht5\\u002ft...\"],[\"The main idea behind SpeechT5 is to pre-train a single model on a mixture of text-to-speech, speech-...\"],[\"Note: Even though the fine-tuned models start out using the same set of weights from the shared pre-...\"],[\"SpeechT5 is not available in the latest release of Transformers yet, so you'll have to install it fr...\"],[\"```\\n\\nFirst, we load the [fine-tuned model](https:\\u002f\\u002fhuggingface.co\\u002fmicrosoft\\u002fspeecht5_tts) from the H...\"],[\"```\\n\\nThe speaker embedding is a tensor of shape (1, 512). This particular speaker embedding describe...\"],[\"```\\n\\nTo make audio from the spectrogram, do the following:\\n\\n```python\\nwith torch.no_grad():\\n    spee...\"],[\"```\\n\\nThe output sounds like this ([download audio](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocum...\"],[\"The **speech encoder pre-net** is the same as the feature encoding module from [wav2vec 2.0](https:\\u002f...\"],[\"```\\n\\nWe will need some speech audio to use as input. For the purpose of this example, we’ll load the...\"],[\"```\\n\\nNow we can perform the speech conversion by calling the model’s `generate_speech` method.\\n\\n```p...\"],[\"```\\n\\nChanging to a different voice is as easy as loading a new speaker embedding. You could even mak...\"],[\"- **Speech encoder pre-net.** This is the same pre-net used by the speech-to-speech model and consis...\"],[\"```\\n\\nAs speech audio, we’ll use the same input as in the previous section, but any audio file will w...\"],[\"```\\n\\nFinally, tell the model to generate text tokens from the speech input, and then use the process...\"],[\"--\\ntitle: \\\"Code Llama: Llama 2 learns to code\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f160_codellama\\u002fthumbnail.jpg\\n...\"],[\"## Table of Contents\\n\\n  - [Introduction](#introduction)\\n  - [Table of Contents](#table-of-contents)\\n...\"],[\"Code Llama was trained on a 16k context window. In addition, the three model variants had additional...\"],[\"\\u003cscript type=\\\"module\\\" src=\\\"https:\\u002f\\u002fgradio.s3-us-west-2.amazonaws.com\\u002f3.28.3\\u002fgradio.js\\\"\\u003e \\u003c\\u002fscript\\u003e\\n\\u003cg...\"],[\"```\\n#### A Note on dtypes\\n\\nWhen using models like Code Llama, it's important to take a look at the d...\"],[\"```python\\nfrom transformers import AutoTokenizer\\nimport transformers\\nimport torch\\n\\ntokenizer = AutoT...\"],[\"```\\n\\nThis may produce output like the following:\\n\\n```python\\nResult: def fibonacci(n):\\n    if n == 0:...\"],[\"```\\n\\nCode Llama is specialized in code understanding, but it's a language model in its own right. Yo...\"],[\"```\\n\\n```Python\\ndef remove_non_ascii(s: str) -\\u003e str:\\n    \\\"\\\"\\\" Remove non-ASCII characters from a strin...\"],[\"```\\n\\nNote that the system prompt is optional - the model will work without it, but you can use it to...\"],[\"```\\n\\n3. **On-going conversation with previous answers**\\n\\nThe process is the same as in [Llama 2](htt...\"],[\"```\\n\\n#### 4-bit Loading\\n\\nIntegration of Code Llama in Transformers means that you get immediate supp...\"],[\"```\\n\\n### Using text-generation-inference and Inference Endpoints\\n\\n[Text Generation Inference](https:...\"],[\"### Using VS Code extension\\n\\n[HF Code Autocomplete](https:\\u002f\\u002fmarketplace.visualstudio.com\\u002fitems?itemN...\"],[\"| Model                  | License            | Dataset known | Commercial use? | Pretraining length...\"],[\"| CodeLlaMa-13B-Instruct | Llama 2 license    | ❌             | ✅               | 2,620B            ...\"],[\"**Note:** The scores presented in the table above are sourced from our code leaderboard, where we ev...\"],[\"--\\ntitle: \\\"Hugging Face and AMD partner on accelerating state-of-the-art models for CPU and GPU plat...\"],[\"## Supported hardware platforms\\n\\nOn the GPU side, AMD and Hugging Face will first collaborate on the...\"],[\"## The road ahead\\n\\nOur initial focus will be ensuring the models most important to our community wor...\"],[\"--\\ntitle: \\\"Accelerating over 130,000 Hugging Face models with ONNX Runtime\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"## Learn More\\nTo learn more about accelerating Hugging Face models with ONNX Runtime, check out our ...\"],[\"--\\ntitle: \\\"AI Speech Recognition in Unity\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games\\u002funity-asr-thumbn...\"],[\"```\\n[SerializeField] private Button startButton;\\n[SerializeField] private Button stopButton;\\n[Serial...\"],[\"```\\nprivate void Update() {\\n    if (recording && Microphone.GetPosition(null) \\u003e= clip.samples) {\\n   ...\"],[\"```\\nusing System.IO;\\nusing TMPro;\\nusing UnityEngine;\\nusing UnityEngine.UI;\\n\\npublic class SpeechRecog...\"],[\"private byte[] EncodeAsWAV(float[] samples, int frequency, int channels) {\\n        using (var memory...\"],[\"```\\n\\nTo test whether this code is working correctly, you can add the following line to the end of th...\"],[\"```\\nusing System.IO;\\nusing HuggingFace.API;\\nusing TMPro;\\nusing UnityEngine;\\nusing UnityEngine.UI;\\n\\np...\"],[\"```\\n\\nCongratulations, you can now use state-of-the-art Speech Recognition in Unity!\\n\\nIf you have any...\"],[\"--\\ntitle: 'Building a Playlist Generator with Sentence Transformers'\\nthumbnail: \\u002fblog\\u002fassets\\u002f87_play...\"],[\"\\u003cdiv class=\\\"hidden xl:block\\\"\\u003e\\n\\u003cdiv style=\\\"display: flex; flex-direction: column; align-items: center...\"],[\"We’ll be looking at a slightly advanced use of Gradio, so if you’re a beginner to the library I reco...\"],[\"[The ST documentation highlights many of the choices](https:\\u002f\\u002fwww.sbert.net\\u002fdocs\\u002fpretrained_models.h...\"],[\"```python\\nfrom sentence_transformers import SentenceTransformer\\nimport pickle\\n\\nembedder = SentenceTr...\"],[\"```\\n\\nTo be able to share you embeddings with others, you can even upload the Pickle file to a Huggin...\"],[\"```\\n\\nSince we’re searching for any verse that matches the text prompt, there’s a good chance that th...\"],[\"```python\\nfrom sentence_transformers import SentenceTransformer, util\\nfrom huggingface_hub import hf...\"],[\"```\\n\\nThe Gradio Blocks API lets you build *multi-step* interfaces, which means that you’re free to c...\"],[\"```\\n\\nIn that function, we use the text prompt to conduct the semantic search. As seen above, to push...\"],[\"While the song *lyrics* aren’t being released, I’ve **[published the verse embeddings along with the...\"],[\"--\\ntitle: \\\"Accelerating Vision-Language Models: BridgeTower on Habana Gaudi2\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"## BridgeTower\\n\\nIn the recent past, [Vision-Language (VL) models](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fvision...\"],[\"[Nvidia A100 Tensor Core GPU](https:\\u002f\\u002fwww.nvidia.com\\u002fen-us\\u002fdata-center\\u002fa100\\u002f) includes the 3rd gener...\"],[\"## Benchmark\\n\\nTo benchmark training, we are going to fine-tune a [BridgeTower Large checkpoint](http...\"],[\"### Making use of `dataloader_num_workers`\\n\\nWhen image loading is done on CPU, a quick way to speed ...\"],[\"Here are the throughputs we got on Gaudi2, H100 and A100:\\n\\n| Device     | `dataloader_num_workers=0`...\"],[\"Second, we see that **allocating more resources for data loading can lead to easy speedups**: x1.28 ...\"],[\"Optimum Habana's fast DDP does not split parameter gradients into buckets as [DDP does](https:\\u002f\\u002fpyto...\"],[\"Given a dataset, most dataloaders follow the following recipe:\\n\\n1. Fetch data (e.g. where your JPEG ...\"],[\"To implement this on Gaudi2, we have got you covered: the [contrastive image-text example](https:\\u002f\\u002fg...\"],[\"We got an additional x1.10 speedup compared to the previous run with `dataloader_num_workers=2` only...\"],[\"```\\n\\nThe base command line to run the script is:\\n```bash\\npython ..\\u002fgaudi_spawn.py --use_mpi --world_...\"],[\"```\\n\\nFor A100 and H100, you can use the same `run_bridgetower.py` script with a few small changes:\\n-...\"],[\"To go further, we are looking forward to using HPU graphs for training models even faster and to pre...\"],[\"--\\ntitle: \\\"Introducing 🤗 Accelerate\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f20_accelerate_library\\u002faccelerate_diff.p...\"],[\"```\\n\\nBy just adding five lines of code to any standard PyTorch training script, you can now run said...\"],[\"```\\n\\nIn contrast, here are the changes needed to have this code run with distributed training are th...\"],[\"```\\n\\nOn top of giving the main object that you will use, this line will analyze from the environment...\"],[\"```\\n\\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torc...\"],[\"```\\n\\nThis last line adds the necessary steps for the backward pass (mostly for mixed precision but o...\"],[\"```\\n\\nLike for the training, you need to add one line to prepare your evaluation dataloader. Then you...\"],[\"```\\n\\nwill launch your training script using those default. The only thing you have to do is provide ...\"],[\"--\\ntitle: \\\"Interactively explore your Huggingface dataset with one line of code\\\"\\nthumbnail: \\u002fblog\\u002fas...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fRenumics\\u002fspotlight\\\"\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface...\"],[\"```\\n\\nData inspection is a very important task in almost all ML development stages, but it can also b...\"],[\"```\\n\\n## **Leveraging model results for data inspection**\\n\\nExploring raw unstructured datasets often ...\"],[\"```\\n\\nNow we can compute the enrichment:\\n\\n\\n```python\\nimport torch\\nimport transformers\\n\\ndevice = torch...\"],[\"```\\n\\nIf you don’t want to perform the full inference run, you can alternatively download pre-compute...\"],[\"```\\n\\n\\u003cfigure class=\\\"image text-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocu...\"],[\"You can optionally choose a dataset that contains model results and other configuration options such...\"],[\"--\\ntitle: Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\nthu...\"],[\"One of the main problems developers and organizations face is how difficult it is to deploy and scal...\"],[\"Within minutes, you can test your endpoint and add its inference API to your application. It’s never...\"],[\"The Hugging Face Blog Repository 🤗\\nThis is the official repository of the [Hugging Face Blog](https:...\"],[\"```\\n---\\ntitle: \\\"PUT YOUR TITLE HERE\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f101_decision-transformers-train\\u002fthumbn...\"],[\"--\\ntitle: Training CodeParrot 🦜 from Scratch\\nthumbnail: \\u002fblog\\u002fassets\\u002f40_codeparrot\\u002fthumbnail.png\\naut...\"],[\"- 0.1% of the unique files make up 15% of all files\\n- 1% of the unique files make up 35% of all file...\"],[\"# Base tokenizer\\ntokenizer = GPT2Tokenizer.from_pretrained(\\\"gpt2\\\")\\nbase_vocab = list(bytes_to_unicod...\"],[\"```\\n\\nLearn more about tokenizers and how to build them in the [Hugging Face course](https:\\u002f\\u002fhuggingf...\"],[\"```\\n\\nNow that we have an efficient tokenizer and a freshly initialized model we can start with the a...\"],[\"```\\n\\nWe can directly load the tokenizer and model from the local repository. Since we are dealing wi...\"],[\"```\\n\\nNext up is the dataset. We make training simpler with a dataset that yields examples with a fix...\"],[\"def __iter__(self):\\n        iterator = iter(self.dataset)\\n        more_examples = True\\n        while...\"],[\"```\\n\\nTexts in the buffer are tokenized in parallel and then concatenated. Chunked samples are then y...\"],[\"```\\n\\nBefore we start training we need to set up the optimizer and learning rate schedule. We don’t w...\"],[\"```\\nUnder the hood it'll use DistributedDataParallel, which means a batch is sent to each GPU worker...\"],[\"```\\n\\nWe are now ready to write the main training loop. It will look pretty much like a normal PyTorc...\"],[\"```\\n\\nWhen we call `wait_for_everyone()` and `unwrap_model()` we make sure that all workers are ready...\"],[\"```\\nDone! That's all the code to train a full GPT-2 model from scratch with as little as 150 lines. ...\"],[\"```Python\\nfrom datasets import load_metric\\n\\ncode_eval = datasets.load_metric(\\\"code_eval\\\")\\ntest_cases...\"],[\"```\\n\\n\\n\\nYou can also load OpenAI's HumanEval dataset with `datasets`:\\n\\n```Python\\nfrom datasets import...\"],[\"```\\n\\nAlthough the test condition itself does not look quite right the model has built all the boiler...\"],[\"```\\n\\n## Summary\\n\\nIn this short blog post we walked through all the steps involved for training a lar...\"],[\"--\\ntitle: \\\"Optimizing Bark using 🤗 Transformers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fbark_optimization\\u002fthumbnai...\"],[\"This tutorial is also a demonstration of how one can benchmark a non-optimized model and its varying...\"],[\"Bark is made of 4 main models:\\n\\n- `BarkSemanticModel` (also referred to as the 'text' model): a caus...\"],[\"```\\n\\nPlace the model to an accelerator device to get the most of the optimization techniques:\\n\\n```py...\"],[\"```\\n\\nMeasuring the latency and GPU memory footprint requires the use of specific CUDA methods. We de...\"],[\"```\\n\\n**Output:**\\n\\n```\\nExecution time: 9.3841625 seconds\\nMax memory footprint 1.914612224  GB\\n```\\n\\nNo...\"],[\"```\\n\\n\\nThe output sounds like this ([download audio](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocu...\"],[\"Flash Attention is a faster and more efficient algorithm for attention computations that combines tr...\"],[\"```\\n\\n**Output:**\\n\\n```\\nExecution time: 5.43284375 seconds\\nMax memory footprint 1.9151841280000002  GB...\"],[\"```\\nExecution time: 5.43284375 seconds\\nMax memory footprint 1.9151841280000002  GB\\n```\\n\\nThe output s...\"],[\"```\\n\\n**Output:**\\n\\n```\\nExecution time: 7.00045390625 seconds\\nMax memory footprint 2.7436124160000004 ...\"],[\"```\\n\\n**Output:**\\n\\n```\\nExecution time: 8.97633828125 seconds\\nMax memory footprint 1.3231160320000002 ...\"],[\"```\\n\\n**Output:**\\n\\n```\\nExecution time: 7.4496484375000005 seconds\\nMax memory footprint 0.468710912000...\"],[\"```\\n\\n\\nThe output sounds like this (download [first](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocu...\"],[\"The benchmark was run on an NVIDIA TITAN RTX 24GB with a maximum of 256 new tokens.\\n\\n## How to read ...\"],[\"| Relative value              | Latency | Memory |\\n|-----------------------------|---------|--------...\"],[\"| Relative value                | Latency | Memory | Throughput |\\n|-------------------------------|-...\"],[\"--\\ntitle: \\\"Director of Machine Learning Insights [Part 4]\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_ml_director_in...\"],[\"**Background:** Seasoned entrepreneur and leader, Javier was co-founder and CTO of Machinalis, a hig...\"],[\"E-commerce is scaling its share of the market year after year, and Machine Learning is always a prob...\"],[\"**Background:** Dr. Shaun Gittens is the Director of the Machine Learning Capability of MasterPeace ...\"],[\"#### **1. How has ML made a positive impact on Engineering?**\\n\\nEngineering is vast in its applicatio...\"],[\"#### **3. What’s a common mistake you see people make when trying to integrate ML into Engineering?*...\"],[\"**Background:** Samuel is a senior Data Science and ML Engineering leader at Pluralsight with a Ph.D...\"],[\"#### **3. What’s a common mistake you see people make trying to integrate ML into existing products?...\"],[\"**Fun Fact:** Met Paul McCarthy. 🎤\\n\\n**MasterPeace Solutions:** MasterPeace Solutions has emerged as ...\"],[\"#### **3. What’s a common mistake you see people make trying to integrate ML into SaaS?**\\nTo get it ...\"],[\"--\\ntitle: \\\"Panel on Hugging Face\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fpanel-on-hugging-face\\u002fthumbnail.png\\nautho...\"],[\"Here are some notable features of Panel that our users find valuable. \\n\\n- Panel provides extensive s...\"],[\"## 🌐 Join Our Community\\nThe Panel community is vibrant and supportive, with experienced developers a...\"],[\"--\\ntitle: \\\"Diffusion Models Live Event\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fdiffusion-models-event\\u002fthumbnail.png...\"],[\"\\u003cdiv\\n    class=\\\"container md:grid md:grid-cols-2 gap-2 max-w-7xl\\\"\\n\\u003e\\n    \\u003cdiv class=\\\"text-center flex...\"],[\"\\u003cp\\u003e\\u003cstrong\\u003eDevi Parikh: \\u003cem\\u003eMake-A-Video: Diffusion Models for Text-to-Video Generation without Text...\"],[\"\\u003cp\\u003e\\u003cstrong\\u003eJustin Pinkney: \\u003cem\\u003eBeyond text - giving Stable Diffusion new abilities\\u003c\\u002fem\\u003e\\u003c\\u002fstrong\\u003e\\u003c\\u002fp\\u003e...\"],[\"--\\ntitle: \\\"Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\\\"\\nthumbnail:...\"],[\"With 60,000+ stars on Github, 30,000+ models, and millions of monthly visits, Hugging Face is one of...\"],[\"--\\ntitle: \\\"Fine-tune Llama 2 with DPO\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f157_dpo_trl\\u002fdpo_thumbnail.png\\nauthor...\"],[\"## DPO vs PPO\\n\\nIn the traditional model of optimising human derived preferences via RL, the goto met...\"],[\"In this respect we would still need to do the step 1, but instead of steps 3 and 4 we need to provid...\"],[\"```\\n\\nOnce we have the dataset sorted the DPO loss is essentially a supervised loss which obtains an ...\"],[\"```\\n\\n## Experiment with Llama v2\\n\\nThe benefit of implementing the DPO trainer in TRL is that one can...\"],[\"# add LoRA layers on top of the quantized base model\\npeft_config = LoraConfig(\\n    r=script_args.lor...\"],[\"```\\n\\n### DPO Training\\n\\nOnce the SFT has finished, we can save the resulting model and move onto the ...\"],[\"```\\n\\nSo as can be seen we load the model in the 4-bit configuration and then train it via the QLora ...\"],[\"We hope with the code release it lowers the barrier to entry for you the readers to try out this met...\"],[\"--\\ntitle: \\\"Welcome fastText to the Hugging Face Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f147_fasttext\\u002fthumbnail....\"],[\"![text_classification_widget](assets\\u002f147_fasttext\\u002ffasttext_text_classification_widget.png)\\n![feature...\"],[\"```\\n\\nHere is how to use this model to query nearest neighbors of an English word vector:\\n\\n```python\\n...\"],[\"```\\n\\n## Would you like to integrate your library to the Hub?\\n\\nThis integration is possible thanks to...\"],[\"--\\ntitle: \\\"Introducing Decision Transformers on Hugging Face 🤗\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f58_decision-...\"],[\"![Offline vs Online RL](assets\\u002f58_decision-transformers\\u002fofflinevsonlinerl.gif)\\n\\n*A comparison betwee...\"],[\"The main idea is that instead of training a policy using RL methods, such as fitting a value functio...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cvideo \\n        alt=\\\"WalkerEd-expert\\\"\\n      ...\"],[\"```\\n\\n### Loading the model\\n\\nUsing the Decision Transformer is relatively easy, but as it is an autor...\"],[\"```\\n### Autoregressive prediction function\\n\\nThe model performs an [autoregressive prediction](https:...\"],[\"`````python\\n# Function that gets an action from the model using autoregressive prediction \\n# with a ...\"],[\"actions = torch.cat([torch.zeros((1, padding, act_dim)), actions], dim=1).float()\\n\\treturns_to_go = t...\"],[\"```\\n### Evaluating the model\\n\\nIn order to evaluate the model, we need some additional information; t...\"],[\"state_mean = torch.from_numpy(state_mean)\\nstate_std = torch.from_numpy(state_std)\\n\\nstate = env.reset...\"],[\"```\\nYou will find a more detailed example, with the creation of videos of the agent in our [Colab no...\"],[\"--\\ntitle: \\\"Course Launch Community Event\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f34_course_launch\\u002fspeakers_day1_thu...\"],[\"To register, please fill out [this form](https:\\u002f\\u002fdocs.google.com\\u002fforms\\u002fd\\u002fe\\u002f1FAIpQLSd17_u-wMCdO4fcOPO...\"],[\"\\u003cdiv\\n    class=\\\"container md:grid md:grid-cols-2 gap-2 max-w-7xl\\\"\\n\\u003e\\n    \\u003cdiv class=\\\"text-center flex...\"],[\"\\u003cp\\u003e\\u003cstrong\\u003eMargaret Mitchell: \\u003cem\\u003eOn Values in ML Development\\u003c\\u002fem\\u003e\\u003c\\u002fstrong\\u003e\\u003c\\u002fp\\u003e\\n        \\u003cp\\u003eMargaret ...\"],[\"\\u003cp\\u003eJakob Uszkoreit is the co-founder of Inceptive. Inceptive designs RNA molecules for vaccines and ...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003cdiv class=\\\"text-center flex flex-col items-center\\\"\\u003e\\n        \\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f34_co...\"],[\"## Day 2 (November 16th): The tools you will use\\n\\nDay 2 will be focused on talks by the Hugging Face...\"],[\"\\u003cdiv\\n    class=\\\"container md:grid md:grid-cols-2 gap-2 max-w-7xl\\\"\\n\\u003e\\n    \\u003cdiv class=\\\"text-center flex...\"],[\"\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f34_course_launch\\u002flysandre_debut.png\\\" width=50% style=\\\"border-radius: 50%;\\\"\\u003e\\n ...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003cdiv class=\\\"text-center flex flex-col items-center\\\"\\u003e\\n        \\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f34_co...\"],[\"\\u003cp\\u003e\\u003cstrong\\u003eMathieu Desvé: \\u003cem\\u003eAWS ML Vision: Making Machine Learning Accessible to all Customers\\u003c\\u002fem...\"],[\"--\\ntitle: 'Accelerated Inference with Optimum and Transformers Pipelines'\\nthumbnail: \\u002fblog\\u002fassets\\u002f66...\"],[\"Let's get started! 🚀\\n\\n## 1. What is Optimum? An ELI5\\n\\n[Hugging Face Optimum](https:\\u002f\\u002fgithub.com\\u002fhugg...\"],[\"**Switching from Transformers to Optimum Inference**\\nThe [Optimum Inference models](https:\\u002f\\u002fhuggingf...\"],[\"```\\n\\nIn the first release, we added [support for ONNX Runtime](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fm...\"],[\"*This tutorial was created and run on an `m5.xlarge` AWS EC2 Instance.*\\n\\n### 3.1 Install `Optimum` f...\"],[\"```\\n\\nThis will install all required packages for us including `transformers`, `torch`, and `onnxrunt...\"],[\"# save onnx checkpoint and tokenizer\\nmodel.save_pretrained(onnx_path)\\ntokenizer.save_pretrained(onnx...\"],[\"```\\n\\nWe successfully converted our vanilla transformers to `onnx` and used the model with the `trans...\"],[\"```\\n\\nTo test performance we can use the `ORTModelForQuestionAnswering` class again and provide an ad...\"],[\"```\\n\\nWe will evaluate the performance changes in step [3.6 Evaluate the performance and speed](#36-e...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f66_optimum_inference\\u002fmod...\"],[\"```\\n\\nNice! The model predicted the same answer.\\n\\n### 3.5 Run accelerated inference using Transformer...\"],[\"```\\n\\nIn addition to this we added a `pipelines` API to Optimum to guarantee more safety for your acc...\"],[\"```\\n\\n### 3.6 Evaluate the performance and speed\\n\\nDuring this [End-to-End tutorial on accelerating Ro...\"],[\"```\\n\\nWe can now leverage the [map](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fv2.1.0\\u002fen\\u002fprocess#map) funct...\"],[\"```\\n\\nNow lets compare the results\\n\\n```python\\ndefault_acc = metric.compute(predictions=result[\\\"defaul...\"],[\"```\\n\\nOur optimized & quantized model achieved an exact match of `78.75%` and an f1 score of `81.83%`...\"],[\"```\\n\\nTo keep it simple, we are going to use a python loop and calculate the avg\\u002fmean latency for our...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f66_optimum_inference\\u002fres...\"],[\"- **Remote Models \\u003e 2GB:** Currently, only models smaller than 2GB can be loaded from the [Hugging F...\"],[\"**How can I use Optimum with Transformers?**\\n\\nYou can find an example and instructions in our [docum...\"],[\"Some important features on the roadmap for Optimum amongst the [current limitations](#4-current-limi...\"],[\"--\\ntitle: \\\"Hugging Face Selected for the French Data Protection Agency Enhanced Support Program\\\"\\nthu...\"],[\"When it comes to respecting people’s privacy rights, the recent developments in ML and AI pose new q...\"],[\"--\\ntitle: Using Stable Diffusion with Core ML on Apple Silicon\\nthumbnail: \\u002fblog\\u002fassets\\u002fdiffusers_cor...\"],[\"## Available Checkpoints\\n\\nThe official Stable Diffusion checkpoints are already converted and ready ...\"],[\"## Notes on Performance\\n\\nThere are several variants per model:\\n\\n- \\\"Original\\\" attention vs \\\"split_ein...\"],[\"```\\ncoreml-stable-diffusion-v1-4\\n├── README.md\\n├── original\\n│   ├── compiled\\n│   └── packages\\n└── sp...\"],[\"```\\n\\n`\\u003coutput-mlpackages-directory\\u003e` should point to the checkpoint you downloaded in the step above...\"],[\"```\\n\\n## Core ML inference in Swift\\n\\nRunning inference in Swift is slightly faster than in Python, be...\"],[\"```\\n\\nYou have to specify in `--resource-path` one of the checkpoints downloaded in the previous step...\"],[\"--\\ntitle: \\\"Exploring simple optimizations for SDXL\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fsimple_sdxl_optimization...\"],[\"```\\n\\nThis isn’t very practical and can slow you down because you’re often generating more than 4 ima...\"],[\"With 🤗 Diffusers, you can use fp16 for inference by specifying the `torch.dtype` parameter to conver...\"],[\"```\\n\\nCompared to a completely unoptimized SDXL pipeline, using fp16 takes 21.7GB of memory and only ...\"],[\"```\\n\\nCompared to a completely unoptimized SDXL pipeline, using fp16 and SDPA takes the same amount o...\"],[\"```\\n\\nCompared to the previous baseline (fp16 + SDPA), wrapping the UNet with `torch.compile` improve...\"],[\"```\\n\\nCompared to the baseline, it now takes 20.2GB of memory which saves you 1.5GB of memory.\\n\\n### S...\"],[\"```\\n\\nWith sliced computations, we reduce the memory to 15.4GB. If we add sequential CPU offloading, ...\"],[\"```\\n\\nNext, flush the GPU memory to remove the text encoders:\\n\\n```jsx\\ndel text_encoder, text_encoder_...\"],[\"```\\n\\nCombined with SDPA and fp16, we can reduce the memory to 21.9GB. Other techniques discussed abo...\"],[\"```\\n\\nWith this setup, we reduce the memory requirement to 15.6GB while reducing the inference latenc...\"],[\"| **Technique** | **Memory (GB)** | **Inference latency (ms)** |\\n| --- | --- | --- |\\n| unoptimized p...\"],[\"--\\ntitle: \\\"Fine-Tune ViT for Image Classification with 🤗 Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f51_fi...\"],[\"It turns out that once you've done the above, you can pre-train and fine-tune transformers just as y...\"],[\"```\\n\\n## Load a dataset\\n\\nLet's start by loading a small image classification dataset and taking a loo...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"```\\n\\n\\n\\n\\n    'bean_rust'\\n\\n\\n\\nTurns out the leaf shown above is infected with Bean Rust, a serious dise...\"],[\"```\\n\\n\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)...\"],[\"```\\n\\nYou can see the image processor configuration by printing it.\\n\\n\\n    ViTImageProcessor {\\n      \\\"...\"],[\"```\\n\\n\\n```python\\nprocess_example(ds['train'][0])\\n```\\n\\n\\n    {\\n      'pixel_values': tensor([[[[-0.6157...\"],[\"```\\n\\n\\nThis time, the resulting `pixel_values` tensor will have shape `(2, 3, 224, 224)`.\\n\\n    {\\n    ...\"],[\"```\\n\\n\\nLet's load the pretrained model. We'll add `num_labels` on init so the model creates a classif...\"],[\"```\\n\\n\\nAlmost ready to train! The last thing needed before that is to set up the training configurati...\"],[\"```\\n\\n### Train 🚀\\n\\n\\n```python\\ntrain_results = trainer.train()\\ntrainer.save_model()\\ntrainer.log_metric...\"],[\"--\\ntitle: \\\"~Don't~ Repeat Yourself\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f59_transformers_philosophy\\u002ftransformers....\"],[\"In short the reasons are:\\n- **1. Transformers is built by and for the open-source community.**\\n- **2...\"],[\"### 3. Machine Learning is evolving at a neck-breaking speed\\nResearch in the field of machine learni...\"],[\"Instead, research teams tend to publish a new model built upon previous models but rarely make signi...\"],[\"Now, we explain why we put the asterisk \\\\\\\\( {}^{\\\\textbf{*}} \\\\\\\\) after *\\\"Repeat Yourself\\\"*. We don't ...\"],[\"### Drawbacks\\nClearly, there are also drawbacks to the single file policy two of which we quickly wa...\"],[\"--\\ntitle: \\\"Introducing Würstchen: Fast Diffusion for Image Generation\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fwuer...\"],[\"![Würstchen images with Prompts](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"You can also find a detailed explanation video here:\\n\\n\\u003ciframe width=\\\"708\\\" height=\\\"398\\\" src=\\\"https:\\u002f\\u002f...\"],[\"```\\n\\n![Anthropomorphic cat dressed as a fire-fighter](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"### Diffusers integration\\nBecause Würstchen is fully integrated in `diffusers`, it automatically com...\"],[\"## Optimisation Technique 1: Flash Attention\\n\\nStarting from version 2.0, PyTorch has integrated a hi...\"],[\"```\\n\\nFor an in-depth look at how `diffusers` leverages SDPA, check out the [documentation](https:\\u002f\\u002fh...\"],[\"```\\n\\nAnd the good news is that this compilation is a one-time execution. Post that, you're set to ex...\"],[\"--\\ntitle: Faster TensorFlow models in Hugging Face Transformers\\nthumbnail: \\u002fblog\\u002fassets\\u002f10_tf-servin...\"],[\"| Batch size | Google implementation | v4.2.0 implementation | Relative difference Google\\u002fv4.2.0 imp...\"],[\"### What is a SavedModel?\\n\\nA SavedModel contains a standalone TensorFlow model, including its weight...\"],[\"```\\nsavedmodel\\n    \\u002fassets\\n        -\\u003e here the needed assets by the model (if any)\\n    \\u002fvariables\\n  ...\"],[\"```\\nThe given SavedModel SignatureDef contains the following input(s):\\n  inputs['attention_mask'] te...\"],[\"```\\n\\nTo directly pass `inputs_embeds` (the token embeddings) instead of `input_ids` (the token IDs) ...\"],[\"```\\n\\nThe serving method has to be overridden by the new `input_signature` argument of the `tf.functi...\"],[\"```\\n\\n## How to deploy and use a SavedModel?\\n\\nLet’s see step by step how to deploy and use a BERT mod...\"],[\"```\\n\\nand kill the serving_base image ran as a daemon because we don't need it anymore:\\n```\\ndocker ki...\"],[\"```\\ndocker run -d -p 8501:8501 -p 8500:8500 --name bert my_bert_model\\n```\\n\\n### Step 3\\n\\nQuery the mod...\"],[\"```\\n\\nThis should return POSITIVE. It is also possible to pass by the gRPC (google Remote Procedure C...\"],[\"# Create a gRPC request made for prediction\\nrequest = predict_pb2.PredictRequest()\\n\\n# Set the name o...\"],[\"```\\n\\n## Conclusion\\nThanks to the last updates applied on the TensorFlow models in transformers, one ...\"],[\"--\\ntitle: \\\"Object Detection Leaderboard\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fobject-detection-leaderboard\\u002fthumbn...\"],[\"So, let's embark on this exploration together and unlock the secrets of the Object Detection Leaderb...\"],[\"## What's Object Detection?\\n\\nIn the field of Computer Vision, Object Detection refers to the task of...\"],[\"The diversity of detectors goes beyond the range of output classes they can recognize. They vary in ...\"],[\"IoU is a metric represented by a number between 0 and 1 that measures the overlap between the predic...\"],[\"Based on predefined \\\\\\\\( \\\\text{T}_{\\\\text{IOU}} \\\\\\\\), we can define True Positives and True Negatives:\\n...\"],[\"* **Recall** gauges a model’s competence in finding all the relevant cases (all ground truth boundin...\"],[\"The Precision x Recall curve illustrates the balance between Precision and Recall based on different...\"],[\"Based on these rules, we can classify each detection as TP or FP, as shown in Table 1:\\n\\n\\u003cdiv display...\"],[\"For example, consider the 12th row (detection \\\"P\\\") of Table 2. The value \\\"acc TP = 4\\\" means that if ...\"],[\"By examining the curve, one may infer the potential trade-offs between Precision and Recall and find...\"],[\"\\u003cdiv display=\\\"block\\\" margin-left=\\\"auto\\\" margin-right=\\\"auto\\\" width=\\\"50%\\\"\\u003e\\n\\u003ccenter\\u003e\\n    \\u003cimg src=\\\"http...\"],[\"### What's Average Recall and how to compute it?\\n\\nAverage Recall (AR) is a metric that's often used ...\"],[\"\\u003cp style=\\\"text-align: center;\\\"\\u003e\\n\\\\\\\\( \\\\text{AP@[.5:.05:0.95} = \\\\frac{\\\\text{AP}_{0.5} + \\\\text{AP}_{0.55...\"],[\"## Object Detection Leaderboard\\n\\nWe recently released the [Object Detection Leaderboard](https:\\u002f\\u002fhug...\"],[\"Next, we will provide tips on choosing the best model based on the metrics and point out which param...\"],[\"Let’s take the DEtection TRansformer (DETR) ([facebook\\u002fdetr-resnet-50](https:\\u002f\\u002fhuggingface.co\\u002ffacebo...\"],[\"```\\n\\nThe parameter `threshold` in function `post_process_object_detection` is used to filter the det...\"],[\"Figure 10 shows the process with batch size = 2, where the same two images are processed with `DetrI...\"],[\"It's important to recognize that models can produce boxes in various formats, and that also may be t...\"],[\"For such models, different prompts (e.g. \\\"Find the dog\\\" and \\\"Where's the bulldog?\\\") may result in th...\"],[\"| Use Case                                     | Real-world Scenarios                  | Recommended...\"],[\"The results shown in our 🤗 [Object Detection Leaderboard](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fhf-vision\\u002fob...\"],[\"--\\ntitle: \\\"The Falcon has landed in the Hugging Face ecosystem\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f147_falcon\\u002f...\"],[\"## The Falcon models\\n\\nThe Falcon family is composed of two base models: [Falcon-40B](https:\\u002f\\u002fhugging...\"],[\"Falcon-7B and Falcon-40B have been trained on 1.5 trillion and 1 trillion tokens respectively, in li...\"],[\"| Model | License | Commercial use? | Pretraining length [tokens] | Pretraining compute [PF-days] | ...\"],[\"Under the hood, this playground uses Hugging Face's [Text Generation Inference](https:\\u002f\\u002fgithub.com\\u002fh...\"],[\"# Inference\\n\\nYou can use the familiar transformers APIs to run the models on your own hardware, but ...\"],[\"```\\n\\nAnd then, you'd run text generation using code like the following:\\n\\n```python\\nsequences = pipel...\"],[\"```\\n\\nNote, however, that mixed 8-bit inference will use `torch.float16` instead of `torch.bfloat16`,...\"],[\"| ![tgi-hfe-screenshot.png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve...\"],[\"* [AI2 Reasoning Challenge](https:\\u002f\\u002fallenai.org\\u002fdata\\u002farc) (ARC): Grade-school multiple choice scienc...\"],[\"| Model | Type | Average leaderboard score |\\n| :---: | :---: | :---: |\\n| [tiiuae\\u002ffalcon-40b-instruct...\"],[\"Although the open LLM leaderboard doesn't measure chat capabilities (where human evaluation is the g...\"],[\"| ![repo-screenshot.png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fma...\"],[\"We fine-tuned the two variants of the Falcon models (7B and 40B) on the Guanaco dataset. We fine-tun...\"],[\"```\\n\\nCheck out the [original qlora repository](https:\\u002f\\u002fgithub.com\\u002fartidoro\\u002fqlora\\u002f) for additional de...\"],[\"--\\ntitle: \\\"Gradio 3.0 is Out!\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f68_gradio_blocks\\u002fblock-party.png\\nauthors:\\n- u...\"],[\"\\u003cimg class=\\\"max-w-full mx-auto my-6\\\" style=\\\"width: 54rem\\\" src=\\\"\\u002fblog\\u002fassets\\u002f68_gradio_blocks\\u002fdalle.j...\"],[\"```python\\nimport numpy as np\\nimport gradio as gr\\n\\ndef flip_text(x):\\n    return x[::-1]\\n\\ndef flip_ima...\"],[\"```\\n\\nOnce you run `launch()`, the following demo will appear:\\n\\n\\u003cimg class=\\\"max-w-full mx-auto my-6\\\" ...\"],[\"--\\ntitle: \\\"Releasing Swift Transformers: Run On-Device LLMs in Apple Devices\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"Let's go!\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n  \\u003cvideo controls title=\\\"Llama 2 (7B) chat model running on an M1 MacB...\"],[\"## Tasks Overview\\n\\nWhen I published tweets showing [Falcon](https:\\u002f\\u002ftwitter.com\\u002fpcuenq\\u002fstatus\\u002f166460...\"],[\"- [Conversion to Core ML](#conversion-to-core-ml). We'll use Llama 2 as a real-life example.\\n- [Opti...\"],[\"## Conversion to Core ML\\n\\nCore ML is Apple's native framework for Machine Learning, and also the nam...\"],[\"2. Use [`exporters`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fexporters), a Python conversion package built on...\"],[\"- If you have to use `coremltools`, use the latest version: `7.0b1`. Despite technically being a bet...\"],[\"There are a few key optimization areas we've identified. They are a very important topic for us and ...\"],[\"### Tokenizers\\n\\nTokenization solves two complementary tasks: adapt text input to the tensor format u...\"],[\"```\\n  \\\"normalizer\\\": {\\n    \\\"type\\\": \\\"Sequence\\\",\\n    \\\"normalizers\\\": [\\n      {\\n        \\\"type\\\": \\\"Prepend\\\"...\"],[\"```\\n\\nIt reads like this: normalization is a sequence of operations applied in order. First, we `Prep...\"],[\"```\\n\\nHowever, you don't usually need to tokenize the input text yourself - the [`Generation` code](h...\"],[\"- Greedy decoding. This is the obvious algorithm: select the token with the highest probability, app...\"],[\"To use it, download a Core ML model from the Hub or create your own, and select it from the UI. All ...\"],[\"### _Appendix: Converting Llama 2 the Hard Way_\\n\\nYou can safely ignore this section unless you've ex...\"],[\"![Llama 2 conversion error](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve...\"],[\"What the error screenshot is telling us is that there's a type mismatch trying to fill the mask tens...\"],[\"Fortunately, `coremltools` coverage for new operations is good and the team reacts very fast.\\n\\n## Re...\"],[\"---\\ntitle: \\\"Making ML-powered web games with Transformers.js\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fml-web-games\\u002f...\"],[\"## Overview\\n\\nBefore we start, let's talk about what we'll be creating. The game is inspired by Googl...\"],[\"### Model architecture\\n\\nWe'll be finetuning [`apple\\u002fmobilevit-small`](https:\\u002f\\u002fhuggingface.co\\u002fapple\\u002fm...\"],[\"3. Defining our [collate function](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain_classes\\u002fdata_collat...\"],[\"### Converting our model to ONNX\\n\\nFortunately, the [🤗 Optimum](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum) ...\"],[\"```\\n\\n2. Run the conversion script (it uses `Optimum` under the hood):\\n\\n    ```bash\\n    python -m scr...\"],[\"```\\n\\nWe can now use this worker in our `App.jsx` file by adding the following code to the `App` comp...\"],[\"```\\n\\n*Woohoo!* 🥳 Although the above code is just a small part of the [final product](https:\\u002f\\u002fgithub....\"],[\"### Quality of life improvements\\n\\nThe original dataset contains 345 different classes, and since our...\"],[\"**PS**: Don't forget to join the [Open Source AI Game Jam](https:\\u002f\\u002fitch.io\\u002fjam\\u002fopen-source-ai-game-j...\"],[\"--\\ntitle: \\\"Deploy MusicGen in no time with Inference Endpoints\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002frun-musicge...\"],[\"### Let's go!\\n\\nFirst, we will duplicate the [facebook\\u002fmusicgen-large](https:\\u002f\\u002fhuggingface.co\\u002ffaceboo...\"],[\"```\\n\\nLet's hear what it sounds like:\\n\\n\\u003caudio controls\\u003e\\n\\u003csource src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002f...\"],[\"```\\n\\nLet's give it a listen:\\n\\n\\u003caudio controls\\u003e\\n\\u003csource src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingf...\"],[\"def __call__(self, data: Dict[str, Any]) -\\u003e Dict[str, str]:\\n        \\\"\\\"\\\"\\n        Args:\\n            da...\"],[\"```\\n\\nTo keep things simple, in this example we are only generating audio from text, and not conditio...\"],[\"```\\n\\nHere's how it sounds like:\\n\\n\\u003caudio controls\\u003e\\n\\u003csource src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggi...\"],[\"--\\n\\ntitle: \\\"Results of the Open Source AI Game Jam\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fgame-jam-first-edition-r...\"],[\"The games were evaluated by their peers and contributors based on three key criteria: **fun, creativ...\"],[\"## Participants Selection: Top 10 🥈🥉🏅\\n\\nOut of the 88 fantastic submissions, these impressive games e...\"],[\"🤖 Used Text To Speech model to generate the voices.\\n\\n🎮👉 https:\\u002f\\u002fzeenaz.itch.io\\u002ffish-dang-rolling-lau...\"],[\"In this sandbox gravity game, you create an expanding universe and try to complete the challenges.\\n\\n...\"],[\"🤖 Elevenlabs - Voice generator\\n\\n🤖 Scenario - Image generator\\n\\n🎮👉 https:\\u002f\\u002fblastergames.itch.io\\u002fgalact...\"],[\"🤖 Used Stable Diffusion\\n\\n🎮👉 https:\\u002f\\u002filumine-ai.itch.io\\u002fdreamlike-hugging-face-open-source-ai-game-ja...\"],[\"Join our Discord Server 👉 **https:\\u002f\\u002fhf.co\\u002fjoin\\u002fdiscord**\\n\\n**Thank you to all the participants, contr...\"],[\"Contrastive Search\\n\\nThis is a companion notebook to the [Hugging Face guest blog post entry about co...\"],[\"```\\n\\n## 3. Contrastive Search:\\n\\n### 3.1. Generating Text with Contrastive Search:\\n\\n\\n```python\\nimport...\"],[\"```\\n\\n#### 4.1.1. Generating Text with Greedy Search:\\n\\n\\n```python\\noutput = model.generate(input_ids, ...\"],[\"```\\n\\n#### 4.2.1. Generating Text with Greedy Search:\\n\\n\\n```python\\noutput = model.generate(input_ids, ...\"],[\"```\\n\\n# TensorFlow\\n\\n⚠️ The TensorFlow version of Contrastive Search is not yet released -- it will be...\"],[\"```\\n\\n### 2.2. Stochastic Methods:\\n\\n\\n```python\\nimport tensorflow as tf\\nfrom transformers import AutoT...\"],[\"```\\n\\n## 4. More Generated Examples:\\n\\n### 4.1. Example One - GPT-2:\\n\\n\\n```python\\n# Load the language m...\"],[\"```\\n\\n#### 4.1.3. Generating Text with Contrastive Search:\\n\\n\\n```python\\noutput = model.generate(input_...\"],[\"```\\n\\n#### 4.2.3. Generating Text with Contrastive Search:\\n\\n\\n```python\\noutput = model.generate(input_...\"],[\"--\\ntitle: 🧨 Accelerating Stable Diffusion XL Inference with JAX on Cloud TPU v5e\\nthumbnail: \\u002fblog\\u002fas...\"],[\"[Google Cloud TPUs](https:\\u002f\\u002fcloud.google.com\\u002ftpu) are custom-designed AI accelerators, which are opt...\"],[\"In this blog post,\\n1. [We describe why JAX + TPU + Diffusers is a powerful framework to run SDXL](#w...\"],[\"#### High-performance throughput for high batch sizes\\n\\nWorkloads can be scaled across multiple devic...\"],[\"```\\n\\nWe'll now load the base SDXL model and the rest of the components required for inference. The d...\"],[\"```\\n\\nThe prompts have to be supplied as tensors to the pipeline, and they always have to have the sa...\"],[\"```\\n\\nWe are now ready to put everything together in a generate function:\\n\\n```python\\ndef generate(\\n  ...\"],[\"```\\n\\nIt now took about 2s to generate the 4 images!\\n\\n## Benchmark\\n\\nThe following measures were obtai...\"],[\"## How does the demo work?\\n\\nThe [demo we showed before](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgoogle\\u002fsdxl) w...\"],[\"--\\ntitle: \\\"Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia\\\"\\nthumbnail: \\u002f...\"],[\"AWS's take to solve this challenge was to design a custom machine learning chip designed for optimiz...\"],[\"You will learn how to: \\n\\n  - [1. Convert your Hugging Face Transformer to AWS Neuron](#1-convert-you...\"],[\"As a first step, we need to install the [Neuron SDK](https:\\u002f\\u002fawsdocs-neuron.readthedocs-hosted.com\\u002fe...\"],[\"```\\n\\nAfter we have installed the Neuron SDK we can load and convert our model. Neuron models are con...\"],[\"```\\n\\nAt the time of writing, the [AWS Neuron SDK does not support dynamic shapes](https:\\u002f\\u002fawsdocs-ne...\"],[\"```\\n\\n## 2. Create a custom `inference.py` script for `text-classification`\\n\\nThe [Hugging Face Infere...\"],[\"```\\n\\nWe are using the `NEURON_RT_NUM_CORES=1` to make sure that each HTTP worker uses 1 Neuron core ...\"],[\"```\\n\\n## 3. Create and upload the neuron model and inference script to Amazon S3\\n\\nBefore we can deplo...\"],[\"```\\n\\nNow we can upload our `model.tar.gz` to our session S3 bucket with `sagemaker`.\\n\\n```python\\nfrom...\"],[\"```\\n\\n## 5. Run and evaluate Inference performance of BERT on Inferentia\\n\\nThe `.deploy()` returns an ...\"],[\"```\\n\\nThe average latency for our BERT model is `5-6ms` for a sequence length of 128.\\n\\u003cbr\\u003e\\n\\u003cfigure cl...\"],[\"```\\n\\n## Conclusion\\n\\nWe successfully managed to compile a vanilla Hugging Face Transformers model to ...\"],[\"--\\ntitle: \\\"How to train your model dynamically using adversarial data\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f88_mn...\"],[\"## Training your model dynamically using adversarial data\\n \\nHere I will walk you through dynamically...\"],[\"This walkthrough will be divided into the following sections:\\n1. Configuring your model\\n2. Interacti...\"],[\"```\\n\\nNow that you have defined the structure of your model, you need to train it on the standard MNI...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fchrisjay-simple-mnist-classification.hf.space\\\" frameBorder=\\\"0\\\" width=\\\"100%\\\" hei...\"],[\"### Putting it all together\\n\\nThe final step is to put all the three components (configuring the mode...\"],[\"This process of fooling and training the model on the adversarially collected data should be repeate...\"],[\"--\\ntitle: \\\"Rocket Money x Hugging Face: Scaling Volatile ML Models in Production​\\\"\\nthumbnail: \\u002fblog\\u002f...\"],[\"We decided to start from a clean slate, assembling both a new team and a new mandate. Our first task...\"],[\"In the beginning, we auditioned a hand-rolled, in-house model hosting solution we had been using for...\"],[\"Once the contract was signed, we began the migration of moving off our regex based system to direct ...\"],[\"Speaking of scale, as we started to witness a significant increase in traffic to our model, it becam...\"],[\"_If you want to learn how Hugging Face can manage your ML inference workloads, contact the Hugging F...\"],[\"--\\ntitle: \\\"Leveraging Hugging Face for complex generative AI use cases\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_m...\"],[\"--\\ntitle: \\\"ControlNet in 🧨 Diffusers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fcontrolnet\\u002fthumbnail.png \\nauthors:\\n- ...\"],[\"Or even  use it as your interior designer.\\n\\n\\u003ctable\\u003e\\n\\u003ctr style=\\\"text-align: center;\\\"\\u003e\\n    \\u003cth\\u003eBefore\\u003c...\"],[\"Also, make some of the famous logos coming to life.\\n\\n\\u003ctable\\u003e\\n\\u003ctr style=\\\"text-align: center;\\\"\\u003e\\n    \\u003ct...\"],[\"Training ControlNet is comprised of the following steps:\\n\\n1. Cloning the pre-trained parameters of a...\"],[\"A sample from the training set for ControlNet-like training looks like this (additional conditioning...\"],[\"Every new type of conditioning requires training a new copy of ControlNet weights. \\nThe paper propos...\"],[\"We will explore different use cases with the `StableDiffusionControlNetPipeline` in this blog post. ...\"],[\"```\\n\\nTo process different conditionings depending on the chosen ControlNet, we also need to install ...\"],[\"```\\n\\nAs we can see, it is essentially edge detection:\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingf...\"],[\"```\\n\\nInstead of using Stable Diffusion's default [PNDMScheduler](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffuse...\"],[\"```\\n\\nInstead of loading our pipeline directly to GPU, we instead enable smart CPU offloading which \\n...\"],[\"```\\n\\nNow we are ready to run the ControlNet pipeline!\\n\\nWe still provide a prompt to guide the image ...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fYiYiXu\\u002ftest-doc-assets\\u002fresolve\\u002fmai...\"],[\"```\\n\\nIt is noticeable that Mr Potato Head is not the best candidate but he tried his best and did a ...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fYiYiXu\\u002ftest-doc-assets\\u002fresolve...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"We welcome you to combine these different elements and share your results with [@diffuserslib](https...\"],[\"If you cannot wait to try out ControlNet directly, we got you covered as well! Simply click on one o...\"],[\"--\\ntitle: Universal Image Segmentation with Mask2Former and OneFormer\\nthumbnail: \\u002fblog\\u002fassets\\u002f127_ma...\"],[\"Image segmentation can largely be split into 3 subtasks - instance, semantic and panoptic segmentati...\"],[\"Over the last years, researchers have come up with several architectures that were typically very ta...\"],[\"[Mask2Former](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fmodel_doc\\u002fmask2former) extends this to i...\"],[\"Note that Mask2Former still needs to be trained on each task separately to obtain state-of-the-art r...\"],[\"```\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\n\\nprocessor = Au...\"],[\"```\\nprediction = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1...\"],[\"```\\nfrom collections import defaultdict\\nimport matplotlib.pyplot as plt\\nimport matplotlib.patches as...\"],[\"```\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f12...\"],[\"# Conclusion\\n\\nThat's it! You now know about the difference between instance, semantic and panoptic s...\"],[\"--\\ntitle: \\\"A Dive into Text-to-Video Models\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f140_text-to-video\\u002fthumbnail.png...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"The text-to-video task faces unique challenges on multiple fronts. Some of these main challenges inc...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"Text2Video-Zero is a text-guided video generation and manipulation framework that works in a fashion...\"],[\"These large datasets experience similar issues to those found in text-to-image datasets. The most co...\"],[\"### Hugging Face Demos\\nAt Hugging Face, our goal is to make it easier to use and build upon state-of...\"],[\"\\u003cgradio-app theme_mode=\\\"light\\\" space=\\\"PAIR\\u002fText2Video-Zero\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\nApart from using demos to...\"],[\"```\\ngit clone https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fdamo-vilab\\u002fmodelscope-text-to-video-synthesis\\ncd modelsc...\"],[\"```\\n\\n### Community Contributions and Open Source Text-to-Video Projects\\nFinally, there are various o...\"],[\"That was it! We are continuing to integrate the most impactful computer vision and multi-modal model...\"],[\"--\\ntitle: \\\"Stable Diffusion XL on Mac with Advanced Core ML Quantization\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fst...\"],[\"For Stable Diffusion XL we’ve done a few things:\\n* Ported the [base model to Core ML](https:\\u002f\\u002fhuggin...\"],[\"## Using SD XL Models from the Hugging Face Hub\\n\\nAs part of this release, we published two different...\"],[\"For reference, these are the performance figures we achieved on different devices:\\n\\n|        Device ...\"],[\"We explored a different alternative instead: **mixed-bit palettization**. Instead of using 6 bits pe...\"],[\"```\\n\\nWhat this tells us is that the original model quality, as measured by PSNR in float16, is about...\"],[\"For visual examples, these are the results on prompt `a high quality photo of a surfing dog` running...\"],[\"Some initial conclusions:\\n- In our opinion, all the images have good quality in terms of how realist...\"],[\"Mixed-bit palettization runs in two phases: _analysis_ and _application_.\\n\\nThe goal of the analysis ...\"],[\"For an introduction to the process, check the [instructions in the repo](https:\\u002f\\u002fgithub.com\\u002fapple\\u002fml...\"],[\"Finally, as mentioned in the introduction, we created a [complete Stable Diffusion XL Core ML pipeli...\"],[\"* [`apple\\u002fml-stable-diffusion`](https:\\u002f\\u002fgithub.com\\u002fapple\\u002fml-stable-diffusion), by Apple. Conversion ...\"],[\"--\\ntitle: \\\"An Introduction to Deep Reinforcement Learning\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f63_deep_rl_intro\\u002f...\"],[\"Deep RL is a type of Machine Learning where an agent learns **how to behave** in an environment **by...\"],[\"In this free course, you will:\\n\\n- 📖 Study Deep Reinforcement Learning in **theory and practice**.\\n- ...\"],[\"If you prefer, you can watch the 📹 video version of this chapter :\\n\\n\\u003ciframe width=\\\"560\\\" height=\\\"315\\\"...\"],[\"## **What is Reinforcement Learning?**\\n\\nTo understand Reinforcement Learning, let’s start with the b...\"],[\"**Without any supervision**, the child will get better and better at playing the game.\\n\\nThat’s how h...\"],[\"- Our Agent receives **state \\\\\\\\(S_0\\\\\\\\)** from the **Environment** — we receive the first frame of ou...\"],[\"### **Observations\\u002fStates Space**\\n\\nObservations\\u002fStates are the **information our agent gets from the...\"],[\"\\u003e In reality, we use the term state in this course but we will make the distinction in implementatio...\"],[\"Taking this information into consideration is crucial because it will **have importance when choosin...\"],[\"As we can see in the diagram, **it’s more probable to eat the cheese near us than the cheese close t...\"],[\"For instance, think about Super Mario Bros: an episode begin at the launch of a new Mario Level and ...\"],[\"Remember, the goal of our RL agent is to maximize the expected cumulative reward. However, **we can ...\"],[\"- *Exploitation*: You go every day to the same one that you know is good and **take the risk to miss...\"],[\"There are two approaches to train our agent to find this optimal policy π*:\\n\\n- **Directly,** by teac...\"],[\"- *Stochastic*: output **a probability distribution over actions.**\\n\\n\\u003cfigure class=\\\"image table text...\"],[\"“Act according to our policy” just means that our policy is **“going to the state with the highest v...\"],[\"Deep Reinforcement Learning introduces **deep neural networks to solve Reinforcement Learning proble...\"],[\"That was a lot of information, if we summarize:\\n\\n- Reinforcement Learning is a computational approac...\"],[\"---\\nNow that you've studied the bases of Reinforcement Learning, you’re ready to train your first la...\"],[\"Naturally, during the course, **we’re going to use and explain these terms again**, but it’s better ...\"],[\"--\\ntitle: \\\"Scaling up BERT-like model Inference on modern CPU  - Part 2\\\"\\nauthors:\\n- user: echarlaix\\n...\"],[\"Back in April, Intel launched its [latest generation of Intel Xeon processors](https:\\u002f\\u002fwww.intel.com...\"],[\"In this area, Intel plays an essential role by providing software components under the oneAPI umbrel...\"],[\"Some of these libraries, especially MKL or oneDNN, are natively included in frameworks such as PyTor...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"Each pushes forward different approaches to improve aspects of the memory allocation and management ...\"],[\"\\u003cbr\\u003e\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7...\"],[\"Finally, on top of this, one can find some domain specific libraries such as Intel's oneDNN which br...\"],[\"We will also provide an initial baseline showing out-of-the-box results and a second baseline applyi...\"],[\"Also, from the comments we had about the previous blog post, we wanted to change the way we present ...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"\\u003c\\u002ffigure\\u003e\\n\\u003cbr\\u003e\\n\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"r...\"],[\"The global trend highlights the positive impact of the number of cores on the observed latencies. \\nI...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"\\u003c\\u002ffigure\\u003e\\n\\u003cbr\\u003e\\n\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"r...\"],[\"This is often referred to as “tracing” the graph and, as you can see here, the results are not that ...\"],[\"The default memory allocator strategies often rely on global memory pools which require the usage of...\"],[\"#### Memory allocator benchmarks\\n\\nAgain, we first compare performance against frameworks executing i...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"\\u003cfigcaption\\u003eFigure 15. Google's TensorFlow with oneDNN enabled memory allocator and cores scaling la...\"],[\"As per the graph above, you can notice that the standard library allocator (glibc) is often behind p...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"\\u003cfigcaption\\u003eFigure 19. Google's TensorFlow with oneDNN enabled memory allocator and cores scaling la...\"],[\"This time, by knowing the underlying structure of the operator flows and matrix shapes involved then...\"],[\"OpenMP exposes [many environment variables](https:\\u002f\\u002fwww.openmp.org\\u002fspec-html\\u002f5.0\\u002fopenmpch6.html) to ...\"],[\"As stated above, tuning OpenMP is something you can start to tweak when you tried all the other, sys...\"],[\"Fortunately, Intel's [SigOpt](https:\\u002f\\u002fsigopt.com\\u002f), through Bayesian optimization, allows us to make...\"],[\"\\u003ctable class=\\\"block mx-auto\\\"\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd\\u003e\\n        \\u003cfigure class=\\\"image table text-center m-0 w-f...\"],[\"\\u003ctable\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd\\u003e\\n        \\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n            \\u003cmed...\"],[\"As expected, the number of cores is, by far, the most important parameter, but the others play a par...\"],[\"## Conclusion - Accelerating Transformers for Production\\n\\nIn this post, we showed how the new Intel ...\"],[\"--\\ntitle: \\\"Supercharged Searching on the 🤗 Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f48_hubsearch\\u002fthumbnail.png\\na...\"],[\"```\\n\\n## Situating the Problem:\\n\\nFirst, let's imagine the scenario you are in. You'd like to find all...\"],[\"```\\n\\n    Available Attributes or Keys:\\n     * author\\n     * dataset\\n     * language\\n     * library\\n ...\"],[\"```\\n```\\n    ModelInfo: {\\n        modelId: Jiva\\u002fxlm-roberta-large-it-mnli\\n        sha: c6e64469ec4aa1...\"],[\"```\\n\\n## Taking it up a Notch\\n\\nWe saw how we could use the `ModelSearchArguments` and `DatasetSearchA...\"],[\"```\\n```\\n    [ModelInfo: {\\n     \\tmodelId: Jiva\\u002fxlm-roberta-large-it-mnli\\n     \\tsha: c6e64469ec4aa17fe...\"],[\"```\\n\\n\\nVery quickly we see that it's a much more coordinated approach for searching through the API, ...\"],[\"```\\n\\n\\n```python\\n\\u003e\\u003e\\u003e # As an attribute\\n\\u003e\\u003e\\u003e ad.3_c\\n```\\n     File \\\"\\u003cipython-input-6-c0fe109cf75d\\u003e\\\", lin...\"],[\"--\\ntitle: \\\"Optimization story: Bloom inference\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fbloom-inference-pytorch-scri...\"],[\"# Porting to transformers\\n\\nBecause of the original training code, we set out to do something which w...\"],[\"The first model (small-testing) is in `bfloat16` like the big bloom so \\neverything should be very si...\"],[\"```\\nNote: Pipeline Parallelism (PP) means in this context that each GPU will own\\nsome layers so each...\"],[\"```\\n\\nNow we have a workable `transformers` clean version of the start\\nworking on running this.\\n\\nBloo...\"],[\"The number of users we can serve at the same time (throughput)\\nHow long does it take for an average ...\"],[\"```\\n**Note: This is not the best nor the only load testing we used, but it was\\nalways the first to b...\"],[\"Let's do the math and we are getting `17 TFlop` for a single forward pass.\\nLooking at the [specs](ht...\"],[\"```\\nNote: Tensor Parallelism (TP) means in this context that each GPU will own\\npart of the weights, ...\"],[\"```\\n\\nNow that we have a good understanding of where we stand it's time to get to work.\\n\\nWe tried man...\"],[\"Results:\\n\\n  - Porting was not an easy task as some conditions and kernels were hard to\\n    reproduce...\"],[\"## Using ONNX\\u002fTRT or other compiled approaches\\n  - They are supposed to handle most of the optimizat...\"],[\"## DeepSpeed\\n  - This is the technology that powered training, it seemed only fair to use\\n    it for...\"],[\"## Webserver ideas\\n  - Given that we are going to run a free server where users are going to \\n    se...\"],[\"- Next chapter.\\n\\n\\n\\n# Final route: PyTorch + TP + 1 custom kernel + torch.jit.script\\n\\n## Writing more...\"],[\"\\u003cimg src=\\\"assets\\u002fbloom-inference-optimization\\u002fprofiler.png\\\"\\u003e\\nWe see many  `cat` operations before `b...\"],[\"## Low-hanging fruits\\n\\nNow that we had a TP implementation, we could start profiling and optimizing ...\"],[\"```\\n\\nto \\n\\n```python\\n@torch.jit.script\\ndef bloom_gelu_forward(x):\\n    return x * 0.5 * (1.0 + torch.t...\"],[\"```\\n\\nThis transforms the operations from multiple small element-wise kernels (and hence tensor copie...\"],[\"Another place where we had to be extra careful, was the initial forward pass (without\\npast) and the ...\"],[\"```\\n\\nThe first masked fill is creating a new tensor, which is here only to \\nsay to the softmax opera...\"],[\"Then we had to drop the use [generate](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fv4.22.2\\u002fen\\u002fmain_clas...\"],[\"## Have you tried ...?\\n\\nStuff we know exists and haven't used because of various reasons. It \\ncould ...\"],[\"## [OpenAI Triton](https:\\u002f\\u002fopenai.com\\u002fblog\\u002ftriton\\u002f)\\n\\n[Triton](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002ftriton) is a...\"],[\"# Acknowledgments\\n\\nAll this work results of the collaboration of many HF team members. In no particu...\"],[\"--\\ntitle: \\\"Welcome spaCy to the Hugging Face Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f23_spacy\\u002fthumbnail.png\\n\\nau...\"],[\"\\u003cdiv\\u003e\\u003ca class=\\\"text-xs block mb-3 text-gray-300\\\" href=\\\"\\u002fspacy\\u002fen_core_web_sm\\\"\\u003e\\u003ccode\\u003espacy\\u002fen_core_we...\"],[\"\\u003cdiv class=\\\"SVELTE_HYDRATER \\\" data-props=\\\"{&quot;apiUrl&quot;:&quot;https:\\u002f\\u002fapi-inference.huggingfac...\"],[\"Score&quot;,&quot;type&quot;:&quot;f_score&quot;,&quot;value&quot;:0.8379609817}]}},{&quot;tasks&quo...\"],[\"Score&quot;,&quot;type&quot;:&quot;f_score&quot;,&quot;value&quot;:0.893607046}]}},{&quot;tasks&quot...\"],[\"sks&quot;:{&quot;name&quot;:&quot;UNLABELED_DEPENDENCIES&quot;,&quot;type&quot;:&quot;token-classifi...\"],[\"classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Accuracy&quot;,&quot;type&quot;:&q...\"],[\"quot;:&quot;accuracy&quot;,&quot;value&quot;:0.9185392711}]}},{&quot;tasks&quot;:{&quot;name&quot;:&...\"],[\"&quot;:&quot;LABELED_DEPENDENCIES&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metr...\"],[\"uot;metrics&quot;:[{&quot;name&quot;:&quot;Accuracy&quot;,&quot;type&quot;:&quot;accuracy&quot;,&quo...\"],[\"ot;,&quot;value&quot;:0.9185392711}]}}]}]},&quot;cardSource&quot;:true,&quot;id&quot;:&quot;spacy\\u002fen...\"],[\"spacy\\u002fen_core_web_sm&quot;,&quot;pipeline_tag&quot;:&quot;token-classification&quot;,&quot;library_n...\"],[\"ibrary_name&quot;:&quot;spacy&quot;,&quot;modelId&quot;:&quot;spacy\\u002fen_core_web_sm&quot;,&quot;priva...\"],[\"ot;private&quot;:false,&quot;siblings&quot;:[{&quot;rfilename&quot;:&quot;.gitattributes&quot;},{&qu...\"],[\"t;},{&quot;rfilename&quot;:&quot;LICENSE&quot;},{&quot;rfilename&quot;:&quot;LICENSES_SOURCES&quot;}...\"],[\"S&quot;},{&quot;rfilename&quot;:&quot;README.md&quot;},{&quot;rfilename&quot;:&quot;accuracy.json&qu...\"],[\".json&quot;},{&quot;rfilename&quot;:&quot;config.cfg&quot;},{&quot;rfilename&quot;:&quot;en_core_web...\"],[\"core_web_sm-any-py3-none-any.whl&quot;},{&quot;rfilename&quot;:&quot;meta.json&quot;},{&quot;rfilena...\"],[\";rfilename&quot;:&quot;tokenizer&quot;},{&quot;rfilename&quot;:&quot;attribute_ruler\\u002fpatterns&quot;}...\"],[\"s&quot;},{&quot;rfilename&quot;:&quot;lemmatizer\\u002flookups\\u002flookups.bin&quot;},{&quot;rfilename&quot;:&...\"],[\"&quot;:&quot;ner\\u002fcfg&quot;},{&quot;rfilename&quot;:&quot;ner\\u002fmodel&quot;},{&quot;rfilename&quot;:&qu...\"],[\"uot;:&quot;ner\\u002fmoves&quot;},{&quot;rfilename&quot;:&quot;vocab\\u002flookups.bin&quot;},{&quot;rfilename&q...\"],[\"lename&quot;:&quot;vocab\\u002fstrings.json&quot;},{&quot;rfilename&quot;:&quot;vocab\\u002fvectors&quot;}],&quo...\"],[\";}],&quot;tags&quot;:[&quot;en&quot;,&quot;spacy&quot;,&quot;token-classification&quot;,&quot;licens...\"],[\"t;license:mit&quot;,&quot;model-index&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;token-class...\"],[\"en-classification&quot;,&quot;label&quot;:&quot;Token...\"],[\"Classification&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quot;id&quot;:&quot;spacy&quot;,&q...\"],[\"name is Clara and I live in Berkeley, California.&quot;}]},&quot;shouldUpdateUrl&quot;:true}\\\" data-t...\"],[\"0 1-14 14zm0-26a12 12 0 1 0 12 12A12 12 0 0 0 16 4z\\\" fill=\\\"currentColor\\\"\\u003e\\u003c\\u002fpath\\u003e\\u003c\\u002fsvg\\u003e\\u003c\\u002fa\\u003e\\u003c\\u002fdiv\\u003e \\u003cdi...\"],[\"d=\\\"M7.8125 3.66254H8.9V4.75004H7.8125V3.66254Z\\\"\\u003e\\u003c\\u002fpath\\u003e\\u003cpath d=\\\"M8.90001 12.3625H6.72501V9.09998C6.7...\"],[\"6.43667 16.7122 6.72501 16.7125H8.90001C9.18834 16.7122 9.46478 16.5975 9.66867 16.3936C9.87255 16.1...\"],[\"12.4813 12.6814C12.6852 12.4775 12.9617 12.3628 13.25 12.3625H15.425C15.7133 12.3628 15.9898 12.4775...\"],[\"6.15636C9.01496 6.36024 8.9003 6.63668 8.90001 6.92502V8.01252C8.9003 8.30085 9.01496 8.5773 9.21885...\"],[\"1.48752ZM9.98751 8.01252V6.92502H11.075V8.01252H9.98751ZM12.1625 5.83752V2.57502H15.425V5.83752H12.1...\"],[\"5.3148 5.31867 5.51868C5.11478 5.72256 4.83834 5.83723 4.55001 5.83752V5.83752ZM2.37501 2.57502V4.75...\"],[\"### Using existing models\\n\\nAll models from the Hub can be directly installed using `pip install`. \\n\\n...\"],[\"```\\n\\n```python\\n# Using spacy.load().\\nimport spacy\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\n\\n# Importing as...\"],[\"```\\n\\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browse...\"],[\"--\\ntitle: \\\"Porting fairseq wmt19 translation system to transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f07_port...\"],[\"Also, as I did the initial porting with the `en-ru`\\u002f`ru-en` models, I was totally unaware that the `...\"],[\"```\\nmkdir ~\\u002fporting\\ncd ~\\u002fporting\\n```\\n\\nWe need to install a few things for this work:\\n\\n```\\n# install ...\"],[\"```\\n\\n## Files\\n\\nAs a quick overview, the following files needed to be created and written:...\"],[\"* [`src\\u002ftransformers\\u002fconfiguration_fsmt.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002f129fda...\"],[\"* [`tests\\u002ftest_tokenization_fsmt.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002f129fdae04033f...\"],[\"There are other files that needed to be modified as well, we will talk about those towards the end.\\n...\"],[\"```\\nimport torch\\ntorch.hub.load('pytorch\\u002ffairseq', 'transformer.wmt19.en-ru', checkpoint_file='model...\"],[\"```\\nYou may have more than one entry there if you have been using the `hub` for other models.\\n\\nLet's...\"],[\"```\\nwe have:\\n1. `model*.pt` - 4 checkpoints (pytorch `state_dict` with all the pre-trained weights, ...\"],[\"```\\n\\nIf we combine the first two and the last two steps we get 3 stages:\\n\\n1. **Encode input**: break...\"],[\"Let's see how this approach helps to reduce memory and computation requirements. If we have an input...\"],[\"### fairseq's tokenizer workings\\n\\nLet's understand how `fairseq`'s tokenizer works.\\n\\n`fairseq` (*) u...\"],[\"```\\nimport torch\\nsentence = \\\"Machine Learning is great\\\"\\ncheckpoint_file='model4.pt'\\nmodel = torch.hu...\"],[\"```\\n\\nYou can see that `model.encode` does `tokenize+apply_bpe+binarize` - as we get the same output....\"],[\"```\\ne n\\u003c\\u002fw\\u003e 1423551864\\ne r\\u003c\\u002fw\\u003e 1142368899\\nth e\\u003c\\u002fw\\u003e 432025210\\n```\\nIf the second column doesn't includ...\"],[\"```\\n$ grep -i ^mach  ~\\u002fporting\\u002fpytorch_fairseq_model\\u002fbpecodes\\nmach ine\\u003c\\u002fw\\u003e 463985\\nMach t 376252\\nMach...\"],[\"```\\n('apply_bpe: ', 'Mach@@ ine Lear@@ ning is great')\\n('binarize: ', 7, tensor([10217,  1419,     3...\"],[\"```\\nfrom fairseq.data.dictionary import Dictionary\\ndef rewrite_dict_keys(d):\\n    # (1) remove word b...\"],[\"```\\n\\nAfter running the conversion script, let's check the converted dictionary:\\n\\n```\\n$ grep '\\\"Mach\\\"'...\"],[\"```\\n['Mach', 'ine\\u003c\\u002fw\\u003e', 'Lear', 'ning\\u003c\\u002fw\\u003e', 'is\\u003c\\u002fw\\u003e', 'great\\u003c\\u002fw\\u003e']\\n```\\nInstead of marking chunks tha...\"],[\"```\\nand with very few changes I had a working encoder part of the tokenizer. There was a lot of code...\"],[\"```\\n\\nIf you're following along, and would like to see all the changes I did to the original `tokeniz...\"],[\"```\\nJust make sure you're checking out the repository [around the time fsmt was released](https:\\u002f\\u002fgi...\"],[\"```\\n\\nThis was my starting point that I needed to tweak to work with the model weights provided by `f...\"],[\"```\\nFirst I looked at the model:\\n```\\nprint(ru2en[\\\"models\\\"][0])\\n```\\n```\\nTransformerModel(\\n  (encoder)...\"],[\"```\\nwhich looked very similar to BART's architecture, with some slight differences in a few layers -...\"],[\"```\\nargs = dict(vars(ru2en[\\\"args\\\"]))\\npprint(args)\\n```\\n```\\n 'activation_dropout': 0.0,\\n 'activation_f...\"],[\"```\\n    model_conf = {\\n        \\\"architectures\\\": [\\\"FSMTForConditionalGeneration\\\"],\\n        \\\"model_typ...\"],[\"```\\nAll that remains is to save the configuration into `config.json` and create a new `state_dict` d...\"],[\"```\\n\\nWe have the configuration and the model's `state_dict` ported - yay!\\n\\nYou will find the final c...\"],[\"![break point group](.\\u002fassets\\u002f07_porting_fsmt\\u002fpycharm-break-point-groups.png)\\n\\nNow that I have used ...\"],[\"I first did this process for the simpler no-beam search, and once the outputs were 100% matching I r...\"],[\"```\\n     def convert_tokens_to_string(self, tokens):\\n         \\\"\\\"\\\" Converts a sequence of tokens (str...\"],[\"```\\nperl -le 'for $f (@ARGV) { print qq[transformers-cli upload -y $_\\u002f$f --filename $_\\u002f$f] \\\\\\nfor map...\"],[\"```\\ntokenizer = FSMTTokenizer.from_pretrained(\\\"\\u002fcode\\u002fhuggingface\\u002ftransformers-fair-wmt\\u002fdata\\u002fwmt19-en...\"],[\"```\\n\\nThen the are the pipelines, which completely hide all the NLP complexities from the end user an...\"],[\"```\\n$ egrep -l \\\"(BartConfig|BartForConditionalGeneration|BartTokenizer)\\\" src\\u002ftransformers\\u002f*.py \\\\\\n| e...\"],[\"```\\nIn the `grep` search I excluded the files that also include those classes.\\n\\n\\n## Manual testing\\n\\n...\"],[\"## Porting other models\\n\\nI next proceeded to port the `en-de` and `de-en` models. \\n\\nI was surprised ...\"],[\"Just like with the code, I started by copying `tests\\u002ftest_modeling_bart.py` and converting it to use...\"],[\"## SinusoidalPositionalEmbedding\\n\\n`fairseq` used a slightly different implementation of `SinusoidalP...\"],[\"```\\nexport PAIR=ru-en\\nexport MODEL=facebook\\u002fwmt19-$PAIR\\nexport DATA_DIR=data\\u002f$PAIR\\nexport SAVE_DIR=d...\"],[\"```\\nYou can see that the BLEU score was `39.0498` and that it evaluated using 2000 test inputs, prov...\"],[\"## Porting new models\\n\\nAfter uploading the 4 `fairseq` models [here](https:\\u002f\\u002fhuggingface.co\\u002fmodels?f...\"],[\"```\\n# search space\\nexport PAIR=ru-en\\nexport DATA_DIR=data\\u002f$PAIR\\nexport SAVE_DIR=data\\u002f$PAIR\\nexport BS...\"],[\"```\\nYou can see that in the case of `transformers` `early_stopping=False` performs better (`fairseq`...\"],[\"```\\n---\\nlanguage: \\n- en\\n- ru\\nthumbnail:\\ntags:\\n- translation\\n- wmt19\\n- facebook\\nlicense: apache-2.0\\nd...\"],[\"```\\nmake docs\\n```\\nto test that the newly added document was building correctly. The file I needed to...\"],[\"```\\n\\nThen I went to github and submitted this [PR](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fpull\\u002f...\"],[\"## Appreciations\\n\\n- Having [Sam Shleifer](https:\\u002f\\u002fgithub.com\\u002fsshleifer) mentor me through this proce...\"],[\"```\\nc = get_config()\\n# Run all nodes interactively\\nc.InteractiveShell.ast_node_interactivity = \\\"all\\\"...\"],[\"--\\ntitle: \\\"Understanding BigBird's Block Sparse Attention\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f18_big_bird\\u002fattn....\"],[\"**BigBird RoBERTa-like** model is now available in 🤗Transformers. The goal of this post is to give t...\"],[\"---\\n\\nIn this blog post, we will try to answer those questions.\\n\\n### What tokens should be attended t...\"],[\"```\\n\\nNearby tokens should be important because, in a sentence (sequence of words), the current word ...\"],[\"```\\n\\n* **Random tokens:** Select some tokens randomly which will transfer information by transferrin...\"],[\"```\\n\\nThis way, the query token attends only to a subset of all possible tokens while yielding a good...\"],[\"![](assets\\u002f18_big_bird\\u002fgraph.gif)\\n\\u003cimg src=\\\"assets\\u002f18_big_bird\\u002ffull.png\\\" width=230 height=230\\u003e\\n\\n**Bi...\"],[\"In case, we have many global tokens, then we may not need random connections since there will be mul...\"],[\"## BigBird block sparse attention\\n\\nBigBird block sparse attention is just an efficient implementatio...\"],[\"```python\\n# pseudo code\\n\\nQ -\\u003e Query martix (seq_length, head_dim)\\nK -\\u003e Key matrix (seq_length, head_...\"],[\"```\\n\\n### Sliding Attention\\n\\nThe sequence of key tokens is copied 2 times with each element shifted t...\"],[\"```\\n\\n### Random Attention\\n\\nRandom attention is ensuring that each query token will attend a few rand...\"],[\"```\\n\\n**Note:** The current implementation further divides sequence into blocks & each notation is de...\"],[\"![BigBird block sparse attention](assets\\u002f18_big_bird\\u002fq1.png)\\n\\\\\\\\(q_1\\\\\\\\) represents 1st block, \\\\\\\\(g_i\\\\...\"],[\"![BigBird block sparse attention](assets\\u002f18_big_bird\\u002fq_middle.png)\\n\\n---\\n\\nFor calculating attention s...\"],[\"Now, we have covered the hardest part of block sparse attention, i.e. its implementation. Hopefully,...\"],[\"When seqlen = 4096\\n=\\u003e time complexity in BERT = (8 x 512)^2\\n=\\u003e **time complexity in BERT = 64 x 512^...\"],[\"```\\n\\n\\u003c\\u002fdetails\\u003e\\n\\n## ITC vs ETC\\n\\nThe BigBird model can be trained using 2 different strategies: **ITC...\"],[\"|                                              | ITC                                   | ETC        ...\"],[\"## Using BigBird with 🤗Transformers\\n\\nYou can use `BigBirdModel` just like any other 🤗 model. Let's s...\"],[\"```\\n\\nThere are total **3 checkpoints** available in **🤗Hub** (at the point of writing this article):...\"],[\"# very minimal training loop\\nfor e in range(epochs):\\n    for batch in dataset:\\n        model.train()...\"],[\"```\\n\\nIt's important to keep the following points in mind while working with big bird:\\n\\n* Sequence le...\"],[\"You will soon find **BigBird Pegasus-like** model in the library for **long document summarization**...\"],[\"--\\ntitle: \\\"Train your first Decision Transformer\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f101_train-decision-transfo...\"],[\"## What are Decision Transformers?\\n\\nThe Decision Transformer model was introduced by **[“Decision Tr...\"],[\"*Decision Transformer architecture. States, actions, and returns are fed into modality-specific line...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cvideo \\n        alt=\\\"CheetahEd-expert\\\"\\n     ...\"],[\"```\\n\\nWhile most datasets on the hub are ready to use out of the box, sometimes we wish to perform so...\"],[\"def __init__(self, dataset) -\\u003e None:\\n        self.act_dim = len(dataset[0][\\\"actions\\\"][0])\\n        se...\"],[\"# get sequences from dataset\\n            s.append(np.array(feature[\\\"observations\\\"][si : si + self.ma...\"],[\"# padding and state + reward normalization\\n            tlen = s[-1].shape[1]\\n            s[-1] = np....\"],[\"s = torch.from_numpy(np.concatenate(s, axis=0)).float()\\n        a = torch.from_numpy(np.concatenate(...\"],[\"```\\n\\nThat was a lot of code, the TLDR is that we defined a class that takes our dataset, performs th...\"],[\"```\\n\\nThe transformers Trainer class required a number of arguments, defined in the TrainingArguments...\"],[\"```\\n\\nNow that we explained the theory behind Decision Transformer, the Trainer, and how to train it....\"],[\"--\\ntitle: Goodbye cold boot - how we made LoRA Inference 300% faster\\nthumbnail: \\u002fblog\\u002fassets\\u002f171_loa...\"],[\"## LoRA\\n\\nLoRA is a fine-tuning technique that belongs to the family of \\\"parameter-efficient\\\" (PEFT) ...\"],[\"If you look, for example, inside the [Stable Diffusion XL Base 1.0 model repo](https:\\u002f\\u002fhuggingface.c...\"],[\"If you were requesting a LoRA that was not so popular, even if it was based on the SDXL model like t...\"],[\"### LoRA structure\\n\\nIn the Hub, LoRAs can be identified with two attributes:\\n\\n![Hub](https:\\u002f\\u002fhugging...\"],[\"We provide an example below on how one can leverage the Diffusers library to quickly load several Lo...\"],[\"print(f\\\"LoRA adapter loaded and fused to main model, elapsed {elapsed:.2f} seconds\\\")\\n\\n    start = ti...\"],[\"```\\n\\n## Loading figures\\n\\nAll numbers below are in seconds:\\n\\n\\u003ctable\\u003e\\n  \\u003ctr\\u003e\\n    \\u003cth\\u003eGPU\\u003c\\u002fth\\u003e\\n    \\u003ctd\\u003e...\"],[\"To serve inference requests, we use [this open source community image](https:\\u002f\\u002fgithub.com\\u002fhuggingfac...\"],[\"```\\n$ git clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fapi-inference-community.git\\n\\n$ cd api-inference-commu...\"],[\"```\\n\\n### What about batching ?\\n\\nRecently a really interesting [paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2311.032...\"],[\"--\\ntitle: \\\"Open-Source Text Generation & LLM Ecosystem at Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fos_l...\"],[\"![Causal LM Output](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fbl...\"],[\"On the Hugging Face Hub, you can find both causal language models and causal language models fine-tu...\"],[\"The second type of text generation model is commonly referred to as the text-to-text generation mode...\"],[\"### Models created with love by Hugging Face with BigScience and BigCode 💗\\n\\nHugging Face has co-led ...\"],[\"- [Falcon 40B](https:\\u002f\\u002fhuggingface.co\\u002ftiiuae\\u002ffalcon-40b)\\n- [XGen](https:\\u002f\\u002fhuggingface.co\\u002ftiiuae\\u002ffalc...\"],[\"There are two code generation models, [StarCoder by BigCode](https:\\u002f\\u002fhuggingface.co\\u002fmodels?sort=tren...\"],[\"- Another popular family of models is OpenAssistant, some of which are built on Meta's Llama model u...\"],[\"If you're looking to fine-tune a model on an existing instruction dataset, you need to know how a da...\"],[\"| Model                                                                                    | Dataset...\"],[\"| [MPT-30B](https:\\u002f\\u002fhuggingface.co\\u002fmosaicml\\u002fmpt-30b)                                       | Mix of ...\"],[\"| [FLAN-T5-XXL](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fflan-t5-xxl)                                 | [gsm8k]...\"],[\"| [Dolly v2](https:\\u002f\\u002fhuggingface.co\\u002fdatabricks\\u002fdolly-v2-12b)                               | [Dolly]...\"],[\"### Text Generation Inference\\n\\nResponse time and latency for concurrent users are a big challenge fo...\"],[\"![HuggingChat Search](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002f...\"],[\"## Parameter Efficient Fine Tuning (PEFT)\\n\\nIf you’d like to fine-tune one of the existing large mode...\"],[\"--\\ntitle: \\\"Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action\\\" \\nthumbnail: \\u002fblog\\u002fass...\"],[\"1. **Define a model**: Before you can use BentoML, you need a machine learning model (or multiple mo...\"],[\"## Table of contents\\n\\n- [A brief introduction to DeepFloyd IF](#a-brief-introduction-to-deepfloyd-if...\"],[\"## Preparing the environment\\n\\n[This GitHub repository](https:\\u002f\\u002fgithub.com\\u002fbentoml\\u002fIF-multi-GPUs-demo...\"],[\"```\\n\\nBefore building the application, let’s briefly explore the key files within this directory:\\n\\n- ...\"],[\"```\\n\\nOnce the downloads are complete, view the models in the Model store.\\n\\n```bash\\n$ bentoml models ...\"],[\"```\\n\\n## Testing the server\\n\\nOnce the server starts, you can visit the web UI at http:\\u002f\\u002flocalhost:786...\"],[\"```\\n\\nView the Bento in the local Bento Store.\\n\\n```bash\\n$ bentoml list\\n\\nTag                          ...\"],[\"```\\n\\nYou can then deploy the model on Kubernetes.\\n\\n## What’s next?\\n\\n[BentoML](https:\\u002f\\u002fgithub.com\\u002fben...\"],[\"--\\ntitle: \\\"Deep Q-Learning with Space Invaders\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_deep_rl_dqn\\u002fthumbnail.gif...\"],[\"We got excellent results with this simple algorithm. But these environments were relatively simple b...\"],[\"To be able to understand this unit, **you need to understand [Q-Learning](https:\\u002f\\u002fhuggingface.co\\u002fblo...\"],[\"The problem is that Q-Learning is a *tabular method*. Aka, a problem in which the state and actions ...\"],[\"\\u003cimg src=\\\"assets\\u002f63_deep_rl_intro\\u002fdeep.jpg\\\" alt=\\\"Deep Q Learning\\\"\\u002f\\u003e\\n\\n\\nNow that we understand Deep Q-...\"],[\"Why do we stack four frames together?\\nWe stack frames together because it helps us **handle the prob...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fassets\\u002f73_deep_rl_q_part2\\u002fq-ex-5.jpg\\\" alt=\\\"Q Loss\\\"\\u002f\\u003e\\n\\nIn Deep ...\"],[\"### Experience Replay to make more efficient use of experiences\\n\\nWhy do we create a replay memory?\\n\\n...\"],[\"### Fixed Q-Target to stabilize the training\\n\\nWhen we want to calculate the TD error (aka the loss),...\"],[\"At each time step, you’re trying to approach the cow, which also moves at each time step (because yo...\"],[\"The solution is: when we compute the Q target, we use two networks to decouple the action selection ...\"],[\"The leaderboard to compare your results with your classmates 🏆 👉 https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fchris...\"],[\"--\\ntitle: \\\"Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"\\u003cp\\u003eIn 2017 a group of Google AI researchers published a paper introducing the transformer model arch...\"],[\"\\u003cp\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002ftransformers_chrono.png?width=1024&amp;name=transf...\"],[\"\\u003cp\\u003eA timeline showing releases of prominent transformer language models (credit: Hugging Face)\\u003c\\u002fp\\u003e\\n\\u003c...\"],[\"\\u003cp\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002fvit%20diag.png?width=1024&amp;name=vit%20diag.png\\\"...\"],[\"\\u003cp\\u003eAn overview of the ViT model structure as introduced in \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2010.11929...\"],[\"\\u003ch2\\u003eViT models – a perfect fit for IPU\\u003c\\u002fh2\\u003e\\n\\u003cp\\u003eGraphcore IPUs are particularly well-suited to ViT mo...\"],[\"\\u003cp\\u003eFor this blog post, we will use a ViT model pre-trained on ImageNet-21k, based on the paper \\u003ca hr...\"],[\"\\u003cli\\u003eThe complexity of multi-class and multi-label problems such as pulmonary diagnosis is exponentia...\"],[\"\\u003cp\\u003eIf this is your first time using IPUs, read the \\u003ca href=\\\"https:\\u002f\\u002fdocs.graphcore.ai\\u002fprojects\\u002fipu-p...\"],[\"\\u003cp\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002fchest%20x-ray%20examples.png?width=700&amp;name=ch...\"],[\"\\u003cdiv class=\\\"blog-caption\\\" style=\\\"max-height: 100%; max-width: 90%; margin-left: auto; margin-right: ...\"],[\"\\u003c\\u002ful\\u003e\\n\\u003cp\\u003eThe Graphcore Tutorials repository contains the step-by-step tutorial notebook and Python s...\"],[\"\\u003cp style=\\\"font-weight: bold;\\\"\\u003eWe’ve even made it easier and created the HF Optimum Gradient so you c...\"],[\"loading=\\\"lazy\\\" style=\\\"width: 200px; float: left;\\\" width=\\\"200\\\" srcset=\\\"https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs...\"],[\"\\u003cp\\u003e&nbsp;\\u003c\\u002fp\\u003e\\n\\u003cp\\u003e&nbsp;\\u003c\\u002fp\\u003e\\n\\u003ch2\\u003eGetting the dataset\\u003c\\u002fh2\\u003e\\n\\u003ca id=\\\"getting-the-dataset\\\" data-hs-anchor=...\"],[\"\\u003cdiv style=\\\"font-size: 14px; line-height: 1.3;\\\"\\u003e\\n\\u003cscript src=\\\"https:\\u002f\\u002fgist.github.com\\u002fnickmaxfield\\u002f2...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003cp\\u003eWe are going to train the Graphcore Optimum ViT model to predict diseases (defined by \\\"Fin...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003cp\\u003eWhen loading data using the \\u003ccode\\u003edatasets.load_dataset\\u003c\\u002fcode\\u003e function, labels can be pro...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003ch2\\u003eCreating the dataset\\u003c\\u002fh2\\u003e\\n\\u003cp\\u003eWe are now ready to create the PyTorch dataset and split it ...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003cp\\u003eTo fine-tune a pre-trained model, the new dataset must have the same properties as the ori...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003ch2\\u003eVisualising the dataset\\u003c\\u002fh2\\u003e\\n\\u003cp\\u003eTo examine the dataset, we display the first 10 rows of m...\"],[\"\\u003cp\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002fx-ray%20images%20transformed.jpg?width=1024&amp;na...\"],[\"2048w, https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002fx-ray%20images%20transformed.jpg?width=2560&amp;name=x-r...\"],[\"\\u003cdiv class=\\\"blog-caption\\\" style=\\\"max-height: 100%; max-width: 90%; margin-left: auto; margin-right: ...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003cp\\u003eTo use this model on the IPU we need to load the IPU configuration, \\u003ccode\\u003eIPUConfig\\u003c\\u002fcode\\u003e...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003ch2\\u003eImplementing a custom performance metric for evaluation\\u003c\\u002fh2\\u003e\\n\\u003cp\\u003eThe performance of multi-...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003cp\\u003eTo train the model, we define a trainer using the \\u003ccode\\u003eIPUTrainer\\u003c\\u002fcode\\u003e class which take...\"],[\"\\u003cp\\u003eNow we are ready to train.\\u003c\\u002fp\\u003e\\n\\u003cdiv style=\\\"font-size: 14px; line-height: 1.3;\\\"\\u003e\\n\\u003cscript src=\\\"http...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003cp\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002fvit%20output.png?width=1024&amp;name=vit%20...\"],[\"\\u003ch2\\u003eRunning the evaluation\\u003c\\u002fh2\\u003e\\n\\u003cp\\u003eNow that we have trained the model, we can evaluate its ability t...\"],[\"\\u003cp\\u003eIn this post, we have introduced ViT models and have provided a tutorial for training a Hugging F...\"],[\"\\u003cp\\u003e\\u003ca href=\\\"https:\\u002f\\u002fconsole.paperspace.com\\u002fgithub\\u002fgradient-ai\\u002fGraphcore-HuggingFace?machine=Free-IPU...\"],[\"300w, https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002fgradient-badge-gradient-05-d-05.png?width=400&amp;name=gr...\"],[\"\\u003cp\\u003e&nbsp;\\u003c\\u002fp\\u003e\\n\\u003cp\\u003e&nbsp;\\u003c\\u002fp\\u003e\\n\\u003cp\\u003eIf you’re interested in trying Hugging Face Optimum with IPUs on Pape...\"],[\"\\u003c\\u002fdiv\\u003e\\n   \\u003c\\u002farticle\\u003e...\"],[\"--\\ntitle: \\\"Nyströmformer: Approximating self-attention in linear time and memory via the Nyström met...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"As shown in the second line, \\\\\\\\(\\\\hat{P}\\\\\\\\) can be expressed as a product of three matrices. The reas...\"],[\"## How can we adapt the Nyström method to approximate self-attention?\\n\\nInstead of sampling from \\\\\\\\(S...\"],[\"This is the Nyström approximation of the softmax matrix in the self-attention mechanism. We multiply...\"],[\"## How is Nyströmformer implemented?\\n\\nThe original implementation of Nyströmformer can be found [her...\"],[\"attention_scores = torch.matmul(q_landmarks, key_layer.transpose(-1, -2)) # \\\\tilde{B} before softmax...\"],[\"```\\n\\n\\n## Using Nyströmformer with HuggingFace\\n\\nNyströmformer for Masked Language Modeling (MLM) is a...\"],[\"```\\n\\n\\u003cdiv class=\\\"output stream stdout\\\"\\u003e\\n\\n    Output:\\n    -------------------------------------------...\"],[\"```\\n\\n\\u003cdiv class=\\\"output stream stdout\\\"\\u003e\\n\\n    Output:\\n    -------------------------------------------...\"],[\"--\\ntitle: \\\"Llama 2 is here - get it on Hugging Face\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fllama2\\u002fthumbnail.jpg\\na...\"],[\"## Why Llama 2?\\n\\nThe Llama 2 release introduces a family of pretrained and fine-tuned LLMs, ranging ...\"],[\"If you’ve been waiting for an open alternative to closed-source chatbots, Llama 2-Chat is likely you...\"],[\"| Model | License | Commercial use? | Pretraining length [tokens] | Leaderboard score |\\n| --- | --- ...\"],[\"*we’re currently running evaluation of the Llama 2 70B (non chatty version). This table will be upda...\"],[\"- training and inference scripts and examples\\n- safe file format (`safetensors`)\\n- integrations with...\"],[\"```\\npip install transformers\\nhuggingface-cli login\\n```\\n\\nIn the following code snippet, we show how t...\"],[\"```\\n\\nAnd although the model has *only* 4k tokens of context, you can use techniques supported in `tr...\"],[\"_Note: You might need to request a quota upgrade via email to **[api-enterprise@huggingface.co](mail...\"],[\"First pip install `trl` and clone the script:\\n```bash\\npip install trl\\ngit clone https:\\u002f\\u002fgithub.com\\u002fl...\"],[\"```\\n\\nThen you can run the script:\\n```bash\\npython trl\\u002fexamples\\u002fscripts\\u002fsft_trainer.py \\\\\\n    --model_n...\"],[\"```\\n\\u003cs\\u003e[INST] \\u003c\\u003cSYS\\u003e\\u003e\\n{{ system_prompt }}\\n\\u003c\\u003c\\u002fSYS\\u003e\\u003e\\n\\n{{ user_message }} [\\u002fINST]\\n```\\n\\nThis template fo...\"],[\"```\\n\\nAs you can see, the instructions between the special `\\u003c\\u003cSYS\\u003e\\u003e` tokens provide context for the m...\"],[\"```\\n\\nThe model is stateless and does not \\\"remember\\\" previous fragments of the conversation, we must ...\"],[\"## Additional Resources\\n\\n- [Paper Page](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2307.09288)\\n- [Models on the H...\"],[\"--\\ntitle: \\\"Introducing The World's Largest Open Multilingual Language Model: BLOOM\\\"\\nthumbnail: \\u002fblog...\"],[\"Researchers can [now download, run and study BLOOM](https:\\u002f\\u002fhuggingface.co\\u002fbigscience\\u002fbloom) to inve...\"],[\"--\\ntitle: \\\"Optimum-NVIDIA Unlocking blazingly fast LLM inference in just 1 line of code\\\" \\nthumbnail:...\"],[\"```diff\\n- from transformers.pipelines import pipeline\\n+ from optimum.nvidia.pipelines import pipelin...\"],[\"```\\nYou can also enable FP8 quantization with a single flag, which allows you to run a bigger model ...\"],[\"```\\n\\nFor more details, check out our [documentation](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-nvidia)\\n...\"],[\"### Next steps\\n\\nOptimum-NVIDIA currently provides peak performance for the LLaMAForCausalLM architec...\"],[\"--\\ntitle: \\\"Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA\\\" \\nthumbn...\"],[\"\\u003e We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a ...\"],[\"## Resources\\n\\nThis blogpost and release come with several resources to get started with 4bit models ...\"],[\"For more information we recommend reading the fundamentals of floating point representation through ...\"],[\"| ![fp8_scheme](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fb...\"],[\"`-1 * 2^(2) * (1 + 2^-1) = -1 * 4 * 1.5 = -6`\\n\\nFor FP4 there is no fixed format and as such one can ...\"],[\"QLoRA tuning is shown to match 16-bit finetuning methods in a wide range of experiments. In addition...\"],[\"```\\n\\n### Quickstart\\n\\nThe basic way to load a model in 4bit is to pass the argument `load_in_4bit=Tru...\"],[\"```\\nThat's all you need!\\n\\nAs a general rule, we recommend users to not manually set a device once th...\"],[\"model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)...\"],[\"```\\n\\n#### Changing the compute dtype\\n\\nAs mentioned above, you can also change the compute dtype of t...\"],[\"```\\n\\nAnd of course, as mentioned in the beginning of the section, all of these components are compos...\"],[\"For text models, at this time of writing, this would include most used architectures such as Llama, ...\"],[\"```\\nNote that if your favorite model is not there, you can open a Pull Request or raise an issue in ...\"],[\"We have also made some benchmarks on the impact of this quantization method on training large models...\"],[\"| Model name                          | Half precision model size (in GB) | Hardware type \\u002f total VR...\"],[\"| decapoda-research\\u002fllama-7b-hf       | 14GB                              | 1xNVIDIA-T4 \\u002f 16GB      ...\"],[\"| decapoda-research\\u002fllama-13b-hf      | 27GB                              | 1xNVIDIA-T4 \\u002f 16GB      ...\"],[\"We have used the recent `SFTTrainer` from TRL library, and the benchmarking script can be found [her...\"],[\"--\\ntitle: \\\"Optimum+ONNX Runtime - Easier, Faster training for your Hugging Face models\\\"\\nthumbnail: \\u002f...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002foptimum_onnxruntime-training\\u002f...\"],[\"```\\nPyTorch: 1.14.0.dev20221103+cu116; ORT: 1.14.0.dev20221103001+cu116; DeepSpeed: 0.6.6; HuggingFa...\"],[\"```\\n\\n## Optimum Library\\n\\nHugging Face is a fast-growing open community and platform aiming to democr...\"],[\"## ONNX Runtime Training\\n\\n[ONNX Runtime](https:\\u002f\\u002fonnxruntime.ai\\u002f) accelerates [large model training]...\"],[\"## ONNX Runtime Training in Optimum\\n\\nOptimum provides an `ORTTrainer` API that extends the `Trainer`...\"],[\"```diff\\n-from transformers import Trainer, TrainingArguments\\n+from optimum.onnxruntime import ORTTra...\"],[\"```\\n\\n## Looking Forward\\n\\nThe Hugging Face team is working on open sourcing more large models and low...\"],[\"## Getting Started\\n\\nWe invite you to check out the links below to learn more about, and get started ...\"],[\"--\\ntitle: \\\"SafeCoder vs. Closed-source Code Assistants\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fsafecoder-vs-closed-...\"],[\"In this post, we'll compare SafeCoder to closed-source services and highlight the benefits you can e...\"],[\"Unfortunately, closed-source code assistant services don't share information about the underlying mo...\"],[\"We also shared the [fine-tuning code](https:\\u002f\\u002fgithub.com\\u002fbigcode-project\\u002fstarcoder\\u002f) on GitHub.\\n \\nEv...\"],[\"Closed-source services rely on the security of the underlying cloud. Whether this works or not for y...\"],[\"--\\ntitle: \\\"Optimizing your LLM in production\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f163_optimize_llm\\u002foptimize_llm....\"],[\"The crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, e...\"],[\"At the time of writing this post, LLMs consist of at least a couple billion parameters. Each paramet...\"],[\"To give some examples of how much VRAM it roughly takes to load a model in bfloat16:\\n\\n-   **GPT3** r...\"],[\"🤗 Transformers does not support tensor parallelism out of the box as it requires the model architect...\"],[\"```\\n```python\\nfrom transformers import AutoModelForCausalLM\\n\\nmodel = AutoModelForCausalLM.from_pretr...\"],[\"```\\n\\n**Output**:\\n```\\nHere is a Python function that transforms bytes to Giga bytes:\\\\n\\\\n```python\\\\nde...\"],[\"```\\n\\n**Output**:\\n```bash\\n29.0260648727417\\n```\\n\\nClose enough to our back-of-the-envelope computation!...\"],[\"```\\n\\nLet's call it now for the next experiment.\\n\\n```python\\nflush()\\n```\\nIn the recent version of the ...\"],[\"```\\n\\nNow what if your GPU does not have 32 GB of VRAM? It has been found that model weights can be q...\"],[\"are changed to\\n\\n$$ Y = X * \\\\text{dequantize}(W); \\\\text{quantize}(W) $$\\n\\nfor every matrix multiplicat...\"],[\"```\\n\\nWe can then load models in 8-bit quantization by simply adding a `load_in_8bit=True` flag to `f...\"],[\"```\\n\\n```python\\nflush()\\n```\\n\\nLet's see what peak GPU memory consumption 4-bit quantization gives. Qua...\"],[\"```\\n\\n**Output**:\\n```\\n9.543574333190918\\n```\\n\\nJust 9.5GB! That's really not a lot for a \\u003e15 billion pa...\"],[\"```\\n\\nOverall, we saw that running OctoCoder in 8-bit precision reduced the required GPU VRAM from 32...\"],[\"Today's top-performing LLMs share more or less the same fundamental architecture that consists of fe...\"],[\"LLMs usually have multiple attention heads, thus doing multiple self-attention computations in paral...\"],[\"with \\\\\\\\( s^a_{ij} \\\\\\\\) and \\\\\\\\( s^b_{ij} \\\\\\\\) being some softmax normalization statistics that need to ...\"],[\"Let's look at a practical example.\\n\\nOur OctoCoder model now gets a significantly longer input prompt...\"],[\"Question: Modify the function so that it returns all input elements when the lists have uneven lengt...\"],[\"```\\nFor demonstration purposes, we duplicate the system by ten so that the input length is long enou...\"],[\"```\\n\\nWe're getting the same output as before, however this time, the model repeats the answer multip...\"],[\"```\\n\\n**Output**:\\n```\\nGenerated in 3.0211617946624756 seconds.\\n Sure. Here is a function that does th...\"],[\"```\\n\\n## 3. The Science Behind LLM Architectures: Strategic Selection for Long Text Inputs and Chat\\n\\n...\"],[\"A LLM based on self-attention, but without position embeddings would have great difficulties in unde...\"],[\"Instead of using fixed position embeddings, others (such as [Devlin et al.](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f18...\"],[\"Without going into too many details, *RoPE* notes that positional information can be encoded into qu...\"],[\"As an alternative, *ALiBi* proposes a much simpler relative position encoding scheme. The relative d...\"],[\"\\u003e Both RoPE and ALiBi are relative positional embeddings that are *not* learned during training, but...\"],[\"Let's run a quick code snippet to show how auto-regressive works in practice. We will simply take th...\"],[\"```\\n\\n**Output**:\\n```\\nshape of input_ids torch.Size([1, 21])\\nshape of input_ids torch.Size([1, 22])\\ns...\"],[\"```\\n\\nAs we can see every time we increase the text input tokens by the just sampled token.\\n\\nWith ver...\"],[\"for _ in range(5):\\n  next_logits, past_key_values = model(next_token_id, past_key_values=past_key_va...\"],[\"```\\n\\n**Output**:\\n```\\nshape of input_ids torch.Size([1, 1])\\nlength of key-value cache 20\\nshape of inp...\"],[\"```\\nUser: How many people live in France?\\nAssistant: Roughly 75 million people live in France\\nUser: ...\"],[\"```\\n\\nIn this chat, the LLM runs auto-regressive decoding twice:\\n- 1. The first time, the key-value c...\"],[\"There is however one catch. While the required peak memory for the \\\\\\\\( \\\\mathbf{QK}^T \\\\\\\\) matrix is s...\"],[\"```\\n\\n**Output**:\\n```\\n7864320000...\"],[\"```\\n\\nRoughly 8 billion float values! Storing 8 billion float values in `float16` precision requires ...\"],[\"The important part to understand here is that reducing the number of key-value attention heads to 1 ...\"],[\"Moreover, the authors of GQA found out that existing model checkpoints can be *uptrained* to have a ...\"],[\"--\\ntitle: \\\"Deploy GPT-J 6B for inference using  Hugging Face Transformers and Amazon SageMaker\\\"\\nthum...\"],[\"There are some hosted solutions to use `GPT-J` for production workloads, like the [Hugging Face Infe...\"],[\"```\\n\\nThe caveat of this example is that it takes a very long time until the model is loaded into mem...\"],[\"*“Saving a model in this way will save the entire module using Python’s [pickle](https:\\u002f\\u002fdocs.python...\"],[\"To create our `torch.load()` compatible model file we load `GPT-J` using Transformers and the `from_...\"],[\"```\\n\\nNow we are able to load our `GPT-J` model with `torch.load()` to run predictions. \\n\\n```python\\nf...\"],[\"```\\n\\n---\\n\\n### Create `model.tar.gz` for the Amazon SageMaker real-time endpoint\\n\\nSince we can load o...\"],[\"If you still want or need to create your own `model.tar.gz`, e.g. because of compliance guidelines, ...\"],[\"```\\n\\nThe `convert_gpt.py` should print out an S3 URI similar to this. `s3:\\u002f\\u002fhf-sagemaker-inference\\u002fg...\"],[\"```\\n\\nIf you want to use your own `model.tar.gz` just replace the `model_uri` with your S3 Uri.\\n\\nThe ...\"],[\"```\\n\\n### Parameterized request\\n\\nThis is an example of a request using a custom parameter, e.g. `min_...\"],[\"```\\n\\n---\\n\\nTo delete your endpoint you can run. \\n\\n```python\\npredictor.delete_endpoint()\\n```\\n\\n## Concl...\"],[\"--\\ntitle: \\\"Visualize proteins on Hugging Face Spaces\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f98_spaces_3dmoljs\\u002fthum...\"],[\"Make sure you have the `gradio` Python package already [installed](\\u002fgetting_started) and basic knowl...\"],[\"```\\n\\n`update`: This is the function that does the processing of our proteins and returns an `iframe`...\"],[\"```\\nThis is a bit clunky to setup but is necessary because of the security rules in modern browsers....\"],[\"```\\nThe styles for `.mol-container` can be used to modify the size of the molecule viewer. \\n\\nThe `bo...\"],[\"```\\nWe use a template literal (denoted by backticks) to store our pdb file in the html document dire...\"],[\"--\\ntitle: \\\"Announcing our new Content Guidelines and Policy\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fcontent-guideli...\"],[\"## Consent as a Core Value\\n\\nAs we prioritize respecting people's rights throughout the development a...\"],[\"--\\ntitle: \\\"Fine-Tune MMS Adapter Models for low-resource ASR\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f151_mms\\u002fmms_ma...\"],[\"**Wav2Vec2** is a pretrained model for Automatic Speech Recognition (ASR) and was released in [Septe...\"],[\"In this blog post, we show how MMS's Adapter training achieves astonishingly low word error rates af...\"],[\"You can find the pretrained-only checkpoints on the 🤗 Hub for model sizes of 300 million parameters ...\"],[\"Three **MMS** checkpoints fine-tuned for speech recognition (ASR) have been released. They include 1...\"],[\"Adapters have a long history in speech recognition and especially **speaker recognition**. In speake...\"],[\"Just like Wav2Vec2 or XLS-R, MMS is fine-tuned using Connectionist Temporal Classification (CTC), wh...\"],[\"```\\n\\nWe strongly suggest to upload your training checkpoints directly to the [🤗 Hub](https:\\u002f\\u002fhugging...\"],[\"```\\n\\n\\n## Prepare Data, Tokenizer, Feature Extractor\\n\\nASR models transcribe speech to text, which mea...\"],[\"The output size of this layer corresponds to the number of tokens in the vocabulary, which we will e...\"],[\"```\\nMany ASR datasets only provide the target text (`'sentence'`) for each audio array (`'audio'`) a...\"],[\"```\\n\\n```python\\nshow_random_elements(common_voice_train.remove_columns([\\\"path\\\", \\\"audio\\\"]), num_exampl...\"],[\"```\\n\\nAlright! The transcriptions look fairly clean. Having translated the transcribed sentences, it ...\"],[\"```\\nLet's look at the processed text labels again.\\n\\n```python\\nshow_random_elements(common_voice_trai...\"],[\"```\\n\\nGood! This looks better. We have removed most special characters from transcriptions and normal...\"],[\"```\\n\\n```python\\ncommon_voice_train = common_voice_train.map(replace_hatted_characters)\\ncommon_voice_t...\"],[\"```\\n\\n```python\\nvocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\\nvocab_dict\\n```\\n\\n```bash...\"],[\"```\\n\\nCool, we see that all letters of the alphabet occur in the dataset (which is not really surpris...\"],[\"```\\n\\n```bash\\n    37\\n```\\n\\nCool, now our vocabulary is complete and consists of 37 tokens, which means...\"],[\"```\\n\\nLet's define an empty dictionary to which we can append the just created vocabulary\\n\\n```python\\n...\"],[\"```\\n\\nIf one wants to re-use the just created tokenizer with the fine-tuned model of this notebook, i...\"],[\"```\\n\\nGreat, you can see the just created repository under `https:\\u002f\\u002fhuggingface.co\\u002f\\u003cyour-username\\u003e\\u002fwa...\"],[\"A `Wav2Vec2FeatureExtractor` object requires the following parameters to be instantiated:\\n\\n-   `feat...\"],[\"```\\n\\nGreat, MMS's feature extraction pipeline is thereby fully defined!\\n\\nFor improved user-friendlin...\"],[\"```\\n\\nIn the example above we can see that the audio data is loaded with a sampling rate of 48kHz whe...\"],[\"```\\n\\nLet's take a look at `\\\"audio\\\"` again.\\n\\n```python\\ncommon_voice_train[0][\\\"audio\\\"]\\n```\\n\\n\\n    {'pat...\"],[\"```\\n\\nGood! Everything looks fine - the data is a 1-dimensional array, the sampling rate always corre...\"],[\"```python\\ndef prepare_dataset(batch):\\n    audio = batch[\\\"audio\\\"]\\n\\n    # batched output is \\\"un-batche...\"],[\"```\\n\\nLet's apply the data preparation function to all examples.\\n\\n```python\\ncommon_voice_train = comm...\"],[\"```\\n\\n**Note**: `datasets` automatically takes care of audio loading and resampling. If you wish to i...\"],[\"After having fine-tuned the model, we will correctly evaluate it on the test data and verify that it...\"],[\"```python\\nimport torch\\n\\nfrom dataclasses import dataclass, field\\nfrom typing import Any, Dict, List,...\"],[\"batch = self.processor.pad(\\n            input_features,\\n            padding=self.padding,\\n          ...\"],[\"```\\n\\n```python\\ndata_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\\n```\\n\\nNe...\"],[\"```\\n\\nThe model will return a sequence of logit vectors:\\n \\\\\\\\( \\\\mathbf{y}_1, \\\\ldots, \\\\mathbf{y}_m \\\\\\\\) ...\"],[\"```\\n\\nNow, we can load the pretrained checkpoint of [`mms-1b-all`](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fmm...\"],[\"```\\n\\n**Note**: It is expected that some weights are newly initialized. Those weights correspond to t...\"],[\"```\\n\\nIn a final step, we define all parameters related to training.\\nTo give more explanation on some...\"],[\"```\\n\\nNow, all instances can be passed to Trainer and we are ready to start training!\\n\\n```python\\nfrom...\"],[\"```\\n\\n| Training Loss | Training Steps | Validation Loss | Wer    |\\n|:-------------:|:----:|:--------...\"],[\"The adapter weights will be uploaded as part of the model checkpoint, but we also want to make sure ...\"],[\"```\\n\\nFinally, you can upload the result of the training to the 🤗 Hub.\\n\\n```python\\ntrainer.push_to_hub...\"],[\"```\\n\\nOne of the main advantages of adapter weights training is that the \\\"base\\\" model which makes up ...\"],[\"```\\n\\nLet's check that the model can correctly transcribe Turkish\\n\\n```python\\nfrom datasets import Aud...\"],[\"```\\n\\nWe again load the Swedish test set from common voice\\n\\n```python\\ncommon_voice_test_swe = load_da...\"],[\"```\\n\\n**Output**:\\n\\n```bash\\n    Prediction:\\n    jag lämnade grovjobbet åt honom\\n\\n    Reference:\\n    ja...\"],[\"--\\ntitle: \\\"Policy Gradient with PyTorch\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f85_policy_gradient\\u002fthumbnail.gif\\nau...\"],[\"Indeed, since the beginning of the course, we only studied value-based methods, **where we estimate ...\"],[\"## What are Policy-Gradient Methods?\\nPolicy-Gradient is a subclass of Policy-Based Methods, a catego...\"],[\"But Deep Q-Learning is excellent! Why use policy gradient methods?\\n\\n### The Advantages of Policy-Gra...\"],[\"Under a value-based RL algorithm, we learn a quasi-deterministic policy (\\\"greedy epsilon strategy\\\")....\"],[\"👉 If you want to go deeper on the why the advantages and disadvantages of Policy Gradients methods, ...\"],[\"The Reinforce algorithm works like this:\\nLoop: \\n- Use the policy \\\\\\\\(\\\\pi_\\\\theta\\\\\\\\)  to collect an epi...\"],[\"Now that we studied the theory behind Reinforce, **you’re ready to code your Reinforce agent with Py...\"],[\"In the next unit, we’re going to learn about a combination of Policy-Based and Value-based methods c...\"],[\"--\\ntitle: \\\"Hugging Face Platform on the AWS Marketplace: Pay with your AWS Account\\\"\\nthumbnail: \\u002fblog...\"],[\"## Getting Started\\n\\nBefore you can connect your AWS Account with your Hugging Face account, you need...\"],[\"![Marketplace Redirect](assets\\u002f158_aws_marketplace\\u002f03_redirect.jpg \\\"Marketplace Redirect\\\")\\n\\nAfter cl...\"],[\"Pricing for Hugging Face Platform through the AWS marketplace offer is identical to the [public Hugg...\"],[\"--\\ntitle: \\\"Practical 3D Asset Generation: A Step-by-Step Guide\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-g...\"],[\"\\u003cgradio-app theme_mode=\\\"light\\\" space=\\\"hysts\\u002fShap-E\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\nEnter \\\"Dilapidated Shack\\\" as your...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f124_ml-...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f124_ml-...\"],[\"--\\ntitle: \\\"Making LLMs lighter with AutoGPTQ and transformers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f159_autogptq...\"],[\"This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs.\\n\\n## Table of contents...\"],[\"## Resources\\n\\nThis blogpost and release come with several resources to get started with GPTQ quantiz...\"],[\"The benefits of this scheme are twofold:\\n\\n- Memory savings close to x4 for int4 quantization, as the...\"],[\"This means that we can quantize each row independently. This is called per-channel quantization. For...\"],[\"Since the AutoGPTQ library has a larger coverage of transformers models, we decided to provide an in...\"],[\"```\\n\\nCheck out the Transformers [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmai...\"],[\"| gptq  | act_order | bits | group_size | kernel            | Load time (s) | Per-token latency (ms)...\"],[\"Quantizing 🤗 Transformers models with the GPTQ method can be done in a few lines:\\n\\n```python\\nfrom tr...\"],[\"```\\n\\nQuantizing a model may take a long time. Note that for a 175B model, at least 4 GPU-hours are r...\"],[\"## **Fine-tune quantized models with PEFT**\\n\\nYou can not further train a quantized model using the r...\"],[\"On the quantization side, let’s emphasize again that this method only quantizes the weights. There h...\"],[\"This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs, which is a huge step ...\"],[\"## Acknowledgements\\n\\nWe would like to thank [William](https:\\u002f\\u002fgithub.com\\u002fPanQiWei) for his support a...\"],[\"--\\ntitle: \\\"Introducing the Data Measurements Tool: an Interactive Tool for Looking at Datasets\\\"\\nthum...\"],[\"## What is the 🤗 Data Measurements Tool?\\nThe [Data Measurements Tool (DMT)](https:\\u002f\\u002fhuggingface.co\\u002fs...\"],[\"A new wave of research in AI has called for a fundamental paradigm shift in how the field approaches...\"],[\"Despite this, there are few tools openly available to the public to enable people from different dis...\"],[\"### Descriptive Statistics\\n**To look at the surface characteristics of the dataset**\\n\\n*This begins t...\"],[\"You can use this to figure out whether your dataset represents language as it tends to behave in the...\"],[\"- The [normalized pointwise mutual information (nPMI)](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fPointwise_mutua...\"],[\"--\\ntitle: \\\"New ViT and ALIGN Models From Kakao Brain\\\" \\nthumbnail: \\u002fblog\\u002f\\u002fassets\\u002f132_vit_align\\u002fthumbn...\"],[\"This blog will introduce the new [COYO](https:\\u002f\\u002fgithub.com\\u002fkakaobrain\\u002fcoyo-dataset) dataset, Kakao B...\"],[\"## COYO DATASET\\n\\n\\u003cp\\u003e\\n\\u003ccenter\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-im...\"],[\"| COYO | LAION 2B| ALIGN 1.8B |\\n| :----: | :----: | :----: |\\n| Image-text similarity score calculate...\"],[\"## How ViT and ALIGN work\\n\\nSo what do these models do? Let's breifly discuss how the ViT and ALIGN m...\"],[\"[Google then introduced ALIGN](https:\\u002f\\u002fai.googleblog.com\\u002f2021\\u002f05\\u002falign-scaling-up-visual-and-vision....\"],[\"## How to use the COYO dataset\\nWe can conveniently download the `COYO` dataset with a single line of...\"],[\"```\\n\\nWhile it is significantly smaller than the `LAION` dataset, the `COYO` dataset is still massive...\"],[\"```shell\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\n\\u003e\\u003e\\u003e dataset = load_dataset('kakaobrain\\u002fcoyo-700m', s...\"],[\"```\\n\\n## How to use ViT and ALIGN from the Hub\\nLet’s go ahead and experiment with the new ViT and ALI...\"],[\"```\\n\\nThe rest is simple, we will forward preprocess the image and use it as input to the model to re...\"],[\"```\\n\\nAnd we are done! To make things even easier and shorter, we can also use the convenient image c...\"],[\"```\\n\\nIf you want to experiment more with the Kakao Brain ViT model, head over to its [Space](https:\\u002f...\"],[\"```\\n\\nWe will start with zero-shot image classification first. To do this, we will suppy candidate la...\"],[\"```\\n\\nAlternatively, we can use the stand-along vision and text encoders of ALIGN to retrieve multi-m...\"],[\"```\\n\\nLet's do the same with `AlignVisionModel` and retrieve the multi-modal embedding of an image.\\n\\n...\"],[\"```\\n\\nSimilar to ViT, we can use the zero-shot image classification [pipeline](https:\\u002f\\u002fhuggingface.co...\"],[\"```\\n\\n## Conclusion\\n\\nThere have been incredible advances in multi-modal models in recent years, with ...\"],[\"--\\ntitle: \\\"Smaller is better: Q8-Chat, an efficient generative AI experience on Xeon\\\"\\nthumbnail: \\u002fbl...\"],[\"In a nutshell, quantization rescales model parameters to smaller value ranges. When successful, it s...\"],[\"\\u003ckbd\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog...\"],[\"Now, let’s see how SmoothQuant works when applied to popular LLMs.\\n\\n## Quantizing LLMs with SmoothQu...\"],[\"The table below presents a summary of their findings. The second column shows the ratio of benchmark...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cvideo \\n        alt=\\\"MPT-7B Demo\\\"\\n        st...\"],[\"## Next steps\\n\\nWe’re currently working on integrating these new quantization techniques into the Hug...\"],[\"--\\ntitle: \\\"An overview of inference solutions on Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f116_inference...\"],[\"Here's a sentence similarity example with the `sentence-transformers\\u002fall-MiniLM-L6-v2` [model](https...\"],[\"```\\ncurl https:\\u002f\\u002fapi-inference.huggingface.co\\u002fmodels\\u002fxlm-roberta-base \\\\\\n\\t-X POST \\\\\\n\\t-d '{\\\"inputs\\\": \\\"...\"],[\"```\\n\\nThe Inference API is the simplest way to build a prediction service that you can immediately ca...\"],[\"\\u003ckbd\\u003e\\n  \\u003cimg src=\\\"assets\\u002f116_inference_update\\u002fendpoints.png\\\"\\u003e\\n\\u003c\\u002fkbd\\u003e\\n\\nTo learn more about Inference ...\"],[\"--\\ntitle: \\\"Hugging Face's TensorFlow Philosophy\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f96_tensorflow_philosophy\\u002fth...\"],[\"```\\n\\nThis one line will instantiate the model architecture and load the weights, giving you an exact...\"],[\"```\\n\\nNow our `model` has an output head and, optionally, a loss function appropriate for its new tas...\"],[\"# Let's load some data and tokenize it\\ntest_strings = [\\\"This is a sentence!\\\", \\\"This is another one!\\\"...\"],[\"```\\n\\nThis is just a taste of the library, of course - if you want more, you can check out our [noteb...\"],[\"```\\n\\nAnd if you want to train that model instead, it's just:\\n\\n```py\\nmodel.fit(my_data, my_labels)\\n``...\"],[\"```\\n\\n#### Philosophy #2: Loss functions are provided by default, but can be easily changed.\\n\\nIn Kera...\"],[\"```\\n\\nBut also, and very importantly, we want to get out of your way as soon as you want to do someth...\"],[\"```\\n\\nIn the past, we instead asked users to pass labels in the input dict when using the default los...\"],[\"# Load and compile our model\\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\\\"bert-base...\"],[\"```\\n\\nThis approach is great when it works, but for larger datasets you might find it starting to bec...\"],[\"```\\nWhy is [prepare_tf_dataset()](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmain_classes\\u002fmode...\"],[\"```\\n\\nWe’ve made a number of major improvements recently in this area. Most significantly, we’ve upda...\"],[\"One major obstacle in deploying NLP models, however, is that inputs will still need to be tokenized,...\"],[\"```\\n\\n#### Conclusion: We’re an open-source project, and that means community is everything\\n\\nMade a c...\"],[\"```\\n\\nI think the fact that there’s no distinction between big famous foundation models and models fi...\"],[\"\\u003csmall\\u003e(And if you can make a meme to troll the PyTorch team with after your cool new feature is mer...\"],[\"--\\ntitle: The Age of Machine Learning As Code Has Arrived\\nthumbnail: \\u002fblog\\u002fassets\\u002f31_age_of_ml_as_co...\"],[\"Well, here's what I think.\\n\\n\\n### Machine Learning For The Masses!\\n\\nMachine Learning is everywhere, o...\"],[\"There's no need to reinvent the wheel either. The DevOps movement solved these problems over 10 year...\"],[\"---\\n\\n### Transformers! Transformers! Transformers! ([Ballmer style](https:\\u002f\\u002fwww.youtube.com\\u002fwatch?v=...\"],[\"It's a Good Thing in so many ways. State of the art is constantly advancing, and hardly anyone can k...\"],[\"We believe in built-in best practices. \\n\\nWe believe in making infrastructure as transparent as possi...\"],[\"--\\ntitle: \\\"Introducing Storage Regions on the HF Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f172_regions\\u002fthumbnail....\"],[\"Any repo (model or dataset) stored in a non-default location will display its Region directly as a t...\"],[\"--\\ntitle: \\\"Announcing the 🤗 AI Research Residency Program\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f57_ai_residency\\u002fr...\"],[\"We are actively working to build a culture that values diversity, equity, and inclusivity. We are in...\"],[\"--\\ntitle: \\\"Hugging Face Reads, Feb. 2021 - Long-range Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f14_long_...\"],[\"In particular, one issue has been at the center of the efforts: the quadratic cost in memory and tim...\"],[\"Iz Beltagy, Matthew E. Peters, Arman Cohan\\n\\nLongformer addresses the memory bottleneck of transforme...\"],[\"Longformer uses different attention patterns for autoregressive language modeling, encoder pre-train...\"],[\"#### Main findings\\n\\n* The authors proposed the dilated windowed self-attention (Figure c) and showed...\"],[\"Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap\\n\\n[Transformer-XL (2019)](ht...\"],[\"A compression factor \\\\\\\\(c\\\\\\\\) (equal to 3 in the illustration) is chosen to decide the rate at which ...\"],[\"#### Follow-up questions\\n\\n* Compressive Transformer requires a special optimization schedule in whic...\"],[\"The theoretical foundations of the proposed approach are based on the Johnson-Lindenstrauss lemma. L...\"],[\"\\u003cfigure\\u003e\\n  \\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f14_long_range_transformers\\u002fLinformer.png\\\" alt=\\\"Linformer performan...\"],[\"$$\\\\text{softmax}(Q * K) \\\\sim Q’ * K’ = \\\\phi(Q) * \\\\phi(K)$$\\n\\n, where \\\\\\\\(phi\\\\\\\\) is a non-linear suitab...\"],[\"## Reading group discussion\\n\\nThe developments in pre-trained transformer-based language models for n...\"],[\"All these works highlight the importance of long-range inputs modeling in natural language. In the i...\"],[\"## @Hugging Face 🤗: Long-range modeling\\n\\nThe Longformer implementation and the associated open-sourc...\"],[\"--\\ntitle: \\\"VQ-Diffusion\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f117_vq_diffusion\\u002fthumbnail.png\\nauthors:\\n- user: wi...\"],[\"```\\n\\n![png](assets\\u002f117_vq_diffusion\\u002fvq_diffusion_teddy_bear_pool.png)\\n\\n### Architecture\\n\\n![svg](asse...\"],[\"VQ-Diffusion uses a pre-trained VQ-VAE which was frozen during the diffusion training process.\\n\\n####...\"],[\"The AR models section provides additional context on VQ-Diffusion's architecture in comparison to AR...\"],[\"There is a smaller amount of literature covering discrete diffusion models than continuous diffusion...\"],[\"AR image generative models have evolved architecturally with much work towards making transformers c...\"],[\"[Image Transformer](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1802.05751) uses transformers by restricting self attentio...\"],[\"Despite having made tremendous strides, AR models still suffer from linear decreases in inference sp...\"],[\"[Improved Vector Quantized Diffusion Models](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2205.16007) improves upon VQ-Diff...\"],[\"--\\ntitle: Zero-shot image segmentation with CLIPSeg\\nthumbnail: \\u002fblog\\u002fassets\\u002f123_clipseg-zero-shot\\u002fth...\"],[\"Image segmentation is a well-known task within the field of computer vision. It allows a computer to...\"],[\"## CLIP: the magic model behind CLIPSeg\\n\\n[CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmod...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"## CLIPSeg: image segmentation with CLIP\\n\\n[CLIPSeg](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2112.10003) is a model tha...\"],[\"One interesting feature of CLIPSeg is that both the query (the image we want to segment) and the pro...\"],[\"```\\n\\nTo download the model, simply instantiate it.\\n\\n```python\\nfrom transformers import CLIPSegProces...\"],[\"```\\n\\nNow that we have our inputs, we can process them and input them to the\\nmodel.\\n\\n```python\\nimport...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-6\\u002f12\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"```\\n\\n```python\\n_, ax = plt.subplots(1, 2, figsize=(6, 4))\\n[a.axis('off') for a in ax.flatten()]\\nax[0...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"```\\n\\n```python\\nfrom segments import SegmentsClient\\nfrom getpass import getpass\\n\\napi_key = getpass('E...\"],[\"```\\n\\nNow we can use CLIPSeg on the image as before. This time, we\\\\'ll also\\nscale up the outputs so t...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"```\\n\\nLet\\\\'s quickly visualize the result.\\n\\n```python\\nplt.imshow(inds)\\n```\\n\\n\\u003cfigure class=\\\"image tabl...\"],[\"```\\n\\nIf you take a look at the [uploaded prediction on\\nSegments.ai](https:\\u002f\\u002fsegments.ai\\u002fadmin-tobias...\"],[\"If you’re interested in learning how to fine-tune a state-of-the-art segmentation model, check out o...\"],[\"--\\ntitle: \\\"Getting Started with Sentiment Analysis on Twitter\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f85_sentiment_...\"],[\"Buckle up and enjoy the ride! 🤗\\n\\n## What is Sentiment Analysis?\\n\\nSentiment analysis uses [machine le...\"],[\"Luckily, recent advancements in AI allowed companies to use machine learning models for sentiment an...\"],[\"## How to do Twitter sentiment analysis with code?\\n\\nNowadays, getting started with sentiment analysi...\"],[\"```\\n\\n2. Setting up Twitter credentials\\n\\nThen, you need to set up the [Twitter API credentials](https...\"],[\"```\\n\\n4. Analyzing tweets with sentiment analysis\\n\\nNow that you have data, you are ready to analyze t...\"],[\"```\\n\\nNext, you will create the API call using the `model id` and `hf_token`:\\n\\n```python\\nAPI_URL = \\\"h...\"],[\"```\\n\\nResults:\\n\\n```\\n@thenotionbar @hypefury @NotionHQ That’s genuinely smart. So basically you’ve set...\"],[\"```\\n\\nIt's cool to see that 50% of all tweets are positive and only 8.2% are negative:\\n\\n\\u003cfigure class...\"],[\"As a last step, let's create some wordclouds to see which words are the most used for each sentiment...\"],[\"```\\n\\nCuriously, some of the words that stand out from the positive tweets include \\\"notes\\\", \\\"cron\\\", a...\"],[\"## How to do Twitter sentiment analysis without coding?\\n\\nTo get started with sentiment analysis, you...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"Once you have your model ID and your Hugging Face token ID, go back to your Zap and follow these ins...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"Then, follow these instructions to configure this last step:\\n1. Select Google Sheets as an app, and ...\"],[\"To turn it on, just click on \\\"Publish\\\" button at the bottom of your screen:\\n\\n\\u003cfigure class=\\\"image ta...\"],[\"Luckily, tools like the [Inference API](https:\\u002f\\u002fhuggingface.co\\u002finference-api) makes it super easy to...\"],[\"--\\ntitle: \\\"Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\\\"...\"],[\"For example, the [IMDB dataset](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fimdb) specifies `en` in the YAML met...\"],[\"*Distribution of language tags for datasets on the hub excluding English*\\n\\nHowever, there is a major...\"],[\"If we switch to the task of finding relevant machine learning models, knowing what languages were in...\"],[\"```\\n\\nHowever, for some of the datasets on the Hub, we might be keen not to download the whole datase...\"],[\"We pass 20 examples to the model representing rows from a dataset. This results in 20 individual lan...\"],[\"For some ISO 639-3 codes, there is no ISO 639-1 equivalent. For these cases we manually specify a ma...\"],[\"As the machine learning librarian at Hugging Face, I continue exploring opportunities for automatic ...\"],[\"--\\ntitle: \\\"Jupyter X Hugging Face\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f135_notebooks-hub\\u002fbefore_after_notebook_...\"],[\"\\u003cfigure\\u003e\\n  \\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f135_notebooks-hub\\u002fbefore_after_notebook_rendering.png\\\" alt=\\\"A side...\"],[\"--\\ntitle: \\\"Gradio is joining Hugging Face!\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f42_gradio_joins_hf\\u002fthumbnail.png...\"],[\"As one of the founders of Gradio, I couldn't be more excited about the next step in our journey. I s...\"],[\"In addition to the shared mission of Gradio and Hugging Face, what delights me is the team that we a...\"],[\"--\\ntitle: \\\"Generating Stories: AI for Game Development #5\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games\\u002f...\"],[\"### Process\\n\\n**Requirements:** I'm using [ChatGPT](https:\\u002f\\u002fopenai.com\\u002fblog\\u002fchatgpt\\u002f) throughout this...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"\\u003e ⚠️ **Limitation:** Using outputs from language models directly may have unintended legal, ethical,...\"],[\"For my simple farming game, this may be an effective approach to producing all the story content for...\"],[\"There are many other models which are not yet publicly accessible. Check out [this](https:\\u002f\\u002fhuggingf...\"],[\"#### In-Game Development\\n\\n**NPCs:** Aside from the clear uses of language models and dialog agents i...\"],[\"--\\ntitle: \\\"Making automatic speech recognition work on large files with Wav2Vec2 in 🤗 Transformers\\\"\\n...\"],[\"```\\n\\n\\n**Wav2Vec2** is a popular pre-trained model for speech recognition.\\nReleased in [September 202...\"],[\"```\\n\\n```python\\nfrom transformers import pipeline\\n\\n# This will work on any of the thousands of models...\"],[\"As it turns out, CTC structure, which is used by Wav2Vec2, can be exploited\\nin order to achieve very...\"],[\"Let's note that you can choose every argument of this technique:\\n\\n```python\\nfrom transformers import...\"],[\"```\\n\\n\\nChunking with stride on LM augmented models\\n-------------------------------------------\\n\\nIn [t...\"],[\"--\\ntitle: \\\"Red-Teaming Large Language Models\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fred-teaming\\u002fthumbnail.png\\naut...\"],[\"Even recent versions of GPT3 produce similarly offensive text when attacked with prompt injection th...\"],[\"Red-teaming can reveal model limitations that can cause upsetting user experiences or enable harm by...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"The caveat in evaluating LLMs for such malicious behaviors is that we don’t know what they are capab...\"],[\"1. Few-shot-prompted LMs with helpful, honest, and harmless behavior are *not* harder to red-team th...\"],[\"Reach out to us (@nazneenrajani @natolambert @lewtun @TristanThrush @yjernite @thomwolf) if you're i...\"],[\"--\\ntitle: \\\"2023, year of open LLMs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fcv_state\\u002fthumbnail.png\\nauthors:\\n- user: ...\"],[\"A **tokenizer** defines how the text from the training dataset is converted to numbers (as a model i...\"],[\"## 🗝️ 2022, from a race for size to a race for data\\nWhat open models were available to the community...\"],[\"2. [OPT](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2205.01068) (Open Pre-trained Transformer)\\nThe OPT [model](ht...\"],[\"4. Smaller or more specialized open LLM\\nSome smaller open-source models were also released, mostly f...\"],[\"This paradigm shift, while probably already known in closed labs took the open science community by ...\"],[\"The first model family in this series was the [LLaMA](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2302.13971) fami...\"],[\"The [MPT models](https:\\u002f\\u002fwww.mosaicml.com\\u002fblog\\u002fmpt-7b), which came out a couple of months later, rel...\"],[\"X-Gen was a bit over-shadowed by the much visible new [LLaMA-2](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2307.0...\"],[\"In parallel, a notable event of the end of the year 2023 was the rise of performances and a number o...\"],[\"**Chat-based fine-tuning** is a variant of supervised fine-tuning, where the annotated data is chat ...\"],[\"Both these methods are relatively easy to implement: you just need to find or generate related datas...\"],[\"**Direct preference optimization** (DPO) is another variation of RLHF, but does not require the trai...\"],[\"At the beginning of 2023, a few datasets for instruction\\u002fchat finetuning were already released. For ...\"],[\"❄️ Winter 2022\\u002f2023: In January this year, the [Human ChatGPT Instruction corpus](https:\\u002f\\u002fhuggingfac...\"],[\"🌱 Spring: In April, BAIR (Berkeley AI Research lab) released [Koala](https:\\u002f\\u002fbair.berkeley.edu\\u002fblog\\u002f...\"],[\"a fine-tune on said dataset. Microsoft then released the [GPT4-LLM](https:\\u002f\\u002fgithub.com\\u002fInstruction-T...\"],[\"of models (Llama, Mistral, ...). In May and June, [Camel-AI](https:\\u002f\\u002fhuggingface.co\\u002fcamel-ai) releas...\"],[\"🌻Summer: In August, [UltraLM](https:\\u002f\\u002fgithub.com\\u002fthunlp\\u002fUltraChat) (a high-performing chat fine-tune...\"],[\"🍂 Autumn: In October, Hugging Face released [Zephyr](https:\\u002f\\u002fhuggingface.co\\u002fHuggingFaceH4\\u002fzephyr-7b-...\"],[\"*Some more specialized datasets (such as [MetaMath](https:\\u002f\\u002fmeta-math.github.io\\u002f) or [MathInstruct](...\"],[\"But what does it mean to merge a model?\\n\\n**Model merging** is a way to fuse the weights of different...\"],[\"You might want to use what is called **parameter efficient fine-tuning** (PEFT).\\nThis technique firs...\"],[\"So, if you reduce the precision, you reduce the memory each model parameter takes in storage, theref...\"],[\"New releases include\\n- A mixture of experts:\\n\\t- [Mixtral](https:\\u002f\\u002fhuggingface.co\\u002fmistralai\\u002fMixtral-8...\"],[\"That's it folks! \\nI hope you enjoyed this year's review, learned a thing or two, and feel as enthusi...\"],[\"--\\ntitle: \\\"The N Implementation Details of RLHF with PPO\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f167_the_n_implemen...\"],[\"- In [Matching Learning Curves](#matching-learning-curves), we show our main contribution: creating ...\"],[\"**Here are the important links:**\\n\\n- 💾 Our reproduction codebase [*https:\\u002f\\u002fgithub.com\\u002fvwxyzjn\\u002flm-hum...\"],[\"- OAI’s dataset was partially corrupted\\u002flost (so we replaced them with similar HF datasets, which ma...\"],[\"1. **The reward model and policy’s value head take input as the concatenation of `query` and `respon...\"],[\"2. So, for example, if `query = \\\"he was quiet for a minute, his eyes unreadable\\\"`., and the `respons...\"],[\"2. **Pad with a special padding token and truncate inputs.** \\n    1. OAI sets a fixed input length f...\"],[\"1. **Note on HF’s transformers — padding token.** According to  ([transformers#2630#issuecomment-578...\"],[\"inputs = tokenizer.pad(\\n        {\\\"input_ids\\\": tokens},\\n        padding=\\\"max_length\\\",\\n        max_len...\"],[\"```\\n    \\n3. **Adjust position indices correspondingly for padding tokens**\\n    1. When calculating t...\"],[\"-35.36577 ]\\n          [ -35.28693   -34.2875    -38.16074  ...  -41.595802  -41.082108\\n            -...\"],[\"```\\n        \\n    3. **Note on HF’s transformers — `position_ids` and `padding_side`.** We can replic...\"],[\"input_ids=input_ids,\\n                attention_mask=attention_mask,\\n                position_ids=pos...\"],[\"```\\n        \\n    4. **Note on HF’s transformers — `position_ids` during `generate`:** during generat...\"],[\"2. **Note on HF’s transformers — sampling could stop at `eos_token`:** in `transformers`, the genera...\"],[\"response_length = 4\\n        temperature = 0.7\\n        pretrained_model = transformers.AutoModelForCa...\"],[\"```\\n        \\n    3. Note that in a more recent codebase https:\\u002f\\u002fgithub.com\\u002fopenai\\u002fsummarize-from-fee...\"],[\"6. **Use different seeds for different processes**\\n    1. When spawning 8 GPU processes to do data p...\"],[\"# Reward Model Implementation Details\\n\\nIn this section, we discuss reward-model-specific implementat...\"],[\"1. **The reward model only outputs the value at the last token.**\\n    1. Notice that the rewards obt...\"],[\"2. Note that in a more recent codebase [*openai\\u002fsummarize-from-feedback*](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002f...\"],[\"2. **Reward head layer initialization**\\n    1. The weight of the reward head is initialized accordin...\"],[\"2. The bias of the reward head is set to 0 ([lm_human_preferences\\u002flanguage\\u002fmodel.py#L254](https:\\u002f\\u002fgi...\"],[\"2. When performing the normalization process, the code first sets `reward_gain=1, reward_bias=0` ([l...\"],[\"$$\\\\begin{aligned}g*\\\\mathcal{N}(\\\\mu_{\\\\mathcal{D}}, \\\\sigma_{\\\\mathcal{D}}) + b &= \\\\mathcal{N}(g*\\\\mu_{\\\\m...\"],[\"5. Note that responses  \\\\\\\\( y \\\\sim \\\\rho(·|x) \\\\\\\\) we generated for the normalization purpose are from...\"],[\"# Policy Training Implementation Details\\n\\nIn this section, we will delve into details, such as layer...\"],[\"1. **Scale the logits by sampling temperature.** \\n    1. When calculating the log probability of res...\"],[\"2. The bias of the reward head is set to 0 ([lm_human_preferences\\u002flanguage\\u002fmodel.py#L254](https:\\u002f\\u002fgi...\"],[\"2. When running `openai\\u002flm-human-preferences`, OAI’s datasets were partially corrupted\\u002flost ([openai...\"],[\"5. **Rejection sampling** \\n    1. Ziegler et al. (2019) suggested, “We use rejection sampling to ens...\"],[\"2. **Run reward model on truncated response:** After the response has been truncated by the token tr...\"],[\"2. Code comment:  “only query humans on responses that pass that function“\\n        4. To give some e...\"],[\"7. **Terminology of the training loop: batches and minibatches in PPO**\\n    1. OAI uses the followin...\"],[\"mini_batch_end = mini_batch_start + mini_batch_size\\n                mini_batch_inds = batch_inds[min...\"],[\"# ____⏩ a forward pass on [3. 2.]\\n        # ⏪ a backward pass on [6. 7. 3. 2.]\\n        # ____⏩ a for...\"],[\"```\\n        \\n8. **Per-token KL penalty**\\n    - The code adds a per-token KL penalty ([lm_human_prefe...\"],[\"- Then the `non_score_reward = beta * kl` , where `beta` is the KL penalty coefficient  \\\\\\\\(\\\\beta\\\\\\\\),...\"],[\"```\\n    \\n    1. In each minibatch, OAI then whitens the reward `whiten(rewards, shift_mean=False)` w...\"],[\"var = tf.Print(var, [var], 'var', summarize=100)\\n            whitened = (values - mean) * tf.rsqrt(v...\"],[\"```\\n        \\n        ```jsx\\n        mean[1.5999999]\\n        var[0.0666666627]\\n        [[0.05080712 0...\"],[\"```\\n        \\n10. **Clipped value function**\\n    1. As done in the original PPO ([baselines\\u002fppo2\\u002fmode...\"],[\"def __init__(self, init_kl_coef, hparams):\\n                self.value = init_kl_coef\\n               ...\"],[\"```\\n        \\n    - For the `sentiment` and `descriptiveness` tasks examined in this work, we have `i...\"],[\"```python\\n### pytorch adam implementation:\\nbias_correction1 = 1 - beta1 ** step\\nbias_correction2 = 1...\"],[\"```\\n\\n- Let’s compare the update equations of pytorch-style and tensorflow-style adam. Following the ...\"],[\"$$\\\\begin{aligned}\\\\text{tensorflow adam:}\\\\quad \\\\theta_t & =\\\\theta_{t-1}-\\\\alpha_t m_t \\u002f\\\\left(\\\\sqrt{v_t...\"],[\"- The equations above highlight that the distinction between pytorch and tensorflow implementation i...\"],[\"- The above figure shows that, if we set the same `eps` in pytorch adam and tensorflow adam, then py...\"],[\"| ratio_mean | 1.0051285 | 1.0105520486831665 | 1.0044583082199097 |\\n    | ratio_var | 0.0007716546 ...\"],[\"- **PyTorch’s `Adam` produces a more aggressive update** for some reason. Here are some evidence:\\n  ...\"],[\"- **Larger models get affected more.** We conducted experiments comparing PyTorch’s `Adam` (codename...\"],[\"![adam_gpt2.png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftrl-internal-testing\\u002fexample-images\\u002fresolve\\u002fmain\\u002frl...\"],[\"# Conclusion\\n\\nIn this work, we took a deep dive into OAI’s original RLHF codebase and compiled a lis...\"],[\"--\\ntitle: \\\"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\\\"\\nthumbnail: \\u002fbl...\"],[\"Distributed training is the key to enable training such large ML models. There have been major recen...\"],[\"In this post we will look at Data Parallelism using ZeRO and more specifically the latest PyTorch fe...\"],[\"Sample FSDP config after running the command `accelerate config`:\\n```bash\\ncompute_environment: LOCAL...\"],[\"```\\n\\n## Multi-GPU FSDP\\n\\nHere, we experiment on the Single-Node Multi-GPU setting. We compare the per...\"],[\"```\\nSample FSDP Run:\\n![Sample FSDP Run](.\\u002fassets\\u002f62_pytorch_fsdp\\u002fsample_fsdp_run.png)\\n\\n\\n| Method | B...\"],[\"### CPU Offloading to enable training humongous models that won’t fit the GPU memory\\n\\nCommand for tr...\"],[\"```\\n\\n| Method | Batch Size Max ($BS) | Num GPUs | Approx Train Time (Hours) | Notes |\\n| --- | --- | ...\"],[\"Table 2: Benchmarking FSDP on GPT-2 XL (1.5B) model\\n\\nFrom Table 2, we can observe that DDP (w and w\\u002f...\"],[\"After creating an instance of this class, users can pass it when creating the Accelerator object.\\n\\nF...\"],[\"We leverage the tracking functionality support in Accelerate to log the train and evaluation peak me...\"],[\"optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr)\\n\\n- model, optimizer, train_dataloade...\"],[\"```\\n\\n- In case of a single model, if you have created optimizer with multiple parameter groups and c...\"],[\"```\\n- In case of multiple models, it is necessary to prepare the models before creating optimizers e...\"],[\"FSDP precisely addresses this by sharding the optimizer states, gradients and model parameters acros...\"],[\"[2] [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f191...\"],[\"[10] [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fz...\"],[\"--\\ntitle: \\\"Hosting your Models and Datasets on Hugging Face Spaces using Streamlit\\\"\\nthumbnail: \\u002fblog...\"],[\"``` python\\nimport streamlit as st\\n\\n# adding the text that will show in the text box as default\\ndefau...\"],[\"```\\n\\nThe inference code returns the generated output, you can print the output using simple ```st.wr...\"],[\"```\\n\\n If you have structured data like mine, you can simply use  ```st.dataframe(df) ``` to show you...\"],[\"--\\ntitle: \\\"Mixture of Experts Explained\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fmoe\\u002fthumbnail.png\\nauthors:\\n- user: ...\"],[\"Let’s dive in!\\n\\n## Table of Contents\\n\\n- [What is a Mixture of Experts?](#what-is-a-mixture-of-expert...\"],[\"## TL;DR\\n\\nMoEs:\\n- Are **pretrained much faster** vs. dense models\\n- Have **faster inference** compar...\"],[\"\\u003cfigure class=\\\"image text-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumenta...\"],[\"Now that we have a rough idea of what a MoE is, let’s take a look at the research developments that ...\"],[\"These works led to exploring a mixture of experts in the context of NLP. Concretely, [Shazeer et al....\"],[\"This setup introduces some challenges. For example, although large batch sizes are usually better fo...\"],[\"$$\\n    \\n2. We only pick the top k\\n\\n$$\\n\\\\text{KeepTopK}(v, k)_i = \\\\begin{cases}\\nv_i & \\\\text{if } v_i \\\\...\"],[\"GShard replaces every other FFN layer with an MoE layer using top-2 gating in both the encoder and t...\"],[\"The GShard paper has contributions by expressing parallel computation patterns that work well for Mo...\"],[\"- The router computation is reduced\\n- The batch size of each expert can be at least halved\\n- Communi...\"],[\"This [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1aGGVHZmtKmcNBbAwa9hbu58DDpIuB5O4?usp=sharin...\"],[\"## What does an expert learn?\\n\\nThe ST-MoE authors observed that encoder experts specialize in a grou...\"],[\"Switch Transformers observed that at a fixed pretrain perplexity, the sparse model does worse than t...\"],[\"One last part to consider when fine-tuning sparse MoEs is that they have different fine-tuning hyper...\"],[\"\\u003cfigure class=\\\"image text-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumenta...\"],[\"With expert parallelism, experts are placed on different workers, and each worker takes a different ...\"],[\"A big downside of MoEs is the large number of parameters. For local use cases, one might want to use...\"],[\"## Open Source MoEs\\n\\nThere are nowadays several open source projects to train MoEs:\\n\\n- Megablocks: h...\"],[\"So, TL;DR, some interesting areas to explore:\\n\\n* Distilling Mixtral into a dense model\\n* Explore mod...\"],[\"- [Adaptive Mixture of Local Experts (1991)](https:\\u002f\\u002fwww.cs.toronto.edu\\u002f~hinton\\u002fabsps\\u002fjjnh91.pdf)\\n- ...\"],[\"- [Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models (May ...\"],[\"## Citation\\n\\n```bibtex\\n@misc {sanseviero2023moe,\\n    author       = { Omar Sanseviero and\\n          ...\"],[\"```\\n\\n```\\nSanseviero, et al., \\\"Mixture of Experts Explained\\\", Hugging Face Blog, 2023.\\n```...\"],[\"--\\ntitle: \\\"An Introduction to Q-Learning Part 1\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f70_deep_rl_q_part1\\u002fthumbnai...\"],[\"So today, we're going to **dive deeper into one of the Reinforcement Learning methods: value-based m...\"],[\"So let's get started!\\n\\n- [What is RL? A short recap](#what-is-rl-a-short-recap)\\n- [The two types of ...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f70_deep_rl_q_part1\\u002fpolicy.jpg...\"],[\"\\u003e But what does it mean to act according to our policy? After all, we don't have a policy in value-b...\"],[\"Consequently, whatever method you use to solve your problem, **you will have a policy**, but in the ...\"],[\"For each state, the state-value function outputs the expected return if the agent **starts at that s...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f70_deep_rl_q_part1\\u002ftwo-types....\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f70_deep_rl_q_part1\\u002fbellman2.j...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f70_deep_rl_q_part1\\u002fbellman4.j...\"],[\"## **Monte Carlo vs Temporal Difference Learning**\\n\\nThe last thing we need to talk about before divi...\"],[\"- We always start the episode **at the same starting point.**\\n- **The agent takes actions using the ...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f70_deep_rl_q_part1\\u002fMC-4p.jpg\\\"...\"],[\"### **Temporal Difference Learning: learning at each step**\\n\\n- **Temporal difference, on the other h...\"],[\"- We just started to train our Value function, so it returns 0 value for each state.\\n- Our learning ...\"],[\"If we summarize:\\n\\n- With Monte Carlo, we update the value function from a complete episode, and so w...\"],[\"---\\nSo that’s all for today. Congrats on finishing this first part of the chapter! There was a lot o...\"],[\"--\\ntitle:  Introducing the Hugging Face LLM Inference Container for Amazon SageMaker\\nthumbnail: \\u002fblo...\"],[\"## What is Hugging Face LLM Inference DLC?\\n\\nHugging Face LLM DLC is a new purpose-built Inference Co...\"],[\"Officially supported model architectures are currently:\\n\\n- [BLOOM](https:\\u002f\\u002fhuggingface.co\\u002fbigscience...\"],[\"Let's get started!\\n\\n## 1. Setup development environment\\n\\nWe are going to use the `sagemaker` python ...\"],[\"```\\n\\nIf you are going to use Sagemaker in a local environment, you need access to an IAM Role with t...\"],[\"```\\n\\n## 2. Retrieve the new Hugging Face LLM DLC\\n\\nCompared to deploying regular Hugging Face models,...\"],[\"```\\n\\n## 3. Deploy Open Assistant 12B to Amazon SageMaker\\n\\n_Note: Quotas for Amazon SageMaker can var...\"],[\"```\\n\\nAfter we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `de...\"],[\"```\\n\\nSageMaker will now create our endpoint and deploy the model to it. This can take 5-10 minutes.\\n...\"],[\"You can find the open api specification of TGI in the [swagger documentation](https:\\u002f\\u002fhuggingface.gi...\"],[\"```\\n\\u003c|prompter|\\u003e[Instruction]\\u003c|endoftext|\\u003e\\n\\u003c|assistant|\\u003e\\n```\\n\\nlets give it a first try and ask about...\"],[\"```\\n\\n## 5. Create Gradio Chatbot backed by Amazon SageMaker\\n\\nWe can also create a gradio application...\"],[\"```\\n\\n```python\\nimport gradio as gr\\n\\n# hyperparameters for llm\\nparameters = {\\n    \\\"do_sample\\\": True,\\n...\"],[\"```\\n\\n![Gradio Chat application](assets\\u002f145_sagemaker-huggingface-llm\\u002fgradio.png \\\"Gradio Chat applica...\"],[\"--\\ntitle: \\\"Fetch Cuts ML Processing Latency by 50% Using Amazon SageMaker & Hugging Face\\\"\\nthumbnail:...\"],[\"Throughout the project, Fetch had weekly calls with the AWS team and received support from a subject...\"],[\"Fetch heavily relied on the ML training features of Amazon SageMaker, particularly its [training job...\"],[\"In addition to its custom ML models, Fetch uses [AWS Deep Learning Containers ](https:\\u002f\\u002faws.amazon.c...\"],[\"Users enjoy the updates too; Fetch has grown from 10 million to 18 million monthly active users sinc...\"],[\"--\\ntitle: \\\"Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator\\\"\\nthumbnail: ...\"],[\"Such large models raise new challenges in terms of memory and speed for both [training](https:\\u002f\\u002fhugg...\"],[\"Moreover, support for [HPU graphs](https:\\u002f\\u002fdocs.habana.ai\\u002fen\\u002flatest\\u002fPyTorch\\u002fInference_on_PyTorch\\u002fInf...\"],[\"## Benchmarks\\n\\nIn this section, we are going to provide an early benchmark of BLOOMZ on Gaudi2, firs...\"],[\"Runs were performed with DeepSpeed-inference in 16-bit precision with 8 devices and using a [key-val...\"],[\"*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, ...\"],[\"### Running inference on a complete dataset\\n\\nThe script we wrote enables using your model to complet...\"],[\"```\\nBatch n°1\\nInput: ['Facebook has released a report that shows what content was most widely viewed...\"],[\"Batch n°3\\nInput: ['A SpaceX Starship rocket prototype has exploded during a pressure test. It was']\\n...\"],[\"Batch n°5\\nInput: ['With the rise of cheap small \\\"Cube Satellites\\\", startups are now']\\nOutput: ['With...\"],[\"```\\n\\nIn the next section, we explain how to use the script we wrote to perform this benchmark or to ...\"],[\"```\\n\\nFor multi-node inference, you can follow [this guide](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fhaban...\"],[\"## Conclusion\\n\\nWe see in this article that **Habana Gaudi2 performs BLOOMZ inference faster than Nvi...\"],[\"---\\n\\nThanks for reading! If you have any questions, feel free to contact me, either through [Github]...\"],[\"--\\ntitle: Deploying 🤗 ViT on Kubernetes with TF Serving\\nthumbnail: \\u002fblog\\u002fassets\\u002f94_tf_serving_kubern...\"],[\"- **Containerizing the application logic**: The application logic\\n  involves a served model that can...\"],[\"**Note**: The code snippets shown in this post can be executed on a Unix terminal\\nas long as you hav...\"],[\"```bash\\n$ MODEL_TAR=model.tar.gz\\n$ MODEL_NAME=hf-vit\\n$ MODEL_VERSION=1\\n$ MODEL_PATH=models\\u002f$MODEL_NA...\"],[\"```\\n\\nBelow, we show how the `models` directory is structured in our case:\\n\\n```bash\\n$ find \\u002fmodels\\n\\u002fm...\"],[\"```\\n\\nWe used the official Docker image of TensorFlow Serving as the base, but\\nyou can use ones that ...\"],[\"```\\n\\n## Running the Docker image locally\\n\\nLastly, you can run the newly built Docker image locally t...\"],[\"```\\n\\nSince we’re using GCR, you need to prefix the\\nDocker image tag ([\\u003cu\\u003enote\\u003c\\u002fu\\u003e](https:\\u002f\\u002fcloud.goo...\"],[\"```\\n\\nGCP offers a variety of machine types to configure the deployment in a\\nway you want. We encoura...\"],[\"```\\n\\nThe `gcloud container clusters get-credentials` command takes care of\\nboth connecting to the cl...\"],[\"Next, we go through the important parts of each of these manifests.\\n\\n**`deployment.yaml`**:\\n\\n```yaml...\"],[\"```\\n\\nYou can configure the names like `tfs-server`, `tfs-k8s` any way you\\nwant. Under `containers`, ...\"],[\"```\\n\\nWe made the service type ‘LoadBalancer’ so the endpoints are\\nexposed externally to the Kubernet...\"],[\"```\\n\\nHPA stands for **H**orizontal **P**od **A**utoscaler. It sets criteria\\nto decide when to scale ...\"],[\"```bash\\n$ kubectl apply -f deployment.yaml\\n$ kubectl apply -f service.yaml\\n$ kubectl apply -f hpa.ya...\"],[\"```\\n\\nWhile using `kubectl` is fine for applying each of the manifests to\\nperform the deployment, it ...\"],[\"```\\n\\nNote down the external IP when it becomes available.\\n\\nAnd that sums up all the steps you need t...\"],[\"```\\n\\nIf you’re interested to know how this deployment would perform if it\\nmeets more traffic then we...\"],[\"# Conclusion\\n\\nIn this post and the associated [repository](https:\\u002f\\u002fgithub.com\\u002fsayakpaul\\u002fdeploy-hf-tf...\"],[\"--\\ntitle: 'Welcome Stable-baselines3 to the Hugging Face Hub 🤗'\\nthumbnail: \\u002fblog\\u002fassets\\u002f47_sb3\\u002fthumb...\"],[\"```\\n\\n### Finding Models\\n\\nWe’re currently uploading saved models of agents playing Space Invaders, Br...\"],[\"```\\n\\n### Sharing a model to the Hub\\nIn just a minute, you can get your saved model in the Hub.\\n\\nFirs...\"],[\"```\\nTry it out and share your models with the community!\\n\\n### What's next?\\n\\nIn the coming weeks and ...\"],[\"And we would love to hear your feedback 💖. 📧 Feel free to [reach us](mailto:thomas.simonini@huggingf...\"],[\"--\\ntitle: \\\"Director of Machine Learning Insights [Part 2: SaaS Edition]\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f67_...\"],[\"### [Omar Rahman](https:\\u002f\\u002fwww.linkedin.com\\u002fin\\u002fomar-rahman-4739713a\\u002f) - Director of Machine Learning ...\"],[\"b. In most large organizations, data is often siloed and not well maintained resulting in significan...\"],[\"Prior to Amplitude, Cao (Danica) was the Global Head of Machine Learning in the Analytics Center of ...\"],[\"\\u003cimg class=\\\"mx-auto\\\" style=\\\"float: left;\\\" padding=\\\"5px\\\" width=\\\"200\\\" src=\\\"\\u002fblog\\u002fassets\\u002f67_ml_director...\"],[\"Once we transcribe a conversation we can look into the content - this is where NLP comes in and we r...\"],[\"#### **3. What’s a common mistake you see people make trying to integrate ML into SaaS?**\\nIs my solu...\"],[\"We are also seeing a lot of tech from NLP entering other domains like speech and vision and being ab...\"],[\"**Fun Fact:**  The first application of ML I used was for Barbie toys. My professor at Schulich Busi...\"],[\"#### **3. What’s a common mistake you see people make trying to integrate ML?**\\nThe most common mist...\"],[\"---\\n\\n🤗   Thank you for joining us in this second installment of ML Director Insights. Stay tuned for...\"],[\"--\\ntitle: \\\"From GPT2 to Stable Diffusion: Hugging Face arrives to the Elixir community\\\" \\nthumbnail: ...\"],[\"Thanks to the concurrency and distribution support in the Erlang Virtual Machine, which Elixir runs ...\"],[\"Several other projects were born from the Nx initiative. [Axon](https:\\u002f\\u002fgithub.com\\u002felixir-nx\\u002faxon) b...\"],[\"* We have also written [single-file Phoenix applications](https:\\u002f\\u002fgithub.com\\u002felixir-nx\\u002fbumblebee\\u002ftre...\"],[\"--\\ntitle: \\\"🐶Safetensors audited as really safe and becoming the default\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f142...\"],[\"```\\n\\nIt also has a number of [cool features](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fsafetensors#yet-another-...\"],[\"```\\n\\nis likely to be the only thing needed to run `safetensors` files safely.\\n\\nGoing forward and tha...\"],[\"Since the Hugging Face Hub is a platform where anyone can upload and share models, it is important t...\"],[\"In the name of openness and transparency, all companies agreed to make the report\\nfully public.\\n\\n[Fu...\"],[\"As for `safetensors` itself, we're looking into adding more advanced features for LLM training,\\nwhic...\"],[\"--\\ntitle: \\\"Federated Learning using Hugging Face and Flower\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002ffl-with-flower...\"],[\"```\\n\\n## Standard Hugging Face workflow\\n\\n### Handling the data\\n\\nTo fetch the IMDB dataset, we will us...\"],[\"tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\\n    tokenized_datasets[\\\"train...\"],[\"```\\n\\n### Training and testing the model\\n\\nOnce we have a way of creating our trainloader and testload...\"],[\"```\\n\\n## Federating the example\\n\\nThe idea behind Federated Learning is to train a model between multi...\"],[\"```\\n\\nThe `get_parameters` function lets the server get the client's parameters. Inversely, the `set_...\"],[\"```\\n\\nThe `weighted_average` function is there to provide a way to aggregate the metrics distributed ...\"],[\"--\\ntitle: \\\"Creating open machine learning datasets? Share them on the Hugging Face Hub!\\\"\\nthumbnail: ...\"],[\"The Hugging Face Hub can help achieve this maximum impact. \\n\\n## What is the Hugging Face Hub?\\n\\nThe [...\"],[\"There are a growing number of tools being created which make it easier to understand datasets hosted...\"],[\"### Community tools \\n\\nAlongside the datasets viewer there are a growing number of community created ...\"],[\"### Support for large datasets\\n\\nThe Hub can host large datasets; it currently hosts datasets with mu...\"],[\"The Hub also has features which allow communities to collaborate more easily. This includes a discus...\"],[\"### How can I share my dataset on the Hugging Face Hub? \\n\\nHere are some resources to help you get st...\"],[\"--\\ntitle: \\\"Assisted Generation: a new direction toward low-latency text generation\\\"\\nthumbnail: \\u002fblog...\"],[\"\\u003c!-- [GIF 1 -- FWD PASS] --\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cvideo\\n        ...\"],[\"From the description above, the latency bottleneck in text generation is clear: running a model forw...\"],[\"```python\\n# Example showcasing the impact of batched generation. Measurement device: RTX3090\\nfrom tr...\"],[\"```\\n\\nFinally, if you have multiple devices available to you, you can distribute the workload using [...\"],[\"## Language decoder forward pass, revisited\\n\\nYou’ve read above that each model forward pass yields t...\"],[\"```\\n\\n\\nThis means that you can use a model forward pass for a different purpose: in addition to feedi...\"],[\"Obviously, there are no latency-free assistant models. Nevertheless, it is relatively easy to find a...\"],[\"Wrapping all up, here’s our original implementation of the assisted generation loop ([code](https:\\u002f\\u002f...\"],[\"We’ve designed the API in 🤗 Transformers such that this process is hassle-free for you. All you need...\"],[\"```\\n\\n\\nIs the additional internal complexity worth it? Let’s have a look at the latency numbers for t...\"],[\"Drawing samples from a probability distribution for the next token will cause our greedy assistant t...\"],[\"Finally, assisted generation resurfaces a crucial question in text generation. The field has been ev...\"],[\"```\\n\\n\\n## Acknowledgements\\n\\nI'd like to thank Sylvain Gugger, Nicolas Patry, and Lewis Tunstall for s...\"],[\"--\\ntitle: \\\"Introducing the Private Hub: A New Way to Build With Machine Learning\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"With this in mind, we launched the [Private Hub](https:\\u002f\\u002fhuggingface.co\\u002fplatform) (PH), a new way to...\"],[\"On the Hugging Face Hub, you’ll be able to create or discover the following ML assets:\\n\\n- [Models](h...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"Now that we have covered the basics, let's dive into the specific characteristics of models, dataset...\"],[\"These models span 180 languages and support up to 25 ML libraries (including Transformers, Keras, sp...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"We provide flexible options for deploying your Private Hub in your private, compliant environment, i...\"],[\"We built the Private Hub to change this. Like Git and GitHub forever changed how companies build sof...\"],[\"For our demo example, one of the requirements for building this ML app for financial analysts is doi...\"],[\"Now that we have a great pre-trained model for financial data, the next step is to fine-tune it usin...\"],[\"Finally, we select the number of candidate models to train with our data. We choose 25 models and vo...\"],[\"In less than 20 minutes, we were able to build an [interactive demo app](https:\\u002f\\u002fhuggingface.co\\u002fspac...\"],[\"Instead of wasting time on Docker\\u002fKubernetes, setting up a server for running these models or optimi...\"],[\"```\\n\\nWith just 12 lines of code, we are up and running in running inferences with an infrastructure ...\"],[\"--\\ntitle: \\\"Speculative Decoding for 2x Faster Whisper Inference\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fwhisper-sp...\"],[\"In this blog post, we demonstrate how Speculative Decoding can be employed to reduce the \\ninference ...\"],[\"While these candidate tokens are generated quickly, they may differ from those predicted by the main...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cvideo\\n        style=\\\"max-width: 90%; margin...\"],[\"The only constraint for selecting an assistant model is that it must share the same vocabulary as th...\"],[\"## English Speech Transcription\\n\\n### Baseline Implementation\\n\\nWe start by benchmarking Whisper [larg...\"],[\"device = \\\"cuda:0\\\" if torch.cuda.is_available() else \\\"cpu\\\"\\ntorch_dtype = torch.float16 if torch.cuda....\"],[\"```\\n\\nLet's load the English speech transcription dataset that we will use for benchmarking. We'll lo...\"],[\"```\\n\\n**Output:**\\n```\\n100%|██████████| 73\\u002f73 [01:37\\u003c00:00,  1.33s\\u002fit]\\n72.99542546272278\\n```\\n\\nAlright!...\"],[\"```\\n**Output:**\\n0.03507271171941831\\n```\\n\\nOur final baseline number is 73 seconds for a WER of 3.5%.\\n...\"],[\"```\\n\\n------------------------------------------------------------------------\\n\\n\\\\\\\\({}^1\\\\\\\\) We intend ...\"],[\"```\\n\\nLet's run the benchmark with speculative decoding, using Distil-Whisper as the assistant to Whi...\"],[\"```\\n**Outputs:**\\n```\\n0.03507271171941831\\n```\\n\\nPerfect! 3.5% WER again, as we have identical outputs ...\"],[\"```\\n\\nAn end-to-end code snippet for running speculative decoding with Whisper and Distil-Whisper can...\"],[\"```\\n\\nFor our benchmarking dataset, we'll load 73 samples from the Dutch (\\\"nl\\\") split of the [VoxPopu...\"],[\"```\\n\\nRight! We have our baseline time of 117 seconds and a WER of 12.8%. Let's re-run the generation...\"],[\"```\\n\\nAgain, we achieve 12.8% WER, but this time in just 62 seconds of inference time, representing a...\"],[\"#### Batch Size\\n\\nIt is worth noting that the largest speed gains with speculative decoding come with...\"],[\"--\\ntitle: \\\"Snorkel AI x Hugging Face: unlock foundation models for enterprises\\\"\\nthumbnail: \\u002fblog\\u002fass...\"],[\"## Foundation models in Snorkel Flow\\n\\nThe Snorkel Flow development platform enables users to [adapt ...\"],[\"Hugging Face’s service allows users to create a model API in a few clicks and begin using it immedia...\"],[\"Clement Delangue, co-founder and CEO, Hugging Face\\n\\n## Conclusion\\n\\nTogether, Snorkel and Hugging Fac...\"],[\"--\\ntitle: \\\"An Introduction to Q-Learning Part 2\\u002f2\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f73_deep_rl_q_part2\\u002fthumbn...\"],[\"So, in the second part, we’ll **study Q-Learning**, **and implement our first RL agent from scratch*...\"],[\"**Q-Learning is the algorithm we use to train our Q-Function**, an **action-value function** that de...\"],[\"Therefore, Q-function contains a Q-table **that has the value of each-state action pair.** And given...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f73_deep_rl_q_part2\\u002fQ-learning...\"],[\"Epsilon Greedy Strategy is a policy that handles the exploration\\u002fexploitation trade-off.\\n\\nThe idea i...\"],[\"Therefore, our \\\\\\\\(Q(S_t, A_t)\\\\\\\\) **update formula goes like this:**\\n\\n  \\u003cfigure class=\\\"image table te...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f73_deep_rl_q_part2\\u002foff-on-1.j...\"],[\"- You're a mouse in this tiny maze. You always **start at the same starting point.**\\n- The goal is *...\"],[\"So, for now, **our Q-Table is useless**; we need **to train our Q-function using the Q-Learning algo...\"],[\"Training timestep 2:\\n\\n**Step 2: Choose action using Epsilon Greedy Strategy**\\n\\n**I take a random act...\"],[\"---\\nNow that we **studied the theory of Q-Learning**, let's **implement it from scratch**. A Q-Learn...\"],[\"Take time to really grasp the material before continuing. \\n\\n  \\nAnd since the best way to learn and a...\"],[\"--\\ntitle: \\\"Overview of natively supported quantization schemes in 🤗 Transformers\\\" \\nthumbnail: \\u002fblog\\u002f...\"],[\"## Resources\\n\\n- [GPTQ blogpost](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fgptq-integration) – gives an overview on...\"],[\"### What are the benefits of bitsandbytes?\\n**easy**: bitsandbytes still remains the easiest way to q...\"],[\"**n-bit support**: The GPTQ algorithm makes it possible to quantize models up to 2 bits! However, th...\"],[\"**works only for language models (for now)**: As of today, the API for quantizing a model with auto-...\"],[\"with batch size = 1: \\n\\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token laten...\"],[\"with batch size = 16:\\n\\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token laten...\"],[\"#### use_cache \\nLet's test `use_cache` to better understand the impact of caching the hidden state d...\"],[\"with a NVIDIA T4: \\n\\n![Benchmark T4](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"From the benchmark above, we can conclude that GPTQ is faster than bitsandbytes independently of the...\"],[\"with 7b model: \\n\\n| model_id                           | Average | ARC   | Hellaswag | MMLU  | Truthf...\"],[\"with 13b model: \\n\\n| model_id                           | Average | ARC   | Hellaswag | MMLU  | Truth...\"],[\"- (1) quantize the base model using bitsandbytes (zero-shot quantization)\\n- (2) add and fine-tune th...\"],[\"--\\ntitle: 'Train and Fine-Tune Sentence Transformers Models'\\nthumbnail: \\u002fblog\\u002fassets\\u002f95_training_st_...\"],[\"This is how the Sentence Transformers models work:\\n\\n1. **Layer 1** – The input text is passed throug...\"],[\"```\\n\\nFrom the code above, you can see that Sentence Transformers models are made up of modules, that...\"],[\"```\\n\\nNow for the most critical part: the dataset format.\\n\\n## How to prepare your dataset for trainin...\"],[\"Most dataset configurations will take one of four forms (below you will see examples of each case):\\n...\"],[\"Note that Sentence Transformers models can be trained with human labeling (cases 1 and 3) or with la...\"],[\"- Case 4: The [Quora Triplets dataset](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fembedding-data\\u002fQQP_triplets) ...\"],[\"```\\n\\nThis guide uses an unlabeled triplets dataset, the fourth case above.\\n\\nWith the `datasets` libr...\"],[\"```\\nYou can see that `query` (the anchor) has a single sentence, `pos` (positive) is a list of sente...\"],[\"```\\n\\nThe next step is to choose a suitable loss function that can be used with the data format.\\n\\n## ...\"],[\"Case 3: When your samples are triplets of the form `[anchor, positive, negative]` and you have an in...\"],[\"The hardest part is choosing a suitable loss function conceptually. In the code, there are only two ...\"],[\"```\\nOnce the dataset is in the desired format and a suitable loss function is in place, fitting and ...\"],[\"```\\n\\nThen, you can share your models by calling the `save_to_hub` method from the trained model. By ...\"],[\"```\\n\\nIn the [Notebook Companion](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fhuggingface\\u002fblog\\u002fblob\\u002fmain...\"],[\"--\\ntitle: \\\"SDXL in 4 steps with Latent Consistency LoRAs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002flcm_sdxl\\u002flcm_thumb...\"],[\"## Contents\\n\\n- [Method Overview](#method-overview)\\n- [Why does this matter](#why-does-this-matter)\\n-...\"],[\"1. Select an available teacher model from the Hub. For example, you can use [SDXL (base)](https:\\u002f\\u002fhu...\"],[\"To gauge the speed difference we are talking about, generating a single 1024x1024 image on an M1 Mac...\"],[\"```\\n\\nNote how the code:\\n- Instantiates a standard diffusion pipeline with the SDXL 1.0 base model.\\n-...\"],[\"```\\n\\nThese are the 8 images displayed in a grid:\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingfa...\"],[\"```\\n\\nThen we can run inference as usual for SDXL. We’ll gather results using varying number of steps...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"prompt = \\\"collage style kid sits looking at the night sky, full of stars\\\"\\n\\ngenerator = torch.Generat...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"## Benchmarks\\n\\nThis section is not meant to be exhaustive, but illustrative of the generation speed ...\"],[\"For cards with a lot of capacity, such as A100, performance increases significantly when generating ...\"],[\"- [`latent-consistency\\u002flcm-sdxl`](https:\\u002f\\u002fhuggingface.co\\u002flatent-consistency\\u002flcm-sdxl). Full fine-tun...\"],[\"pipe.set_adapters([\\\"lora\\\", \\\"toy\\\"], adapter_weights=[1.0, 0.8])\\npipe.to(device=\\\"cuda\\\", dtype=torch.fl...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"We hope these scripts inspire the community to try their own fine-tunes. Please, do let us know if y...\"],[\"- [LoRA the Explorer (experimental LCM version)](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002flatent-consistency\\u002flc...\"],[\"--\\ntitle: \\\"Introducing Skops\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f94_skops\\u002fintroducing_skops.png\\nauthors:\\n- user...\"],[\"```\\n\\nYou can use any model filename and serialization method, like `pickle` or `joblib`. At the mome...\"],[\"```\\n\\nThe repository now contains the serialized model and the configuration file. \\nThe configuration...\"],[\"You can create the model card by instantiating the `Card` class from `skops`. During model serializa...\"],[\"```\\n\\nWe will now evaluate the model and add a description of the evaluation method with `add`. The m...\"],[\"```\\n\\nWe can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub_...\"],[\"```\\n\\nYou can see the example repository pushed with above code [here](https:\\u002f\\u002fhuggingface.co\\u002fscikit-...\"],[\"--\\ntitle: \\\"Run a Chatgpt-like Chatbot on a Single GPU with ROCm\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fchatbot-am...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"- A Linux-based operating system, preferably Ubuntu 18.04 or 20.04\\n\\n- Conda or Docker environment\\n\\n-...\"],[\"```\\nsudo apt update && sudo apt upgrade -y\\nwget https:\\u002f\\u002frepo.radeon.com\\u002famdgpu-install\\u002f5.4.3\\u002fubuntu\\u002f...\"],[\"```\\n**2 Model** **quantization and Model inference (Inside the docker)**\\n\\nYou can either download qu...\"],[\"```\\nNow that you have everything set up, it's time to run the Vicuna 13B\\nmodel on your AMD GPU. Use ...\"],[\"```\\nNow the 4-bit quantized Vicuna-13B model can be fitted in RX6900XT GPU\\nDDR memory, which has 16G...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"- Vicuna 13b – quant (4bit\\u002ffp16): 4bits datatype parameter, fp16 Matmul\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n  \\u003cimg s...\"],[\"**Building Vicuna quantized model from the floating-point LLaMA model**\\n\\n**a. Download LLaMA and Vic...\"],[\"```\\ngit clone https:\\u002f\\u002fgithub.com\\u002flm-sys\\u002fFastChat\\ncd FastChat\\n```\\nConvert the LLaMA parameters by usi...\"],[\"```\\nNow the model is ready and saved as\\n**Vicuna-13b-4bit-act-order.safetensors**.\\n\\n**GPTQ Dequantiz...\"],[\"--\\ntitle: \\\"Zero-shot image-to-text generation with BLIP-2\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fblip-2\\u002fthumbnail...\"],[\"## Introduction\\n\\nRecent years have seen rapid advancements in computer vision and natural language p...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"In the second pre-training stage, the query embeddings now have the relevant visual information to t...\"],[\"```\\n\\nNext, we'll need an input image. Every week The New Yorker runs a [cartoon captioning contest](...\"],[\"```\\n\\nNotice that BLIP-2 is a rare case where you cannot load the model with Auto API (e.g. AutoModel...\"],[\"```\\n\\n```\\n\\\"two monsters sitting around a campfire\\\"\\n```\\n\\n```\\nprompt = \\\"they look like they are\\\"\\n\\ninput...\"],[\"```\\ncontext = [\\n   (\\\"What is a dinosaur holding?\\\", \\\"a torch\\\"),\\n   (\\\"Where are they?\\\", \\\"In the woods....\"],[\"```\\n\\n```\\nTo light a fire.\\n```\\n\\n## Conclusion\\n\\nBLIP-2 is a zero-shot visual-language model that can b...\"],[\"--\\ntitle: \\\"Scaling-up BERT Inference on CPU (Part 1)\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f21_bert_cpu_scaling_pa...\"],[\"This blog post is the first part of a series which will cover most of the hardware and software opti...\"],[\"These two metrics will help us understand the benefits and tradeoffs along this blog post.\\n\\nThe benc...\"],[\"## 3. Baselines\\n\\nAll the results below were run on [Amazon Web Services (AWS) c5.metal instance](htt...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image\\\"\\u003e\\n  \\u003cimg alt=\\\"pytorch versus tensorflow out of the box bigger batch sizes\\\"...\"],[\"For the remainder of this blog post we will focus on the latter, also known as **Multiple Inference ...\"],[\"The figure 3. above simplifies the situation by assuming single core setup. If you want some more de...\"],[\"```shell\\nubuntu@some-ec2-machine:~$ lscpu\\nArchitecture:                    x86_64\\nCPU op-mode(s):   ...\"],[\"```\\n\\nIn our case we have a machine with **2 sockets**, each socket providing **24 physical cores** w...\"],[\"_Note: Setting both cores and memory affinities is important here. Having computations done on socke...\"],[\"```\\n\\nThen we specify the core and memory affinity through `numactl` using all the **physical** cores...\"],[\"```\\n\\n\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image\\\"\\u003e\\n  \\u003cimg class=\\\"centered\\\" alt=\\\"htop CPU usage without and with numact...\"],[\"As we are targeting just 1 thread per physical core, as explained earlier, we pick only thread 0 on ...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image\\\"\\u003e\\n  \\u003cimg alt=\\\"\\\" src=\\\"assets\\u002f21_bert_cpu_scaling_part_1\\u002fimgs\\u002fcore_count_sca...\"],[\"### 7.1. How-to allocate multiple independent instances\\n\\nLet's start simple, if we want to spawn 2 i...\"],[\"```\\n\\nStarting from here, each instance does not share any resource with the other, and everything is...\"],[\"```\\n\\nThe outcomes remain the same, our 4 instances are effectively running in a truly parallel manne...\"],[\"```\\n\\n\\n## 8. Batch size scaling - Improving throughput and latency with multiple parallel & independe...\"],[\"Also, it is important to notice the results might look totally different on another system _(i.e. Op...\"],[\"\\u003cfigure class=\\\"image\\\"\\u003e\\n  \\u003cimg alt=\\\"Batch scaling experiment for PyTorch and Tensorflow\\\" src=\\\"assets\\u002f...\"],[\"Last but not least, many of the knobs discussed along this blog post can be automatically tuned thro...\"],[\"1. [Benchmarking Transformers: PyTorch and TensorFlow](https:\\u002f\\u002fmedium.com\\u002fhuggingface\\u002fbenchmarking-t...\"],[\"12. [Intel® Hyper-Threading Technology - Technical User Guide](http:\\u002f\\u002fwww.cslab.ece.ntua.gr\\u002fcourses\\u002f...\"],[\"--\\ntitle: \\\"Welcome PaddlePaddle to the Hugging Face Hub\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f126_paddlepaddle\\u002ft...\"],[\"![thumbnail](assets\\u002f126_paddlepaddle\\u002fthumbnail.jpg)\\n\\n**With [PaddleNLP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002f...\"],[\"You are also welcome to check out the [PaddlePaddle](https:\\u002f\\u002fhuggingface.co\\u002fPaddlePaddle) org on the...\"],[\"```python\\nfrom paddlenlp.transformers import AutoTokenizer, AutoModelForMaskedLM\\n\\ntokenizer = AutoTo...\"],[\"```\\n\\n## Conclusion\\n\\nPaddlePaddle is an open source Deep Learning platform that originated from indus...\"],[\"--\\ntitle: Image Similarity with Hugging Face Datasets and Transformers\\nthumbnail: \\u002fblog\\u002fassets\\u002fimage...\"],[\"Also, the approach presented in the post can potentially be extended to other modalities as well.\\n\\nT...\"],[\"```py\\nfrom transformers import AutoImageProcessor, AutoModel\\n\\n\\nmodel_ckpt = \\\"nateraw\\u002fvit-base-beans\\\"...\"],[\"```\\n\\nIn this case, the checkpoint was obtained by fine-tuning a [Vision Transformer based model](htt...\"],[\"```\\n\\nThis is how a single sample from the training split looks like:\\n\\n\\u003cdiv align=\\\"center\\\"\\u003e\\n    \\u003cimg ...\"],[\"```\\n\\n## The process of finding similar images\\n\\nBelow, you can find a pictorial overview of the proce...\"],[\"```\\n\\nAnd we can map `extract_embeddings()` like so:\\n\\n```py\\ndevice = \\\"cuda\\\" if torch.cuda.is_availabl...\"],[\"```\\n\\nWe'll use [cosine similarity](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fCosine_similarity) to compute the s...\"],[\"```\\n\\n## Perform a query\\n\\nGiven all the utilities, we're equipped to do a similarity search. Let's ha...\"],[\"```\\n\\nSeems like our system got the right set of similar images. When visualized, we'd get:\\n\\n\\u003cdiv ali...\"],[\"```\\n\\nOnce the index is built, `dataset_with_embeddings` can be used to retrieve the nearest examples...\"],[\"```\\n\\nThe method returns scores and corresponding candidate examples. To know more, you can check out...\"],[\"--\\ntitle: \\\"Japanese Stable Diffusion\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f106_japanese_stable_diffusion\\u002fjsd_thu...\"],[\"[rinna Co., Ltd](https:\\u002f\\u002frinna.co.jp\\u002f). has developed a Japanese-specific text-to-image model named ...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fma...\"],[\"- Generate Japanese-style images\\n- Understand Japanese words adapted from English\\n- Understand Japan...\"],[\"#### 1st stage: Train a Japanese-specific text encoder \\nIn the 1st stage, the latent diffusion model...\"],[\"```\\n\\nOn the other hand, by using our Japanese tokenizer, the prompt is split into interpretable toke...\"],[\"```\\n\\nThis stage enables the model to understand Japanese prompts but does not still output Japanese-...\"],[\"*\\\"サラリーマン 油絵\\\", which means exactly \\\"salary man, oil painting\\\", from the 2nd-stage Japanese Stable Dif...\"],[\"--\\ntitle: \\\"Fine-Tune Wav2Vec2 for English ASR in Hugging Face with 🤗 Transformers\\\"\\nthumbnail: \\u002fblog\\u002f...\"],[\"For the first time, it has been shown that pretraining, followed by\\nfine-tuning on very little label...\"],[\"I highly recommend reading the blog post [Sequence Modeling with CTC\\n(2017)](https:\\u002f\\u002fdistill.pub\\u002f201...\"],[\"```\\n\\nNext we strongly suggest to upload your training checkpoints directly to the [Hugging Face Hub]...\"],[\"```\\n\\n------------------------------------------------------------------------\\n\\n\\\\\\\\({}^1\\\\\\\\) Timit is u...\"],[\"Let\\\\'s start by creating the tokenizer responsible for decoding the\\nmodel\\\\'s predictions.\\n\\n### Creat...\"],[\"```\\n\\n**Print Output:**\\n```bash\\n    DatasetDict({\\n        train: Dataset({\\n            features: ['fi...\"],[\"```\\n\\nLet\\\\'s write a short function to display some random samples of the\\ndataset and run it a couple...\"],[\"```\\n\\n**Print Output:**\\n\\n| Idx |  Transcription     |\\n|----------|:-------------:|\\n|\\t1  | Who took th...\"],[\"```\\n\\nLet's take a look at the preprocessed transcriptions.\\n\\n```python\\nshow_random_elements(timit[\\\"tr...\"],[\"```\\n\\nNow, we create the union of all distinct letters in the training dataset\\nand test dataset and c...\"],[\"```\\n\\nCool, we see that all letters of the alphabet occur in the dataset\\n(which is not really surpris...\"],[\"```\\n\\nIn a final step, we use the json file to instantiate an object of the\\n`Wav2Vec2CTCTokenizer` cl...\"],[\"```\\n\\nGreat, you can see the just created repository under `https:\\u002f\\u002fhuggingface.co\\u002f\\u003cyour-username\\u003e\\u002fwa...\"],[\"A Wav2Vec2 feature extractor object requires the following parameters to\\nbe instantiated:\\n\\n-   `feat...\"],[\"```\\n\\nGreat, Wav2Vec2\\\\'s feature extraction pipeline is thereby fully defined!\\n\\nTo make the usage of ...\"],[\"```\\n\\n**Print Output:**\\n```bash\\n{'array': array([-2.1362305e-04,  6.1035156e-05,  3.0517578e-05, ...,...\"],[\"```\\n\\nIt can be heard, that the speakers change along with their speaking rate, accent, etc. Overall,...\"],[\"```\\n\\nGood! Everything looks fine - the data is a 1-dimensional array, the\\nsampling rate always corre...\"],[\"```\\n\\nLet's apply the data preparation function to all examples.\\n\\n```python\\ntimit = timit.map(prepare...\"],[\"```\\n\\n**Note**: Currently `datasets` make use of [`torchaudio`](https:\\u002f\\u002fpytorch.org\\u002faudio\\u002fstable\\u002finde...\"],[\"After having fine-tuned the model, we will correctly evaluate it on the\\ntest data and verify that it...\"],[\"```python\\nimport torch\\n\\nfrom dataclasses import dataclass, field\\nfrom typing import Any, Dict, List,...\"],[\"processor: Wav2Vec2Processor\\n    padding: Union[bool, str] = True\\n    max_length: Optional[int] = No...\"],[\"```\\n\\nLet's initialize the data collator.\\n\\n```python\\ndata_collator = DataCollatorCTCWithPadding(proce...\"],[\"```\\n\\nThe model will return a sequence of logit vectors:\\n\\n$$ \\\\mathbf{y}_1, \\\\ldots, \\\\mathbf{y}_m $$, \\n...\"],[\"```\\n\\nNow, we can load the pretrained `Wav2Vec2` checkpoint. The tokenizer\\\\'s\\n`pad_token_id` must be ...\"],[\"```\\n\\nIn a final step, we define all parameters related to training. To give\\nmore explanation on some...\"],[\"```\\n\\n------------------------------------------------------------------------\\n\\n\\\\\\\\({}^1\\\\\\\\) To allow m...\"],[\"In case you want to use this google colab to fine-tune your model, you\\nshould make sure that your tr...\"],[\"```\\n\\n```python\\ntrainer.train()...\"],[\"```\\n\\nDepending on your GPU, it might be possible that you are seeing an `\\\"out-of-memory\\\"` error here...\"],[\"You can now upload the result of the training to the Hub, just execute this instruction:\\n\\n```python\\n...\"],[\"```\\n\\nYou can now share this model with all your friends, family, favorite pets: they can all load it...\"],[\"```\\n\\nNow, we will make use of the `map(...)` function to predict the\\ntranscription of every test sam...\"],[\"```\\n\\n**Print Output:**\\n```bash\\n    Test WER: 0.221\\n```\\n\\n22.1% WER - not bad! Our demo model would ha...\"],[\"```\\n\\n| pred_str |  target_text |\\n|----------|:-------------:|\\n| am to balence your employe you benef...\"],[\"pred_ids = torch.argmax(logits, dim=-1)\\n\\n# convert ids to tokens\\n\\\" \\\".join(processor.tokenizer.conver...\"],[\"```\\n\\n**Print Output:**\\n```bash\\n[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] t t h e e | | b b [PAD] u u n n n...\"],[\"--\\ntitle: \\\"Hugging Face and AWS partner to make AI more accessible\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f131_aws...\"],[\"There have been significant advances in new Transformer and Diffuser machine learning models that pr...\"],[\"## Collaborating to scale AI in the cloud\\n\\nThis expanded strategic partnership enables Hugging Face ...\"],[\"--\\ntitle: \\\"Deploy Livebook notebooks as apps to Hugging Face Spaces\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f120_eli...\"],[\"If you don't know Livebook yet, it is an open-source tool for writing interactive code notebooks in ...\"],[\"## Your turn\\n\\nWe hope this new integration between Livebook and Hugging Face empowers even more peop...\"],[\"--\\ntitle: \\\"Getting Started with Sentiment Analysis using Python\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f50_sentimen...\"],[\"- *\\\"dear @verizonsupport your service is straight 💩 in dallas.. been with y’all over a decade and th...\"],[\"There are more than [215 sentiment analysis models](https:\\u002f\\u002fhuggingface.co\\u002fmodels?pipeline_tag=text-...\"],[\"```\\n\\nThis code snippet uses the [pipeline class](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain_class...\"],[\"```\\n\\nYou can test these models with your own data using this [Colab notebook](https:\\u002f\\u002fcolab.research...\"],[\"Are you interested in doing sentiment analysis in languages such as Spanish, French, Italian or Germ...\"],[\"### a. Fine-tuning model with Python\\n\\nIn this tutorial, you'll use the IMDB dataset to fine-tune a D...\"],[\"```\\n\\nThen, install the libraries you will be using in this tutorial:\\n\\n```python\\n!pip install dataset...\"],[\"```\\n\\nNext, you will prepare the text inputs for the model for both splits of our dataset (training a...\"],[\"```\\n\\nThen, let's define the metrics you will be using to evaluate how good is your fine-tuned model ...\"],[\"```\\n\\nYou are almost there! Before training our model, you need to define the training arguments and ...\"],[\"```\\n\\nIn our case, we got 88% accuracy and 89% f1 score. Quite good for a sentiment analysis model ju...\"],[\"```\\n\\nIn the IMDB dataset, `Label 1` means positive and `Label 0` is negative. Quite good! 🔥\\n\\n\\n### b....\"],[\"Next, let's create a [new project on AutoNLP](https:\\u002f\\u002fui.autonlp.huggingface.co\\u002fnew) to train 5 cand...\"],[\"After a few minutes, AutoNLP has trained all models, showing the performance metrics for all of them...\"],[\"```\\n!pip install -q transformers tweepy wordcloud matplotlib\\n```\\n\\n### 2. Set up Twitter API credenti...\"],[\"```\\n\\n### 3. Search for tweets using Tweepy\\nAt this point, you are ready to start using the Twitter A...\"],[\"```\\n\\n\\n### 4. Run sentiment analysis on the tweets\\nNow you can put our new skills to work and run sen...\"],[\"```\\n\\nOutput:\\n\\n```\\nTweet: @NFTGalIery Warm, exquisite and elegant palette of charming beauty Its pric...\"],[\"```\\n\\nInterestingly, most of the tweets about NFTs are positive (56.1%) and almost none are negative ...\"],[\"Finally, let's see what words stand out for each sentiment by creating a word cloud:\\n\\n```python\\nfrom...\"],[\"```\\n\\nSome of the words associated with positive tweets include Discord, Ethereum, Join, Mars4 and Sh...\"],[\"Do you want to train a custom model for sentiment analysis with your own data? Easy peasy! You can f...\"],[\"a href=\\\"https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fsanchit-gandhi\\u002fnotebooks\\u002fblob\\u002fmain\\u002ffine_tune_whispe...\"],[\"When scaled to 680,000 hours of labelled pre-training data, Whisper models \\ndemonstrate a strong abi...\"],[\"For demonstration purposes, we'll fine-tune the multilingual version of the \\n[`\\\"small\\\"`](https:\\u002f\\u002fhug...\"],[\"```\\n\\nNext, we need to update the Unix package `ffmpeg` to version 4:\\n\\n\\n```python\\n!add-apt-repository...\"],[\"```\\n\\n## Load Dataset\\n\\nUsing 🤗 Datasets, downloading and preparing data is extremely simple. \\nWe can ...\"],[\"```\\n\\n## Prepare Feature Extractor, Tokenizer and Data\\n\\nThe ASR pipeline can be de-composed into thre...\"],[\"We'll load the feature extractor from the pre-trained checkpoint with the default values:\\n\\n\\n```pytho...\"],[\"```\\n\\n### Load WhisperTokenizer\\n\\nThe Whisper model outputs a sequence of _token ids_. The tokenizer m...\"],[\"```\\n\\n### Prepare Data\\n\\nLet's print the first example of the Common Voice dataset to see \\nwhat form t...\"],[\"```\\n\\nRe-loading the first audio sample in the Common Voice dataset will resample \\nit to the desired ...\"],[\"```\\n\\n## Training and Evaluation\\n\\nNow that we've prepared our data, we're ready to dive into the trai...\"],[\"The `labels` on the other hand are un-padded. We first pad the sequences\\nto the maximum length in th...\"],[\"batch[\\\"labels\\\"] = labels\\n\\n        return batch...\"],[\"```\\n\\n### Load a Pre-Trained Checkpoint\\n\\nNow let's load the pre-trained Whisper `small` checkpoint. A...\"],[\"```\\n\\n### Define the Training Configuration\\n\\nIn the final step, we define all the parameters related ...\"],[\"```\\n\\n### Training\\n\\nTraining will take approximately 5-10 hours depending on your GPU or the one \\nall...\"],[\"```\\n\\nOur best WER is 32.0% - not bad for 8h of training data! We can submit our checkpoint to the [`...\"],[\"```\\n\\nThe training results can now be uploaded to the Hub. To do so, execute the `push_to_hub` comman...\"],[\"--\\ntitle: How to train a Language Model with Megatron-LM\\nthumbnail: \\u002fblog\\u002fassets\\u002f100_megatron_traini...\"],[\"## Why Megatron-LM?\\n\\nBefore getting into the training details, let’s first understand what makes thi...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"assets\\u002f100_megatron_training\\u002fkernel_fusion.png\\\" width=\\\"600\\\" \\u002f\\u003e\\n\\u003c\\u002fp\\u003e...\"],[\"So after having installed Docker, you can run the container with the following command (`xx.xx` deno...\"],[\"```\\n\\nYou also need to add the vocabulary file `vocab.json` and merges table `merges.txt` of your tok...\"],[\"```\\n\\nThe data is then tokenized, shuffled and processed into a binary format for training using the ...\"],[\"```\\nThe `workers` and `chunk_size` options refer to the number of workers used in the preprocessing ...\"],[\"### Training\\nYou can configure the model architecture and training parameters as shown below, or put...\"],[\"--weight-decay .1\\n--adam-beta2 .999\\n--fp16\\n--log-interval 10\\n--save-interval 2000\\n--eval-interval 20...\"],[\"```\\nWith this setting, the training takes roughly 12 hours.\\n\\nThis setup uses Data Parallelism, but i...\"],[\"```\\nBe careful, you will need to replace the generated vocabulary file and merges table after the co...\"],[\"```\\nThis will use [accelerate](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002faccelerate\\u002findex) library behind the scen...\"],[\"--\\ntitle: \\\"Accelerating Hugging Face Transformers with AWS Inferentia2\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f140...\"],[\"However, for all their greatness, Transformers can be challenging to deploy in production. On top of...\"],[\"## Introducing AWS Inferentia2\\n\\nAWS Inferentia2 is the next generation to Inferentia1 launched in 20...\"],[\"Speaking of, let’s show you how several Hugging Face models run on Inferentia 2. Benchmarking time!\\n...\"],[\"### Results\\n\\nThe benchmark confirms that the performance improvements claimed by AWS can be reproduc...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"The initial benchmarking results are promising, and show that Inferentia2 delivers superior latency ...\"],[\"--\\ntitle: \\\"Introducing SafeCoder\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f159_safecoder\\u002fthumbnail.jpg\\nauthors:\\n- us...\"],[\"However, relying on closed-source Code LLMs to create internal code assistants exposes companies to ...\"],[\"Note: While StarCoder is the inspiration and model powering the initial version of SafeCoder, an imp...\"],[\"BigCode expanded upon this work by implementing novel techniques for the code domain and building Th...\"],[\"### Deploying SafeCoder\\n\\nDuring the setup phase, SafeCoder customers and Hugging Face design and pro...\"],[\"“Our collaboration with Hugging Face around SafeCoder fully aligns to VMware’s goal of enabling cust...\"],[\"--\\ntitle: \\\"The Reformer - Pushing the limits of language modeling\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f03_reform...\"],[\"Recently, long sequence modeling has experienced a surge of interest as can be seen by the many subm...\"],[\"The memory improvements can be attributed to **4** features which the Reformer authors introduced to...\"],[\"This blog post uses the same notation and coloring as the popular blog post [The illustrated transfo...\"],[\"In short, a global self-attention layer projects \\\\\\\\(\\\\mathbf{X}\\\\\\\\) to the query, key and value matric...\"],[\"Important to remember is that for each output vector \\\\\\\\(\\\\mathbf{z}_{i}\\\\\\\\), the whole input sequence ...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"$$\\\\mathbf{Z}^{\\\\text{loc}} = \\\\left[\\\\mathbf{Z}_{1:l_{c}}^{\\\\text{loc}}, \\\\ldots, \\\\mathbf{Z}_{(n_{c} - 1)...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"This enhanced local self-attention is better than the vanilla local self-attention architecture but ...\"],[\"LSH self-attention relies on the LSH algorithm as presented in [Andoni et al (2015)](https:\\u002f\\u002farxiv.o...\"],[\"First, the authors of Reformer notice that sharing the query and key projections: \\\\\\\\(\\\\mathbf{Q} = \\\\m...\"],[\"For each set of indices \\\\\\\\(C_{m}\\\\\\\\), the softmax function on the corresponding bucket of query vecto...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"All in all for all chunks \\\\\\\\( k \\\\in \\\\{1, \\\\ldots, n_{c}\\\\} \\\\\\\\), LSH self-attention can be noted down a...\"],[\"One important feature to mention here as well is that the accuracy of LSH self-attention can be impr...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"Let's recap quickly what we have gone through above:\\n\\n1. We want to approximate global attention usi...\"],[\"\\\\\\\\( {}^3 \\\\\\\\) On a side note, it is to mention the authors put a mask on the query vector \\\\\\\\( \\\\mathbf...\"],[\"```\\n#@title Installs and Imports\\n# pip installs\\n!pip -qq install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002f...\"],[\"```\\n\\n\\n    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1279.0, style=Progr...\"],[\"The longer the input sequence, the more visible is the quadratic relationship \\\\\\\\( \\\\mathcal{O}(n^2) \\\\...\"],[\"```\\n  config = ReformerConfig.from_pretrained(\\\"google\\u002freformer-enwik8\\\")\\n  benchmark_args = PyTorchBe...\"],[\"```\\n\\n    1 \\u002f 1\\n    Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 ...\"],[\"As expected using local and LSH self-attention is much more memory efficient for longer input sequen...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"Let's illustrate the feed forward layers for \\\\\\\\( \\\\mathbf{\\\\overline{z}}_1, \\\\ldots, \\\\mathbf{\\\\overline{...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"Assuming \\\\\\\\( c_{f}=1 \\\\\\\\) for our example we can illustrate the incremental computation of the output...\"],[\"\\\\\\\\( {}^3 \\\\\\\\) As a reminder, the output `config.num_attention_heads` is assumed to be 1 for the sake ...\"],[\"```\\n#@title Installs and Imports\\n# pip installs\\n!pip -qq install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002f...\"],[\"```\\n\\n    1 \\u002f 2\\n    Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 ...\"],[\"Interesting, chunked feed forward layers do not seem to help here at all. The reason is that `config...\"],[\"```\\nconfig_no_chunk = ReformerConfig.from_pretrained(\\\"google\\u002freformer-enwik8\\\", chunk_size_feed_forwa...\"],[\"```\\n\\n    1 \\u002f 2\\n    2 \\u002f 2\\n    \\n    ====================      INFERENCE - MEMORY - RESULT       ======...\"],[\"## 3. Reversible Residual Layers\\n\\nReversible residual layers were first introduced in [N. Gomez et a...\"],[\"Using the same notation as before, the input of a transformer layer *i.e.* \\\\\\\\( \\\\mathbf{X} \\\\\\\\) is fir...\"],[\"Let's illustrate a complete transformer layer using the example of \\\\\\\\( \\\\mathbf{x}_1, \\\\ldots, \\\\mathbf...\"],[\"Here, reversible residual layers come to our help. The idea is relatively straight-forward. The resi...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"If we assume to know \\\\\\\\( \\\\mathbf{\\\\overline{Y}}^{(1)}, \\\\mathbf{\\\\overline{Y}}^{(2)} \\\\\\\\), it can easily...\"],[\"\\\\\\\\). Alright now, \\\\\\\\( \\\\mathbf{Z} \\\\\\\\) and \\\\\\\\( \\\\mathbf{Y} \\\\\\\\) are trivial to compute via \\\\\\\\( \\\\mathbf{Y...\"],[\"use of \\\\\\\\( G \\\\\\\\) and \\\\\\\\( F \\\\\\\\) during the backward pass and passing \\\\\\\\( \\\\mathbf{X}^{(1)} \\\\\\\\) and \\\\\\\\(...\"],[\"**Note**: Since recently, major deep learning frameworks have released code that allows to store onl...\"],[\"```\\n#@title Installs and Imports\\n# pip installs\\n!pip -qq install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002f...\"],[\"```\\n\\n\\n    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=Progre...\"],[\"```\\nconfig_4_layers_reformer = ReformerConfig.from_pretrained(\\\"google\\u002freformer-enwik8\\\", num_hidden_l...\"],[\"```\\n\\n    1 \\u002f 3\\n    2 \\u002f 3\\n    3 \\u002f 3\\n    \\n    ====================        TRAIN - MEMORY - RESULTS    ...\"],[\"**Important:** *Axial Position Encodings were not explained in the official paper, but can be well u...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"Such positional encodings would use an unnecessarily large amount of memory both when loading the mo...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"We can see that we have cut the embedding vectors into \\\\\\\\( \\\\mathbf{e}_\\\\text{down} \\\\\\\\) (*in blue*) an...\"],[\"whereas \\\\\\\\( n_\\\\text{max}^1 = 7 \\\\\\\\) and \\\\\\\\( n_\\\\text{max}^2 = 7 \\\\\\\\) in our example.\\nThese new encoding...\"],[\"To demonstrate the drastic reduction in size, \\nlet's assume we would have set `config.axial_pos_shap...\"],[\"```\\n#@title Installs and Imports\\n# pip installs\\n!pip -qq install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002f...\"],[\"```\\nconfig_no_pos_axial_embeds = ReformerConfig.from_pretrained(\\\"google\\u002freformer-crime-and-punishmen...\"],[\"```\\n\\n\\n    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1151.0, style=Progr...\"],[\"```\\nbenchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512], batch_sizes=[8], models=[\\\"Ref...\"],[\"```\\n\\n    1 \\u002f 2\\n    2 \\u002f 2\\n    \\n    ====================      INFERENCE - MEMORY - RESULT       ======...\"],[\"--\\ntitle: \\\"Chat Templates: An End to the Silent Performance Killer\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fchat-te...\"],[\"```\\nBy loading the tokenizer and model from the same checkpoint, you ensure that inputs are tokenize...\"],[\"```\\nOr you could add special tokens to indicate the roles:\\n```\\n[USER] Hey there! [\\u002fUSER]\\n[ASST] Nice...\"],[\"```\\nThere are lots of ways to do this, and none of them is obviously the best or correct way to do i...\"],[\"```\\n```jinja\\n{% for message in messages %}\\n    {% if message['role'] == 'user' %}\\n        {{ \\\"[USER]...\"],[\"```\\n\\nIf you're unfamiliar with Jinja, I strongly recommend that you take a moment to look at these t...\"],[\"```\\n\\nThere's also a second reason not to hardcode a standard format, though, beyond the proliferatio...\"],[\"Default chat templates are also set at the class level, and tell classes like `ConversationPipeline`...\"],[\"If a tokenizer doesn't have a `chat_template` attribute, it might still work, but it will use the de...\"],[\"--\\ntitle: \\\"Make your llama generation time fly with AWS Inferentia2\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002finferen...\"],[\"Alternatively, you can use the [Hugging Face Neuron SDK DLC](https:\\u002f\\u002fgithub.com\\u002faws\\u002fdeep-learning-co...\"],[\"```\\n\\u003e\\u003e\\u003e from optimum.neuron import NeuronModelForCausalLM\\n\\n\\u003e\\u003e\\u003e compiler_args = {\\\"num_cores\\\": 24, \\\"au...\"],[\"```\\n\\n## Generate Text using Llama 2 on AWS Inferentia2\\n\\nOnce your model has been exported, you can g...\"],[\"```\\n\\n*Note: when passing multiple input prompts to a model, the resulting token sequences must be pa...\"],[\"```\\n\\n## Benchmarks\\n\\nBut how much efficient is text-generation on Inferentia2?  Let's figure out!\\n\\nWe...\"],[\"| Model type                 | num cores | batch_size | Hugging Face Hub model                    |\\n...\"],[\"*Note: all models are compiled with a maximum sequence length of 2048.*\\n\\nThe `llama2 7B` \\\"budget\\\" mo...\"],[\"Encoding time is expressed in **seconds**.\\n\\n|   input tokens  |   Llama2 7B-L  |   Llama2 7B-T  |   ...\"],[\"Latency is expressed in **seconds**.\\n\\n|   new tokens  |   Llama2 7B-L  |   Llama2 7B-T  |   Llama2 1...\"],[\"Throughput is expressed in **tokens\\u002fsecond**.\\n\\n|   new tokens  |   Llama2 7B-L  |   Llama2 7B-T  |  ...\"],[\"The deployed models demonstrate very good performance in terms of encoding time, latency and through...\"],[\"--\\ntitle: \\\"Introduction to 3D Gaussian Splatting\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games\\u002fthumbnail...\"],[\"In practice, multiple gaussians are drawn at once.\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002f...\"],[\"### 3. Training\\n\\nThe training procedure uses Stochastic Gradient Descent, similar to a neural networ...\"],[\"## Who cares?\\n\\nWhy has there been so much attention on 3D Gaussian Splatting? The obvious answer is ...\"],[\"So far, the original CUDA implementation has not been adapted to production rendering pipelines, lik...\"],[\"--\\ntitle: \\\"Train your ControlNet with diffusers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f136_train-your-controlnet\\u002ft...\"],[\"2. **Building your dataset**: Once a condition is decided, it is time to build your dataset. For tha...\"],[\"For this project, we decided to go with the `FaceSynthetics` dataset by Microsoft: it is a dataset t...\"],[\"So we decided to follow another path:\\n- Use the ground truths `image` of faces of the `FaceSynthetic...\"],[\"![New dataset](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f13...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fwandb.ai\\u002fapolinario\\u002fcontrolnet\\u002freports\\u002fControlNet-Uncanny-Faces-Training--Vmlld...\"],[\"```\\n\\nAnd then run the [train_controlnet.py](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmain\\u002fexamp...\"],[\"Let's break down some of the settings, and also let's go over some optimisation tips for going as lo...\"],[\"- `validation_prompt`: A prompt to be ran togehter with your validation image. Can be anything that ...\"],[\"### Fitting on a 16GB VRAM GPU\\n```shell \\npip install bitsandbytes\\n\\n--train_batch_size=1 \\\\\\n--gradient...\"],[\"```\\n\\nThe combination of a batch size of 1 with 4 gradient accumulation steps is equivalent to using ...\"],[\"--\\ntitle: \\\"Spread Your Wings: Falcon 180B is here\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f162_falcon_180b\\u002fthumbnai...\"],[\"In this blog post, we explore what makes Falcon 180B so good by looking at some evaluation results a...\"],[\"The released [chat model](https:\\u002f\\u002fhuggingface.co\\u002ftiiuae\\u002ffalcon-180B-chat) is fine-tuned on chat and ...\"],[\"With 68.74 on the Hugging Face Leaderboard, Falcon 180B is the highest-scoring openly released pre-t...\"],[\"You can easily try the Big Falcon Model (180 billion parameters!) in [this Space](https:\\u002f\\u002fhuggingfac...\"],[\"```bash\\nSystem: Add an optional system prompt here\\nUser: This is the user input\\nFalcon: This is what...\"],[\"```\\n\\n### Transformers\\n\\nWith the release of Transformers 4.33, you can use Falcon 180B and leverage a...\"],[\"```\\n\\nThis could produce an output such as:\\n\\n```\\nMy name is Pedro, I live in Portugal and I am 25 yea...\"],[\"```\\n\\nAs you can see, interactions from the user and responses by the model are preceded by `User: ` ...\"],[\"## Acknowledgments\\n\\nReleasing such a model with support and evaluations in the ecosystem would not b...\"],[\"--\\ntitle: Swift 🧨Diffusers - Fast Stable Diffusion for Mac\\nthumbnail: \\u002fblog\\u002fassets\\u002ffast-mac-diffuser...\"],[\"## What exactly is 🧨Diffusers for Mac anyway?\\n\\nThe Diffusers app ([App Store](https:\\u002f\\u002fapps.apple.com...\"],[\"Why would you want to run a native Mac app then? There are many reasons:\\n- It uses Core ML models, i...\"],[\"Come check out our benchmarks. All the combinations use the CPU in addition to either the GPU or the...\"],[\"We found that the amount of memory does not seem to play a big factor on performance, but the number...\"],[\"## Other Improvements in Version 1.1\\n\\nIn addition to the performance optimization and fixing a few b...\"],[\"--\\ntitle: \\\"Director of Machine Learning Insights\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f61_ml_director_insights\\u002fth...\"],[\"🚀  Let’s meet some top Machine Learning Directors and hear what they have to say about Machine Learn...\"],[\"_Tightened testing:_ In a capital intensive media venture, there is a need to shorten the time betwe...\"],[\"**Background:** Li is an AI\\u002fML veteran with 15+ years of experience leading high-profile Data Scienc...\"],[\"#### **1. How has ML made a positive impact on Pharmaceuticals?**\\nAI\\u002fML applications have exploded i...\"],[\"\\u003cimg class=\\\"mx-auto\\\" style=\\\"float: left;\\\" padding=\\\"5px\\\" width=\\\"200\\\" src=\\\"\\u002fblog\\u002fassets\\u002f61_ml_director...\"],[\"**Machine Learning & Sensing Laboratory:** A University of Florida laboratory that develops machine ...\"],[\"#### **4. What excites you most about the future of ML?**\\nThere are a lot of really exciting directi...\"],[\"For example, when you see a semi-truck driving on the road, there is currently a 20% chance that the...\"],[\"#### **4. What excites you the most about the future of ML?**\\nI think the thing that excites me most...\"],[\"#### **1. How has ML made a positive impact on Marketing?**\\nIn so many ways! It’s completely changin...\"],[\"Understanding what makes a creator\\u002finfluencer successful over time is really hard. There is a lot of...\"],[\"Much like in the beginning of the internet, software developers were few and far between and you nee...\"],[\"\\u003cimg class=\\\"mx-auto\\\" style=\\\"float: left;\\\" padding=\\\"5px\\\" width=\\\"200\\\" src=\\\"\\u002fblog\\u002fassets\\u002f61_ml_director...\"],[\"**E Source:** Provides independent market intelligence, consulting, and predictive data science to u...\"],[\"---\\n\\n🤗   Thank you for joining us in this first installment of ML Director Insights. Stay tuned for ...\"],[\"--\\ntitle: \\\"My Journey to a serverless transformers pipeline on Google Cloud\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"Below is the [official example](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers#quick-tour) from the Tra...\"],[\"```\\n\\n\\n## Deploy transformers to Google Cloud\\n\\u003e GCP is chosen as it is the cloud environment I am usi...\"],[\"### Step 4 - Test on Cloud Run\\n\\nLastly, I moved to [Cloud Run](https:\\u002f\\u002fcloud.google.com\\u002frun) with a ...\"],[\"The content on the `main.py` is really simple. The idea is to receive a `GET` request containing two...\"],[\"```\\n\\nThen the `DockerFile` which will be used to create a docker image of the service. We specify th...\"],[\"```\\n\\n\\n## Deployment instructions\\n\\nFirst, you will need to meet some requirements such as having a pr...\"],[\"```\\n\\nAfter a few minutes, you will also need to upgrade the memory allocated to your Cloud Run insta...\"],[\"For my micro-service, I am planning to near 1,000 requests per month, optimistically. 500 may more l...\"],[\"--\\ntitle: \\\"Machine Learning Experts - Margaret Mitchell\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f57_meg_mitchell_int...\"],[\"### Could you share a little bit about your background and what brought you to Hugging Face?\\n\\n**Dr. ...\"],[\"So I began to run into issues where white people would be described as ‘people’ and black people wou...\"],[\"### How can ML teams be more aware of harmful bias?\\n\\n**Meg:** A primary issue is that these concepts...\"],[\"**Meg:** Diversity is when you have a lot of races, ethnicities, genders, abilities, statuses at the...\"],[\"Just how you want to have a Gaussian approach over different start states, so too do you want that a...\"],[\"Timnit’s paper was called [‘Data Sheets for Datasets’](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1803.09010). So we call...\"],[\"### Decision thresholds & model transparency\\n\\n**Meg:** When Amazon first started putting out facial ...\"],[\"### What are you working on at Hugging Face?\\n\\n- Working on a few different tools designed for engine...\"],[\"### Rapid Fire Questions:\\n\\n### Best piece of advice for someone looking to get into AI?\\n\\n**Meg:** De...\"],[\"### Should people be afraid of AI taking over the world?\\n\\n**Meg:** There are a lot of things to be a...\"],[\"Hopefully, we can focus on the things that are most beneficial and continue heading in that directio...\"],[\"We have a lot of people developing technology, which is great, but we don’t have a lot of people in ...\"],[\"**Honorable mentions + links:**\\n- [Emily Bender](https:\\u002f\\u002ftwitter.com\\u002femilymbender?lang=en)\\n- [Ehud R...\"],[\"--\\ntitle: \\\"Accelerating PyTorch distributed fine-tuning with Intel technologies\\\"\\nthumbnail: \\u002fblog\\u002fas...\"],[\"Running a text classification job, we will fine-tune a [BERT](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-cased...\"],[\"All three major cloud providers offer virtual machines powered by Intel Ice Lake CPUs:\\n\\n- Amazon Web...\"],[\"When it comes to distributed training, the main performance bottleneck is often networking. Indeed, ...\"],[\"From a networking perspective, we will need the following setup: \\n\\n* Open port 22 for ```ssh``` acce...\"],[\"It looks like a lot, but there's nothing complicated. Here we go!\\n\\n__Installing Intel toolkits__\\n\\nFi...\"],[\"```\\nwget https:\\u002f\\u002fregistrationcenter-download.intel.com\\u002fakdlm\\u002firc_nas\\u002f18236\\u002fl_BaseKit_p_2021.4.0.3422...\"],[\"```\\n\\n__Compiling and installing oneCCL__\\n\\nThen, we install some native dependencies required to comp...\"],[\"```\\npython run_glue.py \\\\\\n--model_name_or_path bert-base-cased --task_name mrpc \\\\\\n--do_train --do_eva...\"],[\"```\\nfor nic in eth0 eib0 hib0 enp94s0f0; do\\n  master_addr=$(ifconfig $nic 2\\u003e\\u002fdev\\u002fnull | grep netmask...\"],[\"```\\n+import torch_ccl\\n+\\n import datasets\\n import numpy as np\\n from datasets import load_dataset, loa...\"],[\"```\\n\\nSetup is now complete. Let's scale our training job to 2 nodes and 4 nodes.\\n\\n### Running a dist...\"],[\"```\\n\\nWithin seconds, a job starts on the first two nodes. The job completes in __4 minutes and 39 se...\"],[\"--\\ntitle: \\\"Introducing new audio and vision documentation in 🤗 Datasets\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f87_...\"],[\"\\u003cdiv class=\\\"hidden xl:block\\\"\\u003e\\n\\u003cdiv style=\\\"display: flex; flex-direction: column; align-items: center...\"],[\"Also new in the Quickstart is the `to_tf_dataset` function which takes care of converting a dataset ...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg style=\\\"border:none;\\\" alt=\\\"An overview of ...\"],[\"```\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg style=\\\"border:none;\\\" alt=\\\"A table of...\"],[\"dataset = load_dataset(\\\"imagefolder\\\", data_dir=\\\"\\u002fpath\\u002fto\\u002ffolder\\\", split=\\\"train\\\")\\ndataset[0][\\\"objects...\"],[\"```\\n\\nYou can use `ImageFolder` to load an image dataset for nearly any type of image task if you hav...\"],[\"--\\ntitle: \\\"Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fmixt...\"],[\"## Table of Contents\\n\\n- [What is Mixtral 8x7b](#what-is-mixtral-8x7b)\\n  - [About the name](#about-th...\"],[\"**Mixtral release TL;DR;**\\n\\n- Release of base and Instruct versions\\n- Supports a context length of 3...\"],[\"| Model                                                                             | License       ...\"],[\"For instruct and chat models, evaluating on benchmarks like MT-Bench or AlpacaEval is better. Below,...\"],[\"| Model                                                                                             ...\"],[\"| [meta-llama\\u002fLlama-2-70b-chat-hf](https:\\u002f\\u002fhuggingface.co\\u002fmeta-llama\\u002fLlama-2-70b-chat-hf)           ...\"],[\"Impressively, Mixtral Instruct outperforms all other open-access models on MT-Bench and is the first...\"],[\"```\\n\\nThis format has to be exactly reproduced for effective use. We’ll show later how easy it is to ...\"],[\"### Using 🤗 Transformers\\n\\nWith transformers [release 4.36](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransforme...\"],[\"```\\n\\nIn the following code snippet, we show how to run inference with 🤗 Transformers and 4-bit quant...\"],[\"```\\n\\n\\u003e \\\\\\u003cs\\u003e[INST] Explain what a Mixture of Experts is in less than 100 words. [\\u002fINST] A\\nMixture of ...\"],[\"*Note: You might need to request a quota upgrade via email to **[api-enterprise@huggingface.co](mail...\"],[\"```\\n\\n## Fine-tuning with 🤗 TRL\\n\\nTraining LLMs can be technically and computationally challenging. In...\"],[\"```\\n\\nThis takes about 48 hours to train on a single A100, but can be easily parallelised by tweaking...\"],[\"prompt = \\\"[INST] Explain what a Mixture of Experts is in less than 100 words. [\\u002fINST]\\\"\\ninputs = toke...\"],[\"```\\n\\nThis 4-bit quantization technique was introduced in the [QLoRA paper](https:\\u002f\\u002fhuggingface.co\\u002fpa...\"],[\"```\\n\\nYou also need to install transformers from source:\\n\\n```bash\\npip install -U git+https:\\u002f\\u002fgithub.c...\"],[\"```\\n\\nNote that for both QLoRA and GPTQ you need at least 30 GB of GPU VRAM to fit the model. You can...\"],[\"--\\ntitle: Hyperparameter Search with Transformers and Ray Tune\\nthumbnail: \\u002fblog\\u002fassets\\u002f06_ray_tune\\u002fr...\"],[\"\\u003ctable\\u003e\\n  \\u003ctr\\u003e\\n   \\u003ctd\\u003e\\u003cstrong\\u003eAlgorithm\\u003c\\u002fstrong\\u003e\\n   \\u003c\\u002ftd\\u003e\\n   \\u003ctd\\u003e\\u003cstrong\\u003eBest Val Acc.\\u003c\\u002fstrong\\u003e\\n   \\u003c...\"],[\"![alt_text](\\u002fblog\\u002fassets\\u002f06_ray_tune\\u002fray-hf.jpg \\\"image_tooltip\\\")\\n\\n\\nIn the Transformers 3.1 release, ...\"],[\"def encode(examples):\\n    outputs = tokenizer(\\n        examples['sentence1'], examples['sentence2'],...\"],[\"```\\n\\nBy default, each trial will utilize 1 CPU, and optionally 1 GPU if available.\\nYou can leverage ...\"],[\"```\\n\\n\\nIt also works with [Weights and Biases](https:\\u002f\\u002fwandb.ai\\u002f) out of the box!\\n\\n![alt_text](\\u002fblog\\u002f...\"],[\"--\\ntitle: \\\"Accelerate Large Model Training using DeepSpeed\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f83_accelerate_de...\"],[\"c. **Stage 3**: Shards optimizer states + gradients + model parameters across data parallel workers\\u002f...\"],[\"The code is available here [run_cls_no_trainer.py](https:\\u002f\\u002fgithub.com\\u002fpacman100\\u002faccelerate-deepspeed...\"],[\"```\\n\\nNow, run below command for training:\\n```bash\\naccelerate launch run_cls_no_trainer.py \\\\\\n  --mode...\"],[\"```\\n\\nIn our Single-Node Multi-GPU setup, the maximum batch size that DDP supports without OOM error ...\"],[\"# Accelerate 🚀:  Leverage a DeepSpeed Config file to tweak more options\\n\\nFirst, We will look at the ...\"],[\"We will leverage the DeepSpeed Zero Stage-2 config [zero2_config_accelerate.json](https:\\u002f\\u002fgithub.com...\"],[\"```\\n\\nTo enable DeepSpeed ZeRO Stage-2 with above config, please run `accelerate config` and provide ...\"],[\"Now, run below command for training:\\n```bash\\naccelerate launch run_seq2seq_no_trainer.py \\\\\\n    --dat...\"],[\"```\\n\\nWhen using DeepSpeed config, if user has specified `optimizer` and `scheduler` in config, the u...\"],[\"```\\n\\n---\\n| Method | Batch Size Max | Eval Size Max | Train time per epoch (seconds) | Eval time  per...\"],[\"---\\n## CPU\\u002fDisk Offloading to enable training humongous models that won’t fit the GPU memory\\n\\nOn a s...\"],[\"We will leverage the DeepSpeed Zero Stage-3 CPU offload config [zero3_offload_config_accelerate.json...\"],[\"\\\"pin_memory\\\": true\\n        },\\n        \\\"offload_param\\\": {\\n            \\\"device\\\": \\\"cpu\\\",\\n            \\\"p...\"],[\"```\\n\\n**ZeRO Stage-3 CPU Offload DeepSpeed Config File Example**\\n```bash\\ncompute_environment: LOCAL_M...\"],[\"```\\n\\n---\\n| Method | Batch Size Max | Train time per epoch (seconds) | Notes |\\n| --- | --- | --- | --...\"],[\"--\\ntitle: \\\"Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\\\"\\nthumbnail: \\u002f...\"],[\"The main bottleneck is the latency of predictions which can make large deployments expensive to run ...\"],[\"An Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learnin...\"],[\"In addition to superior performance for machine learning workloads, the Intel Ice Lake C6i instances...\"],[\"In each experiment, we collect numbers for:\\n* Throughput (requests per second)\\n* Latency (min, max, ...\"],[\"```\\n\\n### Throughput\\n\\nBelow you can find the throughput comparison for running infinity on 2 physical...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co\\u002finfinity-...\"],[\"--\\ntitle: \\\"Introducing ⚔️ AI vs. AI ⚔️ a deep reinforcement learning multi-agents competition system...\"],[\"Let’s see how it works with our first competition host: SoccerTwos Challenge.\\n\\n\\u003cdiv align=\\\"center\\\"\\u003e ...\"],[\"If you want to learn more about ELO and see some calculation example, we wrote an explanation in our...\"],[\"To run this matchmaking process continuously, we use **free Hugging Face Spaces hardware with a Sche...\"],[\"## Our first AI vs. AI challenge experimentation: SoccerTwos Challenge ⚽\\n\\nThis challenge is Unit 7 o...\"],[\"We also [created a discord channel called ai-vs-ai-competition](http:\\u002f\\u002fhf.co\\u002fdiscord\\u002fjoin) so that p...\"],[\"```\\n@article{cochet-simonini2023,\\n  author = {Cochet, Carl and Simonini, Thomas},\\n  title = {Introdu...\"],[\"--\\ntitle: \\\"SetFit: Efficient Few-Shot Learning Without Prompts\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f103_setfit\\u002fi...\"],[\"\\u003cp\\u003e🏎 \\u003cstrong\\u003eFast to train\\u003c\\u002fstrong\\u003e: SetFit doesn't require large-scale models like T0 or GPT-3 to a...\"],[\"SetFit takes advantage of Sentence Transformers’ ability to generate dense embeddings based on paire...\"],[\"| Rank | Method | Accuracy | Model Size | \\n| :------: | ------ | :------: | :------: | \\n| 2 | T-Few ...\"],[\"Since SetFit achieves high accuracy with relatively small models, it's blazing fast to train and at ...\"],[\"```\\nNext, we import `SetFitModel` and `SetFitTrainer`, two core classes that streamline the SetFit t...\"],[\"```\\nThe last step is to train and evaluate the model:\\n```python\\n# Train and evaluate!\\ntrainer.train(...\"],[\"--\\ntitle: \\\"Creating a Coding Assistant with StarCoder\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fstarchat_alpha\\u002fthumbn...\"],[\"In this blog post, we’ll show how StarCoder can be fine-tuned for chat to create a personalised codi...\"],[\"To get started, let’s take a look at how language models can be turned into conversational agents wi...\"],[\"```\\nBelow are a series of dialogues between various people and an AI assistant.\\nThe AI tries to be h...\"],[\"```\\n\\nAs we can see, the first part of the prompt “Below are a series...” corresponds to the system m...\"],[\"```\\nBelow are a series of dialogues between various people and an AI technical assistant.\\nThe assist...\"],[\"Human: Modify the function so that it returns all input elements when the lists have uneven length. ...\"],[\"```\\n\\nHere we can see how a well crafted prompt can induce coding behaviour similar to that observed ...\"],[\"Let’s start by downloading the processed dataset from the Hub:\\n\\n```python\\nfrom datasets import load_...\"],[\"```\\n\\n```\\nDatasetDict({\\n    train: Dataset({\\n        features: ['messages'],\\n        num_rows: 19034\\n...\"],[\"```\\n{\\n    \\\"messages\\\": [\\n        {\\n            \\\"content\\\": \\\"Is it possible to imagine a society withou...\"],[\"\\\"role\\\": \\\"user\\\",\\n        },\\n        {\\n            \\\"content\\\": \\\"You are correct that there are other fa...\"],[\"```\\n\\nOK, this looks like an interesting dialogue about moral philosophy, with each turn involving a ...\"],[\"```\\n\\nAlthough this works fine for training, it isn’t ideal for inference because the model will natu...\"],[\"```\\n\\n```\\n\\u003c|system|\\u003e\\nBelow is a dialogue between a human and AI assistant called StarChat.\\n\\u003c|end|\\u003e\\n\\u003c|...\"],[\"```\\n\\n```\\n{\\\"input_ids\\\": [49153], \\\"attention_mask\\\": [1]}\\n```\\n\\nGreat, it works!\\n\\n### Masking user label...\"],[\"```\\n\\nOK, we can see that all the user input IDs have been masked in the labels as desired. These spe...\"],[\"```\\n\\nNext, create a Python virtual environment using e.g. Conda:\\n\\n```shell\\nconda create -n starchat ...\"],[\"```\\n\\nHere the `config.yaml` file specifies all the parameters associated with the dataset, model, an...\"],[\"```\\n\\nResponse:\\n\\n```python\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nplt...\"],[\"```\\nDraw me a map of the world using geopandas. Make it so that only Germany and Spain are colored r...\"],[\"```\\nThere was a basketball game with the following stats. player, points, rebounds and assists: J. H...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"The results are shown in the table below, where we can see the fine-tuned model has improved, but no...\"],[\"```\\nGenerate a bunch of instructions for coding questions in python (in the format of {\\\"prompt\\\": ins...\"],[\"```\\nWrite a Python function called reverse_string that takes a string as its argument and returns th...\"],[\"```\\n\\nBase-model completion (Assistant 1):\\n\\n```\\n\\\"Sure thing! Let's start by writing out the docstring...\"],[\"```\\n\\nWe can compare this to ChatGPT’s response, which seems to miss the fact that the Assistant 1 do...\"],[\"```\\n\\nThis shows us that while there is extremely valuable signal in AI evaluations, we have a lot to...\"],[\"## Links\\n\\n- Code: [https:\\u002f\\u002fgithub.com\\u002fbigcode-project\\u002fstarcoder\\u002ftree\\u002fmain\\u002fchat](https:\\u002f\\u002fgithub.com\\u002fb...\"],[\"```\\n@article{Tunstall2023starchat-alpha,\\n  author = {Tunstall, Lewis and Lambert, Nathan and Rajani,...\"],[\"--\\ntitle: \\\"AI Policy @🤗: Response to the U.S. NTIA's Request for Comment on AI Accountability\\\"\\nthumb...\"],[\"Hugging Face’s mission is to [“democratize good machine learning”](https:\\u002f\\u002fhuggingface.co\\u002fabout). We...\"],[\"Concretely, we make the following recommendations for accountability mechanisms:\\n\\n* Accountability m...\"],[\"--\\ntitle: \\\"Creating Privacy Preserving AI with Substra\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f139_owkin-substra\\u002ft...\"],[\"As the data never leaves its source, federated learning is naturally a privacy-first approach. Not o...\"],[\"![Substra diagram](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblo...\"],[\"--\\ntitle:  Deploy Embedding Models with Hugging Face Inference Endpoints\\nthumbnail: \\u002fblog\\u002fassets\\u002f168...\"],[\"Before we start, let's refresh our knowledge about Inference Endpoints.\\n\\n## 1. What is Hugging Face ...\"],[\"You can get started with Inference Endpoints at: https:\\u002f\\u002fui.endpoints.huggingface.co\\u002f\\n\\n## 2. What is...\"],[\"Those feature enabled industry-leading performance on throughput and cost. In a benchmark for [BAAI\\u002f...\"],[\"*Note: If the instance type cannot be selected, you need to [contact us](mailto:api-enterprise@huggi...\"],[\"```python\\nimport requests\\n\\nAPI_URL = \\\"https:\\u002f\\u002fl2skjfwp9punv393.us-east-1.aws.endpoints.huggingface.c...\"],[\"```\\n\\n## Conclusion\\n\\nTEI on Hugging Face Inference Endpoints enables blazing fast and ultra cost-effi...\"],[\"--\\ntitle: \\\"What Makes a Dialog Agent Useful?\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fdialog-agents\\u002fthumbnail.png\\na...\"],[\"The following table compares these AI chatbots based on the details of their public access, training...\"],[\"| &nbsp;| LaMDA | BlenderBot 3 |Sparrow | ChatGPT\\u002f InstructGPT | Assistant|\\n| --- | --- | --- | --- ...\"],[\"| **RLHF** | ✖️ | ✖️ | ✔ | ✔ | ✔ |\\n| **Hand written rules for safety** | ✔ | ✖️ | ✔ | ✖️ | ✔ |\\n| **E...\"],[\"We observe that albeit there are many differences in the training data, model, and fine-tuning, ther...\"],[\"![Instruction and instance example](assets\\u002fdialog-agents\\u002fift.png)\\n\\nData for IFT is usually a collect...\"],[\"![IFT spectrum](assets\\u002fdialog-agents\\u002fift-spectrum.png)\\n\\nOn one end is the purely model-generated IFT...\"],[\"### Safely following instructions\\n\\nInstruction fine-tuned LMs, however, may not always generate resp...\"],[\"### Fine-tuning the models\\n\\nOn the other hand, Open AI’s InstructGPT, DeepMind’s Sparrow, and Anthro...\"],[\"![Illustration of CoT](assets\\u002fdialog-agents\\u002fcot.png)\\n\\nModels fine-tuned with CoT have shown to perfo...\"],[\"## Next steps for dialogue agents\\n\\nThis blog summarizes many of the existing work on what makes a di...\"],[\"```\\n@article{rajani2023ift,\\n  author = {Rajani, Nazneen and Lambert, Nathan and Sanh, Victor and Wol...\"],[\"--\\ntitle: \\\"AI for Game Development: Creating a Farming Game in 5 Days. Part 1\\\"\\nthumbnail: \\u002fblog\\u002fasse...\"],[\"### Setting up Stable Diffusion\\n\\nThere are a couple options for running Stable Diffusion: *locally* ...\"],[\"```\\ngit clone https:\\u002f\\u002fgithub.com\\u002fAUTOMATIC1111\\u002fstable-diffusion-webui.git...\"],[\"```\\n4. Download the [Stable Diffusion 1.5 weights](https:\\u002f\\u002fhuggingface.co\\u002frunwayml\\u002fstable-diffusion-...\"],[\"*Note:* Parts of this series will use advanced features such as image2image, which may not be availa...\"],[\"I settled on the prompt: *isometric render of a farm by a river, simple, solid shapes, james gillear...\"],[\"4. Set up your [Lighting](https:\\u002f\\u002fdocs.unity3d.com\\u002fManual\\u002fLighting.html). I'm using a warm sun (#FFE...\"],[\"\\u003cfigure class=\\\"image text-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumenta...\"],[\"--\\ntitle: \\\"From PyTorch DDP to Accelerate to Trainer, mastery of distributed training with ease\\\"\\nthu...\"],[\"```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as o...\"],[\"```\\n\\nWe define the training device (`cuda`):\\n\\n```python\\ndevice = \\\"cuda\\\"\\n```\\n\\nBuild some PyTorch Data...\"],[\"```\\n\\nTypically from here, one could either throw all of this into a python script or run it on a Jup...\"],[\"```\\n\\nThe last piece of the puzzle is *how do I send my data and model to another GPU?*\\n\\nThis is wher...\"],[\"```\\n\\nThe above will run the training script on two GPUs that live on a single machine and this is th...\"],[\"# Build optimizer\\n    optimizer = optim.AdamW(ddp_model.parameters(), lr=1e-3)\\n\\n    # Train for a si...\"],[\"```\\n\\nNext let's talk about how Accelerate can help. There's a few issues with the above code:\\n\\n1. Th...\"],[\"# Send everything through `accelerator.prepare`\\n    train_loader, test_loader, model, optimizer = ac...\"],[\"```\\n\\nWith this your PyTorch training loop is now setup to be ran on any distributed setup thanks to ...\"],[\"```\\n\\nOr:\\n\\n```python\\nnotebook_launcher(train_ddp_accelerate, args=(), num_processes=2)\\n```\\n\\n## Using ...\"],[\"```\\n\\n```python\\ntrainer.train()\\n```\\n\\n```python out\\n    ***** Running training *****\\n      Num example...\"],[\"```\\n\\n## Resources\\n\\nTo learn more about PyTorch Distributed Data Parallelism, check out the documenta...\"],[\"--\\ntitle: \\\"Probabilistic Time Series Forecasting with 🤗 Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f118_ti...\"],[\"Some classical methods are point-valued (meaning, they just output a single value per time step) and...\"],[\"To begin with, the use of an Encoder-Decoder architecture is helpful at inference time where typical...\"],[\"A drawback of the Transformer architecture is the limit to the sizes of the context and prediction w...\"],[\"```\\n\\n## Load Dataset\\n\\nIn this blog post, we'll use the `tourism_monthly` dataset, which is available...\"],[\"```\\n\\nThe `start` simply indicates the start of the time series (as a datetime), and the `target` con...\"],[\"```\\n\\nHowever, this example has `prediction_length=24` additional values compared to the training exa...\"],[\"```\\n\\nWe now use `datasets`' [`set_transform`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fv2.7.0\\u002fen\\u002fpackage...\"],[\"```\\n\\n## Define the Model\\n\\nNext, let's instantiate a model. The model will be trained from scratch, h...\"],[\"Let's use the default lags provided by GluonTS for the given frequency (\\\"monthly\\\"):\\n\\n\\n```python\\nfrom...\"],[\"```\\n\\n\\nThis means that we'll look back up to 37 months for each time step, as additional features.\\n\\nL...\"],[\"```\\n\\nNote that, similar to other models in the 🤗 Transformers library, [`TimeSeriesTransformerModel`...\"],[\"```\\n\\nThe transformations below are annotated with comments, to explain what they do. At a high level...\"],[\"# a bit like torchvision.transforms.Compose\\n    return Chain(\\n        # step 1: remove static\\u002fdynami...\"],[\"output_field=FieldName.OBSERVED_VALUES,\\n            ),\\n            # step 4: add temporal features b...\"],[\"FieldName.FEAT_STATIC_REAL: \\\"static_real_features\\\",\\n                    FieldName.FEAT_TIME: \\\"time_f...\"],[\"```\\n\\n## Define `InstanceSplitter`\\n\\nFor training\\u002fvalidation\\u002ftesting we next create an `InstanceSplitt...\"],[\"# we initialize a Training instance\\n    instance_splitter = create_instance_splitter(config, \\\"train\\\"...\"],[\"```\\n\\n\\n```python\\ndef create_backtest_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n  ...\"],[\"```\\n\\nWe have a test dataloader helper for completion, even though we will not use it here. This is u...\"],[\"```\\n\\nLet's check the first batch:\\n\\n\\n```python\\nbatch = next(iter(train_dataloader))\\nfor k, v in batch...\"],[\"```\\n\\n\\nAs can be seen, we don't feed `input_ids` and `attention_mask` to the encoder (as would be the...\"],[\"```\\n\\nNote that the model is returning a loss. This is possible as the decoder automatically shifts t...\"],[\"model, optimizer, train_dataloader = accelerator.prepare(\\n    model,\\n    optimizer,\\n    train_datalo...\"],[\"```\\n\\n\\n## Inference\\n\\nAt inference time, it's recommended to use the `generate()` method for autoregre...\"],[\"```\\n\\nWe can evaluate the resulting forecast with respect to the ground truth out of sample values pr...\"],[\"```\\n\\nWe can also plot the individual metrics of each time series in the dataset and observe that a h...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002ftime-...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002ftime-...\"],[\"Of course, we need to be careful with just claiming state-of-the-art results on time series with neu...\"],[\"Another thing on the roadmap is time series classification. This entails adding a time series model ...\"],[\"--\\ntitle: Image Classification with AutoTrain \\nthumbnail: \\u002fblog\\u002fassets\\u002f105_autotrain-image-classific...\"],[\"[Image Classification](https:\\u002f\\u002fhuggingface.co\\u002ftasks\\u002fimage-classification) models learn to *categoriz...\"],[\"Once AutoTrain creates your project, you just need to connect your data. If you have the data locall...\"],[\"\\u003cdiv class=\\\"grid grid-cols-2 gap-4\\\"\\u003e\\n  \\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cmedi...\"],[\"In the screenshots above you can see that my project started 5 different models, which each reached ...\"],[\"--\\ntitle: \\\"Fine-Tune XLSR-Wav2Vec2 for low-resource ASR with 🤗 Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"XLSR\\\\'s successor, simply called **XLS-R** (refering to the\\n[*\\\\'\\\\'XLM-R*](https:\\u002f\\u002fai.facebook.com\\u002fbl...\"],[\"![wav2vec2\\\\_structure](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fx...\"],[\"```python\\n!pip install datasets==1.18.3\\n!pip install transformers==4.11.3\\n!pip install huggingface_h...\"],[\"```\\n\\nWe strongly suggest to upload your training checkpoints directly to the\\n[Hugging Face Hub](http...\"],[\"```\\n\\n------------------------------------------------------------------------\\n\\n\\\\\\\\( {}^1 \\\\\\\\) In the [...\"],[\"Let\\\\'s start by creating the tokenizer to decode the predicted output\\nclasses to the output transcri...\"],[\"For each language-specific dataset, you can find a language code\\ncorresponding to your chosen langua...\"],[\"```\\n\\nMany ASR datasets only provide the target text, `'sentence'` for each\\naudio array `'audio'` and...\"],[\"```\\n\\n**Print Output:**\\n\\n| Idx |  Sentence |\\n|----------|:-------------:|\\n|\\t1  | Jonuz, kısa süreli g...\"],[\"Let\\\\'s simply remove all characters that don\\\\'t contribute to the\\nmeaning of a word and cannot reall...\"],[\"```\\n\\n```python\\ncommon_voice_train = common_voice_train.map(remove_special_characters)\\ncommon_voice_t...\"],[\"```\\n\\n**Print Output:**\\n\\n| Idx |  Transcription     |\\n|----------|:-------------:|\\n| 1   | birisi bey...\"],[\"Let\\\\'s write another short mapping function to further simplify the text\\nlabels. Remember, the simpl...\"],[\"```\\n\\n```python\\ncommon_voice_train = common_voice_train.map(replace_hatted_characters)\\ncommon_voice_t...\"],[\"```\\n\\n```python\\nvocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\\nvocab_dict\\n```\\n\\n**Print...\"],[\"```\\n\\nCool, we see that all letters of the alphabet occur in the dataset\\n(which is not really surpris...\"],[\"```\\n\\nCool, now our vocabulary is complete and consists of 39 tokens, which\\nmeans that the linear lay...\"],[\"```\\n\\nGreat, you can see the just created repository under\\n`https:\\u002f\\u002fhuggingface.co\\u002f\\u003cyour-username\\u003e\\u002fwa...\"],[\"A `Wav2Vec2FeatureExtractor` object requires the following parameters to\\nbe instantiated:\\n\\n-   `feat...\"],[\"```\\n\\nGreat, XLS-R\\\\'s feature extraction pipeline is thereby fully defined!\\n\\nFor improved user-friend...\"],[\"```\\n\\nGreat, we can see that the audio file has automatically been loaded.\\nThis is thanks to the new ...\"],[\"```\\n\\nThis seemed to have worked! Let\\\\'s listen to a couple of audio files to\\nbetter understand the d...\"],[\"```\\n\\nGood! Everything looks fine - the data is a 1-dimensional array, the\\nsampling rate always corre...\"],[\"```python\\ndef prepare_dataset(batch):\\n    audio = batch[\\\"audio\\\"]\\n\\n    # batched output is \\\"un-batche...\"],[\"```\\n\\nLet\\\\'s apply the data preparation function to all examples.\\n\\n```python\\ncommon_voice_train = com...\"],[\"```\\n\\nAwesome, now we are ready to start training!\\n\\nTraining\\n--------\\n\\nThe data is processed so that ...\"],[\"Without going into too many details, in contrast to the common data\\ncollators, this data collator tr...\"],[\"processor: Wav2Vec2Processor\\n    padding: Union[bool, str] = True\\n\\n    def __call__(self, features: ...\"],[\"```\\n\\n```python\\ndata_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\\n```\\n\\nNe...\"],[\"```\\n\\nThe model will return a sequence of logit vectors:\\n\\\\\\\\( \\\\mathbf{y}_1, \\\\ldots, \\\\mathbf{y}_m \\\\\\\\) w...\"],[\"```\\n\\nNow, we can load the pretrained checkpoint of\\n[Wav2Vec2-XLS-R-300M](https:\\u002f\\u002fhuggingface.co\\u002fface...\"],[\"```\\n\\nThe first component of XLS-R consists of a stack of CNN layers that are\\nused to extract acousti...\"],[\"```\\n\\nIn a final step, we define all parameters related to training. To give\\nmore explanation on some...\"],[\"```\\n\\n------------------------------------------------------------------------\\n\\n\\\\\\\\( {}^1 \\\\\\\\) To allow...\"],[\"```\\n\\n**Print Output:**\\n\\n| Training Loss | Epoch | Step | Validation Loss | Wer    |\\n|:-------------:...\"],[\"```\\n\\nFor more examples of how XLS-R can be fine-tuned, please take a look at the official \\n[🤗 Transf...\"],[\"```\\n\\nFinally, we can decode the example.\\n\\n```python\\nprint(\\\"Prediction:\\\")\\nprint(processor.decode(pred...\"],[\"--\\ntitle: \\\"Advantage Actor Critic (A2C)\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f89_deep_rl_a2c\\u002fthumbnail.gif\\nauthor...\"],[\"[In Unit 5](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fdeep-rl-pg), we learned about our first Policy-Based algorit...\"],[\"Sounds exciting? Let's get started!\\n  \\n- [The Problem of Variance in Reinforce](https:\\u002f\\u002fhuggingface....\"],[\"This return \\\\\\\\(R(\\\\tau)\\\\\\\\) is calculated using a *Monte-Carlo sampling*. Indeed, we collect a traject...\"],[\"However, increasing the batch size significantly **reduces sample efficiency**. So we need to find a...\"],[\"On the other hand, your friend (Critic) will also update their way to provide feedback so it can be ...\"],[\"Let's see the training process to understand how Actor and Critic are optimized:\\n- At each timestep,...\"],[\"The idea is that the Advantage function calculates **how better taking that action at a state is com...\"],[\"Start the tutorial here 👉 [https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fhuggingface\\u002fdeep-rl-class\\u002fblob\\u002fm...\"],[\"In the next unit, we will learn to improve Actor-Critic Methods with Proximal Policy Optimization.\\n\\n...\"],[\"--\\ntitle: \\\"Faster Training and Inference: Habana Gaudi®2 vs Nvidia A100 80GB\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"One of the easy, cost-efficient ways that Intel and Habana have made Gaudi2 available is on the Inte...\"],[\"7. Go to the [Intel Developer Cloud management console](https:\\u002f\\u002fscheduler.cloud.intel.com\\u002f#\\u002fsystems)...\"],[\"Since Gaudi2 has roughly 3 times more memory per device compared to first-gen Gaudi, it is possible ...\"],[\"The following table displays the throughputs we got for first-gen Gaudi, Gaudi2 and Nvidia A100 80GB...\"],[\"[This script](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-habana\\u002ftree\\u002fmain\\u002fexamples\\u002fstable-diffusion) was...\"],[\"### Fine-tuning T5-3B\\n\\nWith 96 GB of memory per device, Gaudi2 enables running much bigger models. F...\"],[\"*BS* is the batch size per device. Gaudi2 and A100 runs were performed in fp32 with gradient checkpo...\"],[\"---\\n\\nThanks for reading! If you have any questions, feel free to contact me, either through [Github]...\"],[\"--\\ntitle: \\\"Parameter-Efficient Fine-Tuning using 🤗 PEFT\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f130_peft\\u002fthumbnail....\"],[\"PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parame...\"],[\"1. LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2106.09685.pdf)\\n...\"],[\"2. Taking the previous example a notch up by enabling INT8 tuning of the `OPT-6.7b` model (6.7 Billi...\"],[\"1. Let's get the necessary imports\\n\\n```diff\\n  from transformers import AutoModelForSeq2SeqLM\\n+ from ...\"],[\"```\\n\\n2. Creating config corresponding to the PEFT method\\n```py\\npeft_config = LoraConfig(\\n    task_ty...\"],[\"```\\n\\nThis will only save the incremental PEFT weights that were trained. For example, you can find t...\"],[\"with torch.no_grad():\\n      outputs = model.generate(input_ids=inputs[\\\"input_ids\\\"].to(\\\"cuda\\\"), max_n...\"],[\"```\\n\\n## Next steps\\nWe've released PEFT as an efficient way of tuning large LLMs on downstream tasks ...\"],[\"--\\ntitle: \\\"MTEB: Massive Text Embedding Benchmark\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f110_mteb\\u002fthumbnail.png\\na...\"],[\"## Why Text Embeddings?\\n\\nText Embeddings are vector representations of text that encode semantic inf...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"assets\\u002f110_mteb\\u002fmteb_diagram_white_background.png\\\" alt=\\\"MTEB Taxono...\"],[\"**💪 Maximum performance** Multi-billion parameter models like [ST5-XXL](https:\\u002f\\u002fhuggingface.co\\u002fsente...\"],[\"```\\n\\nNext, benchmark a model on a dataset, for example [komninos word embeddings](https:\\u002f\\u002fhuggingfac...\"],[\"```\\n\\nNow add the metadata to the top of a `README.md` of any model on the Hub, like this [SGPT-5.8B-...\"],[\"--\\ntitle: Deploying 🤗 ViT on Vertex AI\\nthumbnail: \\u002fblog\\u002fassets\\u002f97_vertex_ai\\u002fimage1.png\\nauthors:\\n- us...\"],[\"\\u003e Vertex AI provides tools to support your entire ML workflow, across\\ndifferent model types and vary...\"],[\"- Vertex AI\\n\\n- Cloud Storage\\n\\n# Revisiting the Serving Model\\n\\nYou’ll use the same [\\u003cu\\u003eViT B\\u002f16 model...\"],[\"```\\n\\nThe model will accept [\\u003cu\\u003ebase64 encoded\\u003c\\u002fu\\u003e](https:\\u002f\\u002fwww.base64encode.org\\u002f) strings of images,...\"],[\"The currently supported model types include `SavedModel` from\\nTensorFlow, scikit-learn, and XGBoost....\"],[\"**1.** The first step in the workflow is to upload the `SavedModel` to\\nVertex AI’s model registry:\\n\\n...\"],[\"```\\n\\nLet’s unpack the code piece by piece:\\n\\n- `GCS_BUCKET` denotes the path of your GCS bucket where...\"],[\"```\\n\\nHere you’re using an `endpoint_service_client` which is an\\n[`EndpointServiceClient`](https:\\u002f\\u002fcl...\"],[\"```\\n\\nHere, you’re chaining together the model you uploaded to the Vertex AI\\nModel Registry and the E...\"],[\"```\\n\\nNotice how you’re defining the traffic split for the model. If you had\\nmultiple versions of the...\"],[\"```\\nfrom google.protobuf import json_format\\nfrom google.protobuf.struct_pb2 import Value\\n\\ndef predic...\"],[\"```\\n\\nNote, however, this is not the only way to obtain predictions using a\\nVertex AI Endpoint. If yo...\"],[\"# Local Load Testing\\n\\nWe conducted a local load test to better understand the limits of the\\nEndpoint...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e\\n\\n| **Machine Type**            | **Hourly Pricing (USD)** |\\n|:-----------------...\"],[\"# Conclusion\\n\\nIn this post, you learned how to deploy a Vision Transformer model with\\nthe Vertex AI ...\"],[\"--\\ntitle: 'Few-shot learning in practice: GPT-Neo and the 🤗 Accelerated Inference API'\\n# thumbnail: ...\"],[\"Few-Shot NLP examples consist of three main components: \\n\\n- **Task Description**: A short descriptio...\"],[\"---\\n\\n## What is GPT-Neo?\\n\\nGPT⁠-⁠Neo is a family of transformer-based language models from [EleutherA...\"],[\"```python\\nimport json\\nimport requests\\n\\nAPI_TOKEN = \\\"\\\"\\n\\ndef query(payload='',parameters=None,options=...\"],[\"```\\n\\n---\\n## Practical Insights\\n\\nHere are some practical insights, which help you get started using `...\"],[\"\\u003e ###  \\n\\u003e Tweet: \\\"I'm a disabled happy person\\\"  \\n\\u003e Sentiment: Negative  \\n\\nWhat could go wrong? Imagi...\"],[\"--\\ntitle: \\\"Director of Machine Learning Insights [Part 3: Finance Edition]\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"### [Ioannis Bakagiannis](https:\\u002f\\u002fwww.linkedin.com\\u002fin\\u002fbakagiannisioannis\\u002f\\u002f) - Director of Machine Le...\"],[\"#### **2. What are the biggest ML challenges within finance?**\\nI can’t speak for companies but estab...\"],[\"#### **4. What excites you most about the future of ML?**\\nIt is difficult not to get excited with ev...\"],[\"#### **1. How has ML made a positive impact on finance?**\\nMachine learning (ML) has made a significa...\"],[\"2. Gap between ML in basic research and education and ML in finance - Due to the regulated nature of...\"],[\"#### **3. What’s a common mistake you see people make trying to integrate ML into financial applicat...\"],[\"Previously, he held the position of Fellows Leader & Senior Fellow, while working at Honeywell Inter...\"],[\"#### **2. What are the biggest ML challenges within finance?**\\nThe finance and banking industry brin...\"],[\"#### **4. What excites you most about the future of ML?**\\nNow is a great time to be in applied ML an...\"],[\"--\\ntitle: \\\"Instruction-tuning Stable Diffusion with InstructPix2Pix\\\" \\nthumbnail: assets\\u002finstruction_...\"],[\"Our code, pre-trained models, and datasets can be found [here](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002finstru...\"],[\"With this approach, one can create exemplars covering many different tasks, which makes instruction-...\"],[\"| ![cartoonization_results](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve...\"],[\"As hinted in the previous section, we wanted to benefit from both worlds:\\n\\n**(1)** training methodol...\"],[\"Our final dataset for cartoonization can be found [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002finstruction...\"],[\"We took different number of samples from the following datasets for each task and constructed a sing...\"],[\"Overall, this setup helps draw parallels from the FLAN setup, where we create a mixture of different...\"],[\"In our experiments, we found out that the first option helps us adapt to our datasets faster (in ter...\"],[\"More comparative results are available [here](https:\\u002f\\u002fwandb.ai\\u002fsayakpaul\\u002finstruction-tuning-sd\\u002fruns\\u002f...\"],[\"However, for low-light image enhancement, it leaves a lot to be desired: \\n\\n| ![image_enhancement_res...\"],[\"\\u003cgradio-app theme_mode=\\\"light\\\" src=\\\"https:\\u002f\\u002finstruction-tuning-sd-instruction-tuned-sd.hf.space\\\"\\u003e\\u003c\\u002fg...\"],[\"## Open questions\\n\\nWe acknowledge that our experiments are preliminary. We did not go deep into abla...\"],[\"## Conclusion\\n\\nIn this post, we presented our exploration of “instruction-tuning” of Stable Diffusio...\"],[\"## Citation\\n\\nTo cite this work, please use the following citation:\\n\\n```bibtex\\n@article{\\n  Paul2023in...\"],[\"--\\ntitle: \\\"Deploying the AI Comic Factory using the Inference API\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f165_ai_co...\"],[\"## Duplicating the Space\\n\\nTo duplicate the AI Comic Factory, go to the Space and [click on \\\"Duplicat...\"],[\"You can find more information about alternative engines and vendors in the project's [README](https:...\"],[\"--\\ntitle: What's new in Diffusers? 🎨\\nthumbnail: \\u002fblog\\u002fassets\\u002f102_diffusers_2nd_month\\u002finpainting.png\\n...\"],[\"## Image to Image pipeline\\n\\nOne of the most requested features was to have image to image generation...\"],[\"```\\n\\nDon't have time for code? No worries, we also created a [Space demo](https:\\u002f\\u002fhuggingface.co\\u002fspa...\"],[\"## Experimental inpainting pipeline\\n\\nInpainting allows to provide an image, then select an area in t...\"],[\"```\\n\\nPlease note this is experimental, so there is room for improvement.\\n\\n## Optimizations for small...\"],[\"```\\n\\n## Experimental ONNX exporter and pipeline\\n\\nThe new experimental pipeline allows users to run S...\"],[\"```\\n\\n## New docs\\n\\nAll of the previous features are very cool. As maintainers of open-source librarie...\"],[\"```\\n\\n\\n### Diffusers Interpret\\n\\n[Diffusers interpret](https:\\u002f\\u002fgithub.com\\u002fJoaoLages\\u002fdiffusers-interpre...\"],[\"```\\n\\n### Japanese Stable Diffusion\\n\\nThe name says it all! The goal of JSD was to train a model that ...\"],[\"## Thanks for reading!\\n\\nI hope you enjoy reading this! Remember to give a Star in our [GitHub Reposi...\"],[\"--\\ntitle: \\\"Announcing the Hugging Face Fellowship Program\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f62_fellowship\\u002ffel...\"],[\"- **María Grandury** - Created the [largest Spanish-speaking NLP community](https:\\u002f\\u002fsomosnlp.org\\u002f) a...\"],[\"- **Christopher Akiki** - Contributed to sprints, workshops, [Big Science](https:\\u002f\\u002ft.co\\u002foIRne5fZYb),...\"],[\"Additionally, there are strategic areas where Hugging Face is looking for open-source contributions....\"],[\"* **Where and how can I contribute?**\\n  \\nIt depends on your interests. Here are some ideas of areas ...\"],[\"Please share in the #looking-for-contributors channel on the [Hugging Face Discord](https:\\u002f\\u002fhf.co\\u002fjo...\"],[\"--\\ntitle: \\\"How 🤗 Accelerate runs very large models thanks to PyTorch\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f104_ac...\"],[\"```\\n\\nWe'll explain what each of those arguments do in a moment, but first just consider the traditio...\"],[\"## Creating an empty model\\n\\nPyTorch 1.9 introduced a new kind of device called the *meta* device. Th...\"],[\"```\\n\\nas this large tensor requires `4 * 10**10` bytes (the default precision is FP32, so each elemen...\"],[\"```\\n\\nThis works on any model, but you get back a shell you can't use directly: some operations are i...\"],[\"```\\n\\nThis will return a dictionary mapping modules or weights to a device. On a machine with one Tit...\"],[\"```\\n\\nAccelerate evaluated that the embeddings and the decoder up until the 9th block could all fit o...\"],[\"```\\n\\nNow, each layer is always on the same device.\\n\\nIn Transformers, when using `device_map` in the ...\"],[\"```\\n\\nIn this precision, we can fit the model up to layer 21 on the GPU:\\n\\n```python out\\n\\n\\n{'model.dec...\"],[\"```\\n\\nThis works pretty well for models with less than 1 billion parameters, but for larger models, t...\"],[\"To load such a sharded checkpoint into a model, we just need to loop over the various shards. Accele...\"],[\"```\\n\\nIf the device map computed automatically requires some weights to be offloaded on disk because ...\"],[\"```\\n\\nThis will fit in Colab, but will be so close to using all the RAM available that it will go out...\"],[\"```\\n\\n## Running a model split on several devices\\n\\nOne last part we haven't touched is how Accelerate...\"],[\"This way, your model can be loaded and run even if you don't have enough GPU RAM and CPU RAM. The on...\"],[\"--\\ntitle: 'The Partnership: Amazon SageMaker and Hugging Face'\\nthumbnail: \\u002fblog\\u002fassets\\u002f17_the_partne...\"],[\"---\\n\\n## **Features & Benefits 🔥**\\n\\n## One Command is All you Need\\n\\nWith the new Hugging Face Deep Le...\"],[\"Below you can find all the important resources to all published blog posts, videos, documentation, a...\"],[\"## Documentation\\n\\n- [Hugging Face documentation for Amazon SageMaker](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fsa...\"],[\"- [all Notebooks](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002ftree\\u002fmaster\\u002fsagemaker)\\n- [Getting Started...\"],[\"- [Image Classification with Vision Transformer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmaste...\"],[\"---\\n\\n## **Getting started: End-to-End Text Classification 🧭**\\n\\nIn this getting started guide, we wil...\"],[\"```\\n\\nTo run training on SageMaker we need to create a sagemaker Session and provide an IAM role with...\"],[\"```\\n\\n## Create the training script `train.py`\\n\\nIn a SageMaker `TrainingJob` we are executing a pytho...\"],[\"# Data, model, and output directories\\n    parser.add_argument(\\\"--output-data-dir\\\", type=str, default...\"],[\"# compute metrics function for binary classification\\n    def compute_metrics(pred):\\n        labels =...\"],[\"# Saves the model to s3; default is \\u002fopt\\u002fml\\u002fmodel which SageMaker sends to S3\\n    trainer.save_model...\"],[\"```\\n\\n## Preprocess our data and upload it to s3\\n\\nWe use the `datasets` library to download and prepr...\"],[\"# set format for pytorch\\ntrain_dataset = train_dataset.rename_column(\\\"label\\\", \\\"labels\\\")\\ntrain_datase...\"],[\"```\\n\\n## Create a HuggingFace Estimator and train our model\\n\\nIn order to create a SageMaker `Training...\"],[\"```\\n\\nTo start our training we call the .fit() method and pass our S3 uri as input.\\n\\n```python\\n# star...\"],[\"```\\n\\nThe \\\"Getting started: End-to-End Text Classification 🧭\\\" example can be used for distributed tra...\"],[\"distribution={\\n    \\\"smdistributed\\\": {\\\"modelparallel\\\": smp_options},\\n    \\\"mpi\\\": mpi_options\\n}\\n\\n # cre...\"],[\"```\\n\\n## Spot instances\\n\\nWith the creation of HuggingFace Framework extension for the SageMaker Pytho...\"],[\"huggingface_estimator = HuggingFace(\\n        entry_point='train.py',\\n        source_dir='.\\u002fscripts',...\"],[\"```\\n\\n## Git Repositories\\n\\nWhen you create an `HuggingFace` Estimator, you can specify a [training sc...\"],[\"```\\n\\n## SageMaker Metrics\\n\\n[SageMaker Metrics](https:\\u002f\\u002fdocs.aws.amazon.com\\u002fsagemaker\\u002flatest\\u002fdg\\u002ftrain...\"],[\"```\\n\\n---\\n\\n## **FAQ 🎯**\\n\\nYou can find the complete [Frequently Asked Questions](https:\\u002f\\u002fhuggingface.c...\"],[\"A: The DLCs are fully tested, maintained, optimized deep learning environments that require no insta...\"],[\"_Q: How is my data and code secured by Amazon SageMaker?_\\n\\nA: Amazon SageMaker provides numerous sec...\"],[\"A: No - the Hugging Face DLCs are open source and licensed under Apache 2.0.\\n\\n_Q: How can I run infe...\"],[\"_Q: I use Hugging Face with Azure Machine Learning or Google Cloud Platform, what does this partners...\"],[\"--\\ntitle: \\\"Introducing Optimum: The Optimization Toolkit for Transformers at Scale\\\"\\nauthors:\\n- user:...\"],[\"### 🏭 Optimum puts Transformers to work\\n\\nTo get optimal performance training and serving models, the...\"],[\"However, putting transformer-based models into production can be tricky and expensive as they need a...\"],[\"### 💡 How Intel is solving quantization and more with Neural Compressor\\n\\nIntel® [Neural Compressor](...\"],[\"### 🌟 A journey of collaboration: join us, follow our progress\\n\\nEvery journey starts with a first st...\"]],\"hovertemplate\":\"source=blog\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"blog, circle\",\"marker\":{\"color\":\"#B6E880\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"blog, circle\",\"showlegend\":true,\"x\":[-6.152668,-2.7279236,-4.7879167,-5.087136,-5.1990952,-4.5390654,-5.863116,-5.404233,-3.9281578,-2.185853,-2.3166924,4.6565576,4.9156284,4.870133,3.5402157,3.690307,3.442827,3.4867027,-6.5979004,-2.228331,10.269221,8.526385,9.3374,7.715061,8.48033,9.209068,-2.7564723,-2.7499237,-2.9895504,-2.9623716,-2.8449323,-2.7003415,-2.6214159,-7.0675516,-8.436045,-8.453107,-8.5440645,-7.451264,-8.771226,5.5699925,5.641563,-8.61837,-6.8014946,-6.2655196,-6.7766385,-8.040714,-7.93095,-7.74651,-0.73311406,-0.6416951,-1.7224003,-0.17005906,-5.201473,-5.774861,0.37202075,0.29699108,0.3737656,-0.5440659,-0.005330863,0.74001896,0.089241855,0.4965327,0.1282082,0.30535993,-0.1901023,2.0588741,0.50315815,0.6230395,0.1395317,0.25883695,0.3980723,0.32026592,0.09304018,-2.22578,-0.5687199,0.25055525,0.73584574,-0.4431679,0.12613802,0.25377592,0.5612813,-0.13040444,-6.0207205,-4.9508533,-4.544475,-4.7529716,-4.734659,-4.682034,-4.7717795,-4.503991,-4.661709,-4.658976,-4.841784,-4.67361,-4.0176363,-6.5269523,-3.3914735,-7.0435276,-4.309569,-3.9981787,-3.4192026,-0.38275144,-6.753917,-6.935834,-4.3168917,-5.051182,-3.1268332,6.39742,6.417359,-2.860288,6.376871,6.422545,-3.1867802,6.3082747,6.4498305,-3.530022,-8.219,-3.342365,-3.1484416,-8.200793,-4.308318,-5.0631475,-5.665688,-1.9556137,-3.896624,-2.9389758,-3.0639548,-2.804544,-2.8596902,-3.1757374,-3.8910234,-7.3349233,-6.346096,-6.2709913,-4.340519,-7.2806525,-6.3785,-7.536451,0.90488684,-7.7167406,-5.5749507,-7.666765,-7.7817707,-7.6898894,-7.5040317,-7.586034,-7.6022735,-6.570995,-6.7917666,-6.771821,3.7557268,3.8353322,2.1084425,3.9559414,2.7964485,2.6321404,2.1141653,1.3662838,-1.8289257,3.9811003,-5.9424434,-4.915254,-4.4641495,-3.1094494,-1.8543063,0.27830923,-5.537247,0.9973719,-4.8752456,-6.4303584,-0.7610534,-0.71054757,-6.561415,-0.73819715,-2.3106685,-6.9855933,-6.154904,-5.3327684,-4.9166956,4.867613,4.893828,4.8232045,4.888893,-2.9885101,4.9681273,4.933642,5.233671,5.2481756,-6.501145,-1.5002272,-1.643146,-2.1502876,-2.34201,-6.4081316,-5.008521,1.5719497,2.5406191,-4.2623134,4.058692,-4.8424406,-5.031943,-2.6323574,3.925376,1.1212068,0.65940213,-2.1126506,-2.2769685,0.05269326,-2.02965,-2.2041638,0.01929855,0.20788832,0.14376071,0.43840167,5.928361,5.919932,0.7999306,0.118996695,-0.039744705,-0.09335951,-1.2056214,0.035354115,2.0214312,-3.621934,4.943188,-7.5903068,-7.9655876,-8.1535015,-8.125437,-8.199055,-8.22488,-7.8399534,-8.035291,-8.572304,-7.8248196,-8.082112,5.7910485,-8.065462,-6.4751587,-7.6935,-6.217845,6.8322616,7.163023,7.1201563,-0.009012245,7.47336,-5.8866963,-3.910078,-3.2407951,-5.4558883,-6.8792377,-6.285288,-6.3909154,-5.0502477,-5.402011,-1.5583444,-3.699443,6.038119,-4.0042706,-3.5448287,-2.6982594,-3.0335917,6.197862,-3.9970875,-3.3106396,6.0173693,-3.1548543,6.176056,-3.777807,-2.655902,6.122781,-3.767448,-3.3375587,-2.798339,6.411733,-4.7613654,-4.341245,-3.4397156,-3.430767,-2.1110766,0.7341043,-2.2900417,-1.4753305,-2.4239616,-2.925487,-4.4772787,-3.567916,-3.7662702,-2.3392715,-3.4449525,-3.5266936,-2.2489538,-3.278109,-3.6301546,-3.5296457,-3.2820153,-3.4828749,-2.3737645,-0.54965615,-3.770466,6.8958015,5.7584653,4.9295135,5.4885964,5.115806,5.430231,6.910014,6.937556,-1.1453265,6.556677,6.4419556,6.7622876,-6.3563643,-4.7578444,-4.6001086,-5.115471,-4.9005723,-4.6747203,5.94132,11.320904,11.385803,-6.109827,-4.668937,-4.215302,-1.2892404,-0.02061819,-1.6296682,-2.1377614,-1.6595247,-0.4950674,-1.0889773,-1.5003598,-1.1719425,-0.49853414,-1.1914127,-1.0843946,-3.6157815,-2.9514735,-7.286264,-6.742249,-7.177507,-6.861537,-7.233766,-5.836481,-6.486175,-2.8741927,-2.1285436,-3.0155678,-5.6371202,-6.199275,-6.304822,-0.50026286,-0.4776756,-0.72850865,-1.0166363,-0.29377937,-0.5213066,-0.56762606,4.5663247,-6.050229,-5.7130713,-5.7712746,-5.714578,-5.724902,-5.856988,-5.8818088,-5.8344874,6.0203214,-5.8229566,-5.5357327,6.053712,6.1222763,6.1464376,-5.801834,-5.7159615,-5.7914677,-5.715181,-5.259132,2.9570334,5.757938,-7.215384,-6.333472,-3.6531286,-3.0542567,-3.9906447,-3.9134476,-3.2294705,-1.6777561,-1.0816897,-1.5465273,-0.669037,1.4167073,-2.0684786,-8.598134,-8.679971,-8.758823,-8.674495,-8.702427,-8.687216,-8.7702055,-8.616106,-8.765485,-8.601106,-8.7643795,-8.632841,-8.761203,-8.628222,-8.287541,-8.492722,-7.353194,-7.2858806,-8.7085285,-8.888831,-0.28043368,-9.125703,-8.971329,-8.609681,-8.776144,-8.54849,-6.458717,-6.577327,-5.007036,-4.6939325,-5.1167803,-1.8074452,-5.7995777,-5.661947,-0.7349032,1.7699403,1.0730202,-2.0452414,-2.0051293,-0.6850462,-2.3799796,-2.6524582,0.44232324,-1.9755946,-2.1897197,0.1271189,-0.4389245,0.016526593,-0.23816408,-1.6590812,-1.7975321,-5.1943436,-4.508388,4.717729,4.4859138,-3.258073,-1.9947171,-1.4351419,3.6678667,2.2005982,4.8590045,0.64784914,-0.17584732,0.41773862,-2.527221,-5.2617097,-1.4580985,-2.8177235,-1.6839631,-1.3907034,-1.3695006,-1.234139,2.741361,-1.3745749,0.44054347,-0.19719648,-1.49715,-1.5917414,-1.2843994,-1.0613061,0.7389611,-0.29016364,-1.0456117,-0.87346244,-1.0685759,1.6488489,-6.502641,-5.088132,-1.2066104,-1.7977078,-1.6990789,-0.04818349,-3.0358617,-6.18879,-6.5948205,5.2730813,5.973862,7.143965,5.9285216,6.534652,7.362874,-7.26078,-6.164255,4.903989,-5.6312423,-4.6476626,-6.209245,-7.031981,-5.9351864,-5.0618925,-1.9983337,-3.0570729,5.6451807,-1.0540643,-2.7216072,4.8535547,5.0719204,5.0426354,3.2549531,6.3577633,5.0671444,4.9809856,-5.669051,7.841849,-3.716304,-3.8177104,-3.375412,-3.8290493,-4.0840178,1.0306407,-6.289594,-6.236041,-5.0239496,-5.0677943,-2.214933,-4.596471,-2.4313607,-2.1706386,-5.9116983,-2.2768385,-2.2042155,-2.0398839,-1.9707938,-2.2685235,-1.6271365,-4.5733886,-5.169679,-5.020781,-1.5869222,-4.9318423,-5.079898,-7.122124,-6.776022,-6.800762,-7.0423756,-6.8124666,-6.4353256,-6.8616247,-6.9373565,-1.5404819,-1.0389246,-1.9163854,-0.9877485,-1.083518,0.7366198,0.524401,-0.8161826,-1.3897331,-6.294562,3.3161824,1.0662161,3.4152243,1.1653396,1.5169305,0.0070493855,-0.29577053,0.17389473,0.2686931,-8.859283,-9.429566,4.038797,-9.192462,-8.953916,1.7266957,2.0454717,1.8861247,1.8562254,2.4577823,0.88546646,-8.643985,0.60470295,0.38957816,0.21234861,0.38764733,3.2561018,-0.2094406,0.54985714,-8.955408,-1.1948298,-0.7773008,-0.46050844,0.47733817,0.8587452,0.2698742,-0.6062077,-4.5879436,-6.9252596,-6.5899134,-6.902839,-6.7570095,-7.6631956,-7.3011227,1.9791845,-7.249589,-7.766781,-7.75611,1.7535057,-7.8040333,-7.3647456,-7.2607055,1.4522238,6.848359,-6.4107804,-6.5578094,-6.700144,-6.241228,-6.758817,-7.154411,-7.238804,-7.2605896,-7.0921073,-6.737062,-6.4677305,-6.7799644,-7.1444254,-6.8977923,-7.4078875,-6.7394586,-7.249057,-7.1933336,-7.3028145,-7.195523,-6.6395364,-7.057316,-7.2505846,-3.3905694,5.3837204,-4.0718284,-7.193417,-7.166656,-7.301192,-7.365162,-7.5410094,-7.5770454,-8.175984,-7.5078187,-7.5418477,-7.585672,-7.3256407,-7.3810277,-3.1592605,-1.7430675,-2.0995293,-7.055836,-6.216284,-7.1964974,-6.6674514,-7.169966,-7.2956614,-7.156204,-7.6092567,-7.8651447,-7.328263,-7.378822,-7.295542,-7.453616,-7.530266,-7.117282,-7.383758,-6.811597,-1.4814914,-1.5353885,-1.19765,-6.95004,-6.0125685,-1.5004514,-1.8426735,-4.516651,-5.9247384,-3.7548258,-1.6612535,-1.9327804,-6.735754,-6.2629156,-6.2701874,-6.1748714,-6.004475,-0.6603876,-7.485317,-0.75538427,-0.25479835,-0.6815999,-0.7351693,-0.36736256,-0.41630015,-6.152686,-5.957322,-5.794968,0.45077938,0.3068828,0.36180118,0.24650222,0.2507804,0.36552253,0.084193304,0.14399514,0.21967593,0.14198977,-0.047905475,0.14212434,-6.032371,-6.2701764,6.2881026,0.8944091,-0.7007993,2.0841851,7.292003,7.288874,7.219535,7.325304,7.337319,7.2153897,7.129272,7.338461,7.2622523,0.6002866,1.030199,1.7703193,1.8791325,3.4430149,3.318122,0.08845434,-0.38735607,0.33907005,0.23488446,1.1155899,-8.955694,0.9111412,-6.450699,-7.416187,-7.675999,-7.457904,-7.5591617,-0.06218424,-2.5416641,-6.2673483,-5.7886057,-6.1793675,-7.640261,-3.854836,-3.5709653,11.382339,-3.6217327,-3.7884786,8.170306,-6.4348683,-6.839567,-7.1839275,-6.2808027,-6.7074323,-6.476048,-7.4158044,-6.993101,-6.59141,5.8540745,5.391725,-6.7296023,-6.7303066,-5.679509,-4.9501495,-2.1948195,-2.09239,-2.1461527,-5.4309297,-3.7551143,-3.809421,-3.6161232,-4.5738616,-6.3200245,-2.4129264,-4.7431054,-4.955236,-0.82749724,-1.8812063,-5.1999054,-6.234728,-2.2359438,-2.0618181,0.26565298,-2.392627,-1.8823671,-2.4515812,-1.9246849,-1.8220083,-1.7006541,-1.8244605,-1.0814425,-2.0801275,-6.767571,7.915205,0.110098876,-5.6431847,-6.0290165,-7.0926,-6.125082,-5.5781665,-8.793551,-5.866239,-5.963361,3.7888072,-5.459289,-4.0303273,-4.7231083,-4.919755,-6.0948734,-5.4075923,-5.5961146,3.4511535,3.5544631,3.635217,3.4734325,3.6331651,-6.382864,-8.37905,-7.747437,-7.135576,-7.1944757,-7.5710344,-3.6749518,-2.712135,-3.8888936,2.2186842,-5.7873454,-5.9418564,-1.1858578,-0.7545028,-0.17313789,0.25423977,-0.32847494,-0.38494664,0.39318812,-1.5260459,-1.5932276,-1.8021832,-1.5214939,-1.3384964,0.21638308,0.009712527,-2.7040768,-0.72344977,-0.68283767,-1.0257179,-4.1368284,-3.2357352,-3.6485357,-0.50361836,-2.4012454,-1.3735962,0.44787395,-0.48629373,2.60455,-3.180815,-4.254442,5.4501357,5.2611046,-6.2509675,-3.062821,-1.7245939,3.2295916,-6.346466,4.907621,5.272045,-6.4079323,-8.063543,-8.258378,-8.357542,-7.4326878,-8.77479,-8.690844,-8.726809,-8.651785,-8.778478,-8.76336,-8.8079195,-8.711648,-7.015679,-7.195424,-7.3284454,-6.03528,-5.458192,-6.795995,-5.3293467,0.8183377,-1.3380107,-1.7375791,-1.7679701,-2.0659125,-1.0017959,-1.2403504,-6.1506743,-1.9532745,-4.7484784,-3.8016763,-2.217784,-2.6212227,-3.3236084,-3.6309454,-1.3135495,-1.796829,0.007439378,-1.9915072,-1.3538413,-2.8382633,0.5176772,-2.005729,-1.0946625,-4.3112893,-4.848016,-6.7588863,-8.701584,-8.631315,-8.7358055,-7.7433476,-1.4491533,-8.742422,-8.879198,-8.3036375,-0.30822697,-0.10391551,-8.974706,-9.058419,-0.4451868,-9.01203,-8.99258,-9.177402,-9.060868,-8.913035,-8.982422,-9.064921,-9.046081,-8.724755,-8.54036,6.160498,-5.1945567,-4.6803803,-4.705193,-4.2269335,-5.005735,-4.7233715,-4.6977696,-6.0166435,-5.7401557,2.5052,4.557027,2.8020518,-4.393066,2.4540136,2.524861,2.660619,0.33655766,1.2778851,2.4957466,2.3415036,2.2294836,1.7457303,1.9133111,2.0755773,2.1936476,-3.983867,-4.915181,-4.730094,-5.3868713,-5.4336624,-5.239649,-5.1343765,-4.5761533,-3.7925677,3.4448318,4.591401,0.20275947,3.2606542,3.271316,2.6939678,-2.3999903,-0.35129404,-4.630795,-6.0166235,4.0571384,4.365105,4.4698315,4.0875077,-3.5618854,-3.167538,-5.6466064,3.8930233,3.5517461,3.6680303,-2.4252625,-2.205492,-2.0418828,0.8351618,0.40611044,-2.0204344,-2.2655294,-1.8942844,-4.0006247,-3.8366363,4.872169,-4.2858586,-4.3507504,-3.9884973,-4.1515026,-4.6472273,0.5809433,4.202208,2.499568,1.211087,4.7685375,-5.5442185,-0.11579226,-0.3791577,-0.39551038,-0.37043986,-1.0180769,-0.6913418,-0.93987894,0.06193207,4.989637,-0.9583951,1.3992801,-3.1593316,0.18887778,-4.0470266,-1.2158958,-0.8209302,0.43415937,0.8110113,-0.7438095,-1.0625204,-0.90212256,-1.0026448,-0.955484,-1.1058159,-0.909815,-5.9839816,-4.943122,-5.1970215,-1.7328796,-1.4653887,-3.1335118,-1.6253555,-3.2229004,-2.4507058,-4.8661222,-5.202263,-4.0018907,-2.0554686,-4.866362,-3.7494688,-0.3257905,-4.215438,-3.8845017,-1.5377384,-2.0015893,0.16736428,-0.30667323,-0.04866901,-5.212456,-4.8661356,7.7905874,7.799143,-4.1445932,4.896444,5.0298896,-4.3700404,-4.3443046,-4.330518,-3.761334,-3.5384188,-3.3392613,-3.5357785,-3.593095,-3.4475706,-3.8652406,1.1559498,0.71427804,0.9861111,0.9429248,0.95551103,0.859258,0.94046855,0.82530975,-6.3527417,-6.159718,-6.3428483,-5.9807906,-6.189249,-5.7791567,-6.2677455,-6.5602427,-6.894505,-6.4218974,-6.0680547,-6.2939935,-6.4266906,-6.4662824,-6.5106106,-6.8968024,-6.6309605,-6.520464,-6.539034,-6.6731215,-6.523554,-6.4939237,-6.593293,-7.437474,-6.8827424,-6.166725,-4.029427,-5.9197497,-4.0442095,-5.3975058,-4.27473,-4.258405,-2.3569183,-6.001188,-6.0794644,-5.732834,-5.9370704,-6.1080136,-6.0011497,-6.1334248,-6.1499515,-6.489651,-5.3819923,-4.828109,0.59324086,-0.43608263,4.628446,4.62933,4.627588,4.6284723,4.6285567,4.6276307,4.628793,4.6286902,4.628397,4.628881,4.628042,4.6281915,4.627623,4.6285973,4.630191,4.628885,4.626755,4.627279,4.6274714,-3.3561747,-2.7152765,-0.09099138,-3.341239,-2.6656473,-0.97412574,-1.1038702,-2.5744247,-2.3074813,-2.9750848,-3.630839,-3.5979426,-3.6416395,-3.5282345,-3.6410553,-3.6427,-3.6025352,-3.7742586,-2.6935623,-1.8236914,-3.564744,-3.4741056,-3.5009563,-3.6071064,-3.4806495,-3.32863,-1.8083539,-1.9057477,-3.074459,-3.9489431,-3.8472886,-2.3690093,-2.6461844,-4.2894692,-1.7502966,-0.062386345,-2.278349,-2.989016,-0.42242265,-7.222697,-6.2198424,-6.29118,-6.113788,-6.120349,-6.4911056,-6.9454885,-5.155559,-4.8127136,-5.0879273,-5.11838,-5.9420524,4.835517,-4.5430136,-5.3542037,-4.988162,4.8542,5.1274276,4.646601,-4.2020965,-3.1715715,0.12168975,-4.451021,-3.5987947,-0.5908702,5.4580517,4.8790474,-5.8134756,-5.09507,-7.342247,-3.7274141,-0.72471285,-0.8660766,-1.7948976,-4.264284,-5.0387793,-5.7562685,-2.7724347,-1.6723349,-1.6264565,0.7039303,-2.108291,-1.1460513,-6.7202353,4.4306607,8.061543,3.770803,3.6312401,3.9144526,3.791259,4.453774,-5.7812834,4.4379525,5.624066,-7.2863655,-7.1377053,-7.37012,-5.503307,-5.2198224,-5.3758273,-5.4864683,-5.414241,-5.5820494,-7.3978,-6.899288,-7.4893517,-7.6414123,-7.4272094,-7.2461147,-6.0739408,-7.3864126,-7.2856264,-7.4197264,-7.222432,-6.899679,-7.18589,-7.1211977,-6.932966,-6.756659,-5.990942,-2.3589096,-2.7321298,-3.587386,-2.5751085,-1.3148372,-2.0655031,-1.2938737,-2.4723248,-7.2687087,-6.876714,-6.658678,-3.421496,-3.2401407,-3.7442422,-3.392501,-6.476663,-6.509191,-5.15352,-4.687695,1.5829808,2.8799202,4.999203,1.6967286,1.9871444,2.2891917,2.5666552,1.8322673,1.342964,1.0720636,1.5333325,1.1811701,2.840758,-4.838063,-1.1673732,1.1461581,0.7746052,-0.33982578,-0.7771873,0.12579413,-1.037726,-4.4089046,-4.5639644,-4.845273,-6.700691,-6.2557855,-5.885391,-6.019624,-5.4210134,-4.990294,-3.5249362,-3.1997304,-3.5376725,-4.5215206,-0.97133124,-1.3261976,-1.0190707,1.2592281,-2.0675275,-4.841756,-2.657283,-3.9687088,-5.87539,10.942692,-5.805005,-4.5530033,-3.9856112,-7.859286,-7.9960933,-0.39259702,-7.9398565,-7.8177223,-7.6021285,1.1754179,-7.5823483,-7.716912,-7.7767987,-0.71341497,-7.606261,-0.99044114,-0.044132598,-0.8733615,-7.412804,-7.7654924,-7.4877105,-0.776125,-7.478119,-7.6392965,-1.7885127,-0.86796993,-0.4049981,-1.1752921,-0.8347414,-1.5771823,-1.0979146,0.32508105,-0.82238626,-1.3680539,-7.1368876,3.2338917,2.7861807,1.1584299,1.3267356,0.8191675,-1.0852996,-6.824083,-7.3808675,-7.3819523,-1.1350434,-7.4204235,-7.1570034,-7.207966,2.6887681,2.603682,-7.1692944,-7.197374,-6.8348956,-6.9864936,-6.8799677,-6.780365,-7.043057,-6.959403,-6.8092318,-7.164383,-7.2150517,-7.181986,-1.6227274,-1.5021316,1.4537563,-1.7035515,-2.0304997,-2.063429,-2.093832,-2.3515487,-7.306961,-2.2493174,-2.3861594,-1.874433,-8.793601,-0.9818362,-0.27600664,-1.639684,-12.281159,-0.19677553,-8.008518,-0.36637366,-0.44457924,-7.9812846,-13.114256,-0.334565,-0.21381693,-0.16371053,-1.3067561,-0.18247108,-0.15883753,-0.76590836,0.54271436,-1.0648237,0.29461026,-0.7043376,-0.5521277,0.8444823,-1.1661036,-0.5532812,0.11649215,-0.44586194,0.42978644,-0.011619449,-0.47062477,-0.726121,-2.0380373,-1.9694101,-1.7330753,-7.257345,-5.787214,-6.24869,-6.149473,-5.743259,1.9779551,2.2586036,-4.723071,-4.595133,-6.912637,-6.814896,-6.6080956,-6.1592884,-4.7529507,-4.6547184,-5.1107883,-5.2645373,-3.22226,-3.2408416,-5.954298,-6.1911,-5.509587,-5.4463882,-5.2995906,-5.7397265,-4.735902,7.1686707,5.0352006,5.036566,4.346092,-2.9696913,-4.576533,4.71269,-1.5687746,4.608023,4.849397,-3.5633118,5.101595,4.832269,4.907013,4.294296,4.901569,4.997864,5.041289,4.922481,6.8443747,-4.648142,4.6238666,1.562758,4.8618,5.158282,7.3469195,6.951993,7.3282743,6.9370823,7.1243415,6.8190503,7.0177007,-4.54785,6.514847,7.451103,5.068621,5.268241,7.376187,7.4344606,7.3729124,5.937363,6.7941046,7.330323,7.346429,6.9094477,4.662617,-3.5271165,-4.238775,-1.3189037,-2.0292559,-1.2982073,4.7737837,4.428392,-3.5730128,-2.4231734,-1.2940209,-0.76346856,-0.34212214,-0.5568848,1.7190018,-6.2814674,-0.32463405,-0.6839731,0.3211607,0.11829379,-0.42679238,-1.1139184,0.23792788,0.12050091,0.24684915,-0.38114434,0.24783549,0.09174166,0.2158483,-4.9239225,-6.9979243,-5.6402335,-5.7977014,-6.609518,-6.5067596,-8.643908,0.46835664,1.0562792,-1.8628511,-7.595548,3.2083068,-0.6897059,-0.7142398,9.558779,-8.66778,-9.129392,-0.06087681,0.21968444,-0.15581268,-9.150352,-8.770962,-4.9379196,-5.6835175,-5.6305013,-1.2408653,-1.0168858,5.8487496,-0.41608608,-0.4005603,1.2898511,0.073859096,4.850059,0.72550267,-0.13291132,0.04601059,-0.06609428,-1.2582842,-1.4237819,-2.7325892,12.354047,12.219008,11.858046,12.182315,12.213277,11.780982,12.051218,12.38483,12.5704155,-5.4136443,-5.1934314,-5.1226215,-5.232401,-5.3235364,7.4485846,6.707481,5.0783906,4.642005,5.2516513,4.420443,5.0346837,4.9945126,-2.0561821,1.6261328,5.9654303,4.6946316,4.949578,5.6630993,4.975262,4.9420266,3.9234762,4.6458573,4.82782,5.0213985,5.1533446,3.7410548,6.0931106,5.59802,4.5031085,4.889714,4.9231043,4.4816775,5.0555153,5.0913386,5.0975504,5.067675,2.9142761,4.901878,4.389673,5.053233,5.2412705,5.2835503,5.1676097,4.9375873,4.913578,4.985623,5.891361,7.3382063,7.303524,5.975892,6.070605,7.3263593,7.100757,7.2440677,7.4150724,-5.4984655,-5.946365,-6.292019,-6.404267,-4.8481407,-5.189708,-4.8266296,-4.7888637,-3.4709558,-3.3081863,-0.2636456,0.07067061,-4.9695687,-4.146218,-4.004721,-4.5815163,-5.0502777,-7.160188,-6.259253,-6.1193213,-4.3719106,-3.9214718,-1.5582278,-4.309323,6.900018,5.032345,5.255006,4.86109,5.337151,5.064688,5.1709104,5.249623,5.0943923,5.121631,5.0827966,4.9743385,4.873786,4.994368,4.8626738,-1.4227635,6.722297,7.3874764,5.8838773,5.9351625,6.512793,7.359779,7.15668,-2.1079724,-3.3678079,7.657667,7.6814213,-3.628554,-4.417658,-4.466406,-7.076544,-1.2640775,-7.6362667,-7.582453,-7.6493607,-7.773212,-7.4662733,-6.6478343,-7.6773777,-7.578685,-6.817923,-4.951224,-4.799731,-4.7615957,0.63183916,1.3031332,-4.680263,-4.9696274,-4.4650555,-7.275465,-6.5657387,-6.539132,-3.3664293,-3.278012,0.7060411,0.29327384,-2.927519,-3.364235,-7.5854073,-3.2840507,-2.9563613,-3.5044076,-6.8494744,3.1198566,-0.93065107,0.093272,-5.9971404,-5.5997467,-5.5154524,-5.6177974,6.0597587,-5.05938,-4.797172,6.136612,6.2015424,-5.6430464,-5.737124,-5.603363,-5.5341578,-5.522804,-5.4423275,-5.583334,-5.3204255,-5.9881554,-5.9014893,-2.3967237,-2.034594,-1.63774,-1.2184827,-1.4742188,-1.5484436,-1.5406494,-1.4748094,-1.2752074,-1.4290802,-1.6576381,-1.7503058,-1.741683,-1.6559689,-1.0268,-1.0518309,-7.056233,-1.795897,-3.5866194,-5.0661616,-2.0530646,6.3233223,-1.0907152,-2.0205226,-0.024321726,-2.613166,-0.6710048,-0.36576313,-1.1927197,0.04240085,-1.3415918,-0.45048282,-6.7582855,-1.9410509,-1.7482785,-1.990237,-1.7970829,-0.57389444,0.117317356,0.30276215,-0.1492491,-0.20793915,-6.1988907,-1.7767079,2.104448,-3.6093895,-2.018599,-3.969213,-4.5813303,-4.5442686,5.2832227,5.044856,-2.117161,-2.2311711,-8.267265,-1.9410801,-0.61284816,2.5125048,-1.3148215,-1.8142022,5.281319,-1.940972,-7.600367,-7.5736985,-7.435729,-7.336293,-7.4196525,5.83499,-7.507088,3.1604044,-6.702361,-6.956042,-6.031781,-7.5530767,-7.8567777,-7.69845,-7.0703964,-6.3775644,-6.5276065,-6.4796557,-7.47121,-7.2564316,-7.4700017,-7.342788,-7.068633,-1.2883527,-0.22471282,-6.933017,0.52516776,-0.48522854,0.6577883,-1.7763957,-7.3258862,1.8663583,-7.3383975,-7.5003114,-7.401085,-7.672326,-7.43565,-6.845644,-7.574673,-7.688901,-7.696604,-7.4626694,-7.6252418,-7.6924753,-7.0778813,-7.159781,-7.610041,-7.7060704,1.982961,1.7279625,1.5386364,1.6448872,3.2723475,-7.6948915,0.39766017,-0.77518344,0.8037592,-7.8324957,2.4198458,-7.7258477,6.546766,-7.266224,-4.676373,-3.656673,-0.060211055,-4.1438236,1.3760159,0.50399816,0.18181212,0.11094125,0.7710644,-0.38857684,1.2389268,1.2257888,-4.0395226,0.88037324,4.077722,3.5597541,-6.449872,-5.0639377,-4.81587,-4.426323,-4.127987,-4.8084416,-4.9247026,5.2764645,-5.3431745,-2.4493463,2.7079341,-4.242978,-0.30727,0.09240547,-5.2533813,-7.74477,-7.692261,-7.787657,2.535759,-7.3297544,-6.792605,-7.2359867,-6.431034,-7.068573,-7.7013154,-7.2120852,-6.2573285,-1.1082333,4.2965703,-7.3321576,-7.431277,-1.4698462,-2.9112294,-0.25262347,4.052195,-2.4963899,0.10288262,-0.6757432,-0.08615321,-2.421195,-0.08961227,-3.3454213,2.2973847,2.8341453,-0.84311056,4.917585,4.783562,-0.5433583,-0.34537452,-1.3871328,-6.0736413,-5.9533143,-6.2495136,-6.4004726,-6.2582397,-6.471215,-6.3550916,-6.5944314,-6.849324,-6.6321974,-6.7694383,-6.216765,-6.495222,-6.3701086,-6.6597357,-6.854737,-6.4362516,-6.284624,-5.2839007,-6.320352,-7.0783224,-0.18818627,2.3804755,2.8219106,0.24210304,-0.04709617,0.0039263857,-0.040475342,0.171529,0.063200176,-0.061675467,-4.5195622,-3.4317086,-5.6148276,-4.2277136,-4.5166492,-3.5402806,-2.9243524,-1.5946155,-2.6659396,-1.9096944,0.1735295,3.380477,1.2092118,-3.418267,-2.9462128,-4.6081576,-7.501158,-6.7950754,-7.5415063,-6.9496765,-7.7021255,-7.5926113,-7.4638753,-7.690824,-7.6821065,-7.3080144,-7.7227435,-7.773939,-7.38389,-7.792868,-7.5670247,-7.021596,-7.6610956,-7.516403,-7.415902,-7.691829,-7.2418814,-7.758856,-7.433801,-7.569134,-7.5542493,-7.799718,-7.7209134,-7.7954807,-7.550327,-7.4346404,-7.2734513,-6.8632026,-4.111895,-4.627175,-2.5537553,-2.7906792,-2.8729424,-4.1315484,-1.7424273,-1.8952417,-4.6490846,-1.0063685,0.099841096,0.2696223,-7.2843237,-7.289053,0.40480727,0.44185877,-6.437681,-6.047502,-2.2993424,-1.6289791,-1.0419596,-1.8179561,-1.3349732,-3.2478163,-1.6457921,-1.9981521,-2.3515348,-0.37511516,-1.4072682,-3.7563126,-3.2619276,-3.3468864,-1.7355298,-3.3009045,-3.1925159,-3.4071667,-3.6688192,-3.548365,-2.7868404,-2.2801719,-3.0271087,-2.8829827,-1.9667046,-1.9411848,-0.60863364,-1.8644105,-1.0230712,-1.0134258,-2.0648148,-1.8737926,-1.1593674,-2.5359492,-2.6731775,-6.5271454,-7.555978,-7.550748,3.1286514,-7.2067485,-7.3333387,-7.188638,-7.3362927,-7.418061,-7.2135787,-7.0439277,-7.4409404,-7.4770727,-6.938982,-7.094026,-5.5239563,-5.2919908,-4.9766965,-4.79608,-2.7287207,-2.3460271,-0.16094981,-3.456341,-3.433245,-1.6618267,-1.9114841,-2.2478814,-4.5554757,-4.93562,-5.06893,12.891005,-5.4165697,-6.1682515,-3.6844783,-4.063288,-4.3162713,-3.4489262,-6.4614744,9.12986,9.0968275,9.177001,7.8627605,9.319437,8.90495,-6.555407,-6.481214,-4.897502,-4.978356,-4.6167874,-2.0404277,2.7492924,-0.36850917,2.5320714,11.307342,11.082354,-5.7702303,-6.039402,-8.730331,-2.5566049,-2.2827759,-1.8399396,-2.2859294,-2.1219702,-1.7498873,2.1013196,-2.499304,-2.2457757,2.650182,-1.7839133,-3.0651407,-0.409862,-0.18807898,-0.0803186,0.513511,-0.5376783,0.53301007,0.4089507,0.76342463,3.4564536,-7.494505,2.2317533,-6.575809,0.27638042,1.1247953,3.60344,3.666385,-6.210846,4.8419933,4.969326,4.8217115,5.965713,-5.0106063,1.4563652,-2.2238526,-2.9317718,0.4049342,-1.2493105,0.21425791,-0.23868077,0.34815067,-0.63191307,0.019073855,0.56525236,0.6085902,-3.2612348,0.29431814,2.1617098,-2.572416,-4.179815,-5.7632184,-7.40071,-7.348156,-7.414048,-1.3074214,-7.4292784,-7.904114,-2.250686,-0.021845642,-7.693523,-7.7367663,-7.764298,-7.749351,-7.8382134,-2.1925263,-2.1811507,-2.059207,-7.383756,-7.457901,-7.6410317,-7.6371055,-7.92717,-7.7634177,-7.610185,-8.005346,-7.6349244,-7.4735503,-6.1039147,5.593776,6.404523,-1.7093077,-2.0763392,-2.3315692,-2.0455236,-6.3650255,-4.792665,-5.842884,-6.648591,-1.6516066,-1.0740565,-3.325256,0.44266608,0.4489716,-3.378529,-4.9196143,-5.419622,-0.027454058,-1.7070497,4.7091455,-7.3773255,-7.9188814,-8.102649,-7.7233768,-0.60620743,-1.2087609,-0.6556991,-0.12733622,-7.2053514,-0.12213296,-7.8955984,-5.7933564,-6.482276,-6.1356034,-7.394817,-6.3499002,-5.7128224,-6.3052216,-6.1341043,-5.8875446,-6.0064945,-6.468959,-5.8579693,-3.1033998,-2.4597843,-3.0230796,0.095974505,-2.287688,-1.705445,-2.355042,-1.7263646,-2.4978974,-1.8551455,-2.0805686,-1.5245388,-3.67691,0.21990672,0.28097513,-4.440383,0.11354739,-2.6267505,-3.2399862,-2.4518824,-2.9022791,-7.217392,-6.8011065,-1.3262419,-1.02858,-1.5346811,-0.57360435,-0.17507043,-1.0953414,-1.1553688,-1.3615234,-1.3256146,0.4339842,-1.2109265,-0.96485287,-0.84275746,-0.81728446,-1.2408596,-0.17433766,-0.90179557,-1.5997438,-1.5119812,-8.513097,-5.8971643,2.2212832,1.5631773,1.6062889,0.6330158,0.6994385,0.9511805,0.05930191,0.21042351,-0.26996967,1.0447428,-3.256547,-3.0902205,-6.3349195,-2.6425798,-2.1699207,-3.143481,-1.7431673,-1.6874887,0.60122716,-1.737455,-0.810927,-0.9348485,-0.76497525,-1.0762084,-3.7591376,-1.954823,-1.0432415,-1.1184103,-1.2781876,-1.3056632,-1.0753969,-2.2560947,4.174783,-1.8200493,-1.9450437,-1.3088471,-1.4326571,-6.9678373,-9.646145,-9.719378,-9.702385,-9.503567,-9.623686,-9.636502,-9.509216,-9.261088,-9.124423,-9.540198,-8.829941,-9.6214905,-9.564407,-9.650979,-9.663634,0.013811538,-9.488577,-9.137231,-8.988325,-9.719389,-9.718681,-9.70447,-5.3091717,-4.550322,-5.778094,-4.886363,-4.8909388,-1.530089,-2.8505006,-2.7654464,-4.3901772,-5.493642,-5.5671315,-4.019365,-3.7569463,-3.2210972,-4.786554,14.610842,11.558362,10.799488,11.586926,-6.605121,-5.1540866,-4.3782954,-3.9162614,-2.9064612,-1.8974266,-3.5223203,-5.8988466,-3.5347261,-3.574824,-3.6268017,-3.4565032,-4.5061255,-4.516758,-1.8588169,-2.544334,-0.7828774,-4.306543,-6.3449683,-6.417678,-9.3518715,-1.830071,-2.5360281,-0.8824316,7.958717,-6.9147525,-6.5455947,-5.736166,3.9416506,3.4994278,-7.5520205,3.6015105,-1.3552811,4.0054975,4.0977173,-6.6309457,-6.438598,-6.3929243,-6.4913797,-6.4472923,-5.842045,-6.5666394,6.5628653,-3.1986043,-3.197917,-3.3260596,-3.2565978,-2.7142746,-2.724711,-2.8647363,-3.3441083,-3.1866105,-0.6075082,-1.5992327,-0.28726903,-0.15522644,0.27685505,-0.032692622,0.16499186,-1.9337993,0.008021366,-6.1172585,-3.5667315,-3.64433,1.7982779,-2.6297572,-1.0392784,-2.4392262,-0.6117061,0.32928488,-0.46550578,-3.0008438,-4.173275,-3.675443,-6.479757,-7.8729224,-0.89215463,-5.3066897,11.502912,-5.4696074,-4.9534755,-7.1831694,-4.986685,-4.8148775,-4.8995657,-4.9168315,4.8715434,-6.952724,-0.7396972,-2.558404,-1.3718516,-1.4708822,-8.437191,-0.5463094,-0.043870963,-0.12389448,-0.100657456,-0.5555326,-0.55298215,0.15321846,-0.24977607,-1.7332109,-0.3766821,-0.48781884,-0.09987858,-0.14397681,-0.6357499,-0.59037226,-1.0546428,-9.70811,-9.60386,-9.622276,-9.806549,-9.636175,0.3554099,0.5126871,1.341457,-9.525484,-9.724695,-2.6073782,-2.5570738,-2.5397289,-2.5967057,-2.4875803,-2.5065496,-2.6760416,-6.6629863,11.31727,0.06610501,-2.354384,-6.997728,-1.6755344,-1.5086893,-1.4779259,-2.7260659,-3.428823,-3.4043608,-1.1107897,-1.544518,-1.3972346,-1.4457358,-1.474608,-1.4409286,-7.609864,-7.521574,-7.246792,-8.059025,-8.071723,-8.18073,-8.1665,-7.461127,-7.4191875,-8.125453,-8.194664,-7.849585,-7.976261,-8.146587,-8.359784,-8.389799,-8.302567,-8.017136,-8.138837,-7.2481527,-7.52998,-3.9652696,-2.0758684,-1.6784275,-1.8916206,-1.4573681,-1.6281195,-1.6480922,-2.2421994,-2.0685446,-1.65568,-2.212271,-1.9256729,-1.5527526,-2.0680976,-2.0111892,-1.3749481,-1.79495,-1.8142936,-1.9295912,-1.993229,-1.9637192,-2.0490723,-1.8346003,-1.6017598,-1.5471542,-1.7536734,-2.3979692,-2.4802592,-2.1966777,-2.0571709,-3.5332742,4.1756887,3.874604,-0.48059332,-1.1689595,-1.0759116,-1.7299924,2.7296383,3.1733143,-5.811015,-3.06981,-2.3655653,-1.2345241,-2.1705232,2.8099477,-2.209482,-1.6553316,-1.0217681,-0.4446389,-2.0055282,-1.6106974,-2.5013497,-1.4428273,-0.9930436,-2.349978,-1.3396603,-0.37502778,-1.0428882,-1.0921001,-1.1665171,-1.7514268,-1.2601792,-1.4451509,-5.9985957,-5.621897,7.312723,5.3213587,1.2477944,1.3167446,4.8738494,0.40266436,2.5783348,4.394645,1.7072977,5.2700677,4.617782,4.6365933,5.0958705,5.0542827,4.8040957,4.895057,4.862698,4.865537,4.9052863,4.6638846,4.834527,4.963964,6.9039097,5.053159,-2.4829996,4.761731,7.15699,7.3479514,6.0064273,5.9649405,5.963837,5.8517113,6.0358477,6.422152,3.2568548,3.1013885,3.968144,-6.3779736,-2.9487064,2.9461572,5.429561,-1.297702,-2.3402262,-0.8969159,1.0829809,2.226632,-3.1796515,-3.445582,-3.734695,-3.4530613,-1.2000763,-3.4790606,-3.1811962,-2.4614522,-0.938502,0.12717737,-1.4274149,-3.388483,-3.3176167,-2.9460404,-2.7814057,-0.38858706,-1.17794,-0.8079233,-0.9884334,-0.6909164,-0.05061402,-0.6113062,-0.7939325,-3.5526597,-3.0219016,2.7208738,-0.83446276,-2.931055,-1.3562115,0.19320238,-0.95636797,-1.2750007,-1.4623507,0.48355514,-0.22791243,-3.1064556,0.47493985,-1.6650605,4.113487,4.745145,-3.1997173,1.3907653,2.316428,-7.1379232,-6.9037724,-2.898503,-2.9493246,-1.9956087,-7.4894104,-7.645405,-7.7869596,-7.4393816,-1.6439533,-1.4736663,-1.2352141,-7.7354317,-7.7723403,-7.757006,-7.5069566,-6.475885,-5.9245496,-7.884947,-3.1609643,-1.1766068,-0.18494934,-7.1564174,-7.0664563,-7.4270096,-8.076295,-8.017709,-0.58339626,0.10018054,0.25752267,0.15081164,-0.5386841,-0.20406355,-1.1797539,-0.35492316,-7.9906425,-4.0770288,-1.01048,-1.8989677,-2.1425412,-0.110666566,0.45430225,0.59775513,-2.2599533,0.29839376,0.37432596,-2.438866,-5.9121766,-5.4203753,-4.9561768,-5.2264657,-5.535533,6.667979,-5.544035,-5.6886554,2.7025747,5.2351274,-5.8329544,-5.6902065,-5.563528,-4.706734,-4.6538095,-1.8826649,-5.400694,-2.3651154,-1.7968048,0.16513911,2.9637916,1.5113066,5.6165047,4.185142,-3.8090239,-7.4135337,-7.824167,-8.171138,-8.048048,-8.145522,-8.11302,-8.273401,-8.197086,-8.207818,-8.178981,-8.167992,-7.241439,-6.240375,-5.994001,7.692014,-9.229546,6.8960257,-9.750757,-2.3172092,-9.506733,-8.362961,-8.226859,6.49259,1.6110045,1.793647,-1.714145,6.653253,2.545295,6.4913335,-8.288745,2.5248227,2.2938812,1.030608,2.1301796,6.5459723,6.341894,-1.5801983,-1.2643014,0.28313205,-0.7356967,-1.3692583,-1.9187459,-8.706377,-3.7821395,0.58602667,6.4162564,-4.284723,7.0821133,-7.8628902,1.1065699,-7.9411297,-8.028935,-8.11477,-8.05504,-0.6599997,-4.8882756,4.9252563,-7.8584757,-5.060527,-5.3557816,-5.437893,12.891105,-4.7388916,-5.27794,1.816148,-3.9490125,-3.6190727,3.0631115,-4.12932,-4.9837847,-4.2338586,-5.192651,-5.109356,-6.385544,-5.394705,-4.331919,-0.6348231,-2.8810673,-2.9008458,-3.5420887,-5.5057187,-3.382232,-4.490198,-3.313667,-3.4161732,-3.2633424,-3.5487814,-0.94940114,-2.2045448,-1.0941962,-2.2344353,-3.09159,-3.418168,-3.5173028,-3.1688457,-2.9555452,5.5619965,5.6588025,-3.807581,-3.349222,-1.9761648,-0.4710484,-4.4005227,-2.6449366,-2.3750725,-1.3530279,-3.256,-3.5643308,-6.449961,-5.5707884,-5.667451,-5.582298,5.7914424,-5.2106295,-6.6921735,-2.3446097,-1.9873614,-1.9395986,-1.6632106,-1.2500098,-1.2300732,0.26459667,-3.317864,-3.349132,-1.9152199,-1.5621299,-1.9917192,-3.2306497,-7.244294,-7.248871,-7.4637156,-1.6089728,-0.584654,-1.5450516,-1.401085,-1.458848,-6.346361,-7.106275,-7.428003,-7.452307,-7.222732,-7.122925,-1.2692609,-0.49618167,-3.608186,-0.9394631,-1.5013998,6.1248293,-5.8274794,-7.177074,0.20045336,-7.2840886,-7.242977,-4.720215,-6.0677447,-1.787195,-0.27281898,-1.2326931,-0.33866337,-1.2850128,-0.64543015,1.3550961,0.14791963,-0.02964038,-2.7423677,-0.9242279,-5.9983654,12.095526,11.438893,7.624622,7.446857,11.987227,-7.241927,-6.911311,-7.6775904,-7.6774526,-7.530388,-7.5580926,-7.499,-7.580546,-7.6212516,3.2200983,-7.641667,-6.913203,-6.2412224,-5.7663593,-5.7502093,-5.717075,-5.0082335,-5.8248754,-1.0948244,-5.444384,-2.9838789,-2.6194372,-2.9914138,-7.7011886,-7.6890545,-7.617264,-7.4478507,-7.0338473,-7.511852,-0.1415179,-6.1303043,-7.302965,-7.22671,-1.1890849,-1.2263441,-0.015451132,-0.86663806,-6.932228,-1.6648748,-0.12409658,-6.9400477,-2.178435,0.22314718,2.013152,-7.253537,-6.7697263,-6.752348,-7.210325,-7.9443073,-8.360882,-8.4146385,-8.319483,-8.36226,-8.441544,-8.425268,-7.27085,-7.4444036,-5.686481,5.5791802,5.324254,5.586658,-3.518128,-3.7039118,-3.7862542,-3.7998629,-3.6825633,-3.3888464,-3.5739896,-3.3654134,-3.417278,-3.2124424,-3.5783246,-3.344542,-2.2096906,-3.4042878,-2.914716,-3.4058769,-3.4976962,-3.4765303,2.7545059,1.6020263,2.211957,1.859,1.8253533,-0.5580771,0.32538012,-7.527298,-7.9226375,-7.2255025,-8.112861,-9.447163,-8.524897,3.1458886,2.5567958,1.205788,0.9867797,0.008530043,0.8970383,-7.9585185,0.26246825,-8.653849,0.41621304,0.4011638,-8.056068,-3.9341722,-3.2638748,-3.6156957,-3.5466933,-3.7001865,-3.7659128,-3.5968714,-4.9614077,-4.1134195,5.2683134,4.981776,4.9770603,-3.2867446,-2.355016,-2.6207218,-1.8191836,-2.397099,-1.6034338,-1.7764724,-1.2485169,-1.2436589,-2.6798944,-0.7531229,-0.9053136,-3.5302732,-3.0386508,-0.10514928,-5.929707,6.567133,-6.8658876,-6.1242347,-6.178169,-6.42931,-5.544429,-6.0766335,4.8560567,4.503488,-7.3151035,-6.2888002,-6.266215,-7.042978,-7.6912875,-7.039563,-7.1423697,-7.039554,-7.103359,-7.4245696,-7.9795756,-7.9155974,-8.022477,-7.3325086,-7.158979,-4.319685,-1.2027028,-1.8733097,-2.0551527,-1.8711936,-1.9503899,-9.557057,-9.34921,-1.8649871,-1.8884056,-9.0800085,-9.376666,-9.001758,-8.976389,-9.254202,-9.133732,0.6167917,0.07952935,1.4082766,0.9535221,1.0507921,1.1006397,-9.315911,1.665371,0.57169664,1.1595869,1.4642837,-9.141447,-8.31885,-4.8140607,-4.7066073,-4.7733645,-4.6368327,4.100086,-4.5938964,-4.5988774,-4.531556,-4.779218,-4.4367886,-4.6736817,-4.6938944,-4.746773,-4.719197,-4.6372023,-4.4987454,-4.6360874,-4.739738,-4.4931684,-4.679223,-4.733051,-4.67983,-4.7884326,-4.7093534,-4.718718,4.1834307,-5.9108434,4.785269,-7.184554,-6.0365458,-6.033707,-6.550905,-6.0352273,6.030688,-5.89471,-6.0961366,-6.141371,-6.511009,-7.683517,-7.7741814,-7.373835,-7.658138,0.705564,-3.6989586,-6.2362995,-6.397188,-6.662777,-6.879049,-6.7865896,-6.824339,6.4854116,-6.00942,-3.5372434,-5.2737374,-6.0570383,-4.6277857,-5.502965,-4.431434,-5.025854,-5.0629582,-5.4118004,-4.858248,-6.166571,-5.714115,-5.680953,-5.4921513,-5.370783,-4.8527956,-4.9682965,-5.472209,-5.8778753,-4.673353,-3.4337707,-2.7015054,-3.274414,-6.040446,-6.487821,-6.469455,-6.346555,-6.2133727,-4.188289,-6.6841955,-6.62636,-6.535653,-2.9948876,-1.7831953,-1.645871,5.8592567,-1.4014844,-1.0779427,-4.1104217,-2.1610801,-2.1090338,-6.4805846,-1.0827662,-6.8291087,-6.6624165,-6.6210055,-6.696502,-6.6248226,-6.508837,-6.5183663,-6.54475,-7.2701116,-6.5309443,-6.559213,-6.3094335,-6.458303,-6.5687885,-6.2291474,-0.5608722,0.061245825,-0.16293734,-6.6907167,-0.3342667,-0.69448876,-0.2172503,0.08386269,-7.991856,0.042214535,-3.0318594,-1.2154003,-2.0603788,-1.2338814,-1.4501064,-1.8841224,5.4126525,-1.9731097,-2.5927415,-5.1309075,-6.028261,-2.0339272,-1.7506497,-1.5731531,0.18579507,-1.4541781,-1.5395175,-0.71678513,-0.62315613,-1.3967314,-0.5721007,-0.6444438,0.3307558,-1.4340028,-1.1900935,-1.3624469,-2.064093,-1.7258989,-5.3362794,6.522105,1.0511572,1.062291,-6.258404,-4.0804524,-4.1334195,-4.076978,-5.4510226,-5.575591,-4.222786,-4.566358,-4.110849,-4.3861146,-4.112483,-4.208851,-4.622826,-3.9808588,-2.9431725,-3.1026535,-3.8107257,-4.148377,-4.415295,-4.001724,-4.9595947,-5.0508647,-1.9566904,-6.903407,-7.5560493,-7.720061,-8.22481,-8.318773,-8.35793,-8.41678,-8.34891,-8.35003,-8.39752,-8.320728,-8.121129,-7.9564085,-8.263945,-8.141494,-8.134036,-8.260317,-7.4610715,-4.3893924,-4.591191,-4.4370575,2.3388424,1.2006428,-3.7365754,0.62877756,-0.71957844,-0.36475575,-4.371559,-1.810629,12.881473,12.561505,-4.9793305,-5.5503216,-4.8135543,-4.2538214,-4.7477865,-4.94171,-6.1122184,-2.4477232,-2.7039988,-2.5344355,-2.1199803,-2.9027605,-4.3305893,6.0915675,5.856011,6.0135565,-2.8522441,-2.5269053,-2.9054027,-7.179119,-2.0530875,3.5925796,-1.2454807,2.951237,-0.9963404,-1.0673695,4.3405256,3.4180229,4.187059,3.404623,3.04426,2.1442273,3.123549,2.2094765,3.1841557,3.0608544,-1.1402873,-1.3279892,-1.7721264,-6.287577,3.0932412,3.463445,-7.020223,5.017152,-7.3263597,-7.5417604,-7.8665056,-7.64309,-6.6804037,-6.9165463,-7.908673,-6.427639,-7.1976705,-7.885655,-6.9985776,-6.4225545,-1.6312582,-6.2742295,4.63552,-5.2634783,2.9623666,1.674826,-4.9253116,-5.450657,-5.608292,-5.6906023,1.6061151,-0.51323146,0.11369304,-0.5497891,2.6224577,-0.33998257,-7.0239553,4.2757316,3.4537518,3.6652884,3.9850945,4.2137976,3.6421173,-5.164596,-4.9169755,-2.000123,-1.4428926,-1.983063,-3.6343389,-4.9786553,-5.285993,-4.3227215,-1.7804734,-4.901603,-4.8249803,-5.1224446,6.553292,-7.08383,4.2223625,4.3888764,4.579704,3.807504,3.6730835,2.0758712,7.4655232,4.34379,4.402603,3.4171703,-4.862168,-4.895429,-4.853361,-5.3620563,4.837117,-6.079548,-7.151493,-6.9732227,-4.5030665,-7.0884037,-6.765771,-7.1871862,-0.8725636,-7.1151805,-0.06522098,-6.786042,-6.48921,-6.941831,-7.1357346,-7.10864,-7.1587486,-6.695433,-6.947539,-6.874441,-7.2578335,-5.051603,-5.4322085,-5.552951,-7.7410245,-7.833814,-8.18819,-8.202132,-8.219005,-8.291586,-8.299649,-8.274808,-8.105302,-8.214535,-8.1589775,-7.6152644,-7.511893,-3.5705357,-3.685274,-3.7390857,-3.5528855,-3.4292326,-3.0270991,-2.7845538,-2.7860146,-3.207789,-3.29982,-5.3309016,-3.7566655,-3.7756686,-4.9283795,-4.310626,-4.7903376,-4.2993426,-3.6806874,-4.5133233,-1.5219007,0.8742699,-1.571576,-3.6948006,-2.2814813,-1.1440111,-3.6514964,2.2936614,-4.454876,-1.0628595,-0.9791972,-1.0489014,-0.7999225,-0.65122086,-0.93143237,0.31371483,-0.9681218,0.95001924,-0.9944548,-2.222693,-1.9668367,-0.978138,0.90351874,-1.1423264,-1.3437414,-1.2695864,-6.064355,2.2119782,3.4583666,3.0505261,0.6994251,3.7658193,3.3513076,-5.075541,-3.4998705,-3.177855,0.7540021,1.9928465,-3.225156,-2.071672,-3.23248,-3.1533976,-3.994484,-4.1051807,-3.8847585,-3.1167905,-7.4827757,-8.672774,-8.344421,-8.142112,0.6008182,-0.20550862,-2.8952894,-1.5473975,-8.595218,-5.245908,-3.5077465,-3.087173,-2.5154293,-1.9851139,-1.7233156,-1.7364134,-0.9576754,-1.4365789,-1.268667,-1.2200267,-1.5374019,-1.5432153,-1.6656276,-0.156271,-0.89150035,-0.92805374,-1.6469326,-1.6570888,-1.6587836,-1.2403585,-5.811609,-1.1036022,-6.8358536,3.977331,3.6360517,-2.219362,-5.577203,-7.7424107,0.37715444,-0.5475039,-8.903158,1.3525862,0.35204345,0.1343478,-0.5675333,0.15437451,0.7939418,-1.1245297,0.4674261,-2.0267417,-2.0831978,-1.9851925,-2.2838705,-4.992861,-3.1576862,-2.0839076,-2.3501232,-7.681087,-7.711044,-7.3818893,3.5727887,-7.6748643,-7.420535,-6.015957,-5.983188,-4.217193,-5.814872,-1.219259,-4.620306,-2.9035635,-7.821566,-7.6614337,-7.6391525,-7.1106615,-6.9238276,-7.556162,-0.07900289,-7.597996,-7.247361,-1.3803399,-1.2837577,0.037763763,-0.76633847,-7.4125657,-0.008891277,-7.26326,1.502177,0.24978933,-0.9758959,1.8897381,-0.9506812,-0.5794044,-6.5129833,-2.9321628,-0.83839154,-6.8345985,-6.9895563,-5.401998,-5.047437,-5.977226,-5.324842,-5.619936,-4.6892395,-4.7759686,-3.8781095,-3.9004421,-4.8662205,-4.6960444,-4.898084,1.7983083,-2.1367877,0.15133639,-0.7464592,-4.23217,-4.7360134,-2.6264503,-4.7979617,3.2467034,-0.20686555,-4.4479594,-4.7302732,-4.7724504,-4.5205054,-4.7283287,-4.7081113,-7.5301647,-7.010191,-7.513085,2.8833928,-6.9441495,-7.698907,-1.0308919,-6.8925,-7.3565145,-7.423824,-7.1926193,-1.3063077,-0.62807083,-4.03004,0.40367836,-1.253202,-7.322753,-7.3678026,-3.9895606,-1.5896528,-1.1901015,1.9101932,-2.361102,-2.5304708,2.1844394,-0.16931893,-0.5929832,-1.3934541,-2.37722,-2.8609567,-6.2109632,-3.4258397,-3.434551,-3.363176,-3.0203497,-3.3385727,-2.8798733,-6.3610396,-5.500883,-5.7539124,-5.5976205,-5.119711,-5.535459,-6.29982,-6.5061364,-7.7480683,-7.936892,-7.9344206,-7.7657866,-7.9609528,-7.96005,-7.934585,-7.376304,-8.148118,-7.710095,-8.016122,-8.01387,-7.992909,-8.055912,-8.042192,-7.819346,-8.076109,-7.7315874,-1.4596624,-0.6662417,-7.6396966,-0.88744926,-0.714655,-7.8607244,-7.689892,-7.750177,-7.4206142,-7.7853684,-7.9805856,-1.0420003,-0.5386317,-7.4621425,-1.1097813,-7.5189743,-7.794925,-7.78721,-7.830646,-7.784471,-7.8561726,-7.5490327,-7.717971,-7.8249183,-7.810565,-1.8310628,-7.2173877,-1.0996877,-7.4749784,-7.5354447,-7.587876,-7.626655,-7.582732,-7.174041,-7.600813,-7.660583,-7.5678005,-0.708132,-7.494131,-1.0722591,-7.5458,-4.827783,-4.030032,-4.845374,-4.684879,-4.977905,-4.7584195,-4.783151,-4.788137,-4.8419013,-4.300195,-3.4610758,-1.4523219,-3.5524893,-4.057419,-4.071712,-4.126706,-3.9166083,-3.827278,-3.5089066,-3.7012804,-2.8441434,-3.942852,-3.6681585,-8.961884,-3.4025917,-3.216486,-1.280722,-1.6232861,-8.292868,-7.4887705,-2.1105776,-1.9768851,0.7933694,-1.1369495,-0.25147262,-0.7757107,-2.36341,-5.467818,-4.553919,-5.0646586,-4.741828,-4.584373,2.6935153,-1.8743999,-3.8250406,-5.364514,-4.7557473,-1.4059541,-0.69342685,-1.4037789,-2.060459,-2.1120524,-1.3038882,-7.3703628,-7.5574894,-7.697856,-7.2508397,-7.9179153,-7.0175533,-7.5500116,-7.8650208,-7.9429326,-7.835608,-7.9293685,-7.880653,-6.609845,-7.2275558,-7.8003545,-6.688774,-6.3510084,-3.8165872,-1.2207339,1.1985881,2.2679758,3.2129269,3.8687706,1.4100633,-2.1572363,-7.410393,-6.582814,-7.3433146,-7.7233686,-7.537041,3.4046352,3.4617946,-7.189826,-6.771936,-7.6755767,-7.726329,-7.6909447,-7.7103653,-7.1647663,-4.5270915,-4.01396,-2.0295038,-1.7273605,4.153633,-1.3305074,2.1932917,1.9685328,0.18732657,0.6797535,1.7208453,0.19685952,-1.9605787,-6.46731,11.394426,0.39713356,2.8152626,2.5641675,2.1141326,2.59639,-5.8248816,-4.6273203,-4.8954034,-5.1135187,-5.1392183,-4.713126,12.880412,-4.394277,-4.6225767,-4.1978493,-2.0913465,-4.5806513,-4.1650167,-3.5343795,-2.0863423,-2.3201442,-3.5254374,-1.1349727,-3.444991,-5.6927195,-2.597787,-3.0123928,-1.3312837,-0.789182,-5.6697807,-2.6523223,-1.5165097,-0.0815106,0.5310114,-1.6105736,-4.233625,-0.4578418,-0.08591286,0.546113,-0.23162796,-2.4520893,-1.1037072,-0.34603453,-0.6439297,0.1680033,-1.6738056,-3.8518946,-3.6879938,-3.5663524,-3.1619136,-2.2385342,-2.3002572,-3.232434,-5.545334,-7.090604,-7.0272813,-6.8193555,-6.9167295,-7.11373,-7.0679674,-7.217157,-5.2026567,-5.1117644,-5.2681737,-3.6974785,-1.7177669,-2.6444957,-3.2240713,-5.776617,-5.7027397,-5.7141166,-6.139921,-5.3972006,-5.3656607,0.056768462,-5.7003694,3.3204212,0.6772146,6.29617,6.692443,-6.0864787,-3.7494082,-3.2708414,-3.3651652,-4.497068,1.9360971,3.6403575,9.78272,9.802182,9.609714,-5.5799537,-5.6793504,-5.395243,-1.2223104,-0.63961154,-0.89145106,-5.694545,-5.5393577,-5.5845084,-7.368211,-6.620487,-7.081646,-7.091282,-6.4899426,-6.330722,4.8830056,4.901037,-4.6817226,-2.9379604,4.6004148,3.8206158,4.878548,-6.087233,-5.686206,-5.6211915,5.2600827,-5.582864,5.755312,-5.258798,-5.2502337,-6.453629,-5.7036567,-6.1812477,6.1997447,-6.573726,-0.53543115,4.9924784,-1.0639778,-2.2589133,-3.524064,-3.6850722,-6.4572177,-0.6129892,-0.1422019,0.13954093,0.27356148,-0.40403235,-0.22588037,0.20240815,-0.14641811,0.41775045,-0.0740452,0.1689363,0.12008309,-0.39310694,-6.2860165,-6.2935915,-6.205356,-5.9985423,0.5192852,0.21908587,0.23689432,0.6703759,-5.7082405,0.2310829,0.12922616,-5.9375324,0.22413258,0.35558927,0.18530774,2.374558,0.32402778,0.35755056,0.2738197,0.2383602,-0.1427731,-2.188307,-0.28334534,0.3208529,-0.49380985,0.25102484,0.25343862,0.2829864,-5.8856454,-6.1609206,-6.158505,-6.642099,-8.608607,-3.2814498,-2.7659638,-3.576868,-7.628978,-7.6704698,-7.564162,2.3210669,3.4862785,-7.4798636,-7.652568,-6.4289794,-6.248565,-5.1977835,-3.904815,-6.011543,-3.8060727,-1.8170102,-5.8467216,-0.6809912,-5.2911453,-3.5390267,-7.6729116,-7.6999216,-7.716659,-7.434255,-6.8648753,-7.6339836,0.015480898,-6.929874,-7.369966,-1.3239167,-1.2923939,-0.02790855,-0.691446,-7.055952,-7.2961307,-7.402715,-6.868386,0.049106456,-7.087024,-6.4770184,-7.584808,-8.304011,-8.151166,-8.254767,-8.041322,-8.085692,-8.254019,-8.195486,-7.293856,-7.783586,-3.5794437,3.9043417,-2.9860508,-2.5956454,-2.3329146,-2.3980505,-2.8269522,-2.8912888,6.5302696,-4.7985425,-1.6279492,-2.2166767,-1.2750887,-1.0378562,0.9732692,0.92131096,-1.2717603,-1.9859854,-6.17455,-5.0220437,-5.4583783,-4.6813636,0.5696083,4.066941,-3.651985,-1.4526935,-1.0421672,-0.9381753,-1.2590086,-0.9809782,-1.0024514,-1.1003525,-0.9983262,-1.1011895,-0.819769,-1.256182,-2.179368,-2.4312046,-1.6840909,-5.892373,-5.269258,-5.1149645,3.8532968,-5.2228985,-5.441846,-7.430332,-7.6553926,-7.9637194,-7.8636847,-7.814619,-7.876119,-7.9509273,-7.254792,-7.891029,-6.899686,-1.5165803,-4.928975,-5.049773,-1.7878361,-8.055966,-1.8176444,-1.8004227,-1.3055232,-1.7265304,-1.8451531,-1.7427304,-1.8423332,-1.8624097,-1.4701952,-1.2328241,-4.591477,7.7991877,-4.1611376,-1.0781622,0.34910795,-1.0060165,-0.09285354,-0.8143907,0.13178538,-0.56937283,-0.09167397,-2.0595198,-0.33093157,-7.2815614,-6.313274,-6.244316,-6.378873,-6.0821714,-6.3288975,-3.7007465,-1.0937862,-0.73008454,-1.0974156,-1.1135508,-0.94703335,-1.1686772,-0.9400972,-0.7745114,-1.4910846,0.32066724,-0.4864441,-0.7226648,-0.61222327,-1.8925879,-5.650095,-4.0355077,-5.404478,-3.827923,-2.0378819,-3.2654383,-3.9281633,1.2072744,0.46937582,1.2295369,0.091900535,0.63189805,2.266468,0.11438802,0.084160715,-0.10251357,-2.323712,0.43784973,-0.040678434,0.14310195,1.8827889,0.6694295,-4.116076,-4.4950786,-4.8874454,-4.199981,-5.6983113,-6.2628536,-3.291955,-3.1475403,-3.3607197,-4.1239815],\"xaxis\":\"x\",\"y\":[1.7719183,2.5757153,0.55974096,-1.1229186,2.448623,1.7798076,2.4234898,2.432174,2.3972154,3.203935,3.2564757,1.0898881,0.7139972,1.2444803,-3.2535338,-4.0819,-4.1112022,-4.244513,3.2517831,7.7561073,-3.9359145,-3.6354475,-4.124695,-3.728398,-4.060749,-4.024,7.5819077,7.697614,7.4874063,7.493422,7.490916,7.6827826,7.2790837,1.0012949,0.16403276,0.006850679,-0.054139063,0.3027125,-0.14474998,-6.603697,-6.547346,-0.1552015,2.0711122,-3.9292068,-3.5740156,-2.5672507,-2.683798,-2.9456868,-2.8090339,-2.789419,-2.761856,-1.6122733,-3.4943414,-4.0730762,-4.240314,-4.3932095,-4.037267,-3.9583843,-3.8994117,-4.51526,-4.11841,-3.815519,-3.9234452,-3.9125433,-4.022699,-4.0236893,-3.1790688,-3.7098153,-2.9750555,-3.3128977,-3.2804718,-3.1382358,-2.6630895,-2.7409894,-1.420914,1.0254611,-3.5051408,-4.1515064,-4.1244984,-5.8932242,-5.0430856,-4.9104304,-3.9653966,0.71056056,-7.636323,-7.7310367,-7.801385,-7.8140755,-7.8104453,-7.638405,-7.8126664,-7.767687,-2.7027593,-2.419355,-3.6213758,2.4264336,-3.464748,2.944152,-3.4961774,-3.6429546,-3.527439,-3.1611001,3.7019713,5.438934,-3.2706857,-3.0642154,-3.6231995,-5.305878,-5.1973286,-3.3067763,-5.0660105,-5.2428365,-3.5641885,-5.301799,-5.2889624,-3.372569,0.060270682,-3.3555384,-3.6340728,0.6596223,-3.3518553,-2.7784653,2.398987,-2.4662182,2.0319767,2.759647,2.6822624,2.685249,2.8859987,2.7239652,2.2749813,2.7504697,2.1899455,2.1584375,-0.20556155,2.7350721,2.306555,2.8973134,-5.6976914,2.8630261,1.2401693,2.9431727,3.0080569,2.9652712,2.8348505,2.923664,2.8704655,2.8641553,2.7404766,2.4370742,-0.6104647,-0.42381185,-0.41499463,-0.5607265,-0.3348934,-0.083172664,-0.38308787,-1.2437983,-4.4637747,-0.7254032,2.429554,1.405933,-1.6927377,2.5729036,3.0626183,2.3540783,1.308876,2.1117582,1.4924434,3.473866,-1.3399022,3.5797713,3.7814376,-3.563693,2.6061018,4.491166,3.1942534,1.6507488,-1.5902647,1.1998553,1.2879355,0.9879822,0.74080086,-3.0643625,-2.0644987,0.2706962,-0.21432324,-1.4753656,4.79886,7.7943807,7.461953,7.3249493,7.4580126,1.121525,-1.3675414,-5.174063,-3.6849377,-1.9318768,-1.6152698,-0.689103,-0.33717948,0.42955592,-0.36799133,-4.11817,-4.4306183,-2.6415868,-3.8411431,-4.268479,-2.3812912,-1.0516723,-6.231657,-5.900087,0.6110571,-4.934104,-5.556047,-4.6288385,0.060779713,-6.1644664,-3.564864,-5.944656,-4.6785088,-6.222278,0.040267024,2.2342534,1.4114232,0.014926696,-0.3224534,-0.2262896,-0.16162544,-0.12455885,-0.1510387,-0.19026121,-0.193007,-0.1801314,-0.5137752,-0.45354065,-3.1440296,-0.12164384,0.12413047,-0.15692492,-0.14376521,-1.6916623,-1.3540256,-1.2085093,-6.660829,-4.4607763,0.5289299,1.9599059,2.416041,2.5405517,2.3934011,2.7243428,3.0617409,-2.0272171,-3.347823,-2.3043015,-3.7316732,-4.8698335,-3.747556,-3.9818966,-4.9018717,-4.301538,-5.0059066,-3.6766663,-3.4435883,-4.827748,-3.8831797,-4.8467507,-3.45982,-3.5037389,-4.8932905,-3.6571617,-3.7841897,-4.1223264,-5.029117,-2.7891536,-2.0334313,2.3049936,2.6396954,1.4356236,-2.8285575,1.3989164,0.5208608,1.9641029,3.0256488,-2.9393148,-4.3049197,-4.062303,-2.764316,-3.5906954,-4.34362,-3.601232,-4.7020307,-4.1570873,-4.177444,-4.2351747,-3.9793472,-4.5248923,-3.314135,-3.9829915,-0.701349,-2.03285,0.332265,-0.98432523,0.33548248,-0.35017097,-3.2316482,-3.2205675,7.914883,-2.898459,-3.0390315,-2.8403008,1.2954506,-2.5344846,-2.154133,-1.4604291,-1.6476834,-1.1342874,-4.417544,-5.425666,-4.576268,1.141208,-1.8109906,2.70086,2.6455646,1.8045917,3.0228531,3.3861191,3.005651,2.3362613,2.023531,3.0130384,2.853071,2.3858309,3.1020172,2.6050901,2.808192,1.8487964,2.5237179,3.2698596,2.9185574,3.05597,2.8999784,1.504801,1.2925893,-5.0700707,-4.5675106,-4.4073114,1.5498744,3.667158,1.7309794,-6.292771,-3.670453,-5.7432337,-6.070785,-5.1978016,-6.3546386,-6.5283666,-4.504537,1.9836568,1.3631012,1.7155856,0.9428084,1.8028781,2.1514442,2.4721882,2.3560503,-5.179063,1.8755848,1.7264696,-5.1603456,-5.0130863,-5.1898484,2.0092335,1.7576984,1.4999723,1.3544517,0.27485585,-4.2152452,-4.092203,2.4209387,3.1856163,-0.063996285,0.2309312,-0.4265777,-0.91235214,-0.47356117,-1.1098642,1.0418155,-1.2782787,-0.5718786,-5.585676,1.899607,-0.29458645,-0.36476701,-0.3263017,-0.29371154,-0.2899371,-0.36621314,-0.35518017,-0.48575112,-0.41462222,-0.31400886,-0.37701282,-0.229721,-0.30958983,-0.3198248,-0.42090264,-0.38249126,-0.14745484,-0.044939496,-0.3837712,-0.08591702,-2.450745,-0.09544704,-0.24484247,-0.32773715,-0.30305356,-0.135437,1.2549769,1.8136837,0.37611032,0.97027004,0.072896935,3.877036,-1.790966,0.17967163,0.25414604,-3.060563,-4.78299,-3.985132,-3.9769135,-3.936127,-2.2662475,-1.985518,3.0276048,-1.0242662,-1.3537632,-6.2491946,-0.17451555,0.20335306,0.55534536,1.7987026,1.5125633,0.6547801,1.2758847,-2.0840373,-1.9603426,2.8725247,2.9292967,2.745601,-0.22338744,0.9631381,-1.3249319,1.9605377,2.3752694,2.19975,3.07145,2.631428,6.8893237,3.0400617,6.2160215,6.8406944,7.0732217,6.343759,-0.93475944,6.6742983,7.5685267,8.172569,6.462649,6.5759215,0.5834808,-0.8603193,-2.293011,-2.4013367,0.1672145,0.17349893,-0.037275106,-0.6363733,1.3824409,-1.3709223,1.3476183,2.659961,2.3022437,0.99154514,1.9796754,1.1876575,2.691548,-1.1670032,-2.1704564,-3.1400404,-2.1609397,-2.6808016,-3.4024503,2.3774884,0.20867264,1.0933472,-0.55733156,-0.7736245,0.8997272,0.7173492,2.2203555,2.5131726,2.4719665,0.4725864,-5.0755086,0.33721942,2.7404904,1.2339824,0.65351796,0.6680459,-3.0541334,-3.745058,0.4642448,0.6034634,1.500168,-2.8070495,1.5875443,1.3969152,1.7797941,1.319005,1.4034652,6.588683,3.353559,1.0668125,-1.0230428,-0.8405552,-1.3366647,-0.81376666,-6.556178,-6.9325566,1.2371895,-6.45515,-6.9342213,-7.030375,-7.113153,-6.91695,-7.107308,-0.8002562,-0.98402023,-0.8398297,-3.7077584,-0.9453327,-0.7983331,2.514332,2.3902545,2.562343,2.6664326,2.5768723,2.2615438,2.5652082,2.7162104,5.579835,6.852184,4.168054,4.7116485,5.426365,2.5583684,6.0718,6.0222735,5.624818,-0.62964725,-2.9026434,-4.605559,-3.9179368,-5.3524776,-4.794449,-3.3412306,-2.954328,0.26099274,0.5187135,0.26046377,0.001308037,-0.84542936,0.16116412,0.24919747,-2.7071967,-2.719398,-3.2583532,-2.7940896,-2.5059695,-2.700368,0.40448585,0.62327707,-6.2838693,-6.445812,0.5305122,-0.2756122,-1.0167384,-2.3155034,0.19169177,6.900272,6.843007,6.715227,7.598726,7.741892,8.024927,7.143173,0.9478911,1.1262182,0.7984653,-0.104947574,-1.092904,-0.5861314,-1.0053726,-5.1969995,-1.2114155,-1.0358627,-1.2915783,-5.031958,-1.7000343,-1.00554,-0.17619891,-5.084867,-4.158281,1.5425484,-3.2518992,-2.8895464,-2.5907855,-3.199243,-3.5845509,-3.4574413,-3.5239108,-3.659116,-3.8367858,-3.5474954,-3.478191,-3.5818694,-3.5139396,-2.5463388,-3.280295,-3.2362554,-3.556762,-3.3373508,-3.508213,-3.330366,-3.631873,-3.5150418,-3.5127606,-4.2050467,-3.4634087,-3.250594,-3.0243568,-3.1075282,-3.14979,-3.31858,-3.0107124,-2.3188176,-3.1927273,-3.1140945,-3.0124753,-3.247506,-3.2218223,-2.4858866,-2.8242705,-2.8035505,-3.1375318,-2.6457763,-3.5922844,-3.019507,-3.4508553,-3.3003414,-3.5053642,-3.0540724,-2.8336873,-3.200638,-3.2293277,-3.3136137,-3.2282937,-3.2904997,-3.3068528,-3.2275932,-2.5760977,-2.7434747,-3.282115,-2.997725,-3.0144498,-2.5479152,-2.8645983,-3.2796133,-3.5739336,1.7826911,2.117149,2.4893248,3.0817728,2.0599244,-3.8986123,-3.8655858,-3.8312378,-3.7327073,-2.309042,-2.7132638,-3.0809627,-2.2150035,-2.7962275,-2.7403686,-2.5531168,-2.471941,-3.768432,-3.8805628,-3.7251384,-4.4904227,-3.6897879,-3.3283463,-3.0532866,-3.124067,-3.3206506,-3.8847754,-4.517501,-4.6806993,-4.519316,-3.7538133,-4.5596538,-3.7384448,-3.9798865,-4.260902,-3.3179758,-3.3808844,-3.6030369,-4.626823,-4.716549,-4.720379,-4.6505914,-4.520359,-4.6331964,-4.6565294,-4.629886,-4.5822177,-3.06346,-3.292693,-3.5640142,-3.5977023,-2.842859,-2.6729298,-2.4851766,-3.3267283,-3.2481065,-3.2457771,-3.2092233,-0.10028659,-2.9302294,1.2664676,1.3418207,0.391764,2.788947,2.542704,-1.2948947,-3.3191686,1.8589467,-1.796431,-1.4827014,-6.102741,2.2052548,2.0634136,-4.5129943,-1.2902632,-1.071945,-3.0683331,2.2947497,2.5566597,2.5229988,3.0835829,4.3510776,5.0537424,2.6718924,2.8674698,3.2473743,-2.1300554,-2.1966662,3.1463568,3.242759,1.2995948,1.1482238,2.8487544,2.7762508,2.7851076,-0.99244654,-5.4296403,-5.3960686,-5.7262926,-1.0186548,3.4675834,2.604527,-1.9240192,-0.50623786,-0.5661369,-2.1562595,-0.29434642,2.4827275,3.0157094,3.3195899,-3.718983,3.0769496,3.0976005,3.3090866,2.6069846,3.0305521,2.9456778,3.0389907,2.9591522,3.3713121,2.3627567,-2.9365041,-5.7309165,0.092624806,2.1526535,0.89773715,1.2939581,0.73100483,-0.07088327,-0.4603868,-0.19951041,-2.967658,0.5502574,2.1884587,1.0574945,1.0920061,1.6919097,0.40595567,0.50057185,-1.9593415,-1.9704677,-1.8999829,-1.8838164,-1.8684536,1.5019363,-1.3401892,-2.5070782,-2.186254,-1.8558108,-1.2066951,0.15341982,-3.2530744,-3.2608757,0.5128916,-1.708293,2.0049298,7.4734783,7.3566623,7.606555,8.529129,7.3660183,7.8780875,8.507951,7.750093,7.957352,7.8148293,7.849363,7.8926516,-1.1073529,7.0987606,-2.7067761,7.6263456,7.662844,7.6897645,2.0039504,1.6972951,1.8049889,0.8972996,1.6044334,1.4650337,0.9809618,0.93250144,0.45563194,1.8479953,2.171772,-2.26025,-0.04413052,2.1622453,0.709079,-1.6821183,-2.387981,4.486143,0.31700936,-3.4791253,4.477734,6.2937202,6.2816267,6.1966524,5.956878,6.1945996,6.1785717,6.229251,6.212504,6.266744,6.2110248,6.158617,6.268273,6.2731776,6.1272516,5.6516647,1.0367506,-0.28776884,2.1364124,-0.17933081,-5.7816415,7.094428,4.0570035,3.2574863,4.4889007,5.6307836,6.636534,-0.78890616,3.7943187,-1.5253371,-2.541643,-2.9408379,-2.9163895,-3.4841647,-2.073083,-0.6234816,-1.2196944,-2.8302813,-1.7018074,1.0915215,-2.7987835,-3.6337962,-2.2518744,3.8504486,-1.9217526,-1.3317376,0.093560316,-0.99421656,-1.7584308,-1.8245841,-1.7679855,-1.8566434,-1.7892962,-1.6507902,-1.6429679,-1.6999129,-1.567472,-1.6476172,-1.3136184,-1.4503397,-1.64771,-1.577566,-1.2223837,-1.6069816,-1.691721,-1.7554319,-1.6870269,-1.4006084,-1.4864066,-1.6115891,-4.591981,-1.9167303,-2.7196825,-2.7962942,-1.358556,-1.7861114,-2.0721853,-1.8799793,1.0293838,0.8907503,-4.645144,-4.94095,-5.3928723,-0.085020475,-5.138847,-5.268335,-5.1815085,-4.2336745,-4.404113,-5.2121315,-5.0384717,-5.2317934,-4.880417,-4.575452,-4.891329,-4.9183826,-0.900807,-1.9349744,-0.8455803,-0.18737265,-0.2985605,-1.265971,-1.7398155,-1.7755793,-2.1833224,-0.7658386,0.90645397,-4.629927,-3.1475089,-3.0840242,-3.0101192,-3.0738194,-4.3919716,-2.2555702,1.5570207,-0.67019,-0.6726488,-0.8674785,-0.6582664,0.31888166,2.429513,1.4421412,-2.4898708,-2.6024604,-3.4023957,2.6677616,3.200236,3.339841,1.6287755,3.178674,2.882151,2.5059075,3.7301598,-0.08008125,1.1360674,1.1682113,1.2817396,0.7696358,0.24750677,0.87904996,0.92793965,2.6180298,-1.3070801,0.2542143,-0.4368873,0.37033582,1.6067008,1.2720968,1.9250983,1.6942196,0.33889306,1.7939659,1.3325835,0.4560576,-6.1308904,-5.7511864,1.3023686,0.90668434,0.65677565,1.5682404,1.2031405,-0.25097844,-0.7612039,-2.2371757,-2.4043055,-1.1366248,-1.1659216,-0.7224187,-0.2899434,-0.5845812,-0.23358345,-0.66804785,2.6709344,2.276372,-0.16264139,2.7882597,2.9681537,2.796178,4.107928,2.665527,2.727173,-1.1536696,-0.79085004,1.3070418,1.7135644,-2.1656663,-2.8715568,-6.2291236,-2.107324,-2.7717485,-1.6162533,-2.7716203,-7.315857,-3.278441,-1.7126989,0.7647929,-0.3896013,-2.8656156,-2.8421834,7.0457325,-1.15438,-0.7719069,2.2308636,2.0113428,1.9246899,2.7173412,2.9360435,2.9723828,3.0130885,2.9368665,2.9112244,2.6463795,-5.329199,-5.839902,-5.6483564,-5.5136585,-5.5035644,-5.4348874,-5.713064,-5.711537,1.617268,-2.9904065,-2.975404,-1.957503,-2.4461746,-2.2368727,-2.6967459,-3.0940146,-3.2360249,-2.6540353,-2.7900412,-2.9565883,-2.851588,-3.02558,-3.2523084,-3.3592463,-3.0971189,-3.034571,-3.0520122,-2.8117642,-3.1639433,-2.9392622,-3.0427568,-3.250598,-3.1388185,-2.7619822,-0.7993743,-2.5699863,-2.897753,-1.5353394,-2.5760467,-3.0025122,-5.5150976,-2.758286,-2.64463,-0.23270488,-2.6844003,-2.3464172,-2.6159785,-2.83507,-2.9557106,-1.0751237,-1.8704575,-2.07987,-3.9807553,-4.184927,22.443378,22.44422,22.444479,22.443321,22.443398,22.44322,22.444834,22.443592,22.444458,22.44521,22.444239,22.443607,22.445827,22.44335,22.44501,22.445024,22.443577,22.445215,22.444954,-4.1305494,-3.6326137,-4.8496575,-3.8415496,-3.489374,-3.24074,-3.1890845,-1.4305542,-1.1877028,-1.2303475,-1.6244395,-1.6265363,-1.5958779,-1.5658555,-1.6083952,-1.610349,-1.6428813,-0.36273327,-1.4163834,-3.3522778,-1.4445152,-1.7878959,-1.4844543,-1.5588957,-1.5352968,-1.7016116,-0.9292697,-4.164924,-2.7977455,-1.9341664,-2.6673822,-1.4106208,-3.7466574,-1.6372178,-0.7193075,-6.777254,0.05540045,-1.4611305,-6.518755,2.5463777,2.741321,2.857189,2.5715196,2.3223858,2.537592,2.1310759,-1.2756833,-1.7467732,-1.0883075,-0.2535461,2.5123773,1.154261,2.1554232,1.3988167,1.5274067,0.87478787,-2.4995072,-1.6327918,-2.0591514,-4.305504,8.006415,-1.7097018,-3.0717456,7.6000776,-1.7809204,1.1210966,0.9591473,0.79075044,-1.6646788,2.7276075,-2.2475667,-3.3130817,0.033016082,1.9655607,-0.39484218,0.7864184,2.7946634,6.403277,7.3308477,8.526454,3.885911,5.074542,2.3253498,-1.1409669,-2.9831202,-1.359783,-2.9077842,-2.7701964,-2.8908157,-1.5355293,1.9499152,-1.6061856,-0.82945746,2.7507422,3.0303128,2.8413608,1.1516674,0.6935287,1.2754151,1.2813178,1.0704871,1.60377,2.6342123,3.1859615,2.9234657,2.8930168,2.8906121,2.847123,2.060398,2.842468,2.7806027,2.898533,2.8034165,3.1070116,2.8251412,2.853297,3.1535623,3.0568805,1.6267443,1.1899222,1.4135659,1.2163647,0.27511588,0.21193914,0.35824466,0.24474011,1.0854032,2.5442877,2.9397843,4.7649784,7.252495,7.408698,7.16915,7.2184114,4.1576233,1.1985207,-0.22598109,-2.3718677,-3.695946,-3.367,-2.005197,-3.8055358,-3.3189788,-4.262652,-4.234377,-3.9823558,-4.391375,-4.22233,-4.2585454,-4.4045744,-2.5812464,-1.101562,0.9677513,-2.8881724,-4.251044,-1.1780334,-4.1066165,-3.7952287,0.8272228,-1.8192334,-2.2398067,-0.87773776,4.9090524,1.7790402,0.84828454,1.2539941,-0.6682418,-1.4687228,-5.0161343,-5.17261,-5.1617913,-1.4164134,-3.3958561,-3.7931345,1.6902831,-5.4699154,-4.7716455,-1.7656698,-3.4313025,-0.37658247,2.4614203,-4.6003637,2.2657213,1.7779596,1.281457,-6.658779,-6.751978,7.190328,-6.5179048,-6.688836,-6.8869534,7.0297766,-7.1790085,-7.0272527,-6.921997,5.9386253,-7.0385013,5.7851734,-4.2409534,7.8201585,-7.0889535,-6.867659,-7.0568423,2.6062858,-7.175173,-6.862861,6.1324687,7.492339,6.207292,4.841524,5.666985,4.769959,2.702563,2.2652948,5.7993746,5.3489666,-6.842043,-3.2386909,-4.013565,-4.3343234,-4.4888587,-4.437815,-4.8408246,-6.75913,-7.6152883,-7.729442,-4.929556,-7.5296903,-7.264055,-7.143921,-4.3771825,-4.4504485,-6.66647,-6.2449737,-6.8504515,-7.0230384,-6.6213202,-6.9925966,-6.5343933,-6.5892363,-6.645009,-6.2444925,-6.450929,-6.665585,7.024793,7.8536997,-0.87808585,8.051201,8.104229,8.092605,8.194413,8.034804,-3.2635581,8.08842,8.141234,8.087692,-0.8656436,-1.0744878,-2.409139,-1.139635,0.03677206,-2.1776755,-2.2530735,-2.3159988,-2.4918797,-1.8660187,-0.07313752,-2.1018388,-2.0822146,-2.4171813,7.8137937,-2.3400934,-2.5548217,7.7719193,-2.3520968,7.868552,8.1870575,7.876339,7.5845723,-3.2207282,7.9222956,7.8865566,-1.7603195,7.411773,0.6637625,-4.085598,7.5069966,7.797756,7.878638,7.8458695,7.874207,2.5614014,1.4201818,2.7824142,2.6669133,1.7395325,-4.3287525,-4.2677197,2.183817,2.5134041,5.0465097,5.5933013,5.5254593,0.799993,-0.7928048,-2.0816734,-1.4012933,-0.22000329,-5.3526053,-4.5322933,0.40946412,0.24748152,-0.31720135,-0.4506776,0.96777034,0.028890155,-1.3849033,-4.4499316,-3.9171317,-3.9831877,-3.4261992,-3.0997064,-2.2661026,-3.5713508,-1.3745371,-4.028255,-3.9442685,-2.8360956,-4.000005,-4.0693126,-4.043848,-3.4624617,-4.1302285,-4.17157,-4.0901523,-4.0303187,-3.8183098,-1.9333531,-3.8598006,-3.8750525,-3.7330515,-4.139194,-5.389168,-3.928458,-5.48167,-4.73668,-4.3337736,-5.6443725,-4.7192016,-2.6516943,-3.9701176,-4.5473866,-4.306728,-4.1324973,-4.5721893,-5.3039813,-5.4889817,-6.4831233,-4.776348,-4.5874996,-5.400667,-4.625603,0.674112,-1.3381991,-1.1370001,6.9587817,7.8538504,6.400844,0.23942903,-0.29458532,2.4258435,2.8338237,4.856846,2.6298087,2.5221288,2.6042192,1.469528,-0.16547766,7.4385076,3.5317023,7.8321285,7.768311,7.7166147,7.514084,7.8557158,7.5652905,8.100204,8.085947,8.546253,8.161384,8.335119,0.2415629,2.4582434,2.4076517,2.316662,2.975959,1.8676375,-0.13968036,-1.529828,-2.4770048,0.1869431,0.06508861,-3.2241337,7.428322,6.263104,-3.6725142,-0.045078482,-0.33883092,-2.303981,-2.2911284,-4.8089676,-0.45392704,-0.1930789,1.9317204,2.3086998,2.5442169,6.328186,2.5148587,-4.1212106,2.4529977,2.3577633,1.6119378,5.905768,-2.135553,2.314255,2.5317256,4.0733404,5.025104,0.7440365,6.512593,2.6217043,-4.612382,-4.3340554,-4.422177,-4.86894,-4.93953,-5.4665375,-4.5601206,-4.577381,-4.6449223,-1.4617918,-1.2956008,-1.1470793,-1.7015635,-1.7972428,-4.5158277,-4.2823596,-4.074638,-3.7955117,-4.434048,-3.8994317,-3.8037465,-3.6558523,-1.201872,6.700034,-2.4848497,-3.4293656,-3.8924615,-4.006289,-4.1440754,-4.058301,-3.0093155,-3.5747766,-4.12117,-4.12601,-3.9575684,-4.1779823,-3.4088194,-3.4747868,-4.2333975,-4.0506926,-4.20962,-4.1433697,-4.4488287,-4.3406734,-4.13576,-4.375815,-4.2730975,-4.4433336,-4.433512,-4.54774,-4.522876,-4.1393557,-4.396004,-4.4576535,-4.4611526,-4.4618235,-3.4491487,-4.75643,-5.483678,-6.3920703,-6.3368053,-4.867949,-4.3762016,-5.355192,-4.541569,-1.8290185,-0.715479,-2.2538652,-2.1445057,-0.8349623,-0.89005524,-0.6940279,-1.1263833,0.029697422,-3.4773197,-5.5731587,-5.7230463,-1.350556,-0.6766116,-0.46978763,-1.4011903,-1.2557911,2.5610545,2.8844373,3.1402693,-2.3336923,-2.6867826,-3.7260458,-0.7063014,-4.4222984,-4.057136,-3.9720802,-3.6083398,-4.1925874,-4.4054356,-4.386023,-4.8582506,-5.0163918,-4.3520393,-4.405212,-4.4266224,-4.4023466,-4.5307007,-4.2130556,-1.4245389,-3.5056589,-5.411656,-6.13564,-6.267789,-5.07701,-5.006102,-4.682233,-3.8985498,-5.7884808,-4.656483,-4.6446695,-2.6372252,-2.201677,-2.2066436,2.2632346,-6.6313443,2.7707856,2.6756735,2.5629623,2.8425567,2.908162,2.361456,2.853512,2.8263314,3.1401381,-0.3752841,-1.2865709,-0.94080985,-3.5885537,-4.1053467,-1.2441858,-0.49138287,0.2940286,2.578163,2.7738836,2.217098,2.375163,2.1694329,1.478858,-5.4330273,2.2049484,2.8716018,-1.0898482,2.26669,2.7411501,2.0101004,2.4348075,-0.62639683,-0.630157,-0.24479647,1.5802913,1.5730878,1.5361698,1.4448032,-5.1416836,-0.98299265,0.6514169,-5.175226,-5.1993275,1.4025599,1.3144487,1.6366627,0.73603904,1.791936,1.4793309,1.5469539,1.7852688,2.0357049,2.660167,3.0367212,2.8558695,3.068746,2.9029,2.7546265,3.103495,2.8414571,3.0389538,2.8367062,2.9468443,2.9658496,3.0480685,2.9187348,2.5003645,2.8152688,2.7447865,-2.2459428,2.5149999,2.0716057,0.19256516,3.02722,-4.5160866,6.887458,7.671866,7.842792,7.6564016,6.953897,6.6375313,5.9218607,6.762938,7.108355,6.7973423,2.9361565,7.705613,7.5996113,7.523376,7.5514135,6.2728715,6.7896385,6.7642345,6.7165527,6.915022,1.8211714,2.3553889,0.69827724,-0.24089825,0.66543734,-0.13651888,-1.0818136,-0.4329909,-0.8323728,-0.8282017,7.6022005,7.597919,6.1451917,7.1000814,6.009541,0.7210622,2.423046,4.7626433,-4.56382,7.6630487,-5.9235783,-5.933497,-5.8258557,-5.5700455,-6.164932,-6.6170993,-6.3416767,0.17372982,-7.081182,-6.868323,-6.8576684,-6.598319,-6.3699794,-6.6375594,-5.9726872,-6.5116243,-6.686145,-7.0407195,-7.7512393,-7.5780277,-7.55951,-6.680434,-6.448348,-3.902814,-6.225566,-6.2155375,0.76169944,0.19800216,0.52554727,2.2301075,-6.4411316,0.13472036,-6.379641,-6.322671,2.6698492,2.8405359,2.8026226,3.1815426,2.9055312,2.8956907,2.874148,2.9255133,2.8254316,2.873566,2.6322956,2.4570186,2.786525,2.9023743,-5.060831,-5.334555,-5.4990373,-5.3029933,-2.1964362,2.8129032,-5.9987955,-6.3909388,-5.801344,2.9392054,-4.6805315,2.7565677,-4.258112,3.0649066,1.3553032,1.7640749,1.5182364,0.992013,1.0982252,-0.27901274,1.5354818,1.2694712,2.4351792,1.6568544,-2.008753,-2.2041514,1.7921023,-3.3296254,-0.38221562,1.3258651,1.3695083,-0.8725308,-1.3840461,-1.9853321,-1.5386882,-1.2556978,-1.0651758,-4.8165317,0.21658935,3.6479983,-0.0017005067,-1.3072289,-3.1596925,0.53027683,-0.69247574,-6.152368,-6.343796,-6.2344527,0.6287323,-6.937739,-6.7048073,-6.5613985,-5.7955894,-6.190952,-6.2125072,-6.553929,-6.7465024,-3.7918878,-0.26774317,-6.151895,-6.158195,-4.712731,-4.86712,-4.6004295,-5.341583,3.0587106,-4.320227,2.440727,-4.535261,-4.73449,-4.66498,-4.7264266,-1.4965494,-1.1808821,-4.3111124,-3.9194434,-3.2109406,-4.8230395,-4.3404746,2.165676,1.861091,1.8281053,2.886947,3.4478726,3.1259027,3.5339074,3.389934,3.9084885,4.1237535,3.8684545,4.0490465,3.6071,3.5460188,3.2852833,3.9948695,4.297065,3.387111,2.8505394,1.7023779,2.9930782,4.8817005,5.2928677,0.9627464,0.59188443,6.2191086,4.497641,4.8183217,4.7349,8.2940035,5.186066,4.6135554,-0.73197836,1.1236241,-1.8673382,-1.3478024,-1.0666014,-3.0577686,-3.807438,-4.124372,0.7551811,0.9836182,1.6395816,-2.1262445,1.3891225,0.38063142,2.1407657,-1.0694468,2.8156905,2.8892589,2.9515846,2.1839867,2.95213,2.8571954,2.7489178,2.9598157,2.8857405,2.614068,2.9318087,2.9825332,2.6621687,2.9524457,2.8372643,2.898687,2.9357827,2.8164387,2.9102345,2.9506998,2.7192404,2.9622273,2.8323123,2.8927572,2.9230287,2.9553711,2.9873214,2.9270208,2.863352,2.7906816,2.8605535,2.160675,0.2580125,-1.9532956,-0.35915115,-0.23039345,1.8684757,-0.025542658,2.8974576,3.0774505,0.8258081,2.5513346,2.7648547,2.8006208,-1.7121416,-1.9270985,2.6080925,3.0671299,1.1516469,1.2479131,2.915489,6.7545214,6.3046637,6.775123,6.1936765,3.2780755,5.1726494,4.449714,4.746691,4.8035426,6.768436,2.4178126,2.673746,3.2805107,2.472992,2.9844832,2.9750013,2.9414864,2.5263329,3.0366392,3.0041974,3.3324652,2.6390872,3.3825972,0.5763187,1.3779618,-2.1274114,1.0658976,0.72786766,0.26117498,1.3224229,1.623044,1.4014609,2.7184749,2.5352352,1.2685115,-5.82091,-5.938156,0.31018725,-6.8894334,-6.9124074,-6.965745,-6.473163,-6.187263,-7.0410824,-6.679658,-6.341511,-5.7297955,-6.8373084,-6.1417794,1.8054141,0.8167283,1.313853,1.3636608,1.4371275,-3.035646,-4.4070415,-2.8362074,-5.1143003,-3.0285573,-3.1895452,1.839498,1.4026585,1.1715788,1.831452,16.000973,1.465172,2.2644637,2.079714,2.3838856,2.2000442,2.0272183,4.5160365,-4.7701235,-4.571756,-4.6419706,-4.2780504,-4.6662846,-4.603219,4.5282764,2.172612,-1.3721143,-2.006583,-2.1084561,-4.3812532,-3.1171017,-4.1094484,-3.1903815,-5.0332265,-4.73027,-0.36358654,1.9297928,-0.49008697,3.1286395,2.971092,3.0958176,3.2634492,3.2973015,3.0521748,-4.667684,6.638378,3.0644186,0.36696324,2.6426737,2.786163,2.3186843,1.8684365,2.2344043,2.130253,1.972417,0.30708122,2.3754823,2.2569256,-3.3969765,-6.3330503,-3.933092,0.85110134,-2.23563,-3.4191234,-2.8444934,-3.456304,2.3265305,1.1832185,1.1780932,-0.70574814,-2.4151068,0.38728848,-3.5998306,-4.543398,-1.3255248,0.8131201,-1.2664897,-4.17197,-3.9339921,-4.3153663,1.2401217,0.4844285,0.9842255,0.37740058,-0.49614546,-7.7016587,-4.130454,-1.1499944,-0.43171,-0.12662259,-6.3241234,-6.1882205,-6.57451,2.3474612,-7.2222815,-6.735723,3.3906326,-3.8911011,-7.0892467,-7.1420374,-7.186805,-7.039362,-7.0080028,3.315244,3.4050174,3.3520527,2.8041594,2.9591877,2.9696076,3.0082796,3.0794632,3.0084016,2.9961967,3.0931113,2.9996789,2.9275498,2.3410094,-2.5145066,-3.9978447,6.8931756,7.4688535,7.802433,7.5989003,1.9491949,2.4158812,2.4992518,3.9128141,-3.7907252,-0.513355,1.9708275,2.976939,1.0865808,2.317117,1.3562028,-0.48650244,-2.2025428,-2.9795775,-0.8231382,5.188649,5.6732583,5.0689816,4.8018627,0.122524336,-3.2293706,-2.0267484,-1.8094885,5.8703265,-1.6124905,5.0684123,0.9959241,1.4211763,2.3531148,2.7974014,2.0650873,1.3680903,3.225627,2.0037236,2.6070232,2.674943,2.7256446,0.8279734,1.8596467,0.85618794,0.43954182,0.9705889,1.0845572,-0.021182982,0.75948876,0.47692463,1.554931,1.5440437,0.54282665,0.3181131,-0.6167881,-5.161753,-7.694861,-1.2210393,-3.1977117,2.9888234,1.286713,1.4910179,2.1295512,2.5064216,3.1581867,6.7771196,7.0847034,5.951594,6.0721684,5.5477676,5.8233113,6.188611,6.4718766,5.2461863,6.4303503,4.833503,4.826659,4.8201995,5.306347,4.8074822,5.771449,5.312972,4.4977975,4.737675,-0.11114353,-0.84722257,-3.237885,-3.4297383,-3.1362374,-1.8004749,-2.43204,-3.0634682,-5.8413076,-1.5587701,0.7288651,0.836424,-0.6613385,-0.20252512,-1.8494388,-0.5340648,-0.72802275,-0.29690066,7.3079567,6.3231335,7.7454734,6.015536,6.101772,5.355215,5.581609,4.7504406,1.4586198,1.2228388,-0.6105072,-0.76415867,-1.9952745,-1.735819,-1.4445542,-1.1416614,-0.8474952,-1.4883268,-2.7517645,-2.3476117,-0.7619717,1.6452419,0.08440691,-0.24278015,0.088962,0.20375435,0.27401042,0.25459313,0.2129181,0.3561037,0.36503944,0.26689455,0.54158324,0.16404831,0.1669177,0.027646085,0.17804265,-1.9836425,-0.12066706,-0.27623907,0.27027902,0.053287063,0.009774088,0.047916535,1.6463057,1.8755822,-0.21213478,2.244463,0.9557471,1.0573401,-3.0983407,2.4929106,1.5563633,1.1269784,2.0365493,2.0631256,2.226062,2.1071758,1.2550915,-4.13125,-5.043805,-5.3035984,-4.550963,2.452142,0.9574853,0.89919436,-0.22857423,1.558513,-0.15295127,1.9509748,-1.240302,-4.3496366,-5.7441096,-4.876904,-2.5604444,-1.0850388,0.9334411,0.5099301,-0.3459578,-1.4398848,-0.19218113,4.867796,3.4464004,-0.3291268,0.50371844,0.531045,-0.16874714,-3.3206372,5.4844947,5.2295585,1.0100567,0.31220236,0.14019579,-7.2575426,-1.1934661,-2.75989,0.19178456,-0.41702253,4.120588,4.3703003,4.676047,5.2891917,5.9289193,5.588197,5.112817,-4.416159,-3.046977,-3.4498765,-3.303894,-3.5273373,-1.7574972,-3.052396,-3.2131398,-3.2496936,-3.5340302,5.607745,3.7414668,4.883193,4.7134204,5.725194,4.6336627,5.374605,3.7072713,5.6414857,1.2457881,1.765366,1.452906,1.0492356,-1.3978156,-0.1670403,0.96105134,-0.12265415,1.1140534,1.2134429,1.4503089,0.2866041,2.1529994,1.9458021,-0.47231194,-0.6305099,0.26533362,-4.652077,0.8771866,-0.2977479,2.6035779,0.17697407,2.3633626,1.8727788,1.9999145,1.2752572,2.5945523,7.351262,7.4504166,7.7492948,7.2968216,0.42249534,6.9945273,7.3898425,7.656027,8.1120405,7.5772343,5.7738304,8.277308,7.780269,7.2642107,8.157673,7.841582,8.521311,8.186899,7.5832148,6.82964,7.6834636,-0.22053088,-0.13814053,-0.12870508,-0.27433094,-0.64633596,-2.0967264,-2.0649984,-3.0237045,-0.2496098,-0.34753403,7.8953657,7.9330883,7.9179482,7.8823977,7.865534,7.917798,7.877118,1.3209281,-4.4640346,8.082648,7.8746777,1.2571262,6.937843,6.721694,6.716671,3.4627795,3.173698,2.8866384,6.8773108,6.8324594,6.822413,6.8494596,6.944987,6.7345014,6.2247305,6.257536,6.252384,6.5163875,6.5470657,6.6001725,6.6119967,6.5282965,6.5179768,6.609124,6.7456098,6.5282326,6.5460434,6.6240087,6.869212,6.723654,6.7890253,6.762784,6.6680026,6.3036647,6.106429,1.6253383,2.8017275,2.9339943,2.7908168,2.6371894,2.987025,2.6374397,2.343499,3.1348684,2.3377762,2.0217865,1.9958838,2.953548,2.147445,2.0776598,2.9128983,3.244681,3.0607202,2.1936493,2.1171927,3.2050698,2.1481488,2.1422858,2.8621473,2.890657,3.177593,3.0046916,2.388057,2.5858834,2.9677038,2.3176754,-1.9446707,-1.2293843,-2.2914932,-1.335552,-2.2671626,-1.2525502,-3.0841238,-2.2186093,2.260446,0.24449582,2.1151571,2.836959,2.5930333,-2.205914,3.25768,2.9746099,2.6873376,3.9061184,3.3745239,2.5355363,2.9184952,2.8333259,2.334068,3.296679,3.070312,-2.3490412,2.7105663,2.9498405,2.3932276,3.2692883,2.4379513,2.0037363,2.5196567,0.10547388,-4.219539,-4.2984705,-6.25116,-6.0297265,-4.215351,-6.979337,-5.4652023,-4.4379277,-5.937788,-4.982481,-3.9291112,-3.13006,-3.9357965,-4.2219353,-4.1970043,-4.053266,-3.474511,-4.126577,-3.9401643,-3.737823,-3.9136653,-4.0828114,-3.8633454,-4.216176,-4.556698,-4.0510745,-4.5076165,-5.503145,-6.30846,-6.3342404,-6.3398833,-6.2457676,-6.364838,-5.3506603,-0.38622048,-0.5242953,-0.6190884,1.5693259,-0.9508807,0.4312662,-2.7905207,-0.79109865,-2.7284522,-0.32609528,-0.2640953,-1.0184164,-4.368412,-5.0530844,-5.4778013,-5.408868,-2.9564552,-5.1031055,-4.7646093,-4.788031,-1.9190067,-3.5674284,-3.9035327,-4.9887433,-5.3528914,-5.1022477,-2.4582314,0.07332574,-1.321292,-0.50602627,-1.0115397,-0.73077035,-0.79804426,-0.3309856,-0.90997666,-4.2621374,-3.576958,-1.2820292,-0.7989572,-2.4678814,-1.6593916,-2.4963882,-0.62214965,-0.72401154,-0.6811214,-4.0575805,-6.630123,0.7439527,-4.042467,-0.50168264,-1.5762743,-0.7268921,-0.13466989,-1.0418913,-0.705263,-2.2894714,-2.1177046,-4.489424,-4.3405466,-4.2725787,-2.152292,-2.2720191,-2.31402,-2.2763326,-4.4578238,-4.2717447,-4.1576757,-2.3672395,-2.3310013,-2.340115,-2.2824597,-1.8467261,-1.6511471,-2.4359264,-1.5294254,0.2011626,0.35196283,-2.134853,-2.115127,4.05835,5.144785,4.9386315,0.18910673,-3.5819297,-3.827498,-4.0558743,-3.0880833,-2.0594506,-1.6449206,0.5772715,4.820849,2.8712804,4.3228474,4.4441724,3.5940313,5.2085147,6.0161514,-1.4816674,3.5702794,6.107356,5.6986213,3.7236006,1.4157315,0.5020829,1.5128657,-0.9447449,1.3234594,-2.6970153,1.2156036,1.6974764,-3.3710399,-3.646697,-0.3828331,-0.4417889,0.8848616,1.1291318,1.3596331,3.8081114,2.2293162,0.64110106,7.459648,2.2205832,-0.36370054,0.31947598,-3.0273166,-0.9245932,2.0072873,6.142408,6.855448,7.041352,7.0178857,7.0623684,7.0265017,7.2296915,6.960361,6.90164,7.0786853,7.158864,6.241517,1.8293518,-0.7045055,-1.0716493,-0.6943311,-4.725023,-0.42919958,2.6462946,-0.46581155,0.4711792,0.5112371,-4.5813723,0.2983013,0.36349764,2.4253016,-4.9930253,-3.3115387,-4.3833776,0.22211477,-3.5944421,-2.6110616,-2.1867628,-4.215672,-4.278503,-4.765859,0.4329258,1.8295794,-6.9893174,0.9038071,0.9457068,0.9894832,0.36137882,1.8447284,1.8485221,-4.8030887,1.9241999,-4.364337,-2.285843,-3.602452,-2.7202494,-2.6919756,-2.4085584,-1.8513919,-2.5093913,-2.3352666,-1.9987451,-2.1957211,1.7968434,1.6385418,1.4826243,15.993726,1.7117606,1.1382222,0.24005495,1.782107,2.3412204,0.20943272,-2.982559,-2.4709425,-2.3841772,-0.39454436,1.5794364,1.5995467,1.4556618,2.0116625,-0.46629703,2.1727464,2.958725,2.5612,1.5313815,2.5501416,2.032274,3.4288151,3.4172199,2.7623549,2.415622,0.012098241,1.9447055,-0.89368343,1.6593328,2.7940886,-0.7914006,2.1408417,2.9740458,3.2356367,-4.3012333,-4.32812,2.3108087,1.9678637,1.3688318,2.2624128,2.1474016,1.7596902,1.6155761,0.75784343,2.2743194,1.9958835,2.261484,1.45126,1.2436236,1.5479684,-2.1352706,0.43585473,-1.4559953,2.818951,3.1047075,1.9315239,1.7778535,2.2282116,2.3208945,1.3354753,2.9374268,2.8164587,1.87071,2.2745018,2.164229,2.8029323,-2.0767615,-1.9933509,-1.9625506,-2.6253555,-4.240491,2.3267107,2.9709857,2.518741,-1.2529292,-2.8987868,-2.7535691,-2.8231354,-2.6553853,-2.609503,-2.4415934,-3.548951,-3.8939881,-2.987363,-2.5699253,-5.2741723,-2.0632348,-1.8580897,-4.4682693,-1.7954847,-1.8212242,1.4603115,1.5780433,1.8452944,0.89089215,-0.4547004,-0.9408804,-1.7174472,1.2665914,0.6327542,1.1237028,0.62977326,-3.0257072,0.79963464,1.2002838,-4.4747524,-5.0801115,-3.4466298,-3.714011,-4.7937984,2.5295472,3.116733,-5.9455857,-6.154614,-6.1600056,-6.128992,-6.3593802,-6.2295084,-6.5959606,0.036020514,-6.3904085,-6.825892,-6.8797636,-6.7699866,-6.480743,-6.8489804,-6.087713,-6.582877,-5.1265907,-6.2196503,-5.053971,-4.103744,-4.599317,-6.6928535,-6.484744,-6.9497747,-7.770233,-7.249513,-7.27364,-4.3314,-6.921385,-6.584212,-6.8133793,-3.770231,-3.7990892,-6.684849,-4.8235703,-6.395331,-1.4898257,0.99958867,-6.392219,2.429927,-0.068362266,0.11753953,-6.370546,-6.6375656,-6.9648595,-6.1828995,6.653611,6.7624855,6.7679443,6.804957,6.762614,6.613213,6.530926,6.2880282,5.854626,2.314761,-0.23284636,0.08051487,0.11591249,7.265281,7.2683606,7.2808204,7.240983,2.7348738,2.8148706,2.7960472,2.970083,2.7334492,2.1778355,2.8675015,2.7567594,1.0584896,2.820905,2.7563603,2.8116696,2.7665088,2.7777925,-4.1056395,-5.2711177,-4.217382,-4.741507,-4.7511845,-5.80461,-5.919075,0.80129075,0.6105503,0.79223365,0.4131557,-0.4784948,-0.063431814,-3.051326,-4.211555,-4.1607437,-1.7361538,-2.1804943,-1.4371296,0.34346336,-2.5355442,-0.46994781,-2.7280266,-2.0545611,0.42190474,2.0014906,2.6949184,2.780882,2.3635423,2.543501,2.5146213,2.5197017,2.4289362,-1.1931727,-1.7748121,1.0652761,1.1250025,0.543571,-0.90528166,-0.8975754,-3.4797323,-0.4137127,-1.355229,-1.3435088,-1.3790294,-2.1484797,-1.1711212,-3.8440602,-1.8362688,-1.3207877,-3.2551568,0.20780885,2.4796617,-4.1137776,2.193293,1.9550841,2.1525867,-0.57880205,2.407076,2.68369,-1.6247468,-1.7375075,2.537487,3.1840856,1.7288804,-2.4454975,-2.4495134,-2.460352,-2.3231544,-2.415555,-2.5046937,-2.5961354,-2.5926826,-2.343599,-2.3811417,-2.5754387,-2.5018485,1.81159,7.619958,7.641024,7.894337,7.921197,7.959217,-0.8251196,-0.92303586,7.7499123,7.5502872,0.06188478,0.06387262,-0.082044065,-0.06563105,-0.030966977,-0.061681062,-1.8057632,-2.278237,-2.236339,-2.0172515,-1.853048,-3.014218,0.052749887,-3.0902057,-2.4162238,-2.4146097,-2.8270633,0.039691005,0.21177836,-2.7967458,-2.7657678,-2.6731887,-2.8947442,-2.3631358,-2.3233633,-2.7124135,-3.075765,-2.9726193,-3.06137,-2.914029,-2.6718793,-2.1782908,-1.8889892,-3.1592224,-2.969868,-2.977781,-1.9871644,-7.5629034,-7.724329,-7.8139663,-7.8304644,-7.8668575,-7.7703714,-7.7902074,-1.659365,1.6590784,-1.25539,2.395505,2.9029808,3.0347192,4.6899357,1.533268,-2.9833243,1.32011,1.7000268,1.7396541,5.156162,-6.351509,-6.2476764,-6.542626,-6.386039,-3.6786273,1.5980195,1.6668897,1.8378989,2.2037306,2.58844,2.5538635,2.5689983,-3.723313,1.3387895,-4.799053,1.7857454,-0.6941744,1.7028334,0.98020786,1.9971788,1.1313678,1.7351452,1.184942,0.32429174,2.6563895,2.1871903,1.6709553,1.5957623,1.4955943,1.5161535,1.3953491,1.5808043,1.6649159,0.8126944,0.9425521,2.63033,2.866402,1.5113554,2.7333262,3.627093,3.6802328,3.1588457,2.6481092,4.0644426,4.0952883,3.7934985,-3.4290977,-3.5562754,-3.1158137,-6.1558323,-2.8384397,-2.4847155,-2.759488,-2.2486587,-2.5694287,3.7142973,2.7060707,4.433371,4.1064262,4.005467,4.143882,4.126864,4.118592,4.0954247,3.9588723,5.0881557,3.929383,3.9166021,3.1709144,3.786315,3.8872952,2.8670144,1.0629754,0.49245507,-1.3604615,4.184906,-2.0000184,-1.8056101,-3.2193513,-4.005851,5.7069306,-0.97495854,0.41717228,-0.46325615,0.5783646,-1.1605924,0.53617007,0.42496896,-6.264299,0.43487054,1.5876842,2.9025335,3.773727,2.85381,3.1087086,2.8175466,2.863284,2.9582682,2.9927204,2.4437697,2.4582548,2.9843695,2.2633586,2.7470996,1.1957399,1.1580018,2.6092315,2.7021048,2.9864302,2.8761554,0.72769845,-4.1402025,-4.4254,-4.48834,1.9214109,2.0962358,2.011169,2.0474055,1.8575997,-0.7385392,2.051286,1.7621143,1.9728649,1.9160509,2.0554185,1.6160696,0.7940185,1.7455189,2.407776,2.5142903,2.1305723,2.2515004,1.8121074,1.9284332,1.1458637,0.3945739,-7.1808605,3.0597363,6.248947,6.594125,6.689537,6.8472795,6.839788,6.9463367,6.8272057,6.865975,6.9575734,6.8189983,6.6195955,6.607302,6.8091817,6.792496,6.750835,6.87247,6.197015,2.1761308,1.6091828,2.050849,0.4983707,0.99259704,1.9971766,1.0520742,1.1435069,0.98086447,-3.2645805,-2.833878,-5.7303042,-5.676889,2.315433,2.6483262,2.3616235,2.3372025,2.3655133,2.4136758,1.7491888,2.7848656,2.664615,3.1043742,3.0438027,2.7519445,-1.4090734,-5.1487675,-5.0455937,-5.122658,3.023846,1.3923851,2.8836098,-0.2514684,0.5033161,-0.26054683,0.09876784,-0.8400224,-0.04789343,0.17593059,-1.0754662,-0.3144208,-1.0439637,-0.4354196,-0.2238188,0.17930797,-0.28304917,0.09317653,-0.17212467,-0.098496124,0.21502744,0.6296316,0.565527,2.9210806,0.13059579,0.025343493,6.1317706,-0.93603826,2.7135956,2.9750993,3.0647593,3.0386143,2.2019956,2.0238752,3.0243344,2.4483194,2.3118567,3.0635226,2.9388182,2.4161847,2.5238557,2.0767386,-2.2337265,1.7760429,-0.12242772,-0.8423102,1.6035492,1.9241791,1.9329718,0.055353303,-3.0542827,-4.1391253,0.79478735,0.46661532,-2.35195,-6.601686,2.4364681,-1.5466927,-3.1261806,-3.4384115,-2.6070035,-2.2028723,-2.9010842,-1.7294405,-2.309962,3.0436745,-2.33956,2.9897728,-3.5141332,-2.3733838,-1.8930248,-2.9724164,-1.8808647,-0.8397495,-1.887369,-2.4607186,-4.305671,2.6249735,-1.2030858,-1.1726856,-1.3960505,-1.2526227,-1.7754836,-4.594376,-2.5401423,-1.4037501,-1.3655527,-1.3265597,-0.7864284,-0.8065996,-0.9039284,1.3867667,1.0725951,2.5945697,-5.4835505,-5.4327536,-2.4934394,-5.1238213,-5.087505,-6.342805,-1.7723025,-6.9394255,-4.839414,-5.839754,-5.454792,-6.008043,-6.480082,-5.8200517,-6.750797,-6.411611,-5.3415565,-5.497017,2.5242836,1.9881321,2.2213175,2.4773374,6.3899293,6.7208033,7.0789113,7.1437483,7.021879,6.9049363,6.8938622,6.9161854,6.830124,7.015227,6.8836174,6.5400715,6.259757,2.5131247,2.7519546,2.6229272,2.7968829,2.6173363,3.6134856,3.2530527,3.0788887,3.0712266,2.8917377,2.1087031,2.9177697,2.3684654,-1.6264169,-2.0863574,-2.056234,-2.2099206,-2.711765,-2.2413044,-3.3620155,-4.4831614,-3.7886922,-2.7103555,-3.2481742,-1.0398074,-1.9791114,0.06943087,-1.9639943,6.4758277,5.8497834,6.2216067,6.041287,6.960837,7.209877,5.7405577,6.800776,8.66637,6.5770555,3.48619,4.029613,6.5639305,8.567515,6.2687287,6.8107,6.5118103,1.6583325,-0.35466692,-1.6896098,-1.868275,-1.88993,-0.22945273,-1.6219258,1.7262226,2.7180622,2.7178438,1.2831692,0.88791794,2.0966563,1.5632225,2.8619654,2.741945,2.2683187,1.7723798,1.8289015,2.4678257,-0.017023994,-0.39957038,-0.6008537,-0.55798733,-1.5110424,-1.6615622,-3.014954,-3.3631105,-0.4619744,0.49217603,1.8202627,2.4191973,2.8232276,1.9356014,3.0104427,3.0977066,2.5621843,2.9823687,2.6961768,2.455797,2.7249358,2.8134391,2.9169526,2.508076,2.3684237,2.3979177,2.9207177,2.6270473,2.3380213,2.5088208,21.462986,2.5603693,2.5541797,-1.0192446,-1.0582513,-2.7690394,2.0607073,0.06135045,-2.8886073,-1.14001,-0.35056522,-3.0447052,-2.8359442,-2.8493872,-3.431719,-4.314548,-3.211932,-3.5996878,-3.089655,7.0187325,7.652287,7.6949487,7.5099964,-2.8355944,-4.54911,7.4444127,7.451972,-5.981288,-6.34433,-6.7434516,-0.006486465,-6.3251176,-6.047365,-6.6553435,-6.8759,-5.7333083,-6.7105455,-5.196222,-5.9036837,-4.213602,-6.5804615,-6.5557175,-6.774532,-7.4804406,-7.09468,-7.2841754,-4.052677,-6.8728547,-6.6941714,-3.751842,-3.772517,-6.708606,-4.6110086,-6.5495176,0.7124049,-6.2844334,-0.3202803,0.43602484,2.1891625,0.1621739,-0.55806357,-3.6214542,-6.919512,-4.130208,-3.436386,-6.4318533,2.5111995,2.4658532,2.4599147,2.0576746,1.5759239,2.1003237,-2.6599162,-2.462609,-2.7804818,-2.9084046,-1.2846318,-1.4852457,-1.0447251,-0.37067163,-3.1134775,-5.9797955,-0.32267898,-2.4487019,-2.0265934,1.3107074,-2.3249433,-0.117829174,-4.1631436,-2.7301426,-2.9955118,-2.958767,-2.9730678,-2.775075,-0.723843,-6.2673917,-4.9566703,-6.5676346,0.04053804,-6.8779607,-6.552373,-0.31933776,-6.5997314,-7.766169,-7.563547,-6.676638,-3.8884826,-4.173155,-4.0218806,0.7083497,2.3475814,-6.665109,-6.529347,-0.031508703,2.8850489,2.7587404,0.89386916,-3.72692,-4.7293177,-4.1115856,2.9305422,0.6746104,2.4604511,-2.1405878,0.8035549,1.5376527,2.1292903,2.4095426,2.560443,2.7728872,2.401735,2.7921891,2.0696633,1.4672345,1.8104414,1.4748596,1.7974646,1.958194,1.5880408,-2.3523161,-2.6036117,-2.508335,-2.6124592,-2.5257068,-2.4839377,-2.6901162,-2.4188488,-2.3292673,-2.6869829,-2.4959106,-2.7743578,-2.638806,-2.4563372,-2.5842516,-2.622424,-2.4441788,-2.7611518,-2.3818853,1.0402145,2.328614,-2.230939,1.2122525,2.6165822,-2.497978,-2.9777658,-2.8776152,-3.0575254,-2.8614414,-2.2586021,0.75085807,2.538405,-2.3238678,1.3677667,-2.2696476,-2.9085395,-2.98524,-2.8941185,-3.0131757,-2.9808404,-3.3039544,-3.017249,-3.1659846,-2.5913281,0.49787757,-2.2486756,1.1813552,-2.4879637,-2.677335,-2.7347608,-2.8313897,-3.0417564,-2.9855206,-2.8648262,-2.766348,-2.518383,-1.562668,-2.7414818,1.3990963,-2.5133536,-3.6555383,-3.4876695,-4.2006025,-3.6073084,-4.3884664,-4.098433,-3.5615768,-3.5830998,-3.7212148,2.012063,1.7944933,0.8549229,-2.1618257,-1.982849,2.1795876,2.2416806,2.5018098,2.5989654,2.8739119,2.6244206,2.8415961,7.0008264,6.975349,-0.5371102,7.1116757,6.329688,7.5586877,5.0731416,0.4765658,0.6035324,4.311193,3.6430447,3.9839835,5.108901,3.4046757,2.9769607,3.918329,1.2752599,2.034644,1.6981107,2.040454,1.1899923,-0.79884654,-0.07425748,-2.8590395,0.08300142,1.5238576,7.0590396,6.9301715,3.6342165,3.2731,3.360503,7.259167,2.7920108,2.8924568,2.9188178,2.9539406,3.0902526,2.6164274,2.9181774,3.0683992,3.0310483,3.062003,3.1295907,3.0783756,2.6301548,2.8841107,3.0663478,2.8589423,1.7556187,-2.7795575,0.36617103,0.575376,-2.415161,-0.67541164,-0.7128075,0.6651444,1.8887944,2.8406794,2.9950478,2.8519688,2.9332163,2.8297913,-1.9780967,-1.972008,2.8171332,2.8206968,2.935908,2.9349427,2.9676635,3.003644,2.7766428,2.4466522,0.35736004,3.0115104,2.7991297,-0.64033294,2.6727936,0.7425887,0.92577016,2.0875816,1.7088392,0.55808246,2.281933,2.759248,1.2546023,-4.605602,-2.7094429,-3.401048,-3.2026374,-3.737061,-3.6050258,2.059103,1.3325464,1.7409031,1.8751044,1.0783411,1.9992902,16.029495,1.6892596,1.159263,0.7004794,1.8357595,0.7633076,1.6326685,2.3166485,2.7982109,-3.522923,2.700529,-1.0953957,2.8178344,0.88414043,2.408098,1.8531965,-3.693124,2.022551,-0.41723293,2.9796302,2.9620745,2.7667456,2.77262,3.1516268,1.5924342,3.0263457,2.9332776,2.6459758,2.8238237,2.891561,3.0574179,3.0129616,2.6320348,2.832729,3.0784369,2.425629,2.3746426,2.457546,2.1847951,3.1402967,3.1014178,2.6667292,1.5678127,5.475001,5.400865,5.4015102,5.4784636,5.8408704,5.7576675,5.8607845,-0.4050532,-0.98601466,-0.9678567,1.6350456,2.5658967,-1.8302765,-1.5487009,1.4663961,1.1076132,0.9372253,1.5206211,0.061827436,0.41623256,-4.36489,0.7660585,-3.138958,-4.4747667,-4.712261,-4.8713655,1.6867156,-3.2006538,-4.6404266,-3.2232318,0.54158616,0.5507613,-3.132578,-5.92424,-5.9452677,-5.9568305,0.77551925,1.2135353,0.85352415,-4.0553026,-2.5473552,-3.5431192,1.1992222,0.8569651,1.3789446,2.5066564,3.045121,2.8038356,2.5061405,2.3644645,2.8900683,1.4112871,1.2642833,0.42918694,2.8344457,0.53982013,-3.035377,1.2958245,1.7410246,1.2512988,1.1621256,-4.4903994,0.9390986,-3.9919333,0.3041006,0.70671844,3.663789,1.2543828,2.2618628,-3.1878915,4.8391275,6.9183335,-0.5435992,7.376373,7.535184,7.2769117,7.3258805,5.4904118,2.147208,-2.0607512,1.3314056,2.267302,2.2397668,2.5762668,0.9521981,2.5486934,1.0178572,2.2518556,0.81613225,0.9184366,1.984568,-3.9014595,-4.0064006,-3.6305637,-3.8795493,-4.4019513,-4.1122975,-4.4469647,-3.7040372,-3.8868062,-4.207989,-4.219238,-3.6716926,-3.9582171,-3.7989962,-4.0381207,-3.913097,-3.3611765,-3.0822318,-3.2591245,-3.1490095,-2.747124,-2.952125,-0.34614664,1.0409715,-4.042074,-5.9002237,-4.8841004,-4.7768097,-3.80303,-3.9291263,-3.8973362,1.2024295,0.4430336,1.3486408,1.7412931,0.7305944,-6.1956267,-6.228079,-6.315458,0.5117347,0.0875783,-6.3982244,-6.143955,-6.6389976,-6.8270993,-6.1865306,-5.5275784,-6.7929273,-5.4467597,-4.1818666,-6.6075926,-5.138361,-6.217529,-5.0330305,-6.749633,-6.5376906,-6.750626,-7.7677307,-7.2609696,-7.186386,-4.269854,-7.127617,-6.643848,-3.934195,-3.9086225,-6.873115,-5.083243,-6.449868,-6.4946275,-6.5449696,-6.3628125,-2.0726378,-6.6318426,-6.63021,6.0647407,6.621536,6.3820844,6.224536,6.2546616,6.305823,6.619621,6.447911,6.150793,6.08398,2.4503849,-0.29082248,1.943137,2.500408,3.7133095,3.5337512,2.6440005,2.7664614,-0.6117567,0.8413238,4.090686,3.5794961,5.448873,-0.6465148,2.4121783,2.3450444,-2.9501321,3.92316,1.3131009,-1.8234564,-0.4714378,0.6123227,-3.8271842,-1.733279,1.220962,0.5628908,-0.43882355,0.024707366,0.20154202,0.03278841,0.21379706,0.17076154,0.27019426,0.032463007,-0.3607278,0.34968197,2.9447746,2.1811013,0.509849,0.30897436,-0.45290038,0.009682665,-2.6818042,-0.5327145,-0.955243,2.766793,2.9844713,3.1291676,3.1064565,2.954338,3.0595443,3.0728388,2.8693693,3.0953221,2.2064502,6.8448524,0.33154213,-0.5588151,6.236957,-0.09919485,6.20556,6.1136365,6.399471,6.005255,5.537399,6.451294,7.47967,6.358761,6.621252,6.9058576,2.277902,-2.6947503,1.8224727,7.1881223,8.3818245,7.18653,8.241085,5.851321,7.2703614,7.427997,7.213225,7.497151,7.0727596,2.514135,1.7116177,2.152571,3.1932538,2.752858,3.326707,2.6408293,1.0072815,0.4951532,1.0745744,1.1047212,-0.51078403,2.47068,1.4304727,0.10360762,1.6755999,0.15635674,0.5648484,0.386294,1.588519,2.436723,2.4028242,1.9739044,1.0762916,1.8961452,1.7597302,1.7048906,1.1203821,1.0132027,0.74971503,-0.16850908,-6.2083936,0.56671816,-4.0114274,-3.1023483,1.2308898,1.395042,0.83601797,0.9868259,1.4388651,1.2363229,-0.10058339,0.17632091,2.0913992,2.3631983,2.5782042,2.133366,2.6106498,1.6280793,2.2766979,2.500122,2.4772625,2.3207858],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"hat is dynamic padding? In the \\\"Batching Inputs together\\\" video, we have seen that to be able to gro...\"],[\"we can apply. The most obvious one is to pad all the elements of the dataset to the same length: the...\"],[\"sentence inside the batch. This way batches composed of short inputs will be smaller than the batch ...\"],[\"and tokenizer, we applied the tokenization to all the dataset with padding and truncation to make al...\"],[\"(usually 512) get truncated to that length. Then we pad our samples dynamically by using a data coll...\"],[\"on CPUs and GPUs, so you should apply it if you can. Remember to switch back to fixed padding howeve...\"],[\"ow to slice and dice a dataset. Most of the time, the data you work with won’t be perfectly prepared...\"],[\"shuffling. It is generally a good idea to apply shuffling to the training set so that your model doe...\"],[\"if you have to create your own test splits from raw data. To do this, you just apply the train_test_...\"],[\"most common way to do this is with the select method. This method expects a list or generator of the...\"],[\"checks whether each rows fulfills some condition or not. For example, here we've created a small lam...\"],[\"method to delete them. You can see examples of both these method here. Some datasets have nested col...\"],[\"in the dataset. For example,here we first define a lowercase_title function that simply lowercases t...\"],[\"he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library t...\"],[\"determine the paraphrases. The object returned by the load_dataset function is a DatasetDict, which ...\"],[\"even if your dataset is huge you won't get out of RAM: only the elements you request are loaded in m...\"],[\"and names for the labels. 0 stands for not equivalent and 1 for equivalent. To preprocess all the el...\"],[\"directly apply to all the splits in our dataset with the map method. As long as the function returns...\"],[\"not need to change for this. You can also use multiprocessing with the map method, check out its doc...\"],[\"efore diving in character-based tokenization, understanding why this kind of tokenization is interes...\"],[\"estimated 170,000 different words, we would need a very large vocabulary to encompass all words. Wit...\"],[\"in a language, even words unseen during the tokenizer training can still be tokenized, so out-of-voc...\"],[\"of information held in single characters, but for others like roman-based languages, the model will ...\"],[\"This tokenization, while it has some issues, has seen some very good results in the past and should ...\"],[\"Normalization and pre-tokenization[[normalization-and-pre-tokenization]]\\n\\n\\u003cCourseFloatingBanner chap...\"],[\"## Normalization[[normalization]]\\n\\n\\u003cYoutube id=\\\"4IIC2jI9CaU\\\"\\u002f\\u003e\\n\\nThe normalization step involves some...\"],[\"```\\n\\n```python out\\n\\u003cclass 'tokenizers.Tokenizer'\\u003e\\n```\\n\\nThe `normalizer` attribute of the `tokenizer`...\"],[\"```\\n\\n```python out\\n[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (...\"],[\"```\\n\\nAlso note that unlike the BERT tokenizer, this tokenizer does not ignore the double space.\\n\\nFor...\"],[\"```\\n\\nLike the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token (`_`), ...\"],[\"## Algorithm overview[[algorithm-overview]]\\n\\nIn the following sections, we'll dive into the three ma...\"],[\"ow to preprocess pairs of sentences? We have seen how to tokenize single sentences and batch them to...\"],[\"duplicates; in the second, they are not. Another pair classification problem is when we want to know...\"],[\"an academic benchmark for text classification), 8 of the 10 datasets are focused on tasks using pair...\"],[\"deal with pairs of sentences: you just have to pass them as two arguments to the tokenizer. On top o...\"],[\"also added special tokens so we have a CLS token, the tokens from the first sentence, a SEP token, t...\"],[\"outputs the same length, and properly dealt with token type IDS and attention masks for the two sent...\"],[\"The Hugging Face Course\\n\\nThis repo contains the content that's used to create the **[Hugging Face co...\"],[\"| Language                                                                      | Source            ...\"],[\"|:------------------------------------------------------------------------------|:------------------...\"],[\"-------------------------------------------------------------------------|:-------------------------...\"],[\"----------------------------------------------------------------------------------------------------...\"],[\"----------------------------------------------------|...\"],[\"| [English](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fen\\u002fchapter1\\u002f1)                        | [`chapters\\u002fen`](ht...\"],[\"| [Spanish](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fes\\u002fchapter1\\u002f1) (WIP)                  | [`chapters\\u002fes`](ht...\"],[\"| [Hebrew](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fhe\\u002fchapter1\\u002f1) (WIP)                   | [`chapters\\u002fhe`](ht...\"],[\"| [Japanese](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fja\\u002fchapter1\\u002f1) (WIP)                 | [`chapters\\u002fja`](ht...\"],[\"| [Russian](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fru\\u002fchapter1\\u002f1) (WIP)                  | [`chapters\\u002fru`](ht...\"],[\"| [Vietnamese](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fvi\\u002fchapter1\\u002f1)               | [`chapters\\u002fvi`](https:\\u002f\\u002f...\"],[\"### Translating the course into your language\\n\\nAs part of our mission to democratise machine learnin...\"],[\"```\\n\\n**📋 Copy-paste the English files with a new language code**\\n\\nThe course files are organised und...\"],[\"```\\n\\n\\u003e 🚨 Make sure the `_toctree.yml` file only contains the sections that have been translated! Oth...\"],[\"```\\npip install -r requirements.txt\\nmake style\\n```\\n\\nOnce that's run, commit any changes, open a pull...\"],[\"```\\n\\nThen run the following script:\\n\\n```bash\\npython utils\\u002fgenerate_notebooks.py --output_dir nbs\\n```...\"],[\"What to do when you get an error[[what-to-do-when-you-get-an-error]]\\n\\n\\u003cCourseFloatingBanner chapter=...\"],[\"```\\n\\nor the following in your favorite terminal:\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nThis will promp...\"],[\"```\\n\\nNow when you call `copy_repository_template()`, it will create a copy of the template repositor...\"],[\"```\\n\\nOh no, something seems to have gone wrong! If you're new to programming, these kind of errors c...\"],[\"```python out\\n\\\"\\\"\\\"\\nMake sure that:\\n\\n- 'lewtun\\u002fdistillbert-base-uncased-finetuned-squad-d5716d28' is a...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n💡 If you encounter an error message that is difficult to understand, just copy and paste...\"],[\"Okay, this got a hit. Now let's try to download the model again with the correct model ID:\\n\\n```pytho...\"],[\"```\\n\\n```python out\\n\\\"\\\"\\\"\\nOSError: Can't load config for 'lewtun\\u002fdistilbert-base-uncased-finetuned-squa...\"],[\"```\\n\\nInteresting -- there doesn't seem to be a *config.json* file in the repository! No wonder our `...\"],[\"```\\n\\nNow we can test if this worked by loading the model from the latest commit on the `main` branch...\"],[\"```\\n\\nWoohoo, it worked! Let's recap what you've just learned:\\n\\n- The error messages in Python are kn...\"],[\"```\\n\\nNext we need a question, so let's see if our favorite frameworks are supported:\\n\\n```python\\nques...\"],[\"```\\n\\n```python out\\n\\\"\\\"\\\"\\n---------------------------------------------------------------------------\\nA...\"],[\"~\\u002fminiconda3\\u002fenvs\\u002fhuggingface\\u002flib\\u002fpython3.8\\u002fsite-packages\\u002ftransformers\\u002fmodels\\u002fdistilbert\\u002fmodeling_di...\"],[\"AttributeError: 'list' object has no attribute 'size'\\n\\\"\\\"\\\"...\"],[\"```\\n\\nOh dear, it looks like we have a bug in our code! But we're not afraid of a little debugging. Y...\"],[\"```\\n~\\u002fminiconda3\\u002fenvs\\u002fhuggingface\\u002flib\\u002fpython3.8\\u002fsite-packages\\u002ftransformers\\u002fmodels\\u002fdistilbert\\u002fmodelin...\"],[\"```\\n\\nIt looks like our code tried to call `input_ids.size()`, but this clearly won't work for a Pyth...\"],[\"```\\n\\n```python out\\n\\\"\\\"\\\"\\nQuestion: Which frameworks can I use?\\nAnswer: pytorch, tensorflow, and jax\\n\\\"\\\"...\"],[\"hat is transfer learning? The idea of Transfer Learning is to leverage the knowledge acquired by a m...\"],[\"B. When training from scratch, all the model’s weight are initialized randomly. In this example, we ...\"],[\"86% easily. This is because pretrained models are usually trained on large amounts of data that prov...\"],[\"key difference with ImageNet is that the pretraining is usually self-supervised, which means it does...\"],[\"have done in school. BERT was pretrained this way using the English Wikipedia and 11,000 unpublished...\"],[\"labels. To be as efficient as possible, the pretrained model used should be as similar as possible t...\"],[\"from these countries. OpenAI also studied the bias in the predictions of its GPT-3 model (which was ...\"],[\"ow to batch inputs together? In this video, we will see how to batch input sequences together. In ge...\"],[\"error, because all arrays and tensors should be rectangular. One way to overcome this limit is to ma...\"],[\"used to pad the second sentence should not be picked randomly: the model has been pretrained with a ...\"],[\"should not come as a total surprise: when computing the contextual representation of each token, the...\"],[\"input IDs, with zeros and ones. Ones indicate the tokens the attention layers should consider in the...\"],[\"upercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on whi...\"],[\"or another and to learn a new API. All those setups are handled by the Trainer API, and there are se...\"],[\"training loop (here shown on the code of the training loop from the \\\"Raw training loop\\\" video), Acce...\"],[\"main method to remember. Accelerate handles device placement, so you don't need to put your batch on...\"],[\"this: pass along the evaluation dataloader to the accelerator.prepare method, like for training. The...\"],[\"easy API to configure your setup and launch your training script. In a terminal, run accelerate conf...\"],[\"Building your first demo[[building-your-first-demo]]\\n\\n\\u003cCourseFloatingBanner chapter={9}\\n  classNames...\"],[\"```\\n\\nLet's walk through the code above:\\n\\n- First, we define a function called `greet()`. In this cas...\"],[\"Try using this GUI right now with your own name or some other input!\\n\\nYou'll notice that in this GUI...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-hello-world-custom.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"300\\\" tit...\"],[\"```\\n\\nThis function completes prompts that you provide, and you can run it with your own input prompt...\"],[\"Natural Language Processing[[natural-language-processing]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n  ...\"],[\"## Why is it challenging?[[why-is-it-challenging]]\\n\\nComputers don't process information in the same ...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={8}\\n    classNames=\\\"absolute z-10 ri...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Sharing pretrained models[[sharing-pretrained-models]]\\n\\n{#if fw ===...\"],[\"\\u003cYoutube id=\\\"9yY3RB_GSPM\\\"\\u002f\\u003e\\n\\nWe encourage all users that train models to contribute by sharing them ...\"],[\"```\\n\\nIn a terminal, you can run:\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nIn both cases, you should be pr...\"],[\"```\\n\\nWhen you call `trainer.train()`, the `Trainer` will then upload your model to the Hub each time...\"],[\"```\\n\\nThen you should add `callbacks=[callback]` in your call to `model.fit()`. The callback will the...\"],[\"```\\n\\nThis will create the new repository `dummy-model` in your profile, and populate it with your mo...\"],[\"```\\n\\nNow head to the Model Hub to find your newly uploaded model: *https:\\u002f\\u002fhuggingface.co\\u002fuser-or-or...\"],[\"The `push_to_hub()` method is backed by the [`huggingface_hub`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggi...\"],[\"```\\n\\nThe `huggingface_hub` package offers several methods and classes which are useful for our purpo...\"],[\"```\\n\\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belo...\"],[\"After creating your model repository, you should see a page like this:\\n\\n\\u003cdiv class=\\\"flex justify-cen...\"],[\"We'll take a look at how to add some new files next.\\n\\n## Uploading the model files[[uploading-the-mo...\"],[\"```\\n\\nThis will upload the file `config.json` available at `\\u003cpath_to_file\\u003e` to the root of the reposi...\"],[\"```\\n\\nAnd others! We recommend taking a look at the `Repository` documentation available [here](https...\"],[\"```\\n\\n```bash\\nUpdated git hooks.\\nGit LFS initialized.\\n```\\n\\nOnce that's done, the first step is to clo...\"],[\"```\\n\\n```bash\\nREADME.md\\n```\\n\\nIf you just created your repository using Hugging Face Hub's `create_rep...\"],[\"```\\n{\\u002fif}\\n\\nNow that we've saved some model and tokenizer artifacts, let's take another look at the *...\"],[\"```\\n\\nWe can then have a look at the files that are currently staged:\\n\\n```bash\\ngit status\\n```\\n\\n{#if f...\"],[\"```\\n\\nWe can see that all files have `Git` as a handler, except *pytorch_model.bin* and *sentencepiec...\"],[\"```\\n{\\u002fif}\\n\\nPushing can take a bit of time, depending on the speed of your internet connection and th...\"],[\"```\\n\\n{#if fw === 'pt'}\\nIf we take a look at the model repository when this is finished, we can see a...\"],[\"n these few videos, we'll take a look at the tokenizers. In Natural Language Processing, most of the...\"],[\"one, so we recommend you look at the videos in the following order: Word-based, Character-based, and...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fine-tuning a masked language model[[fine-tuning-a-masked-language-...\"],[\"However, there are a few cases where you'll want to first fine-tune the language models on your data...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-distilbert-base-uncased-finetuned-imdb.hf.space\\\" frameBorder=\\\"0\\\" h...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-course\\u002fdocum...\"],[\"```\\n\\nWe can see how many parameters this model has by calling the `num_parameters()` method:\\n\\n```pyt...\"],[\"```python out\\nModel: \\\"tf_distil_bert_for_masked_lm\\\"\\n________________________________________________...\"],[\"vocab_projector (TFDistilBer multiple                  23866170  \\n==================================...\"],[\"```\\n\\n{\\u002fif}\\n\\nWith around 67 million parameters, DistilBERT is approximately two times smaller than th...\"],[\"```\\n\\nWith a tokenizer and a model, we can now pass our text example to the model, extract the logits...\"],[\"```\\n\\n{\\u002fif}\\n\\n```python out\\n'\\u003e\\u003e\\u003e This is a great deal.'\\n'\\u003e\\u003e\\u003e This is a great success.'\\n'\\u003e\\u003e\\u003e This is a ...\"],[\"```\\n\\nWe can see that the `train` and `test` splits each consist of 25,000 reviews, while there is an...\"],[\"```\\n\\n```python out\\n\\n'\\u003e\\u003e\\u003e Review: This is your typical Priyadarshan movie--a bunch of loony character...\"],[\"'\\u003e\\u003e\\u003e Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, an...\"],[\"```\\n\\nYep, these are certainly movie reviews, and if you're old enough you may even understand the co...\"],[\"So to get started, we'll first tokenize our corpus as usual, but _without_ setting the `truncation=T...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['attention_mask', 'input_id...\"],[\"```\\n\\n```python out\\n512\\n```\\n\\nThis value is derived from the *tokenizer_config.json* file associated w...\"],[\"```\\n\\n```python out\\n'\\u003e\\u003e\\u003e Review 0 length: 200'\\n'\\u003e\\u003e\\u003e Review 1 length: 559'\\n'\\u003e\\u003e\\u003e Review 2 length: 192'\\n...\"],[\"```\\n\\nAs you can see in this example, the last chunk will generally be smaller than the maximum chunk...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['attention_mask', 'input_id...\"],[\"```\\n\\n```python out\\n\\\".... at.......... high. a classic line : inspector : i'm here to sack one of you...\"],[\"```\\n\\nTo see how the random masking works, let's feed a few examples to the data collator. Since it e...\"],[\"```\\n\\nNice, it worked! We can see that the `[MASK]` token has been randomly inserted at various locat...\"],[\"```py\\nimport collections\\nimport numpy as np\\n\\nfrom transformers import default_data_collator\\n\\nwwm_pro...\"],[\"```\\n\\n{:else}\\n\\n```py\\nimport collections\\nimport numpy as np\\n\\nfrom transformers.data.data_collator impo...\"],[\"```\\n\\n```python out\\n'\\u003e\\u003e\\u003e [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as so...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Try it out!** Run the code snippet above several times to see the random masking ha...\"],[\"```\\n\\nThis has automatically created new `train` and `test` splits, with the training set size set to...\"],[\"```\\n\\nNext, we set up our training hyperparameters and compile our model. We use the `create_optimize...\"],[\"# Train in mixed-precision float16\\ntf.keras.mixed_precision.set_global_policy(\\\"mixed_float16\\\")\\n\\nmode...\"],[\"```\\n\\nWe're now ready to run `model.fit()` -- but before doing so let's briefly look at _perplexity_,...\"],[\"```\\n\\nHere we tweaked a few of the default options, including `logging_steps` to ensure we track the ...\"],[\"```\\n\\nWe're now ready to run `trainer.train()` -- but before doing so let's briefly look at _perplexi...\"],[\"```\\n\\n{:else}\\n\\nAssuming our test set consists mostly of sentences that are grammatically correct, the...\"],[\"```\\n\\n{:else}\\n\\n```python\\neval_loss = model.evaluate(tf_eval_dataset)\\nprint(f\\\"Perplexity: {math.exp(ev...\"],[\"```\\n\\n{\\u002fif}\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Your turn!** Run the training above after changing the data collator to the ...\"],[\"```\\n\\nNext, we'll apply this function to our test set and drop the unmasked columns so we can replace...\"],[\"```\\n\\nWith these objects, we can now prepare everything for training with the `Accelerator` object:\\n\\n...\"],[\"```\\n\\nWith that done, it's just a simple matter of writing out the full training and evaluation loop:...\"],[\"```\\n\\nCool, we've been able to evaluate perplexity with each epoch and ensure that multiple training ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Using pretrained models[[using-pretrained-models]]\\n\\n{#if fw === 'pt...\"],[\"Let's say we're looking for a French-based model that can perform mask filling.\\n\\n\\u003cdiv class=\\\"flex ju...\"],[\"```\\n\\n```python out\\n[\\n  {'sequence': 'Le camembert est délicieux :)', 'score': 0.49091005325317383, '...\"],[\"```\\n\\nAs you can see, loading a model within a pipeline is extremely simple. The only thing you need ...\"],[\"```\\n{:else}\\n```py\\nfrom transformers import CamembertTokenizer, TFCamembertForMaskedLM\\n\\ntokenizer = C...\"],[\"Understanding the Interface class[[understanding-the-interface-class]]\\n\\n\\u003cCourseFloatingBanner chapte...\"],[\"For a complete list of components, [see the Gradio docs ](https:\\u002f\\u002fgradio.app\\u002fdocs). Each pre-built c...\"],[\"```py\\nimport numpy as np\\nimport gradio as gr\\n\\n\\ndef reverse_audio(audio):\\n    sr, data = audio\\n    re...\"],[\"```\\n\\nThe code above will produce an interface like the one below (if your browser doesn't\\nask you fo...\"],[\"The code snippet below shows how three input components line up with the three arguments of the `gen...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-generate-tone.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"450\\\" title=\\\"G...\"],[\"We'll cover the `share` parameter in a lot more detail in the next section!\\n\\n## ✏️ Let's apply it![[...\"],[\"```\\n\\nIf your browser doesn't ask you for microphone permissions, \\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fspa...\"],[\"ou are at the right place if you want to understand what the Byte pair Encoding subword tokenization...\"],[\"corpus we used to train it. How is a BPE tokenizer trained? First of all, we have to get a corpus of...\"],[\"of the following words: huggingface, hugging, hug, hugger, etc. BPE is an algorithm that starts with...\"],[\"how to increase it. We return to our split corpus, we will go through the words one by one and count...\"],[\"note our first merging rule and we add the new token to our vocabulary. We can then apply this mergi...\"],[\"the vocabulary and then we merge all the pairs of tokens composed of the token \\\"le\\\" and \\\"a\\\" into our...\"],[\"have learned our vocabulary and our merging rules, we can tokenize new texts. For example, if we wan...\"],[\"that's it, I hope that now the BPE algorithm has no more secret for you!...\"],[\"ome bugs in your code are very straightforward. You try running it, you get a syntax error somewhere...\"],[\"added bonus problem that your models are often compiled before execution, which is great for perform...\"],[\"we create our tokenizer and we tokenize the dataset. Next, we convert our datasets to TensorFlow dat...\"],[\"- how do we even begin to debug something like that? When the error you get doesn't immediately sugg...\"],[\"like so, by looping over the dataset for one iteration and then breaking. So what do we get when we ...\"],[\"need to be passed in the input dictionary, where the model can see them. This internal loss is the l...\"],[\"are, or we keep using Keras losses, but we move the labels to the place Keras expects them. For simp...\"],[\"all the outputs nan , all the weights are nan too. Once a single nan creeps into your computations, ...\"],[\"You can see this in more detail in the accompanying section of the course notes, but we find that if...\"],[\"loss of nan because we got an \\\"impossible\\\" label. To fix that, we need to go back and set the model ...\"],[\"at a fairly high value. What's going on? Well, when things are mostly working, but training is just ...\"],[\"but in the process we invisibly got the default learning rate, which is 1e-3, or ten to the power of...\"],[\"this to see this in more detail and to experiment with the code yourself. Good luck, and remember to...\"],[\"How to write a good issue[[how-to-write-a-good-issue]]\\n\\n\\u003cCourseFloatingBanner chapter={8}\\n  classNam...\"],[\"\\u003cTip\\u003e\\n\\n🚨 Many issues in the 🤗 Transformers repository are unsolved because the data used to reproduc...\"],[\"```\\ntransformers-cli env\\n```\\n\\nand you should get something like this:\\n\\n```out\\nCopy-and-paste the tex...\"],[\"```\\n```python\\n```\\n\\nthen paste in your minimal reproducible example and type a new line with three ba...\"],[\"oading a custom dataset. Although the Hugging Face Hub hosts over a thousand public datasets, you'll...\"],[\"need to provide the name of the format to the load_dataset function, along with a data_files argumen...\"],[\"data_files argument. The CSV loading script also allows you to pass several keyword arguments, so he...\"],[\"filepath. Let's now take a look at loading raw text files. This format is quite common in NLP and yo...\"],[\"text are also represented as a row in the dataset. For JSON files, there are two main formats to kno...\"],[\"Summary[[summary]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    classNames=\\\"absolute z-10 right-0 top-...\"],[\"What if my dataset isn't on the Hub?[[what-if-my-dataset-isnt-on-the-hub]]\\n\\n\\u003cCourseFloatingBanner ch...\"],[\"🤗 Datasets provides loading scripts to handle the loading of local and remote datasets. It supports ...\"],[\"The training and test splits are hosted on GitHub, so we can download them with a simple `wget` comm...\"],[\"```\\n\\nThis will download two compressed files called *SQuAD_it-train.json.gz* and *SQuAD_it-test.json...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['title', 'paragraphs'],\\n   ...\"],[\"```\\n\\nThis is exactly what we wanted. Now, we can apply various preprocessing techniques to clean up ...\"],[\"```\\n\\nThis can be useful if you don't want to manually decompress many GZIP files. The automatic deco...\"],[\"```\\n\\nThis returns the same `DatasetDict` object obtained above, but saves us the step of manually do...\"],[\"rite your own training loop in PyTorch. In this video, we will look at how we can do the same fine-t...\"],[\"it to the model. With the labels, we can then compute a loss. That number is not useful on its own, ...\"],[\"on your favorite deep learning course. We will use the GLUE MRPC dataset here again, and we have see...\"],[\"we try to grab a batch of data and inspect it. Like our dataset elements, it's a dictionary, but thi...\"],[\"here two. Again, to be sure everything is going well, we pass the batch we grabbed to our model and ...\"],[\"you like. Using the previous loss and computing the gradients with loss.backward(), we check that we...\"],[\"the Transformers library is just a convenience function to easily build such a scheduler, you can ag...\"],[\"for you! We can now put everything together! First we put our model in training mode (which will act...\"],[\"in our scheduler for the next iteration and zero the gradients of the optimizer. Once this is finish...\"],[\"send it those intermediate predictions. Once the evaluation loop is finished, we just have to call t...\"],[\"atasets and DataFrames equals love. Although the processing functions of Datasets will cover most th...\"],[\"an example, let's suppose we're analysing Supreme Court cases from Switzerland. As usual we download...\"],[\"regions. Answering these questions with the native Arrow format isn't easy, but we can easily switch...\"],[\"that the Datasets library changes the magic __getitem__() method of the dataset. The __getitem__() m...\"],[\"one go. And once you have a DataFrame, you can find answers to all sorts of complex questions or mak...\"],[\"n this video, we'll study the decoder architecture. An example of a popular decoder-only architectur...\"],[\"use a small example, using three words. We pass them through the decoder. We retrieve a numerical re...\"],[\"representation of the word in question. The dimension of that vector is defined by the architecture ...\"],[\"the words on the left and right, I.e., the bidirectional context, decoders only have access to the w...\"],[\"used in a wide variety of tasks. However, the strength of a decoder lies in the way a word has acces...\"],[\"We use this as input for the decoder. The model outputs a vectors of dimension 768. This vector cont...\"],[\"we are now at \\\"My name\\\". This is where the \\\"autoregressive\\\" aspect comes in. Auto-regressive models ...\"],[\"a while; GPT-2, for example, has a maximum context size of 1024. We could eventually generate up to ...\"],[\"It is based off of the masked self-attention layer, which allows to have word embeddings which have ...\"],[\"\\\"sequence-to-sequence\\\" transformer (which can generally be used interchangeably). We recommend you c...\"],[\"n our other videos, and as always, there'll be links below if you want to check those out, we showed...\"],[\"can use the standard Keras predict() method, as shown here. You simply pass in tokenized text to thi...\"],[\"into the model’s probability outputs, you just apply a softmax, like so. What if we want to turn tho...\"],[\"skip the softmax step entirely, because the largest logit will always be the largest probability too...\"],[\"data from the MRPC dataset, which is part of the GLUE benchmark. Each of the GLUE datasets, as well ...\"],[\"compute those metrics to benchmark our model, we just pass them the model’s predictions, and the gro...\"],[\"'metric' argument to compile(). As with things like loss and optimizer, you can specify the metrics ...\"],[\"a bit beyond the scope of this course, I'll link to the relevant TF docs below because it can be ver...\"],[\"Unigram tokenization[[unigram-tokenization]]\\n\\n\\u003cCourseFloatingBanner chapter={6}\\n  classNames=\\\"absolu...\"],[\"This is all a very costly operation, so we don't just remove the single symbol associated with the l...\"],[\"```\\n(\\\"hug\\\", 10), (\\\"pug\\\", 5), (\\\"pun\\\", 12), (\\\"bun\\\", 4), (\\\"hugs\\\", 5)\\n```\\n\\nand for this example, we will...\"],[\"```\\n\\nSo, the sum of all frequencies is 210, and the probability of the subword `\\\"ug\\\"` is thus 20\\u002f210...\"],[\"```\\n\\nSo, `\\\"pug\\\"` would be tokenized as `[\\\"p\\\", \\\"ug\\\"]` or `[\\\"pu\\\", \\\"g\\\"]`, depending on which of those s...\"],[\"```\\n\\nThus `\\\"unhug\\\"` would be tokenized as `[\\\"un\\\", \\\"hug\\\"]`.\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Now your turn!** Determine t...\"],[\"```\\n\\nNow we need to compute how removing each token affects the loss. This is rather tedious, so we'...\"],[\"```\\n\\nLike for BPE and WordPiece, we begin by counting the number of occurrences of each word in the ...\"],[\"```\\n\\nWe group the characters with the best subwords to arrive at an initial vocabulary of size 300:\\n...\"],[\"```\\n\\nNow the main function is the one that tokenizes words using the Viterbi algorithm. As we saw be...\"],[\"Once the main loop is finished, we just start from the end and hop from one start position to the ne...\"],[\"```\\n\\nWe can already try our initial model on some words:\\n\\n```python\\nprint(encode_word(\\\"Hopefully\\\", m...\"],[\"```\\n\\nWe can try it on a given token:\\n\\n```python\\nscores = compute_scores(model)\\nprint(scores[\\\"ll\\\"])\\np...\"],[\"```\\n\\nThen, to tokenize some text, we just need to apply the pre-tokenization and then use our `encod...\"],[\"n this video, we're going to go over the HuggingFace Model Hub navigation. This is the huggingface.c...\"],[\"tasks, such as question answering or text classification, but it isn't only limited to NLP. Other ta...\"],[\"it isn't limited to it. The model Hub is used to host a lot of different frameworks' models, and we ...\"],[\"which the model is shared. On the right, you'll find the models available on the model Hub! The mode...\"],[\"crafted a model card is, the easier it will be for other users to leverage your model in their appli...\"],[\"we have just seen. The \\\"Files & Versions tab\\\" displays the architecture of the repository of that mo...\"],[\"shows how to load that model within the appropriate library. For BERT, this is transformers....\"],[\"n this video, we're going to see how to load and fine-tune a pre-trained model. It's very quick, and...\"],[\"To start, we pick which model we want to start with - in this case we're going to use the famous, th...\"],[\"of interest. We load the language model with this one line of code here, using the \\\"from_pretrained\\\"...\"],[\"probably seen this already, but if not, this is one of its core methods - you always need to \\\"compil...\"],[\"for the wrong classes. Note that you can specify the loss function as a string, like we did with the...\"],[\"In fact, if you remember absolutely nothing else from this video, remember to always check whether y...\"],[\"rate, and to do that we'll need to import the actual optimizer rather than just calling it by string...\"],[\"to break the data into batches and train on it.  So the first input is tokenized text - you will alm...\"],[\"classes for our examples, and that’s it. If you're following along with the data from our datasets v...\"],[\"everything works out, you should see a little training progress bar as your loss goes down. And whil...\"],[\"get an even lower loss, and an even more accurate model. And what do we do with our model once it's ...\"],[\"Part 2 Release Event[[part-2-release-event]]\\n\\nFor the release of part 2 of the course, we organized ...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cYoutube id=\\\"VzvG23gmcYU\\\"\\u002f\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"h...\"],[\"Margaret Mitchell is a researcher working on Ethical AI, currently focused on the ins and outs of et...\"],[\"Chen Qian is a software engineer from Keras team, with a focus on high-level modeling APIs. Chen got...\"],[\"## Day 2: The tools to use[[day-2-the-tools-to-use]]\\n\\n**Lewis Tunstall:** *Simple Training with the ...\"],[\"**Lucile Saulnier:** *Get your own tokenizer with 🤗 Transformers & 🤗 Tokenizers*\\n\\n\\u003cdiv class=\\\"flex j...\"],[\"Abubakar Abid is the CEO of [Gradio](www.gradio.app). He received his Bachelor's of Science in Elect...\"],[\"How do Transformers work?[[how-do-transformers-work]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    cla...\"],[\"- **February 2019**: [GPT-2](https:\\u002f\\u002fcdn.openai.com\\u002fbetter-language-models\\u002flanguage_models_are_unsup...\"],[\"We will dive into these families in more depth later on.\\n\\n## Transformers are language models[[trans...\"],[\"Another example is *masked language modeling*, in which the model predicts a masked word in the sent...\"],[\"Unfortunately, training a model, especially a large one, requires a large amount of data. This becom...\"],[\"This is why sharing language models is paramount: sharing the trained weights and building on top of...\"],[\"This pretraining is usually done on very large amounts of data. Therefore, it requires a very large ...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"To put this into context, consider the task of translating text from English to French. Given the in...\"],[\"To speed things up during training (when the model has access to target sentences), the decoder is f...\"],[\"As we dive into Transformer models in this course, you'll see mentions of *architectures* and *check...\"],[\"n this video we'll take a look at how you upload your very own dataset to the Hub. The first you'll ...\"],[\"specify whether it is a public or private dataset. Public datasets can be accessed by anyone, while ...\"],[\"uploaded the files, you'll see them appear in the repository under the \\\"Files and versions\\\" tab. The...\"],[\"First you need to create some metadata that will allow your dataset to be easily found by others on ...\"],[\"name of your repository and a data_files argument for the files and you're good to go!...\"],[\"n this video we take a look at the mysterious sounding metric called Perplexity. You might have enco...\"],[\"the likelihood. We can calculate the likelihood as the product of each token’s probability What this...\"],[\"which also is  a classification task. Therefore, if we want to calculate the cross entropy of an exa...\"],[\"ability to generate quality text and the same is true for perplexity. For this reason one usually al...\"],[\"Decoder models[[decoder-models]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    classNames=\\\"absolute z-1...\"],[\"n this video, we'll study the encoder architecture. An example of a popular encoder-only architectur...\"],[\"Let's dive in this representation. It contains one vector per word that was passed through the encod...\"],[\"which we call the “context”.As in, it looks to the left context, the word on the left of the one we'...\"],[\"positions (or different words) in a single sequence, in order to compute a representation of that se...\"],[\"BERT, arguably the most famous transformer model, is a standalone encoder model and at the time of r...\"],[\"shine. First of all, Masked Language Modeling, or MLM. It's the task of predicting a hidden word in ...\"],[\"little chance that BERT would have been able to identify \\\"name\\\" as the correct word. The encoder nee...\"],[\"giving a sequence a rating from one to five stars if doing review analysis, to giving a positive or ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Translation[[translation]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCourseFloatingBanne...\"],[\"- **Style transfer**: Creating a model that *translates* texts written in a certain style to another...\"],[\"Once we're finished, we will have a model able to make predictions like this one:\\n\\n\\u003ciframe src=\\\"http...\"],[\"As in the previous sections, you can find the actual model that we'll train and upload to the Hub us...\"],[\"```\\n\\nIf you want to work with a different pair of languages, you can specify them by their codes. A ...\"],[\"```\\n\\nNow let's take a look at one element of the dataset:\\n\\n```py\\nsplit_datasets[\\\"train\\\"][1][\\\"transla...\"],[\"```\\n\\nOur pretrained model, however, sticks with the compact and familiar English word:\\n\\n```py\\ntransl...\"],[\"```\\n\\nYou can also replace the `model_checkpoint` with any other model you prefer from the [Hub](http...\"],[\"```\\n\\nAs we can see, the output contains the input IDs associated with the English sentence, while th...\"],[\"```\\n\\nNote that we set the same maximum length for our inputs and outputs. Since the texts we're deal...\"],[\"```\\n\\nNow that the data has been preprocessed, we are ready to fine-tune our pretrained model!\\n\\n{#if ...\"],[\"```\\n\\n\\u003cTip warning={false}\\u003e\\n\\n💡 The `Helsinki-NLP\\u002fopus-mt-en-fr` checkpoint only has PyTorch weights, ...\"],[\"{#if fw === 'pt'}\\n\\n```py\\nfrom transformers import DataCollatorForSeq2Seq\\n\\ndata_collator = DataCollat...\"],[\"```\\n\\n{:else}\\n\\n```py\\nfrom transformers import DataCollatorForSeq2Seq\\n\\ndata_collator = DataCollatorFor...\"],[\"```\\n\\nHere are the labels for the first and second elements in our dataset:\\n\\n```py\\nfor i in range(1, ...\"],[\"```\\n\\n{\\u002fif}\\n\\n\\n### Metrics[[metrics]]\\n\\n\\u003cYoutube id=\\\"M05L1DhFqcw\\\"\\u002f\\u003e\\n\\n{#if fw === 'pt'}\\n\\nThe feature tha...\"],[\"One weakness with BLEU is that it expects the text to already be tokenized, which makes it difficult...\"],[\"```\\n\\nWe can then load it via `evaluate.load()` like we did in [Chapter 3](\\u002fcourse\\u002fchapter3):\\n\\n```py\\n...\"],[\"```\\n\\nThis gets a BLEU score of 46.75, which is rather good -- for reference, the original Transforme...\"],[\"```\\n\\n```python out\\n{'score': 0.0,\\n 'counts': [2, 1, 0, 0],\\n 'totals': [2, 1, 0, 0],\\n 'precisions': [...\"],[\"```\\n\\nThe score can go from 0 to 100, and higher is better.\\n\\n{#if fw === 'tf'}\\n\\nTo get from the model...\"],[\"@tf.function(jit_compile=True)\\ndef generate_with_xla(batch):\\n    return model.generate(\\n        inpu...\"],[\"```\\n\\n{:else}\\n\\nTo get from the model outputs to texts the metric can use, we will use the `tokenizer....\"],[\"```\\n\\n{#if fw === 'tf'}\\n\\nBefore we start, let's see what kind of results we get from our model withou...\"],[\"```\\n\\nNext, we define a `PushToHubCallback` to upload our model to the Hub during training, as we saw...\"],[\"```\\n\\n```\\n{'bleu': 57.334066271545865}\\n```\\n\\nAt this stage, you can use the inference widget on the Mo...\"],[\"```\\n\\nApart from the usual hyperparameters (like learning rate, number of epochs, batch size, and som...\"],[\"```\\n\\nBefore training, we'll first look at the score our model gets, to double-check that we're not m...\"],[\"```\\n\\nThat's a nearly 14-point improvement, which is great.\\n\\nFinally, we use the `push_to_hub()` meth...\"],[\"```\\n\\nAt this stage, you can use the inference widget on the Model Hub to test your model and share i...\"],[\"```\\n\\nThen we will need an optimizer:\\n\\n```py\\nfrom transformers import AdamW\\n\\noptimizer = AdamW(model....\"],[\"```\\n\\nLastly, to push our model to the Hub, we will need to create a `Repository` object in a working...\"],[\"```\\n\\nWe can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. ...\"],[\"```\\n\\nThe training loop looks a lot like the ones in [section 2](\\u002fcourse\\u002fchapter7\\u002f2) and [Chapter 3](...\"],[\"# Necessary to pad predictions and labels for being gathered\\n        generated_tokens = accelerator....\"],[\"```\\n\\n```python out\\nepoch 0, BLEU score: 53.47\\nepoch 1, BLEU score: 54.24\\nepoch 2, BLEU score: 54.44\\n...\"],[\"```\\n\\nAnother great example of domain adaptation!\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Your turn!** What does the model retur...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={6}\\n    classNames=\\\"absolute z-10 ri...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={5}\\n    classNames=\\\"absolute z-10 ri...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Question answering[[question-answering]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCours...\"],[\"\\u003cYoutube id=\\\"ajPx5LwJD-I\\\"\\u002f\\u003e\\n\\nWe will fine-tune a BERT model on the [SQuAD dataset](https:\\u002f\\u002frajpurkar...\"],[\"This is actually showcasing the model that was trained and uploaded to the Hub using the code shown ...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Preparing the data[[preparing-the-data]]\\n\\nThe dataset that is used the most as an academi...\"],[\"```\\n\\nWe can then have a look at this object to learn more about the SQuAD dataset:\\n\\n```py\\nraw_datase...\"],[\"```\\n\\nThe `context` and `question` fields are very straightforward to use. The `answers` field is a b...\"],[\"```\\n\\nWe won't dive into the evaluation script as it will all be wrapped up by a 🤗 Datasets metric fo...\"],[\"```\\n\\nAs mentioned previously, we'll be fine-tuning a BERT model, but you can use any other model typ...\"],[\"```\\n\\nThe labels will then be the index of the tokens starting and ending the answer, and the model w...\"],[\"```\\n\\n```python out\\n'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [S...\"],[\"```\\n\\nAs we can see, our example has been in split into four inputs, each of them containing the ques...\"],[\"```\\n\\nAs we can see, we get back the usual input IDs, token type IDs, and attention mask, as well as ...\"],[\"```\\n\\nAs we can see, the first three examples (at indices 2, 3, and 4 in the training set) each gave ...\"],[\"```py\\nanswers = raw_datasets[\\\"train\\\"][2:6][\\\"answers\\\"]\\nstart_positions = []\\nend_positions = []\\n\\nfor i...\"],[\"```\\n\\n```python out\\n([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],\\n [85, 53, 21,...\"],[\"```\\n\\n```python out\\n'Theoretical answer: a Marian place of prayer and reflection, decoded example: [C...\"],[\"```\\n\\nIndeed, we don't see the answer inside the context.\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Your turn!** When using the XL...\"],[\"# Find the start and end of the context\\n        idx = 0\\n        while sequence_ids[idx] != 1:\\n      ...\"],[\"```\\n\\nNote that we defined two constants to determine the maximum length used as well as the length o...\"],[\"```\\n\\nAs we can see, the preprocessing added roughly 1,000 features. Our training set is now ready to...\"],[\"sequence_ids = inputs.sequence_ids(i)\\n        offset = inputs[\\\"offset_mapping\\\"][i]\\n        inputs[\\\"o...\"],[\"```\\n\\nWe can apply this function on the whole validation dataset like before:\\n\\n```py\\nvalidation_datas...\"],[\"```\\n\\nIn this case we've only added a couple of hundred samples, so it appears the contexts in the va...\"],[\"{:else}\\n\\n\\u003cYoutube id=\\\"VN67ZpN33Ss\\\"\\u002f\\u003e\\n\\n{\\u002fif}\\n\\nThe model will output logits for the start and end posi...\"],[\"```python\\nsmall_eval_set = raw_datasets[\\\"validation\\\"].select(range(100))\\ntrained_checkpoint = \\\"disti...\"],[\"```\\n\\nNow that the preprocessing is done, we change the tokenizer back to the one we originally picke...\"],[\"```\\n\\n{:else}\\n\\n```python\\nimport tensorflow as tf\\nfrom transformers import TFAutoModelForQuestionAnswe...\"],[\"```\\n\\nWith this in hand, we can really get to work by looping through all the examples and, for each ...\"],[\"answers.append(\\n                    {\\n                        \\\"text\\\": context[offsets[start_index][0...\"],[\"```\\n\\nThe final format of the predicted answers is the one that will be expected by the metric we wil...\"],[\"```\\n\\nAgain, that's rather good considering that according to [its paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1910....\"],[\"predicted_answers = []\\n    for example in tqdm(examples):\\n        example_id = example[\\\"id\\\"]\\n       ...\"],[\"```\\n\\nWe can check it works on our predictions:\\n\\n```python\\ncompute_metrics(start_logits, end_logits, ...\"],[\"```\\n\\nIf you aren't working in a notebook, just type the following line in your terminal:\\n\\n```bash\\nhu...\"],[\"```\\n\\nWe've seen most of these before: we set some hyperparameters (like the learning rate, the numbe...\"],[\"```\\n\\nNext, we set up our training hyperparameters and compile our model:\\n\\n```python\\nfrom transformer...\"],[\"```\\n\\nFinally, we're ready to train with `model.fit()`. We use a `PushToHubCallback` to upload the mo...\"],[\"```\\n\\n{\\u002fif}\\n\\nNote that while the training happens, each time the model is saved (here, every epoch) i...\"],[\"```\\n\\n{\\u002fif}\\n\\n```python out\\n{'exact_match': 81.18259224219489, 'f1': 88.67381321905516}\\n```\\n\\nGreat! As...\"],[\"```\\n\\nThe `Trainer` also drafts a model card with all the evaluation results and uploads it.\\n\\n{\\u002fif}\\n\\n...\"],[\"```\\n\\nNext we reinstantiate our model, to make sure we're not continuing the fine-tuning from before ...\"],[\"```\\n\\nTo push our model to the Hub, we will need to create a `Repository` object in a working folder....\"],[\"```\\n\\nWe can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. ...\"],[\"optimizer.step()\\n        lr_scheduler.step()\\n        optimizer.zero_grad()\\n        progress_bar.upda...\"],[\"```\\n\\nIn case this is the first time you're seeing a model saved with 🤗 Accelerate, let's take a mome...\"],[\"```\\n\\nThe first line is self-explanatory: it tells all the processes to wait until everyone is at tha...\"],[\"```\\n\\n```python out\\n{'score': 0.9979003071784973,\\n 'start': 78,\\n 'end': 105,\\n 'answer': 'Jax, PyTorch...\"],[\"ow to write a good issue on GitHub? GitHub is the main place for the Hugging Face open source librar...\"],[\"it by executing this command in a notebook (remove the exclamation mark to execute it in a terminal)...\"],[\"it calls AutoTokenizer.from_pretrained. Using the debugger, we find the values passed to that method...\"],[\"not just this one, and that it disappears when we use use_fast=False inside our tokenizer call. The ...\"],[\"is to properly name your issue. Don't pick a title that is too vague! Then you have to fill your env...\"],[\"with tokenizers, we pick the maintainer associated with them. There is no point tagging more than 3 ...\"],[\"issue at hand. With all of this, you should expect an answer to your issue pretty fast, and hopefull...\"],[\"The Hugging Face Hub[[the-hugging-face-hub]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={4}\\n    classNames=\\\"...\"],[\"The video below shows how to navigate the Hub.\\n\\n\\u003cYoutube id=\\\"XvSGPZFEjDY\\\"\\u002f\\u003e\\n\\nHaving a huggingface.co...\"],[\"n this video, we will study together \\\"the Unigram Language Model subword tokenization algorithm\\\".\\n\\nT...\"],[\"Before going further in the explanation of the training algorithm, I need to explain what is an Unig...\"],[\"previous word. This \\\"assumption\\\" allows us to write that the probability of a text is equal to the p...\"],[\"We are now ready to return to our explanation of the training algorithm. Let's say that we have as a...\"],[\"possible strict substrings that's what we'll do here. We could also have used the BPE algorithm with...\"],[\"characters to be able to tokenize any word. Let's go for it! The probability of a token is simply es...\"],[\"of our text \\\"Hug\\\" will be the one with the highest probability of occurrence according to our Unigra...\"],[\"need to tokenize as we just did all the remaining words in the corpus. The loss is then the sum over...\"],[\"for example the token 'ug'. We notice that the tokenization for \\\"hug\\\" with the letter h and the tupl...\"],[\"continue the calculation, we would notice that we could remove any token without it impacting the lo...\"],[\"end, we obtain by removing the token composed of the letters \\\"h\\\" and \\\"u\\\" from the vocabulary a loss ...\"],[\"could remove p % of the tokens by iteration. The second token that could be removed at this iteratio...\"],[\"And that's it! I hope that this example has allowed you to better understand the Unigram tokenizatio...\"],[\"Creating your own dataset[[creating-your-own-dataset]]\\n\\n\\u003cCourseFloatingBanner chapter={5}\\n  classNam...\"],[\"## Getting the data[[getting-the-data]]\\n\\nYou can find all the issues in 🤗 Datasets by navigating to ...\"],[\"```\\n\\nOnce the library is installed, you can make GET requests to the `Issues` endpoint by invoking t...\"],[\"```python out\\n[{'url': 'https:\\u002f\\u002fapi.github.com\\u002frepos\\u002fhuggingface\\u002fdatasets\\u002fissues\\u002f2792',\\n  'repositor...\"],[\"'html_url': 'https:\\u002f\\u002fgithub.com\\u002fbhavitvyamalik',\\n   'followers_url': 'https:\\u002f\\u002fapi.github.com\\u002fusers\\u002fb...\"],[\"'locked': False,\\n  'assignee': None,\\n  'assignees': [],\\n  'milestone': None,\\n  'comments': 1,\\n  'cre...\"],[\"```\\n\\nWhoa, that's a lot of information! We can see useful fields like `title`, `body`, and `number` ...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\n⚠️ Do not share a notebook with your `GITHUB_TOKEN` pasted in it. We reco...\"],[\"if len(batch) \\u003e rate_limit and len(all_issues) \\u003c num_issues:\\n            all_issues.extend(batch)\\n  ...\"],[\"```\\n\\nNow when we call `fetch_issues()` it will download all the issues in batches to avoid exceeding...\"],[\"```\\n\\nGreat, we've created our first dataset from scratch! But why are there several thousand issues ...\"],[\"```\\n\\n```python out\\n\\u003e\\u003e URL: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets\\u002fpull\\u002f850\\n\\u003e\\u003e Pull request: {'url':...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Try it out!** Calculate the average time it takes to close issues in 🤗 Datasets. Yo...\"],[\"```py\\nissue_number = 2792\\nurl = f\\\"https:\\u002f\\u002fapi.github.com\\u002frepos\\u002fhuggingface\\u002fdatasets\\u002fissues\\u002f{issue_nu...\"],[\"```python out\\n[{'url': 'https:\\u002f\\u002fapi.github.com\\u002frepos\\u002fhuggingface\\u002fdatasets\\u002fissues\\u002fcomments\\u002f897594128'...\"],[\"'gists_url': 'https:\\u002f\\u002fapi.github.com\\u002fusers\\u002fbhavitvyamalik\\u002fgists{\\u002fgist_id}',\\n   'starred_url': 'https...\"],[\"'updated_at': '2021-08-12T12:31:17Z',\\n  'author_association': 'CONTRIBUTOR',\\n  'body': \\\"@albertvilla...\"],[\"```\\n\\nWe can see that the comment is stored in the `body` field, so let's write a simple function tha...\"],[\"```\\n\\nThis looks good, so let's use `Dataset.map()` to add a new `comments` column to each issue in o...\"],[\"```\\n\\n```python out\\nDataset({\\n    features: ['url', 'repository_url', 'labels_url', 'comments_url', '...\"],[\"```\\n\\nCool, we've pushed our dataset to the Hub and it's available for others to use! There's just on...\"],[\"2. Read the [🤗 Datasets guide](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets\\u002fblob\\u002fmaster\\u002ftemplates\\u002fREADME_...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n\\u003c!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-...\"],[\"### 3. Which of the following is an example of subword tokenization?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\tt...\"],[\"{#if fw === 'pt'}\\n### 5. What is an AutoModel?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"A model that aut...\"],[\"{\\u002fif}\\n\\n### 6. What are the techniques to be aware of when batching sequences of different lengths to...\"],[\"### 8. What method is most of the tokenizer API centered around?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext:...\"],[\"```\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"A list of strings, each string being a token\\\",\\n\\t\\t\\texplain: ...\"],[\"```\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"No, it seems correct.\\\",\\n\\t\\t\\texplain: \\\"Unfortunately, couplin...\"],[\"```\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"No, it seems correct.\\\",\\n\\t\\t\\texplain: \\\"Unfortunately, couplin...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Processing the data[[processing-the-data]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCou...\"],[\"```python\\nimport torch\\nfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassifica...\"],[\"```\\n{:else}\\nContinuing with the example from the [previous chapter](\\u002fcourse\\u002fchapter2), here is how w...\"],[\"```\\n{\\u002fif}\\n\\nOf course, just training the model on two sentences is not going to yield very good resul...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['sentence1', 'sentence2', '...\"],[\"```\\n\\nWe can see the labels are already integers, so we won't have to do any preprocessing there. To ...\"],[\"```\\n\\nBehind the scenes, `label` is of type `ClassLabel`, and the mapping of integers to label name i...\"],[\"```\\n\\n```python out\\n{ \\n  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996...\"],[\"```\\n\\nSo we see the model expects the inputs to be of the form `[CLS] sentence1 [SEP] sentence2 [SEP]...\"],[\"```\\n\\nAs you can see, the parts of the input corresponding to `[CLS] sentence1 [SEP]` all have a toke...\"],[\"```\\n\\nThis works well, but it has the disadvantage of returning a dictionary (with our keys, `input_i...\"],[\"```\\n\\nThis function takes a dictionary (like the items of our dataset) and returns a new dictionary w...\"],[\"```\\n\\nThe way the 🤗 Datasets library applies this processing is by adding new fields to the datasets,...\"],[\"```\\n\\nYou can even use multiprocessing when applying your preprocessing function with `map()` by pass...\"],[\"{:else}\\n\\nThe function that is responsible for putting together samples inside a batch is called a *c...\"],[\"```\\n{:else}\\n```py\\nfrom transformers import DataCollatorWithPadding\\n\\ndata_collator = DataCollatorWith...\"],[\"```\\n\\n{:else}\\n\\n```python out\\n{'attention_mask': torch.Size([8, 67]),\\n 'input_ids': torch.Size([8, 67]...\"],[\"```\\n\\nLooking good! Now that we've gone from raw text to batches our model can deal with, we're ready...\"],[\"```\\n\\nAnd that's it! We can take those datasets forward into the next lecture, where training will be...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Models[[models]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCourseFloatingBanner chapter=...\"],[\"The `AutoModel` class and all of its relatives are actually simple wrappers over the wide variety of...\"],[\"```\\n{:else}\\n```py\\nfrom transformers import BertConfig, TFBertModel\\n\\n# Building the config\\nconfig = B...\"],[\"```\\n{\\u002fif}\\n\\nThe model can be used in this state, but it will output gibberish; it needs to be trained...\"],[\"```\\n\\nAs you saw earlier, we could replace `TFBertModel` with the equivalent `TFAutoModel` class. We'...\"],[\"```\\n\\nThis saves two files to your disk:\\n\\n{#if fw === 'pt'}\\n```\\nls directory_on_my_computer\\n\\nconfig.j...\"],[\"```\\n\\nThe tokenizer converts these to vocabulary indices which are typically called *input IDs*. Each...\"],[\"ow to instantiate a Transformers model? In this video we will look at how we can create and use a mo...\"],[\"similarly for GPT-2 or BART. Behind the scenes, this API can take the name of a checkpoint on the Hu...\"],[\"type of the model (BERT, GPT-2 or BART for instance). Once it has the proper configuration class, it...\"],[\"To easily load the configuration of a model from any checkpoint or a folder containing the configura...\"],[\"instance the BERT model associated with the bert-base-cased checkpoint has 12 layers, a hidden size ...\"],[\"Saving a model once it's trained or fine-tuned is very easy: we just have to use the save_pretrained...\"],[\"Introduction to Gradio[[introduction-to-gradio]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={9}\\n    classNam...\"],[\"* An extractive **question answering** model that takes in a context paragraph and a quest and outpu...\"],[\"This chapter is broken down into sections which include both _concepts_ and _applications_. After yo...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Handling multiple sequences[[handling-multiple-sequences]]\\n\\n{#if fw...\"],[\"In the previous section, we explored the simplest of use cases: doing inference on a single sequence...\"],[\"```\\n\\n```python out\\nIndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1...\"],[\"```\\n{:else}\\n```py\\ntokenized_inputs = tokenizer(sequence, return_tensors=\\\"tf\\\")\\nprint(tokenized_inputs...\"],[\"```\\n{:else}\\n```py\\nimport tensorflow as tf\\nfrom transformers import AutoTokenizer, TFAutoModelForSequ...\"],[\"```\\nbatched_ids = [ids, ids]\\n```\\n\\nThis is a batch of two identical sequences!\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Try it ou...\"],[\"```\\n\\nThe padding token ID can be found in `tokenizer.pad_token_id`. Let's use it and send our two se...\"],[\"```\\n\\n```py out\\ntf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)\\ntf.Tensor([[ 0.5803...\"],[\"```\\n\\n```python out\\ntensor([[ 1.5694, -1.3895],\\n        [ 0.5803, -0.4125]], grad_fn=\\u003cAddmmBackward\\u003e)...\"],[\"```\\n{\\u002fif}\\n\\nNow we get the same logits for the second sentence in the batch.\\n\\nNotice how the last val...\"],[\"Advanced Interface features[[advanced-interface-features]]\\n\\n\\u003cCourseFloatingBanner chapter={9}\\n  clas...\"],[\"See the chatbot example below:\\n\\n```py\\nimport random\\n\\nimport gradio as gr\\n\\n\\ndef chat(message, history...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-Chatbot-Demo.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"350\\\" title=\\\"Gr...\"],[\"# Download human-readable labels for ImageNet.\\nresponse = requests.get(\\\"https:\\u002f\\u002fgit.io\\u002fJJkYN\\\")\\nlabel...\"],[\"```\\n\\nTest the interpretation function by submitting an input then clicking Interpret under the outpu...\"],[\"n this video we will see together what is the purpose of training a tokenizer, what are the key step...\"],[\"legal, or uses a different style, a language from another century for instance. For example, if I ta...\"],[\"that either a word is divided into many sub tokens or that the tokenizer does not know one of the un...\"],[\"part of the text. In this other example, we can see that the tokenizer replaces words containing cha...\"],[\"language model. This training consists in learning rules to divide the text into tokens and the way ...\"],[\"completely design your tokenizer but it requires more experience and attention. Once the architectur...\"],[\"tokenizer trained on it - to convince you of this we will see at the end the difference produced on ...\"],[\"open source libraries on Github. It's good timing, this dataset is known by the datasets library and...\"],[\"it on our new corpus. An argument that is common to most of the tokenization algorithms used at the ...\"],[\"we see that all spaces are isolated and the method name \\\"randn\\\" relatively common in python code is ...\"],[\"he Trainer API. The Transformers library provides a Trainer API that allows you to easily fine-tune ...\"],[\"tokenizer or a given data collator. We will try this API on the MRPC dataset, since it's relatively ...\"],[\"the model signature. The last steps before creating the Trainer are to define our model and some tra...\"],[\"display a progress bar and after a few minutes (if you are running on a GPU) you should have the tra...\"],[\"contains the model predictions), label_ids (which contains the labels if your dataset had them) and ...\"],[\"it can be loaded as easily as our dataset with the load_metric function, and it returns the evaluati...\"],[\"our TrainingArguments, we tell the Trainer to evaluate at the end of every epoch. Launching a traini...\"],[\"he tokenizer pipeline. In this video, we'll look at how a tokenizer converts raw text to numbers tha...\"],[\"happen in this order, but viewing it like this is better for understanding what happens. The first s...\"],[\"algorithms\\\" videos linked below for more information! The ## prefix we see in front of ize is the co...\"],[\"the vocabulary of the tokenizer.  This is why we need to download a file when we instantiate a token...\"],[\"had a number at the beginning and at the end that are missing, those are the special tokens. The spe...\"],[\"depending on which tokenizer you are using. The BERT tokenizer uses [CLS] and [SEP] but the roberta ...\"],[\"Sharing demos with others[[sharing-demos-with-others]]\\n\\n\\u003cCourseFloatingBanner chapter={9}\\n  classNam...\"],[\"To add additional content to your demo, the `Interface` class supports some optional parameters:\\n   ...\"],[\"article = \\\"Check out [the original Rick and Morty Bot](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fkingabzpro\\u002fRick...\"],[\"```\\n\\nUsing the options above, we end up with a more complete interface. Try the interface below:\\n\\n\\u003ci...\"],[\"```\\n\\nThis generates a public, shareable link that you can send to anybody! When you send this link, ...\"],[\"\\u003cYoutube id=\\\"LS9Y2wDVI0k\\\" \\u002f\\u003e\\n\\n## ✏️ Let's apply it![[lets-apply-it]]\\n\\nUsing what we just learned in ...\"],[\"LABELS = Path(\\\"class_names.txt\\\").read_text().splitlines()\\n\\nmodel = nn.Sequential(\\n    nn.Conv2d(1, 3...\"],[\"```\\n\\nNow that we have a `predict()` function. The next step is to define and launch our gradio inter...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fast tokenizers in the QA pipeline[[fast-tokenizers-in-the-qa-pipel...\"],[\"{#if fw === 'pt'}\\n\\n\\u003cYoutube id=\\\"_wxyB3j3mk4\\\"\\u002f\\u003e\\n\\n{:else}\\n\\n\\u003cYoutube id=\\\"b3u8RzBCX9Y\\\"\\u002f\\u003e\\n\\n{\\u002fif}\\n\\n## Usin...\"],[\"```\\n\\n```python out\\n{'score': 0.97773,\\n 'start': 78,\\n 'end': 105,\\n 'answer': 'Jax, PyTorch and Tensor...\"],[\"```\\n\\nUnlike the other pipelines, which can't truncate and split texts that are longer than the maxim...\"],[\"🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and Tensor...\"],[\"```\\n\\n```python out\\n{'score': 0.97149,\\n 'start': 1892,\\n 'end': 1919,\\n 'answer': 'Jax, PyTorch and Ten...\"],[\"```\\n\\n{\\u002fif}\\n\\nNote that we tokenize the question and the context as a pair, with the question first.\\n\\n...\"],[\"```\\n\\n{:else}\\n\\n```python out\\n(1, 66) (1, 66)\\n```\\n\\n{\\u002fif}\\n\\nTo convert those logits into probabilities, ...\"],[\"```\\n\\n{\\u002fif}\\n\\nNow that we have properly masked the logits corresponding to positions we don't want to ...\"],[\"```\\n\\n{\\u002fif}\\n\\nAt this stage, we could take the argmax of the start and end probabilities -- but we mig...\"],[\"```\\n\\n{:else}\\n\\nThen we'll mask the values where `start_index \\u003e end_index` by setting them to `0` (the...\"],[\"```\\n\\nNow we just have to format everything to get our result:\\n\\n```py\\nresult = {\\n    \\\"answer\\\": answer...\"],[\"```\\n\\n```python out\\n\\\"\\\"\\\"\\n[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Tran...\"],[\"```\\n\\nThis means the model will have a hard time picking the correct answer. To fix this, the `questi...\"],[\"```\\n\\n```python out\\ndict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])\\n```\\n\\nAs ...\"],[\"```\\n\\ngets us:\\n\\n```python out\\n[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\\n```\\n\\nwhich means the first sentence i...\"],[\"```\\n\\n```python out\\ntorch.Size([2, 384])\\n```\\n\\n{:else}\\n\\n```py\\n_ = inputs.pop(\\\"overflow_to_sample_mappi...\"],[\"```\\n\\n{:else}\\n\\n```py\\nsequence_ids = inputs.sequence_ids()\\n# Mask everything apart from the tokens of ...\"],[\"```\\n\\n{\\u002fif}\\n\\nThe next step is similar to what we did for the small context, but we repeat it for each...\"],[\"```\\n\\n{\\u002fif}\\n\\n```python out\\n[(0, 18, 0.33867), (173, 184, 0.97149)]\\n```\\n\\nThose two candidates correspo...\"],[\"```\\n\\nIf we ignore the first result, we get the same result as our pipeline for this long context -- ...\"],[\"he tokenization pipeline involves several steps that convert raw text into numbers. In this video, w...\"],[\"- but the apostrophe is not a division criterion for example. We also notice that spaces have been r...\"],[\"examples, we could observe the two main types of operations brought by the pre-tokenization: some ch...\"],[\"method. This operation defines the largest tokens that can be produced by the tokenization or in oth...\"],[\"et's see how to preprocess a dataset for summarization. This is the task of well summarizing a long ...\"],[\"For once, our labels are not integers corresponding to some classes, but plain text. We will thus ne...\"],[\"usually much shorter than the documents, you should definitely pick different maximum lengths for th...\"],[\"and same for the targets. We pad the inputs with the pad token and the targets with the -100 index, ...\"],[\"he post-processing step in a question answering task. When doing question answering, the processing ...\"],[\"THE answer for a given example. For the processing step, you should refer to the video linked below....\"],[\"back feature to the examples that they originated from. If you don't want to compute the validation ...\"],[\"you want to evaluate. With the to_tf_dataset method, we can just sent our processed dataset to model...\"],[\"and end logits and be done, but if our model predicts something impossible, like tokens in the quest...\"],[\"answers or answer that are too long. As we saw in the preprocessing, the labels (0, 0) correspond to...\"],[\"logit score in all the features the example generated. Now you know how to get answers from your mod...\"],[\"A full training[[a-full-training]]\\n\\n\\u003cCourseFloatingBanner chapter={3}\\n  classNames=\\\"absolute z-10 ri...\"],[\"```\\n\\n### Prepare for training[[prepare-for-training]]\\n\\nBefore actually writing our training loop, we...\"],[\"```\\n\\nTo quickly check there is no mistake in the data processing, we can inspect a batch like this:\\n...\"],[\"```\\n\\n```python out\\ntensor(0.5441, grad_fn=\\u003cNllLossBackward\\u003e) torch.Size([8, 2])\\n```\\n\\nAll 🤗 Transform...\"],[\"```\\n\\n```python out\\n1377\\n```\\n\\n### The training loop[[the-training-loop]]\\n\\nOne last thing: we will wan...\"],[\"```\\n\\nYou can see that the core of the training loop looks a lot like the one in the introduction. We...\"],[\"```\\n\\nAgain, your results will be slightly different because of the randomness in the model head init...\"],[\"```\\n\\nAnd here are the changes:\\n\\n```diff\\n+ from accelerate import Accelerator\\n  from transformers imp...\"],[\"```\\n\\nThe first line to add is the import line. The second line instantiates an `Accelerator` object ...\"],[\"train_dl, eval_dl, model, optimizer = accelerator.prepare(\\n    train_dataloader, eval_dataloader, mo...\"],[\"```\\n\\nPutting this in a `train.py` script will make that script runnable on any kind of distributed s...\"],[\"Transformers, what can they do?[[transformers-what-can-they-do]]\\n\\n\\u003cCourseFloatingBanner chapter={1}\\n...\"],[\"The [🤗 Transformers library](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers) provides the functionality...\"],[\"```\\n\\n```python out\\n[{'label': 'POSITIVE', 'score': 0.9598047137260437}]\\n```\\n\\nWe can even pass severa...\"],[\"```\\n\\nBy default, this pipeline selects a particular pretrained model that has been fine-tuned for se...\"],[\"```\\n\\n```python out\\n{'sequence': 'This is a course about the Transformers library',\\n 'labels': ['educ...\"],[\"```\\n\\nYou can control how many different sequences are generated with the argument `num_return_sequen...\"],[\"```\\n\\nYou can refine your search for a model by clicking on the language tags, and pick a model that ...\"],[\"```\\n\\nThe `top_k` argument controls how many possibilities you want to be displayed. Note that here t...\"],[\"```\\n\\nHere the model correctly identified that Sylvain is a person (PER), Hugging Face an organizatio...\"],[\"```\\n\\n```python out\\n{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}\\n`...\"],[\"```\\n\\nLike with text generation, you can specify a `max_length` or a `min_length` for the result.\\n\\n\\n#...\"],[\"sing the Python debugger in a notebook. In this video, we'll learn how to use the Python debugger in...\"],[\"then we get the following error. We use PyTorch here but you will get the same error with TensorFlow...\"],[\"by typing %debug in any cell. When executing that cell, you go to the very bottom of the traceback w...\"],[\"can see the value of return_tensors or batch_outputs to try to understand what triggered the error. ...\"],[\"with them! This is because the pad method only takes care of the tokenizer outptus: input IDs, atten...\"],[\"ow to instantiate a Transformers model? In this video we will look at how we can create and use a mo...\"],[\"and similarly for GPT-2 or BART. Behind the scenes, this API can take the name of a checkpoint on th...\"],[\"the type of the model (BERT, GPT-2 or BART for instance). Once it has the proper configuration class...\"],[\"model. To easily load the configuration of a model from any checkpoint or a folder containing the co...\"],[\"model architecture. For instance the BERT model associated with the bert-base-cased checkpoint has 1...\"],[\"layers instead of 12. Saving a model once it's trained or fine-tuned is very easy: we just have to u...\"],[\"n this video, we're going to understand how to manage a model repository on the HuggingFace model hu...\"],[\"The model name is the model identifier that will then be used to identify your model on your chosen ...\"],[\"users won't know it exists and will not be able to use it. Let's create a dummy model to play with. ...\"],[\"you're unaware of what is a git repository, you can think of it as a folder containing files. If you...\"],[\"first start by adding files to the repository. Files can be added through the web interface thanks t...\"],[\"model files. First, I make sure that both git and git-lfs are correctly installed on my system. Link...\"],[\"used for sentiment analysis. I'll simply copy over the contents to this folder. This includes the mo...\"],[\"seen two ways of adding files to a repository, a third way is explored in the video about the push t...\"],[\"model, ensuring reusability by fellow community members and reproducibility of results, and providin...\"],[\"be used in downstream libraries simply by specifying your model identifier....\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Behind the pipeline[[behind-the-pipeline]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCou...\"],[\"Let's start with a complete example, taking a look at what happened behind the scenes when we execut...\"],[\"```\\n\\nand obtained:\\n\\n```python out\\n[{'label': 'POSITIVE', 'score': 0.9598047137260437},\\n {'label': 'N...\"],[\"```\\n\\nAs we saw in [Chapter 1](\\u002fcourse\\u002fchapter1), this pipeline groups together three steps: preproce...\"],[\"- Splitting the input into words, subwords, or symbols (like punctuation) that are called *tokens*\\n-...\"],[\"```\\n\\nOnce we have the tokenizer, we can directly pass our sentences to it and we'll get back a dicti...\"],[\"```\\n{\\u002fif}\\n\\nDon't worry about padding and truncation just yet; we'll explain those later. The main th...\"],[\"```\\n{:else}\\n\\nHere's what the results look like as TensorFlow tensors:\\n\\n```python out\\n{\\n    'input_id...\"],[\"```\\n{:else}\\nWe can download our pretrained model the same way we did with our tokenizer. 🤗 Transform...\"],[\"```\\n\\n```python out\\ntorch.Size([2, 16, 768])\\n```\\n{:else}\\n```py\\noutputs = model(inputs)\\nprint(outputs....\"],[\"```\\n{\\u002fif}\\n\\nNote that the outputs of 🤗 Transformers models behave like `namedtuple`s or dictionaries....\"],[\"There are many different architectures available in 🤗 Transformers, with each one designed around ta...\"],[\"```\\n{:else}\\nFor our example, we will need a model with a sequence classification head (to be able to...\"],[\"```\\n{\\u002fif}\\n\\nOur model predicted `[-1.5607, 1.6123]` for the first sentence and `[ 4.1692, -3.3464]` f...\"],[\"```\\n{\\u002fif}\\n\\nNow we can see that the model predicted `[0.0402, 0.9598]` for the first sentence and `[0...\"],[\"🤗 Datasets, check![[datasets-check]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={5}\\n    classNames=\\\"absolute...\"],[\"n this video we will see together what is the normalizer component that we find at the beginning of ...\"],[\"FNet model has transformed the letters with font variants or circled into their basic version and ha...\"],[\"it is very easy to observe the normalization chosen for the currently loaded tokenizer. Indeed, each...\"],[\"to tokenize a text. For example, if we hadn't included the albert normalizer we would have had a lot...\"],[\"must then decode this sequence of bytes into a sequence of \\\"code points\\\". In our example the 2 bytes...\"],[\"Latin Small Letter Cand the combining cedilla. But it's annoying because what appears to us to be a ...\"],[\"sentence\\\". However, you must be aware that some normalizations can be very harmful if they are not a...\"],[\"but I advise you to take the time to select them so that they do not make you lose important informa...\"],[\"Building a tokenizer, block by block[[building-a-tokenizer-block-by-block]]\\n\\n\\u003cCourseFloatingBanner c...\"],[\"The 🤗 Tokenizers library has been built to provide several options for each of those steps, which yo...\"],[\"\\u003cYoutube id=\\\"MR8tZm5ViWU\\\"\\u002f\\u003e\\n\\nMore precisely, the library is built around a central `Tokenizer` class...\"],[\"You can find the whole list of building blocks [here](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftokenizers\\u002fpython\\u002f...\"],[\"```\\n\\nThe function `get_training_corpus()` is a generator that will yield batches of 1,000 texts, whi...\"],[\"```\\n\\nWe have to specify the `unk_token` so the model knows what to return when it encounters charact...\"],[\"```\\n\\n```python out\\nhello how are u?\\n```\\n\\n\\u003cTip\\u003e\\n\\n**To go further** If you test the two versions of th...\"],[\"```\\n\\nIf you only want to split on whitespace, you should use the `WhitespaceSplit` pre-tokenizer ins...\"],[\"```\\n\\nThe next step in the tokenization pipeline is running the inputs through the model. We already ...\"],[\"```\\n\\nThe `encoding` obtained is an `Encoding`, which contains all the necessary outputs of the token...\"],[\"```\\n\\nNote that we need to pass along the IDs of the special tokens, so the tokenizer can properly co...\"],[\"```\\n\\nGreat! We can save our tokenizer in a single JSON file like this:\\n\\n```python\\ntokenizer.save(\\\"to...\"],[\"```\\n\\nYou can then use this tokenizer like any other 🤗 Transformers tokenizer. You can save it with t...\"],[\"```\\n\\n```python out\\n[('Let', (0, 3)), (\\\"'s\\\", (3, 5)), ('Ġtest', (5, 10)), ('Ġpre', (10, 14)), ('-', (...\"],[\"```\\n\\nWe apply the byte-level post-processing for the GPT-2 tokenizer as follows:\\n\\n```python\\ntokenize...\"],[\"```\\n\\nor:\\n\\n```python\\nfrom transformers import GPT2TokenizerFast\\n\\nwrapped_tokenizer = GPT2TokenizerFas...\"],[\"```\\n\\n```python out\\n[(\\\"▁Let's\\\", (0, 5)), ('▁test', (5, 10)), ('▁the', (10, 14)), ('▁pre-tokenizer!', ...\"],[\"```\\n\\n```python out\\n['▁Let', \\\"'\\\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.']\\n```\\n\\nA peculiarit...\"],[\"```\\n\\n```python out\\n['▁Let', \\\"'\\\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.', '.', '.', '\\u003csep\\u003e'...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n\\u003c!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-...\"],[\"### 2. What part of the preprocessing for token classification differs from the other preprocessing ...\"],[\"### 4. What does \\\"domain adaptation\\\" mean?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It's when we run a m...\"],[\"### 6. Which of these tasks can be seen as a sequence-to-sequence problem?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t...\"],[\"{#if fw === 'pt'}\\n\\n### 8. Why is there a specific subclass of `Trainer` for sequence-to-sequence pro...\"],[\"{\\u002fif}\\n\\n### 10. When should you pretrain a new model?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"When there...\"],[\"### 12. What are the main challenges when preprocessing data for a question answering task?\\n\\n\\u003cQuesti...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n\\u003c!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-...\"],[\"### 3. What can you do using the Hugging Face Hub web interface? \\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext...\"],[\"{#if fw === 'pt'}\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"A tokenizer\\\",\\n\\t\\t\\texplain: \\\"Correct! All tokeni...\"],[\"]}\\n\\u002f\\u003e\\n{:else}\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"A tokenizer\\\",\\n\\t\\t\\texplain: \\\"Correct! All tokenizers...\"],[\"### 6. What is the first step when using the `push_to_hub()` method or the CLI tools?\\n\\n\\u003cQuestion\\n\\tch...\"],[\"### 8. Which git operations can you do with the `Repository` class?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\tte...\"],[\"ext embeddings and semantic search. In this video we’ll explore how Transformer models represent tex...\"],[\"Reading the text, we can see that walking the dog seems to be most similar to walking the cat, but l...\"],[\"this example, our embedding vectors live in 3D and we can see that the orange and grey vectors are c...\"],[\"per sentence, and each vector has 384 dimensions. But what we really want is a single embedding vect...\"],[\"the attention mask being used here. This now gives us one 384 dimensional vector per sentence which ...\"],[\"similarity between a question and a corpus of documents. For example, suppose we embed every post in...\"],[\"embedding logic as before. This gives us a new column called \\\"embeddings\\\" that stores the embedding ...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"### 3. Suppose you try to run the following code, which throws an error:\\n\\n```py\\nfrom transformers im...\"],[\"```\\n\\nWhich of the following might be a good choice for the title of a forum topic to ask for help?\\n\\n...\"],[\"\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"The optimization step where we compute gradients and perform bac...\"],[\"### 6. What is the best way to get an issue on GitHub fixed?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"Po...\"],[\"### 8. Why is it a good idea to include details on your compute environment with `transformers-cli e...\"],[\"et's study the transformer architecture. This video is the introductory video to the encoders, decod...\"],[\"leverage only some parts of it, according to what we're trying to do. We won't dive into the specifi...\"],[\"converts this text, these words, into numerical representations. These numerical representations can...\"],[\"It uses a similar mechanism as the encoder, which is the masked self-attention as well. It differs f...\"],[\"passed to the decoder. The decoder uses the encoder's output alongside other inputs, in order to gen...\"],[\"Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\\n\\n\\u003cCourseFloatingBanner chapter={9}\\n  ...\"],[\"- Group together related demos as multiple tabs in one web application\\n- Change the layout of your d...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-flip-text.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"400\\\" title=\\\"Gradi...\"],[\"3. You can assign events to any `Blocks` component. This will run your function when the component i...\"],[\"Here's what you should keep in mind: any components created under a `Column` (this is also the defau...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-flip-text-image.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"450\\\" title=...\"],[\"- `fn`: the function to run\\n- `inputs`: a (list of) component(s) whose values should supplied as the...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-blocks-gpt.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"300\\\" title=\\\"Grad...\"],[\"def text_to_sentiment(text):\\n    return classifier(text)[0][\\\"label\\\"]\\n\\n\\ndemo = gr.Blocks()\\n\\nwith demo...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-blocks-multi-step.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"600\\\" titl...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-blocks-update-component-properties.hf.space\\\" frameBorder=\\\"0\\\" ...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"### 3. Where can you launch a Gradio demo from?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n        {\\n\\t\\t\\ttext: \\\"Standard ...\"],[\"### 6. Which of the following are valid ways of loading a Hugging Face model from Hub or Spaces?\\n\\n\\u003cQ...\"],[\"### 8. Which of the following are components included in the Gradio library?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n...\"],[\"### 10. You can share a public link to a `Blocks` demo and host a `Blocks` demo on Hugging Face spac...\"],[\"i, this is going to be a video about the push_to_hub API for Tensorflow and Keras. So, to get starte...\"],[\"just getting everything ready for training. So we're just going to load a dataset, we're going to to...\"],[\"PushToHubCallback. So a callback in Keras is a function that's called regularly during training. You...\"],[\"means you can resume from that save, so you get this automatic cloud-saving of your model, and you c...\"],[\"are going to be saved to before they're uploaded to the Hub. The second argument is the tokenizer, a...\"],[\"us know extremely urgently. But if you do have access to your own organization then you can use that...\"],[\"a once-off method - it's not called regularly during training. You can just call this manually whene...\"],[\"the way. So I'm going to run both of these cells and then I'm going to cut the video here, just beca...\"],[\"our call to model.push_to_hub() after training. So everything's looking good! So now if we drop over...\"],[\"the Glue CoLA dataset, and CoLA is an acronym for Corpus of Linguistic Acceptability. So what that m...\"],[\"a couple of seconds out of this video here. Okay, we're back! The model loaded and we got an output,...\"],[\"we can get that from the dataset we loaded, from the 'features' attribute it has. And then we can cr...\"],[\"generate new results I'm going to use something slightly different. So let's try an incorrect senten...\"],[\"be unacceptable than acceptable. Presumably if we tried a bit harder with training we could get a mu...\"],[\"and a very low probability of being unacceptable. So you can use this inference API even with the ch...\"],[\"pipeline or you can just load it with, for example, TFAutoModelForSequenceClassification and then fo...\"],[\"you wanna do. So that was a quick overview of how, after your training or during your training, you ...\"],[\"et's take a look at word-based tokenization. Word-based tokenization is the idea of splitting the ra...\"],[\"single number is high as a word contains a lot of contextual and semantic information in a sentence....\"],[\"plural form of the word dog. Another issue with this approach is that there are a lot of different w...\"],[\"mappings requires an enormous number of weights when the vocabulary size is large. If we want our mo...\"],[\"be converted to the out-of-vocabulary word, or the \\\"unknown\\\" word. This can rapidly become an issue:...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Putting it all together[[putting-it-all-together]]\\n\\n{#if fw === 'pt...\"],[\"```py\\nfrom transformers import AutoTokenizer\\n\\ncheckpoint = \\\"distilbert-base-uncased-finetuned-sst-2-...\"],[\"```\\n\\nHere, the `model_inputs` variable contains everything that's necessary for a model to operate w...\"],[\"```\\n\\nThe `tokenizer` object can handle the conversion to specific framework tensors, which can then ...\"],[\"```\\n\\nOne token ID was added at the beginning, and one at the end. Let's decode the two sequences of ...\"],[\"```\\n{:else}\\n```py\\nimport tensorflow as tf\\nfrom transformers import AutoTokenizer, TFAutoModelForSequ...\"],[\"n this video, I'm going to give you a very quick introduction to how our transformers models work to...\"],[\"fold, I'm going to quickly introduce Keras models, and how we work with them. In other videos, which...\"],[\"by building your model out by hand - you added one layer after another, maybe using model.add() or t...\"],[\"all packed inside a Model, you don't need to worry about that complexity if you don't want to! You h...\"],[\"we'll cover concrete examples of how to use those methods in other videos that I'll link below. For ...\"],[\"Gradio, check![[gradio-check]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={9}\\n    classNames=\\\"absolute z-10 ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fast tokenizers' special powers[[fast-tokenizers-special-powers]]\\n\\n...\"],[\"{\\u002fif}\\n\\nIn this section we will take a closer look at the capabilities of the tokenizers in 🤗 Transfo...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Batch encoding[[batch-encoding]]\\n\\n\\u003cYoutube id=\\\"3umI3tm27Vw\\\"\\u002f\\u003e\\n\\nThe output of a tokenizer ...\"],[\"```\\n\\nAs mentioned previously, we get a `BatchEncoding` object in the tokenizer's output:\\n\\n```python ...\"],[\"```\\n\\nWe can see that the tokenizer's special tokens `[CLS]` and `[SEP]` are mapped to `None`, and th...\"],[\"Lastly, we can map any word or token to characters in the original text, and vice versa, via the `wo...\"],[\"```\\n\\n```python out\\nSylvain...\"],[\"```\\n\\nAs we mentioned previously, this is all powered by the fact the fast tokenizer keeps track of t...\"],[\"{\\u002fif}\\n\\n### Getting the base results with the pipeline[[getting-the-base-results-with-the-pipeline]]\\n...\"],[\"```\\n\\n```python out\\n[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'e...\"],[\"```\\n\\nThe model properly identified each token generated by \\\"Sylvain\\\" as a person, each token generat...\"],[\"```\\n\\nThe `aggregation_strategy` picked will change the scores computed for each grouped entity. With...\"],[\"```py\\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\\n\\nmodel_checkpoint = \\\"d...\"],[\"```\\n\\nSince we're using `AutoModelForTokenClassification` here, we get one set of logits for each tok...\"],[\"```\\n\\n```python out\\n(1, 19)\\n(1, 19, 9)\\n```\\n\\n{\\u002fif}\\n\\nWe have a batch with 1 sequence of 19 tokens and t...\"],[\"```\\n\\nAs we saw earlier, there are 9 labels: `O` is the label for the tokens that are not in any name...\"],[\"With this map, we are ready to reproduce (almost entirely) the results of the first pipeline -- we c...\"],[\"```\\n\\n```python out\\n[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},\\n {'entity': 'I...\"],[\"```\\n\\n```python out\\n[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22...\"],[\"```\\n\\nThis is the same as what we got from the first pipeline!\\n\\n### Grouping entities[[grouping-entit...\"],[\"```\\n\\n```python out\\nHugging Face\\n```\\n\\nTo write the code that post-processes the predictions while gro...\"],[\"```\\n\\nAnd we get the same results as with our second pipeline!\\n\\n```python out\\n[{'entity_group': 'PER'...\"],[\"et's take a look at subword-based tokenization. Understanding why subword-based tokenization is inte...\"],[\"very long sequences, less meaningful individual tokens for character-based tokenizers. These algorit...\"],[\"still the word dog, with an added s while slightly changes the meaning while keeping the original id...\"],[\"the model will now be able to make sense of token in different situations. It will understand that t...\"],[\"completing a word. Here the ## prefix indicates that ization is part of a word rather than the begin...\"],[\"in reducing the vocabulary sizes by sharing information across different words, having the ability t...\"],[\"et's see together what is the training strategy of the WordPiece algorithm and how it performs the t...\"],[\"increases this vocabulary to the desired size. To build the initial vocabulary, we divide each word ...\"],[\"each of these pairs. As for the BPE algorithm, we will select the pair with the highest score. Takin...\"],[\"this score will be decreased. In our example, the pair \\\"hu\\\" appears 4 times, the letter \\\"h\\\" 4 times ...\"],[\"operations until we have the vocabulary at the desired size! Let's look at a few more steps to see t...\"],[\"of our word. And so on until we reach the end! And that's it, huggingface is divided into 4 sub-toke...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Summarization[[summarization]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCourseFloatingB...\"],[\"\\u003cYoutube id=\\\"yHnr5Dk2zCI\\\"\\u002f\\u003e\\n\\nAlthough there already exist various fine-tuned models for summarizatio...\"],[\"## Preparing a multilingual corpus[[preparing-a-multilingual-corpus]]\\n\\nWe'll use the [Multilingual A...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['review_id', 'product_id', ...\"],[\"```\\n\\n```python out\\n'\\u003e\\u003e Title: Worked in front position, not rear'\\n'\\u003e\\u003e Review: 3 stars because these ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Try it out!** Change the random seed in the `Dataset.shuffle()` command to explore ...\"],[\"```\\n\\nThe most popular products in the English dataset are about household items, clothing, and wirel...\"],[\"```\\n\\n```python out\\n'\\u003e\\u003e Title: I\\\\'m dissapointed.'\\n'\\u003e\\u003e Review: I guess I had higher expectations for ...\"],[\"```\\n\\nOkay, we can see that the reviews are not strictly about books and might refer to things like c...\"],[\"```\\n\\nThis certainly looks like a mix of English and Spanish reviews! Now that we have a training cor...\"],[\"```\\n\\nNow that we've prepared our corpus, let's take a look at a few possible Transformer models that...\"],[\"| Transformer model | Description                                                                   ...\"],[\"|     [mT5](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fmt5-base)     | A multilingual version of T5, pretrained o...\"],[\"As you can see from this table, the majority of Transformer models for summarization (and indeed mos...\"],[\"\\u003cTip\\u003e\\n\\n✏️ **Try it out!** Once you've worked through this section, see how well mT5 compares to mBAR...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n💡 In the early stages of your NLP projects, a good practice is to train a class of \\\"smal...\"],[\"```\\n\\n```python out\\n['▁I', '▁', 'loved', '▁reading', '▁the', '▁Hung', 'er', '▁Games', '\\u003c\\u002fs\\u003e']\\n```\\n\\nTh...\"],[\"```\\n\\nLet's walk through this code to understand what's happening. The first thing we've done is defi...\"],[\"```\\n\\nNow that the corpus has been preprocessed, let's take a look at some metrics that are commonly ...\"],[\"```\\n\\nOne way to compare them could be to count the number of overlapping words, which in this case w...\"],[\"Applying this to our verbose summary gives a precision of 6\\u002f10  = 0.6, which is considerably worse t...\"],[\"```\\n\\nand then loading the ROUGE metric as follows:\\n\\n```python\\nimport evaluate\\n\\nrouge_score = evaluat...\"],[\"```\\n\\nWhoa, there's a lot of information in that output -- what does it all mean? First, 🤗 Datasets a...\"],[\"```\\n\\n```python out\\nScore(precision=0.86, recall=1.0, fmeasure=0.92)\\n```\\n\\nGreat, the precision and re...\"],[\"```\\n\\nand then download the punctuation rules:\\n\\n```python\\nimport nltk\\n\\nnltk.download(\\\"punkt\\\")\\n```\\n\\nNe...\"],[\"```\\n\\n```python out\\n{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}\\n```\\n\\nWe can...\"],[\"```\\n\\n{\\u002fif}\\n\\n\\u003cTip\\u003e\\n\\n💡 If you're wondering why you don't see any warnings about fine-tuning the model ...\"],[\"```\\n\\nwhich will display a widget where you can enter your credentials. Alternatively, you can run th...\"],[\"```\\n\\nHere, the `predict_with_generate` argument has been set to indicate that we should generate sum...\"],[\"```python\\nimport numpy as np\\n\\n\\ndef compute_metrics(eval_pred):\\n    predictions, labels = eval_pred\\n ...\"],[\"```\\n\\n{\\u002fif}\\n\\nNext, we need to define a data collator for our sequence-to-sequence task. Since mT5 is ...\"],[\"```\\n\\n```python out\\n{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\"],[\"```\\n\\nThe main thing to notice here is that the first example is longer than the second one, so the `...\"],[\"```\\n\\nFrom the scores we can see that our model has handily outperformed our lead-3 baseline -- nice!...\"],[\"```\\n\\nThis will save the checkpoint and configuration files to `output_dir`, before uploading all the...\"],[\"```\\n\\nNow, we define our training hyperparameters and compile:\\n\\n```python\\nfrom transformers import cr...\"],[\"```\\n\\nWe got some loss values during training, but really we'd like to see the ROUGE metrics we compu...\"],[\"all_preds = []\\nall_labels = []\\nfor batch, labels in tqdm(tf_generate_dataset):\\n    predictions = gen...\"],[\"```\\n\\nOnce we have our lists of label and prediction strings, computing the ROUGE score is easy:\\n\\n```...\"],[\"```\\n\\nWe can then instantiate the data collator and use this to define our dataloaders:\\n\\n```python\\nfr...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n🚨 If you're training on a TPU, you'll need to move all the code above into a dedicated t...\"],[\"```\\n\\nThis should look familiar to you if you recall how we defined the `compute_metrics()` function ...\"],[\"```\\n\\nThis will allow us to push the artifacts back to the Hub by calling the `repo.push_to_hub()` me...\"],[\"generated_tokens = accelerator.pad_across_processes(\\n                generated_tokens, dim=1, pad_in...\"],[\"# Save and upload\\n    accelerator.wait_for_everyone()\\n    unwrapped_model = accelerator.unwrap_model...\"],[\"```\\n\\n```python out\\nEpoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.50...\"],[\"```\\n\\nAnd that's it! Once you run this, you'll have a model and results that are pretty similar to th...\"],[\"```\\n\\nLet's take a look at one of the English examples we get:\\n\\n```python\\nprint_summary(100)\\n```\\n\\n```...\"],[\"et's study how to preprocess a dataset for token classification! Token classification regroups any t...\"],[\"to get to the same point, with one column containing words (as list of strings) and another containi...\"],[\"location, PER, for person, ORG for organization and MISC for miscellaneous. Each label has two versi...\"],[\"word may have been split into several tokens, our labels won't match the tokens anymore. This is whe...\"],[\"we tell the Transformer loss functions to ignore them when computing the loss. The code is then pret...\"],[\"a batch. Unless you changed the preprocessing function to apply some fixed padding, we will get sent...\"],[\"are either ready to send your data and this data collator to the Trainer, or to use the to_tf_datase...\"],[\"he fast tokenizers of the Transformers library are fast, but they also implement features that will ...\"],[\"is thus not enough if we want to match some tokens with a span of text (something we will need to do...\"],[\"at the beginning of a word, and T5 uses this special underscore symbol for the same purpose. Thankfu...\"],[\"calling it on one (or several) text by adding the return_offsets_mapping=True argument. In this inst...\"],[\"then we apply the model of the tokenizer, which is where the words are splits into tokens,() before ...\"],[\"instead of randomly chosen tokens. This will require us to use the word IDs we saw. When doing token...\"],[\"et's have a look inside the question answering pipeline. The question answering pipeline can extract...\"],[\"post-processing is applied. The tokenization and model steps should be familiar. We use the auto cla...\"],[\"SEP special tokens). The answer is a part of those tokens. So we ask the model to predict which toke...\"],[\"an impossible answer. This is what it looks in terms of code. We use a large negative number for the...\"],[\"we have the start and end positions of the tokens, we use the offset mappings provided by our tokeni...\"],[\"context. If we take disjoint chunks of texts, we might end up with the answer being split between tw...\"],[\"post-processing we saw before for each feature, we get the answer with a score for each of them, and...\"],[\"Basic usage completed![[basic-usage-completed]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={2}\\n    className...\"],[\"et's study how to preprocess a dataset for question answering! Question answering is the task of fin...\"],[\"containing the questions, one column containing the contexts, one column for the index of the start ...\"],[\"In this case, we wont have any proper labels for our model. So we should keep the truncated part as ...\"],[\"only and the padding to the maximum length. The stride argument is where we set the number of overla...\"],[\"contain the answer, we set the two labels to the index of the CLS token. We also do this if the cont...\"],[\"on our previous example. Putting it all together looks like this big function, which we can apply to...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={7}...\"],[\"Each section can be read independently.\\n\\n{\\u002fif}\\n\\n\\n\\u003cTip\\u003e\\n\\nIf you read the sections in sequence, you wi...\"],[\"hat happens inside the pipeline function? In this video, we will look at what actually happens when ...\"],[\"the model, which outputs logits. Finally, the post-processing steps transforms those logits into lab...\"],[\"at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer match...\"],[\"We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since th...\"],[\"a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding ...\"],[\"the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that...\"],[\"to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except...\"],[\"returns logits. To make sense of those logits, we need to dig into the third and last step of the pi...\"],[\"the negative label, and the seconds (index 1) correspond to the positive label. This is how our clas...\"],[\"et's see how to preprocess a dataset for translation. This is the task of well translating a sentenc...\"],[\"be able to follow the same steps. For once, our labels are not integers corresponding to some classe...\"],[\"as a single word. That's because our inputs have been tokenized as English. Since our model knows tw...\"],[\"setting padding=max_length. Here we will show you how to pad dynamically as it requires one more ste...\"],[\"library provides us with a data collator to do this all automatically. You can then pass it to the T...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fine-tuning, Check![[fine-tuning-check]]\\n\\n\\u003cCourseFloatingBanner\\n   ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Debugging the training pipeline[[debugging-the-training-pipeline]]\\n...\"],[\"The best way to debug an error that arises in `model.fit()` is to manually go through this whole pip...\"],[\"```\\n\\nIf you try to execute it, you might get some `VisibleDeprecationWarning`s when doing the datase...\"],[\"```\\n\\n`break` ends the loop after one iteration, so this grabs the first batch that comes out of `tra...\"],[\"```\\n\\nThis looks right, doesn't it? We're passing the `labels`, `attention_mask`, and `input_ids` to ...\"],[\"```\\n\\nNow we'll use the model's internal loss, and this problem should be resolved!\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Your...\"],[\"```\\n\\nOh no. \\n\\n`nan` is not a very encouraging loss value. Still, we've checked our data, and it look...\"],[\"```\\n\\nWell, this is tricky. Everything is `nan`! But that's strange, isn't it? How would all our logi...\"],[\"```\\n\\nWhen we run that, we get:\\n\\n```py out\\nTFSequenceClassifierOutput(loss=\\u003ctf.Tensor: shape=(16,), d...\"],[\"```\\n\\n*Now* we're getting somewhere! There are no `nan` values in our logits, which is reassuring. Bu...\"],[\"```python out\\narray([[  101,  2007,  2032,  2001,  1037, 16480,  3917,  2594,  4135,\\n        23212, ...\"],[\"0,     0,     0,     0,     0,     0,     0,     0,     0,\\n            0,     0,     0,     0],\\n    ...\"],[\"0,     0,     0,     0,     0,     0,     0,     0,     0,\\n            0,     0,     0,     0,     0...\"],[\"0,     0,     0,     0,     0,     0,     0,     0,     0,\\n            0,     0,     0,     0,     0...\"],[\"0,     0,     0,     0,     0,     0,     0,     0,     0,\\n            0,     0,     0,     0,     0...\"],[\"```\\n\\nWell, there's a lot in here, but nothing stands out as unusual. Let's look at the labels:\\n\\n```p...\"],[\"```\\n\\nWe're training! No more `nan`s, and our loss is declining... sort of. If you watch it for a whi...\"],[\"Does anything stand out here? That's right -- the learning rate! When we just use the string `'adam'...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n💡 You can also import the `create_optimizer()` function from 🤗 Transformers, which will ...\"],[\"```\\n\\nNow our loss is really going somewhere! Training finally looks like it's working. There's a les...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Hungry Hungry TensorFlow 🦛[[hungry-hungry-tensorflow]]\\n\\nOne particular quirk of TensorFl...\"],[\"### Check your data (again!)[[check-your-data-again]]\\n\\nYour model will only learn something if it's ...\"],[\"```\\n\\nThen you can compare it with the first label, like so:\\n\\n```py\\nlabels = batch[\\\"labels\\\"].numpy()\\n...\"],[\"```\\n\\nOnce you can view your data like this, you can ask yourself the following questions:\\n\\n- Is the ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n💡 If your training data is unbalanced, make sure to build a batch of training data conta...\"],[\"If you are tweaking the model itself, keep it simple and don't try anything you can't reasonably jus...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Debugging the training pipeline[[debugging-the-training-pipeline]]\\n...\"],[\"The best way to debug an error that arises in `trainer.train()` is to manually go through this whole...\"],[\"```\\n\\nIf you try to execute it, you will be met with a rather cryptic error:\\n\\n```python out\\n'ValueErr...\"],[\"```\\n\\nDo you notice something wrong? This, in conjunction with the error message about `input_ids` mi...\"],[\"metric = evaluate.load(\\\"glue\\\", \\\"mnli\\\")\\n\\n\\ndef compute_metrics(eval_pred):\\n    predictions, labels = e...\"],[\"```\\n\\nThis new code will now give a different error (progress!):\\n\\n```python out\\n'ValueError: expected...\"],[\"```\\n\\n```python out\\ndict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'...\"],[\"```\\n\\n```python out\\nTrue\\n```\\n\\nThat's good! Lastly, let's check our label:\\n\\n```py\\ntrainer.train_datase...\"],[\"```\\n\\nThis code creates the training dataloader, then iterates through it, stopping at the first iter...\"],[\"```\\n\\nSo this is the `default_data_collator`, but that's not what we want in this case. We want to pa...\"],[\"def compute_metrics(eval_pred):\\n    predictions, labels = eval_pred\\n    return metric.compute(predic...\"],[\"```\\n\\nThe good news? We don't get the same error as before, which is definitely progress. The bad new...\"],[\"```\\n\\nIf you're running this code in a notebook, you may get a CUDA error that's similar to the one w...\"],[\"```\\n\\n```python out\\n~\\u002f.pyenv\\u002fversions\\u002f3.7.9\\u002fenvs\\u002fbase\\u002flib\\u002fpython3.7\\u002fsite-packages\\u002ftorch\\u002fnn\\u002ffunctional...\"],[\"```\\n\\nWith two labels, only 0s and 1s are allowed as targets, but according to the error message we g...\"],[\"```\\n\\nWe aren't including the `trainer.train()` line yet, to take the time to check that everything l...\"],[\"```\\n\\nAgain, if you're using the default optimizer in the `Trainer`, you shouldn't get an error at th...\"],[\"```\\n\\n```python out\\nTypeError: only size-1 arrays can be converted to Python scalars\\n```\\n\\nYou will re...\"],[\"```\\n\\nThis tells us that the error originates in the `datasets\\u002fmetric.py` module -- so this is a prob...\"],[\"```\\n\\n```python out\\n{'accuracy': 0.625}\\n```\\n\\nNow our error is fixed! This was the last one, so our sc...\"],[\"```\\n\\nIn this instance, there are no more problems, and our script will fine-tune a model that should...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nAfter looking at your data, go through a few of the model's predictions and decode them too....\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n💡 If your training data is unbalanced, make sure to build a batch of training data conta...\"],[\"```\\n\\n100% accuracy, now this is a nice example of overfitting (meaning that if you try your model on...\"],[\"Here are some additional resources that may prove helpful:\\n\\n- [\\\"Reproducibility as a vehicle for eng...\"],[\"Part 1 completed![[part-1-completed]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={4}\\n    classNames=\\\"absolut...\"],[\"Bias and limitations[[bias-and-limitations]]\\n\\n\\u003cCourseFloatingBanner chapter={1}\\n  classNames=\\\"absolu...\"],[\"```\\n\\nWhen asked to fill in the missing word in these two sentences, the model gives only one gender-...\"],[\"n this video, we'll study the encoder-decoder architecture. An example of a popular encoder-decoder ...\"],[\"now know that the numerical representation holds information about the meaning of the sequence. Let'...\"],[\"that's where the encoder-decoder magic happens. The encoder accepts a sequence as input. It computes...\"],[\"decoder is essentially decoding what the encoder has output. The \\\"start of sequence word\\\" indicates ...\"],[\"second word. Please note that the first word is still here; as the model still outputs it. However, ...\"],[\"That encoder output is then sent to the decoder, for it to be decoded. While we can now discard the ...\"],[\"encoder to create a representation of the English sentence. We cast this to the decoder and, with th...\"],[\"We've translated the sentence! Where the encoder-decoder really shines, is that we have an encoder a...\"],[\"other hand, we have the decoder, whose sole purpose is to decode the feature output by the encoder. ...\"],[\"Firstly, this means that from a sequence of three words, we're able to generate a sequence of four w...\"],[\"example a very long context for the encoder which handles the text, and a smaller context for the de...\"],[\"tasks. This wraps things up for the encoder-decoders. Thanks for watching!...\"],[\"et's have a look inside the token classification pipeline. In the pipeline video, we looked at the d...\"],[\"the text classification pipeline we studied in a previous video. There are three steps: the tokeniza...\"],[\"all the other models of the Transformers library, our model outputs logits, which we turn into predi...\"],[\"is why we didn't see it in our results in the first slide. On top of the label and the probability, ...\"],[\"group together tokens that correspond to the same entity.This is why we had two labels for each type...\"],[\"cases, we can flag a new entity each time we see a new label appearing (either with the I or B prefi...\"],[\"hat is the ROUGE metric? For many NLP tasks we can use common metrics like accuracy or F1 score, but...\"],[\"a summary that tells us how \\\"good\\\" it is compared to one or more reference summaries. In this exampl...\"],[\"n-grams of the references. An n-gram is just a fancy way of saying \\\"a chunk of n words\\\", so let's st...\"],[\"unigrams. This means we just count the number of matching words in the generated and reference summa...\"],[\"This would also have perfect recall, but is arguably a worse summary since it is verbose. To deal wi...\"],[\"then count how many pairs in the generated summary are present in the reference one. This gives us R...\"],[\"instead treats each summary as a sequence of words and then looks for the longest common subsequence...\"],[\"very simple: just use the load_metric() function, provide your model's summaries along with the refe...\"],[\"score. We've already seen ROUGE-1, ROUGE-2 and ROUGE-L, so what is ROUGE-LSUM? Well, the “sum” in RO...\"],[\"WordPiece tokenization[[wordpiece-tokenization]]\\n\\n\\u003cCourseFloatingBanner chapter={6}\\n  classNames=\\\"ab...\"],[\"```\\nw ##o ##r ##d\\n```\\n\\nThus, the initial alphabet contains all the characters present at the beginni...\"],[\"```\\n\\nso the initial vocabulary will be `[\\\"b\\\", \\\"h\\\", \\\"p\\\", \\\"##g\\\", \\\"##n\\\", \\\"##s\\\", \\\"##u\\\"]` (if we forget a...\"],[\"```\\nVocabulary: [\\\"b\\\", \\\"h\\\", \\\"p\\\", \\\"##g\\\", \\\"##n\\\", \\\"##s\\\", \\\"##u\\\", \\\"##gs\\\", \\\"hu\\\"]\\nCorpus: (\\\"hu\\\" \\\"##g\\\", 10), ...\"],[\"```\\n\\nand we continue like this until we reach the desired vocabulary size.\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Now your tur...\"],[\"When the tokenization gets to a stage where it's not possible to find a subword in the vocabulary, t...\"],[\"```\\n\\nFirst, we need to pre-tokenize the corpus into words. Since we are replicating a WordPiece toke...\"],[\"```\\n\\nAs we saw before, the alphabet is the unique set composed of all the first letters of words, an...\"],[\"```\\n\\nNext we need to split each word, with all the letters that are not the first prefixed by `##`:\\n...\"],[\"```\\n\\n```python out\\n('T', '##h'): 0.125\\n('##h', '##i'): 0.03409090909090909\\n('##i', '##s'): 0.0272727...\"],[\"```\\n\\nAnd we can have a look at the result of the first merge:\\n\\n```py\\nsplits = merge_pair(\\\"a\\\", \\\"##b\\\",...\"],[\"```\\n\\nWe can then look at the generated vocabulary:\\n\\n```py\\nprint(vocab)\\n```\\n\\n```python out\\n['[PAD]', ...\"],[\"```\\n\\nAs we can see, compared to BPE, this tokenizer learns parts of words as tokens a bit faster.\\n\\n\\u003c...\"],[\"```\\n\\nWe can try it on any text:\\n\\n```python\\ntokenize(\\\"This is the Hugging Face course!\\\")\\n```\\n\\n```pyth...\"],[\"emory mapping and streaming. In this video we'll take a look at two core features of the Datasets li...\"],[\"To handle these large datasets, the Datasets library is built on two core features: the Apache Arrow...\"],[\"your hard disk. For these cases, the Datasets library provides a streaming API that allows you to pr...\"],[\"virtual memory. This allows applications to access can access segments in an extremely large file wi...\"],[\"laptop - that's not too bad at all! Let's now take a look at how we can stream a large dataset. The ...\"],[\"will download and access a single example from the dataset, which  means you can progressively itera...\"],[\"methods because we can't index into the dataset. The take() method returns the first N examples in t...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={2}\\n    classNames=\\\"absolute z-10 ri...\"],[\"Then we'll look at the tokenizer API, which is the other main component of the `pipeline()` function...\"],[\"Introduction[[introduction]]\\n\\nWelcome to the Hugging Face course! This introduction will guide you t...\"],[\"Once you're comfortable moving around in Colab, create a new notebook and get started with the setup...\"],[\"```\\n!pip install transformers\\n```\\n\\nYou can make sure the package was correctly installed by importin...\"],[\"```\\n!pip install transformers[sentencepiece]\\n```\\n\\nThis will take a bit of time, but then you'll be r...\"],[\"```\\n\\nFrom inside this directory, create a virtual environment using the Python `venv` module:\\n\\n```\\np...\"],[\"n this video we take a look at the data processing necessary to train causal language models. Causal...\"],[\"text files. These files can webpages scraped from the internet such as the Common Crawl dataset or t...\"],[\"length and depending on the data source it is possible that the tokenized texts are much longer than...\"],[\"fill it. In this case we would like to remove it. With the return_length keyword we also get the len...\"],[\"existing columns. We need to remove columns because we can create multiple samples per text and the ...\"],[\"for short, token in between. Finally we can chunk this long sequence with the context length and we ...\"],[\"predict is “formers”. In the next step we feed “Trans” and “formers” to the model and the label is t...\"],[\"the sequence ends. Let’s have a look at what we need to do to create the labels for causal language ...\"],[\"hat is the BLEU metric? For many NLP tasks we can use common metrics like accuracy or F1 score, but ...\"],[\"to one or more reference translations. In this example we have a sentence in Spanish that has been t...\"],[\"the n-grams of the generated translation to the n-grams of the references. An n-gram is just a fancy...\"],[\"This means we just count the number of matching words in the generated and reference translations an...\"],[\"times. If we just count the number of word matches, we can get really high precision scores even tho...\"],[\"clip the numerator to one and the modified unigram precision now gives a much lower score. Another p...\"],[\"for several different n-grams and then averages the result. For example, if we compare 4-grams, then...\"],[\"score itself is then calculated by taking the geometric mean of the precision scores. By default, th...\"],[\"on a benchmark. On the other hand, there are several problems with BLEU, including the fact it doesn...\"],[\"see in this example, computing the SacreBLEU score is almost identical to the BLEU one. The main dif...\"],[\"elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging ...\"],[\"result with the community. The second will dive deeper into our libraries and teach you how to tackl...\"],[\"Learning. If you don't know what a training and validation set is or what gradient descent means, yo...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    classNames=\\\"absolute z-10 ri...\"],[\"- Chapters 1 to 4 provide an introduction to the main concepts of the 🤗 Transformers library. By the...\"],[\"## Who are we?[[who-are-we]]\\n\\nAbout the authors:\\n\\n[**Abubakar Abid**](https:\\u002f\\u002fhuggingface.co\\u002fabidlab...\"],[\"[**Dawood Khan**](https:\\u002f\\u002fhuggingface.co\\u002fdawoodkhan82) is a Machine Learning Engineer at Hugging Fac...\"],[\"- **Does taking this course lead to a certification?**\\nCurrently we do not have any certification fo...\"],[\"The Jupyter notebooks containing all the code from the course are hosted on the [`huggingface\\u002fnotebo...\"],[\"```\\n@misc{huggingfacecourse,\\n  author = {Hugging Face},\\n  title = {The Hugging Face Course, 2022},\\n ...\"],[\"ote: the following transcripts are associated with Merve Noyan's videos in the Hugging Face Tasks pl...\"],[\"Question Answering video\\n\\nWelcome to the Hugging Face tasks series. In this video, we will take a lo...\"],[\"Masked Language Modeling video\\n\\nWelcome to the Hugging Face tasks series! In this video we’ll take a...\"],[\"Translation video\\n\\nWelcome to the Hugging Face tasks series. In this video, we will take a look at t...\"],[\"as recorded adlib - need to generate transcript with Whisper :)...\"],[\"Big data? 🤗 Datasets to the rescue![[big-data-datasets-to-the-rescue]]\\n\\n\\u003cCourseFloatingBanner chapte...\"],[\"## What is the Pile?[[what-is-the-pile]]\\n\\nThe Pile is an English text corpus that was created by [El...\"],[\"```\\n\\nNext, we can load the dataset using the method for remote files that we learned in [section 2](...\"],[\"```\\n\\nOkay, this looks like the abstract from a medical article. Now let's see how much RAM we've use...\"],[\"```\\n\\nNice -- despite it being almost 20 GB large, we're able to load and access the dataset with muc...\"],[\"Memory-mapped files can also be shared across multiple processes, which enables methods like `Datase...\"],[\"```\\n\\n```python out\\n'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB\\u002fs'\\n```\\n\\n...\"],[\"```\\n\\n```python out\\n{'meta': {'pmid': 11409574, 'language': 'eng'},\\n 'text': 'Epidemiology of hypoxae...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n💡 To speed up tokenization with streaming you can pass `batched=True`, as we saw in the ...\"],[\"```\\n\\n```python out\\n[{'meta': {'pmid': 11409574, 'language': 'eng'},\\n  'text': 'Epidemiology of hypox...\"],[\"```\\n\\nLet's round out our exploration of dataset streaming with a common application: combining multi...\"],[\"```\\n\\nThis dataset is large enough to stress the RAM of most laptops, yet we've been able to load and...\"],[\"```\\n\\nHere we've used the `islice()` function from Python's `itertools` module to select the first tw...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Try it out!** Use one of the large Common Crawl corpora like [`mc4`](https:\\u002f\\u002fhuggin...\"],[\"et's see how we can preprocess our data for masked language modeling. As a reminder, masked language...\"],[\"together. The first way to make all the texts the same length is the one we used in text classificat...\"],[\"lost. This is why a second way to generate samples of text with the same length is to chunk our text...\"],[\"of chunking is ideal if all your texts are very long, but it won't work as nicely if you have a vari...\"],[\"chunk it. Notice how it reduces the number of samples in our dataset here, there must have been quit...\"],[\"Subtitles for the course videos\\n\\nThis folder contains all the subtitles for the course videos on You...\"],[\"```\\n\\nTo upload the SRT file to YouTube, we need the subtitle in monolingual format, i.e. the above b...\"],[\"he pipeline function. The pipeline function is the most high-level API of the Transformers library. ...\"],[\"given input, and determines if it's positive or negative. Here, it attributed the positive label on ...\"],[\"classification pipeline is a more general text-classification pipeline: it allows you to provide the...\"],[\"until now, we have used the pipeline API with the default model associated to each task, but you can...\"],[\"a lighter version of gpt2 created by the Hugging Face team. When applying the pipeline to a given pr...\"],[\"we ask the two most likely values for the missing words (according to the model) and get mathematica...\"],[\"text. The grouped_entities=True argument used is to make the pipeline group together the different w...\"],[\"API is translation. Here we use a French\\u002fEnglish model found on the model hub to get the English ver...\"],[\"Sequence-to-sequence models[sequence-to-sequence-models]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    ...\"],[\"aving and reloading a dataset. In this video we'll take a look saving a dataset in various formats, ...\"],[\"downloaded the allocine dataset from the Hugging Face Hub and you can see there are three Arrow file...\"],[\"want to save it in either the Arrow or Parquet formats. Arrow files are great if you plan to reload ...\"],[\"library will automatically create a directory for each split to store the Arrow table and metadata. ...\"],[\"need to loop over the splits of the DatasetDict object and save each dataset as an individual CSV fi...\"],[\"associated with each split. As you can see in this example, by providing all the splits and their fi...\"],[\"or Parquet files, we can reload them again with the appropriate script in the load_dataset function,...\"],[\"n our other videos we talked about the basics of fine-tuning a language model with Tensorflow (and a...\"],[\"two things we want to change about the default learning rate for Adam. The first is that it's way to...\"],[\"down to a tiny value, or even 0, over the course of training. That's what this PolynomialDecay sched...\"],[\"is the size of the training set, divided by the batch_size to get the number of batches per epoch, a...\"],[\"5e-5, which means 5 times ten to the minus 5, and then decays down at a constant rate until it hits ...\"],[\"just passed it the string \\\"adam\\\". Keras recognizes the names of common optimizers and loss functions...\"],[\"be sparse categorical crossentropy if you're following along from the fine-tuning video. And now we ...\"],[\"Tokenizers, check![[tokenizers-check]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={6}\\n    classNames=\\\"absolu...\"],[\"We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since th...\"],[\"dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is...\"],[\"weights. However, the TFAutoModel API will only instantiate the body of the model, that is, the part...\"],[\"the TFAutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that...\"],[\"To make sense of those logits, we need to dig into the third and last step of the pipeline: post-pro...\"],[\"and the seconds (index 1) correspond to the positive label. This is how our classifier built with th...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"```\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It will return classification scores for this sentence, wit...\"],[\"```\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"This pipeline requires that labels be given to classify thi...\"],[\"### 6. True or false? A language model usually does not need labels for its pretraining.\\n\\n\\u003cQuestion\\n...\"],[\"### 9. Which of those types of models would you use for summarizing texts?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t...\"],[\"### 11. What possible source can the bias observed in a model have?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\tte...\"],[\"n this video we will see how you can create your own tokenizer from scratch! To create your own toke...\"],[\"the creation of an attention mask but also the generation of a list of token ids. The decoding opera...\"],[\"a tokenizer from the tokenizers library. So, to create your own transformers tokenizer you will have...\"],[\"English. We attack here the big part: the design of our tokenizer with the tokenizers library. We st...\"],[\"the level of spaces and the second one isolating the punctuation marks. Now, we can define the train...\"],[\"can retrieve the ids of the special class and separation tokens because we will need them to post-pr...\"],[\"there it ist, you have all the necessary lines of code to define your own tokenizer. Now that we hav...\"],[\"that you are ready to navigate the tokenizers library documentation to choose the components for you...\"],[\"Gradio Blocks Party[[gradio-blocks-party]]\\n\\nAlong with the release of the Gradio chapter of the cour...\"],[\"Training a new tokenizer from an old one[[training-a-new-tokenizer-from-an-old-one]]\\n\\n\\u003cCourseFloatin...\"],[\"\\u003cYoutube id=\\\"DJimQynXZsQ\\\"\\u002f\\u003e\\n\\n\\u003cTip warning={true}\\u003e\\n\\n⚠️ Training a tokenizer is not the same as traini...\"],[\"```py\\nfrom datasets import load_dataset\\n\\n# This can take a few minutes to load, so grab a coffee or ...\"],[\"```\\n\\nWe can have a look at the training split to see which columns we have access to:\\n\\n```py\\nraw_dat...\"],[\"```\\n\\nThe first thing we need to do is transform the dataset into an _iterator_ of lists of texts -- ...\"],[\"```\\n\\nwe get them once and then an empty list:\\n\\n```python out\\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\n[]\\n```\\n\\n...\"],[\"```\\n\\nEven though we are going to train a new tokenizer, it's a good idea to do this to avoid startin...\"],[\"```\\n\\nThis tokenizer has a few special symbols, like `Ġ` and `Ċ`, which denote spaces and newlines, r...\"],[\"```\\n\\nThis command might take a bit of time if your corpus is very large, but for this dataset of 1.6...\"],[\"Most of the Transformer models have a fast tokenizer available (there are some exceptions that you c...\"],[\"```\\n\\n```python out\\n['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ\\\"\\\"\\\"', 'Add',...\"],[\"```\\n\\n```python out\\n['class', 'ĠLinear', 'Layer', '():', 'ĊĠĠĠ', 'Ġdef', 'Ġ__', 'init', '__(', 'self'...\"],[\"```\\n\\nIn addition to the token corresponding to an indentation, here we can also see a token for a do...\"],[\"```\\n\\nOnce you've logged in, you can push your tokenizer by executing the following command:\\n\\n```py\\nt...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n\\u003c!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-...\"],[\"### 3. How does the BERT model expect a pair of sentences to be processed?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t...\"],[\"### 5. What does dynamic padding mean?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It's when you pad the in...\"],[\"### 6. What is the purpose of a collate function?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It ensures al...\"],[\"\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"Nothing, but you get a warning.\\\",\\n\\t\\t\\texplain: \\\"You do get a warn...\"],[\"### 9. Why should you use the 🤗 Accelerate library?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It provides...\"],[\"### 5. The TensorFlow models from `transformers` are already Keras models. What benefit does this of...\"],[\"is why we didn't see it in our results in the first slide. On top of the label and the probability, ...\"],[\"group together tokens that correspond to the same entity. This is why we had two labels for each typ...\"],[\"both cases, we can flag a new entity each time we see a new label appearing (either with the I or B ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fine-tuning a model with Keras[[fine-tuning-a-model-with-keras]]\\n\\n\\u003c...\"],[\"tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\\n\\ndata_collator = DataCollator...\"],[\"```\\n\\n### Training[[training]]\\n\\nTensorFlow models imported from 🤗 Transformers are already Keras mode...\"],[\"```\\n\\nYou will notice that unlike in [Chapter 2](\\u002fcourse\\u002fchapter2), you get a warning after instantia...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nNote a very common pitfall here — you *can* just pass the name of the los...\"],[\"```py\\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\\n\\nbatch_size = 8\\nnum_epochs =...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nThe 🤗 Transformers library also has a `create_optimizer()` function that will create an ...\"],[\"```\\n\\nWe can convert these logits into the model's class predictions by using `argmax` to find the hi...\"],[\"back feature to the examples that they originated from. If you don't want to compute the validation ...\"],[\"you want to evaluate. We will run a manual evaluation loop, so we create a PyTorch DataLoader with o...\"],[\"just take the best index for the start and end logits and be done, but if our model predicts somethi...\"],[\"ones. We ignore the logits that spawn impossible answers or answer that are too long. As we saw in t...\"],[\"picking for each the answer with the best logit score in all the features the example generated. Now...\"],[\"Asking for help on the forums[[asking-for-help-on-the-forums]]\\n\\n\\u003cCourseFloatingBanner chapter={8}\\n  ...\"],[\"On the lefthand side you can see all the categories that the various topics are grouped into, while ...\"],[\"```python\\nfrom transformers import AutoTokenizer, AutoModel\\n\\nmodel_checkpoint = \\\"distilbert-base-unc...\"],[\"```\\n\\nNow suppose we try to embed a whole section of the [Wikipedia article](https:\\u002f\\u002fen.wikipedia.org...\"],[\"The Transformers TV series began around the same time. Produced by Sunbow\\nProductions and Marvel Pro...\"],[\"```\\n\\n```python output\\nIndexError: index out of range in self...\"],[\"```\\n\\nUh-oh, we've hit a problem -- and the error message is far more cryptic than the ones we saw in...\"],[\"Although this topic contains the error message we need help with, there are a few problems with the ...\"],[\"### Formatting your code snippets[[formatting-your-code-snippets]]\\n\\nReading source code is hard enou...\"],[\"### Including the full traceback[[including-the-full-traceback]]\\n\\nSince the last line of the traceba...\"],[\"### Providing a reproducible example[[providing-a-reproducible-example]]\\n\\nIf you've ever tried to de...\"],[\"n this video, we will see how to debug an error you encounter when running trainer.train(). As an ex...\"],[\"us there is a problem there, but the problem could come from many different causes. To debug an erro...\"],[\"a problem there as we see texts and not numbers. The error message was telling us the model did not ...\"],[\"can confirm this by asking the Trainer to get us a batch of the training data loader, which reproduc...\"],[\"to do that. So let's fix the issue and run again. This time we get a nasty CUDA error. They are very...\"],[\"is because everything that happens on the GPU is done asynchronously: when you execute the model cal...\"],[\"hasn't finished the forward pass of the model since all that took no time at all. The CPU stops movi...\"],[\"a traceback we can trust this time. As we said before, the error happens during the forward pass of ...\"],[\"when we create the model. Now the training script will run to completion! We did not need it yet, bu...\"],[\"Encoder models[[encoder-models]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    classNames=\\\"absolute z-1...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Token classification[[token-classification]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cC...\"],[\"{\\u002fif}\\n\\nThe first application we'll explore is token classification. This generic task encompasses an...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-bert-finetuned-ner.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"350\\\" title=\\\"G...\"],[\"## Preparing the data[[preparing-the-data]]\\n\\nFirst things first, we need a dataset suitable for toke...\"],[\"```\\n\\nThis will download and cache the dataset, like we saw in [Chapter 3](\\u002fcourse\\u002fchapter3) for the ...\"],[\"```\\n\\n```python out\\n[3, 0, 7, 0, 0, 0, 7, 0, 0]\\n```\\n\\nThose are the labels as integers ready for train...\"],[\"```\\n\\nWe already saw these labels when digging into the `token-classification` pipeline in [Chapter 6...\"],[\"```\\n\\n```python out\\n'EU    rejects German call to boycott British lamb .'\\n'B-ORG O       B-MISC O    ...\"],[\"```\\n\\nYou can replace the `model_checkpoint` with any other model you prefer from the [Hub](https:\\u002f\\u002fh...\"],[\"```\\n\\nAs we can see, the tokenizer added the special tokens used by the model (`[CLS]` at the beginni...\"],[\"```\\n\\n```python out\\n[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]\\n```\\n\\nWith a tiny bit of work, we can t...\"],[\"```\\n\\nAs we can see, our function added the `-100` for the two special tokens at the beginning and th...\"],[\"```\\n\\nNote that we haven't padded our inputs yet; we'll do that later, when creating the batches with...\"],[\"```\\n\\nWe've done the hardest part! Now that the data has been preprocessed, the actual training will ...\"],[\"```\\n\\n{\\u002fif}\\n\\nTo test this on a few samples, we can just call it on a list of examples from our tokeni...\"],[\"```\\n\\n```python out\\n[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\\n[-100, 1, 2, -100]\\n```\\n\\n{#if fw === 'p...\"],[\"```\\n\\n\\n Next stop: the model itself.\\n\\n{\\u002fif}\\n\\n{#if fw === 'tf'}\\n\\n### Defining the model[[defining-the-...\"],[\"```\\n\\n```python out\\n9\\n```\\n\\n\\u003cTip warning={true}\\u003e\\n\\n⚠️ If you have a model with the wrong number of labe...\"],[\"```\\n\\nAfter logging in, we can prepare everything we need to compile our model. 🤗 Transformers provid...\"],[\"```\\n\\nNote also that we don't supply a `loss` argument to `compile()`. This is because the models can...\"],[\"```\\n\\nYou can specify the full name of the repository you want to push to with the `hub_model_id` arg...\"],[\"```\\n\\nWe can then load it via the `evaluate.load()` function like we did in [Chapter 3](\\u002fcourse\\u002fchapt...\"],[\"```\\n\\nNote that the metric takes a list of predictions (not just one) and a list of labels. Here's th...\"],[\"```\\n\\n{#if fw === 'pt'}\\n\\nThis is sending back a lot of information! We get the precision, recall, and...\"],[\"```\\n\\nNow that this is done, we are almost ready to define our `Trainer`. We just need a `model` to f...\"],[\"```\\n\\n\\n```python out\\n{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},\\n 'MISC'...\"],[\"```\\n\\nNow we can just pass them to the `AutoModelForTokenClassification.from_pretrained()` method, an...\"],[\"```\\n\\nOnce this is done, we can define our `TrainingArguments`:\\n\\n```python\\nfrom transformers import T...\"],[\"```\\n\\nYou've seen most of those before: we set some hyperparameters (like the learning rate, the numb...\"],[\"```\\n\\nNote that while the training happens, each time the model is saved (here, every epoch) it is up...\"],[\"```\\n\\nThe `Trainer` also drafts a model card with all the evaluation results and uploads it. At this ...\"],[\"```\\n\\nOnce we have all those objects, we can send them to the `accelerator.prepare()` method:\\n\\n```py\\n...\"],[\"```\\n\\n```python out\\n'sgugger\\u002fbert-finetuned-ner-accelerate'\\n```\\n\\nThen we can clone that repository in...\"],[\"```\\n\\nThen we can write the training loop. After defining a progress bar to follow how training goes,...\"],[\"predictions = outputs.logits.argmax(dim=-1)\\n        labels = batch[\\\"labels\\\"]\\n\\n        # Necessary to...\"],[\"```\\n\\nThe first line is self-explanatory: it tells all the processes to wait until everyone is at tha...\"],[\"```\\n\\n```python out\\n[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end...\"],[\"n this video we take a look at setting up a custom loss function for training. In the default loss f...\"],[\"other rules. For each sample we get a loss value during training and we can combine that loss with a...\"],[\"see a loss function that does exactly that for causal language modeling. It takes the models it take...\"],[\"over the batch. With the view we unflatten the tensor to get a matrix with a row for each sample in ...\"],[\"get the information for each keyword in a separate matrix. Only want to know how many times keywords...\"],[\"weighted loss. Let’s see how we can make use of that custom loss with Accelerate and the Trainer In ...\"],[\"you can integrate your own awesome loss function with both the trainer and accelerates....\"],[\"Time to slice and dice[[time-to-slice-and-dice]]\\n\\n\\u003cCourseFloatingBanner chapter={5}\\n  classNames=\\\"ab...\"],[\"First we need to download and extract the data, which can be done with the `wget` and `unzip` comman...\"],[\"```\\n\\nSince TSV is just a variant of CSV that uses tabs instead of commas as the separator, we can lo...\"],[\"```\\n\\n```python out\\n{'Unnamed: 0': [87571, 178045, 80482],\\n 'drugName': ['Naproxen', 'Duloxetine', 'M...\"],[\"```\\n\\nNote that we've fixed the seed in `Dataset.shuffle()` for reproducibility purposes. `Dataset.se...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['patient_id', 'drugName', '...\"],[\"```\\nlambda \\u003carguments\\u003e : \\u003cexpression\\u003e\\n```\\n\\nwhere `lambda` is one of Python's special [keywords](http...\"],[\"```\\n\\n```python out\\n['left ventricular dysfunction', 'adhd', 'birth control']\\n```\\n\\nIt works! Now that...\"],[\"```\\n\\nAs expected, we can see a `review_length` column has been added to our training set. We can sor...\"],[\"```\\n\\n```python out\\n{'train': 138514, 'test': 46108}\\n```\\n\\nAs you can see, this has removed around 15%...\"],[\"```\\n\\nAs you can see, the `Dataset.map()` method is quite useful for processing data -- and we haven'...\"],[\"```\\n\\nIf you're running this code in a notebook, you'll see that this command executes way faster tha...\"],[\"```\\n\\nYou can also time a whole cell by putting `%%time` at the beginning of the cell. On the hardwar...\"],[\"`Dataset.map()` also has some parallelization capabilities of its own. Since they are not backed by ...\"],[\"```\\n\\nYou can experiment a little with timing to determine the optimal number of processes to use; in...\"],[\"\\u003cTip\\u003e\\n\\n💡 In machine learning, an _example_ is usually defined as the set of _features_ that we feed ...\"],[\"```\\n\\nLet's test this on one example before using `Dataset.map()` on the whole dataset:\\n\\n```py\\nresult...\"],[\"```\\n\\n```python out\\nArrowInvalid: Column 1 named condition expected length 1463 but got length 1000\\n`...\"],[\"```\\n\\n```python out\\n(206772, 138514)\\n```\\n\\nWe mentioned that we can also deal with the mismatched leng...\"],[\"```\\n\\nWe get the same number of training features as before, but here we've kept all the old fields. ...\"],[\"```\\n\\n\\u003ctable border=\\\"1\\\" class=\\\"dataframe\\\"\\u003e\\n  \\u003cthead\\u003e\\n    \\u003ctr style=\\\"text-align: right;\\\"\\u003e\\n      \\u003cth\\u003e\\u003c\\u002f...\"],[\"Let's create a `pandas.DataFrame` for the whole training set by selecting all the elements of `drug_...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n🚨 Under the hood, `Dataset.set_format()` changes the return format for the dataset's `__...\"],[\"```\\n\\n\\u003ctable border=\\\"1\\\" class=\\\"dataframe\\\"\\u003e\\n  \\u003cthead\\u003e\\n    \\u003ctr style=\\\"text-align: right;\\\"\\u003e\\n      \\u003cth\\u003e\\u003c\\u002f...\"],[\"```\\n\\n## Creating a validation set[[creating-a-validation-set]]\\n\\nAlthough we have a test set we could...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['patient_id', 'drugName', '...\"],[\"```\\n\\nGreat, we've now prepared a dataset that's ready for training some models on! In [section 5](\\u002fc...\"],[\"```\\n\\nwhere we can see that each split is associated with its own *dataset.arrow* table, and some met...\"],[\"```\\n\\nThis saves each split in [JSON Lines format](https:\\u002f\\u002fjsonlines.org), where each row in the data...\"],[\"```\\n\\nAnd that's it for our excursion into data wrangling with 🤗 Datasets! Now that we have a cleaned...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={3}...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"```\\n\\nWhich of the following commands will produce a random sample of 50 elements from `dataset`?\\n\\n\\u003cQ...\"],[\"\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"\\u003ccode\\u003epets_dataset.filter(lambda x : x['name'].startswith('L'))\\u003c...\"],[\"### 5. Which of the following are the main benefits of memory mapping?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t...\"],[\"```\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It tries to stream a dataset that's too large to fit in RAM...\"],[\"### 7. Which of the following are the main benefits of creating a dataset card?\\n\\n\\u003cQuestion\\n\\tchoices=...\"],[\"### 9. For asymmetric semantic search, you usually have:\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"A shor...\"],[\"hy are fast tokenizers called fast? In this video we will see exactly how much faster the so-called ...\"],[\"use_fast=False to define the slow one. In a notebook, we can time the execution of a cell with the t...\"],[\"text at a time is like sending a cargo ship between two continents with just one container, it's ver...\"],[\"fast. And this is only for tokenizing texts. If you ever need to train a new tokenizer, they do this...\"],[\"Part 2 completed![[part-2-completed]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={8}\\n    classNames=\\\"absolut...\"],[\"n a lot of our examples, you're going to see DataCollators popping up over and over. They're used in...\"],[\"example, when you're doing sequence classification, all you really need from your data collator is t...\"],[\"You'll see these approaches used in the examples and notebooks throughout this course. In both cases...\"],[\"of the time, and so are often totally unaware that this option exists. This is a valuable lesson abo...\"],[\"need any special processing before being ready for training. Most sequence classification tasks, for...\"],[\"are the same length then you can use the even simpler DefaultDataCollator, but it'll give you an err...\"],[\"labels are also a sequence of tokens that can have variable length. In both of these cases, we handl...\"],[\"do in NLP, and secondly because it has two modes that do two very different things. You choose which...\"],[\"When you set mlm to True, though, you get quite different behaviour! That's because masked language ...\"],[\"some tokens with a masking token, other tokens with a random token and then keep a third set of toke...\"],[\"Integrations with the Hugging Face Hub[[integrations-with-the-hugging-face-hub]]\\n\\n\\u003cCourseFloatingBan...\"],[\"```py\\nimport gradio as gr\\n\\ntitle = \\\"GPT-J-6B\\\"\\ndescription = \\\"Gradio Demo for GPT-J 6B, a transformer...\"],[\"```\\n    \\nThe code above will produce the interface below:\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-gpt-j-6...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-remove-bg-original.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"650\\\" tit...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-Remove-bg.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"550\\\" title=\\\"Gradi...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fine-tuning a model with the Trainer API[[fine-tuning-a-model-with-...\"],[\"raw_datasets = load_dataset(\\\"glue\\\", \\\"mrpc\\\")\\ncheckpoint = \\\"bert-base-uncased\\\"\\ntokenizer = AutoTokeniz...\"],[\"```\\n\\n### Training[[training]]\\n\\nThe first step before we can define our `Trainer` is to define a `Tra...\"],[\"```\\n\\nYou will notice that unlike in [Chapter 2](\\u002fcourse\\u002fchapter2), you get a warning after instantia...\"],[\"```\\n\\nThis will start the fine-tuning (which should take a couple of minutes on a GPU) and report the...\"],[\"```\\n\\n```python out\\n(408, 2) (408,)\\n```\\n\\nThe output of the `predict()` method is another named tuple ...\"],[\"```\\n\\n```python out\\n{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}\\n```\\n\\nThe exact results...\"],[\"```\\n\\nNote that we create a new `TrainingArguments` with its `evaluation_strategy` set to `\\\"epoch\\\"` a...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Tokenizers[[tokenizers]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCourseFloatingBanner ...\"],[\"```\\nJim Henson was a puppeteer\\n```\\n\\nHowever, models can only process numbers, so we need to find a w...\"],[\"```\\n\\nThere are also variations of word tokenizers that have extra rules for punctuation. With this k...\"],[\"\\u003cYoutube id=\\\"ssLq_EK2jLE\\\"\\u002f\\u003e\\n\\nCharacter-based tokenizers split the text into characters, rather than ...\"],[\"## Subword tokenization[[subword-tokenization]]\\n\\n\\u003cYoutube id=\\\"zHvTiHr506c\\\"\\u002f\\u003e\\n\\nSubword tokenization a...\"],[\"### And more![[and-more]]\\n\\nUnsurprisingly, there are many more techniques out there. To name a few:\\n...\"],[\"```\\n\\n{#if fw === 'pt'}\\nSimilar to `AutoModel`, the `AutoTokenizer` class will grab the proper tokeni...\"],[\"```\\n\\nSaving a tokenizer is identical to saving a model:\\n\\n```py\\ntokenizer.save_pretrained(\\\"directory_...\"],[\"```\\n\\nThe output of this method is a list of strings, or tokens:\\n\\n```python out\\n['Using', 'a', 'trans...\"],[\"```\\n\\n```python out\\n'Using a Transformer network is simple'\\n```\\n\\nNote that the `decode` method not on...\"],[\"Building a model card[[building-a-model-card]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={4}\\n    classNames...\"],[\"Let's take a look at what each of these sections should contain.\\n\\n### Model description[[model-descr...\"],[\"### Evaluation results[[evaluation-results]]\\n\\nFinally, provide an indication of how well the model p...\"],[\"```\\n---\\nlanguage: fr\\nlicense: mit\\ndatasets:\\n- oscar\\n---\\n```\\n\\nThis metadata is parsed by the Hugging ...\"],[\"Byte-Pair Encoding tokenization[[byte-pair-encoding-tokenization]]\\n\\n\\u003cCourseFloatingBanner chapter={6...\"],[\"```\\n\\\"hug\\\", \\\"pug\\\", \\\"pun\\\", \\\"bun\\\", \\\"hugs\\\"\\n```\\n\\nThe base vocabulary will then be `[\\\"b\\\", \\\"g\\\", \\\"h\\\", \\\"n\\\", \\\"...\"],[\"```\\n(\\\"hug\\\", 10), (\\\"pug\\\", 5), (\\\"pun\\\", 12), (\\\"bun\\\", 4), (\\\"hugs\\\", 5)\\n```\\n\\nmeaning `\\\"hug\\\"` was present 1...\"],[\"```\\n\\nNow we have some pairs that result in a token longer than two characters: the pair `(\\\"h\\\", \\\"ug\\\")...\"],[\"```\\n\\nAnd we continue like this until we reach the desired vocabulary size.\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Now your tur...\"],[\"```\\n\\nNext, we need to pre-tokenize that corpus into words. Since we are replicating a BPE tokenizer ...\"],[\"```\\n\\nThe next step is to compute the base vocabulary, formed by all the characters used in the corpu...\"],[\"```\\n\\nLet's have a look at a part of this dictionary after the initial splits:\\n\\n```python\\npair_freqs ...\"],[\"```\\n\\nAnd we can have a look at the result of the first merge:\\n\\n```py\\nsplits = merge_pair(\\\"Ġ\\\", \\\"t\\\", s...\"],[\"```\\n\\nAs a result, we've learned 19 merge rules (the initial vocabulary had a size of 31 -- 30 charac...\"],[\"```\\n\\nAnd the vocabulary is composed of the special token, the initial alphabet, and all the results ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n💡 Using `train_new_from_iterator()` on the same corpus won't result in the exact same vo...\"],[\"```\\n\\n```python out\\n['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']\\n```\\n\\n\\u003cTip warning={true}...\"],[\"Live sessions and workshops[[live-sessions-and-workshops]]\\n\\nFor the release of parts 1 and 2 of the ...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cYoutube id=\\\"Ihgk8kGLpIE\\\"\\u002f\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\nFor the second workshop, Merve...\"],[\"sing the Python debugger in a terminal. In this video, we'll learn how to use the Python debugger in...\"],[\"here but you will get the same error with TensorFlow. As we have seen in the \\\"How to debug an error?...\"],[\"command, you are sent to the first instruction of your script. You can run just the next instruction...\"],[\"is p, for print. It allows you to print any value you want. For instance here, we can see the value ...\"],[\"can actually confirm by printing the sizes. No wonder the tokenizer wasn't able to create a tensor w...\"],[\"in the script. It will interrupt the execution and launch the Python debugger at this place, and we ...\"],[\"n this video, we will learn the first things to do when you get an error. Let's say we want to use t...\"],[\"is that Python shows you with a clear arrow the line of code that triggered the error. So you don't ...\"],[\"KeyError we see displayed. Note that Python tells you exactly where the functions it's executing liv...\"],[\"it's telling us it doesn't know the question answering task, and helpfully gives us the list of supp...\"],[\"telling us that we should check our model is a correct model identifier, so let's hop on to hf.co\\u002fmo...\"],[\"Mastering NLP[[mastering-nlp]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={7}\\n    classNames=\\\"absolute z-10 ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Semantic search with FAISS[[semantic-search-with-faiss]]\\n\\n{#if fw =...\"],[\"\\u003cYoutube id=\\\"OATCgQtNX2o\\\"\\u002f\\u003e\\n\\n## Using embeddings for semantic search[[using-embeddings-for-semantic-...\"],[\"```\\n\\n```python out\\nDataset({\\n    features: ['url', 'repository_url', 'labels_url', 'comments_url', '...\"],[\"```\\n\\n```python out\\nDataset({\\n    features: ['url', 'repository_url', 'labels_url', 'comments_url', '...\"],[\"```\\n\\n```python out\\nDataset({\\n    features: ['html_url', 'title', 'comments', 'body'],\\n    num_rows: ...\"],[\"```\\n\\nIf we inspect the first row in this `DataFrame` we can see there are four comments associated w...\"],[\"\\u003ctable border=\\\"1\\\" class=\\\"dataframe\\\" style=\\\"table-layout: fixed; word-wrap:break-word; width: 100%;\\\"\\u003e...\"],[\"\\u003ctd\\u003eHello,\\\\r\\\\nI am trying to run run_glue.py and it gives me this error...\\u003c\\u002ftd\\u003e\\n    \\u003c\\u002ftr\\u003e\\n    \\u003ctr\\u003e\\n ...\"],[\"Great, we can see the rows have been replicated, with the `comments` column containing the individua...\"],[\"```\\n\\n```python out\\nDataset({\\n    features: ['html_url', 'title', 'comments', 'body'],\\n    num_rows: ...\"],[\"```\\n\\nHaving cleaned up our dataset a bit, let's concatenate the issue title, description, and commen...\"],[\"```\\n\\nWe're finally ready to create some embeddings! Let's take a look.\\n\\n## Creating text embeddings[...\"],[\"```\\n\\n{:else}\\n\\n```py\\nfrom transformers import AutoTokenizer, TFAutoModel\\n\\nmodel_ckpt = \\\"sentence-tran...\"],[\"```\\n\\nWe can test the function works by feeding it the first text entry in our corpus and inspecting ...\"],[\"```\\n\\n```python out\\nTensorShape([1, 768])\\n```\\n\\nGreat, we've converted the first entry in our corpus i...\"],[\"```\\n\\nWe can now perform queries on this index by doing a nearest neighbor lookup with the `Dataset.g...\"],[\"```\\n\\nNow we can iterate over the first few rows to see how well our query matched the available comm...\"],[\"```\\n\\n```python out\\n\\\"\\\"\\\"\\nCOMMENT: Requiring online connection is a deal breaker in some cases unfortun...\"],[\"I already note the \\\"freeze\\\" modules option, to prevent local modules updates. It would be a cool fea...\"],[\"```\\n\\u003e\\n\\u003e import datasets\\n\\u003e\\n\\u003e data = datasets.load_dataset(...)\\n\\u003e\\n\\u003e data.save_to_disk(\\u002fYOUR\\u002fDATASET\\u002fDI...\"],[\"```\\n\\nNot bad! Our second hit seems to match the query.\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Try it out!** Create your own qu...\"],[\"he Push to Hub API. Let's have a look at the push_to_hub API. You will need to be logged in with you...\"],[\"you can find it in any Transformers tutorial, or by looking at the videos linked below. What interes...\"],[\"of the output directory as a repository name. You can pick another name by passing it to the hub_mod...\"],[\"start playing with its inference widget while it's training! There is something wrong with the label...\"],[\"repo. Going back to the model page, you can see the Trainer included some metadata that is interpret...\"],[\"not using the Trainer API to fine-tune your model, you can use the push_to_hub method on the model a...\"],[\"done and we can check on the website the model is now showing the proper labels! Now that the model ...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"### 2. What is the advantage of using a generator of lists of texts compared to a list of lists of t...\"],[\"### 4. How does the `token-classification` pipeline handle entities that span over several tokens?\\n\\n...\"],[\"### 6. What is normalization?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It's any cleanup the tokenizer pe...\"],[\"### 8. Select the sentences that apply to the BPE model of tokenization.\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n...\"],[\"### 9. Select the sentences that apply to the WordPiece model of tokenization.\\n\\n\\u003cQuestion\\n\\tchoices={...\"],[\"### 10. Select the sentences that apply to the Unigram model of tokenization.\\n\\n\\u003cQuestion\\n\\tchoices={[...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Training a causal language model from scratch[[training-a-causal-la...\"],[\"{\\u002fif}\\n\\nUp until now, we've mostly been using pretrained models and fine-tuning them for new use case...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-codeparrot-ds.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"300\\\" title=\\\"Gradio...\"],[\"However, training on the full corpus is time- and compute-consuming, and we only need the subset of ...\"],[\"```\\n\\nLet's test it on two examples:\\n\\n```py\\nfilters = [\\\"pandas\\\", \\\"sklearn\\\", \\\"matplotlib\\\", \\\"seaborn\\\"]\\n...\"],[\"```\\n\\n```python out\\n3.26% of data after filtering.\\n```\\n\\nThis leaves us with about 3% of the original ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nPretraining the language model will take a while. We suggest that you first run the trai...\"],[\"```\\n\\nWe can see that the `content` field contains the code that we want our model to train on. Now t...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\n```python out\\nInput IDs length: 34\\nInput chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128...\"],[\"```\\n\\nWe can see that we get 34 segments in total from those two examples. Looking at the chunk lengt...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['input_ids'],\\n        num_r...\"],[\"```\\n\\nWe now have 16.7 million examples with 128 tokens each, which corresponds to about 2.1 billion ...\"],[\"{#if fw === 'pt'}\\n\\n```py\\nfrom transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\\n\\nconfig...\"],[\"```\\n\\nWith that configuration, we can load a new model. Note that this is the first time we don't use...\"],[\"```\\n\\n```python out\\n_________________________________________________________________\\nLayer (type)   ...\"],[\"```\\n\\n{\\u002fif}\\n\\nOur model has 124M parameters that we'll have to tune. Before we can start training, we ...\"],[\"```\\n\\n{:else}\\n\\n```python out\\ninput_ids shape: (5, 128)\\nattention_mask shape: (5, 128)\\nlabels shape: (...\"],[\"```\\n\\n{#if fw === 'pt'}\\n\\nAll that's left to do is configure the training arguments and fire up the `T...\"],[\"```\\n\\nAfter training completes, we can push the model and tokenizer to the Hub:\\n\\n```py\\ntrainer.push_t...\"],[\"```\\n\\n{\\u002fif}\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Try it out!** It only took us about 30 lines of code in addition to the `Tra...\"],[\"```\\n\\n{:else}\\n\\n```py\\nfrom transformers import pipeline\\n\\ncourse_model = TFGPT2LMHeadModel.from_pretrai...\"],[\"```\\n\\nNice, that's the correct answer -- although it then inserts the column `x` again. Since the num...\"],[\"```\\n\\n{#if fw === 'tf'}\\n\\nLooking at these few examples, it seems that the model has learned some of t...\"],[\"\\u003cYoutube id=\\\"Hm8_PgVTFuc\\\"\\u002f\\u003e\\n\\nSince we are mainly interested in sensible autocompletion for the the d...\"],[\"```\\n\\n```python out\\n'Keyword has not single token: testtest'\\n```\\n\\nGreat, that seems to work nicely! W...\"],[\"```\\n\\nBefore we can start training with this awesome new loss function, we need to prepare a few thin...\"],[\"```\\n\\nSince we want to evaluate the model regularly on the validation set during training, let's writ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n🚨 If you're training on a TPU, you'll need to move all the code starting at the cell abo...\"],[\"```\\n\\n```python out\\n'sgugger\\u002fcodeparrot-ds-accelerate'\\n```\\n\\nThen we can clone that repository in a lo...\"],[\"```\\n\\nThose are very high values for loss and perplexity, but that's not surprising as we haven't tra...\"],[\"```py\\nfrom tqdm.notebook import tqdm\\n\\ngradient_accumulation_steps = 8\\neval_steps = 5_000\\n\\nmodel.trai...\"],[\"```\\n\\nAnd that's it -- you now have your own custom training loop for causal language models such as ...\"],[\"ow to ask a question on the Hugging Face forums?\\n\\nIf you have a general question or are looking to d...\"],[\"For this example, we will use the following code,\\n\\nthat produces an error, as we saw in the \\\"What to...\"],[\"hat is domain adaptation? When fine-tuning a pretrained model on a new dataset, the fine-tuned model...\"],[\"the pretrained distilBERT model with the version fine-tuned in chapter 7 of the course (linked below...\"],[\"is another example on a translation task. On top we use a pretrained French\\u002fEnglish model and at the...\"]],\"hovertemplate\":\"source=course\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"course, circle\",\"marker\":{\"color\":\"#FF97FF\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"course, circle\",\"showlegend\":true,\"x\":[-2.2852967,-1.9634644,-2.1562006,-1.2333684,-1.5184546,-1.2749401,1.3366235,1.1962276,1.0361323,1.5892221,1.3938805,0.47739336,-0.9230684,3.2287424,2.296719,1.8849602,-2.1142673,-0.81487864,-0.24847598,-3.4700787,-3.4768696,-3.5308695,-3.498086,-3.3707602,-3.6405988,-3.8214035,-3.6324444,-3.5704606,-3.5515869,-3.6727004,-3.579086,-2.6475677,-4.2340617,-4.6544137,-3.110339,-3.271671,-3.0315356,-5.628107,5.8550906,19.515717,19.55638,19.584251,19.519354,7.044399,6.9801526,7.0041637,7.0555043,7.1504803,7.033373,-5.451792,6.291404,6.0452666,5.4033904,5.176844,2.3556063,4.635969,2.3861182,2.4477654,2.2631407,5.2239637,1.4020014,2.4251733,1.8899723,-2.6042824,1.9360548,-1.9389958,2.0573053,-0.6495171,2.3012419,-0.36261386,-0.35754034,-1.0494066,-1.7582897,-3.6595747,-4.132437,-4.169972,-4.42256,-3.7621367,-4.1717453,-5.9728465,-1.8472581,-2.1589162,-2.9930654,-3.3692021,-3.236039,-0.35457498,0.4635759,0.40728962,0.30644363,0.29867998,0.8259568,11.559057,11.35471,11.792929,11.340416,11.375374,-4.9947515,-4.745739,5.3188977,-2.3615654,3.581021,3.870597,1.5753225,2.0357692,3.5260737,3.46083,3.7620013,4.698356,4.1398816,4.029688,4.6546807,4.2084785,4.587052,4.355902,4.349109,4.2530856,4.5218205,4.3490024,4.481456,4.6078277,-3.621512,5.662055,-3.9918118,-4.948029,-5.3347697,-4.236422,-1.6003788,-3.1526213,-0.11649709,-4.6790075,-2.0053701,-4.02565,-0.2574021,6.335131,6.3117023,-4.8888083,-2.00061,-1.9634067,-2.2793887,-0.86109304,-1.8382207,-0.98485595,-3.128221,-2.5110872,-4.0280685,-1.2707423,-1.2554013,6.4323707,-3.7764897,2.185167,0.35171497,-0.029705787,-0.3013719,-0.08869787,-2.179836,-1.8509026,-0.24109435,-3.9825907,-0.7646362,1.1248038,0.40214837,-4.6502585,-2.7732263,-0.75979626,0.027674265,-1.1111332,-1.3398734,11.794495,11.239717,11.198049,10.417789,11.367309,11.198841,11.071029,10.07859,-3.561015,-3.5196896,-3.3756604,-3.275713,-3.2699034,-3.447258,-3.4774535,-6.8448596,-0.24581777,-1.2645568,-0.9144397,-0.640395,-1.2868744,-1.231298,-1.3048897,-1.0892605,-1.1953727,-1.28762,-1.6374716,-1.8016522,4.868528,5.2261643,5.3203797,3.1616805,2.499554,3.4487355,2.8309066,2.657624,2.9017076,2.719103,-5.0013595,3.4252124,2.6980062,1.4550875,2.6862295,1.495728,2.6484106,2.8487887,3.0695431,-0.12021297,-1.1751992,0.11727051,-1.1560721,-1.0412785,-0.97435975,-0.8524648,-0.89498985,0.027456582,-0.27184758,1.3769355,0.5156612,0.9052477,1.2015727,0.9525792,-6.6749797,-6.3577185,-6.9343657,-7.017697,-5.923795,-6.17774,-6.0902853,-6.7066875,-6.9468236,-6.556209,-1.9797668,-0.9643008,-0.83353406,-0.9481847,-0.0012457371,0.038581323,0.3570495,0.11099808,-3.5696528,-3.3536575,-3.4901571,-3.30589,-3.1583645,-3.4812317,-3.4116275,-2.6484694,-2.3220391,-2.9919338,-2.1939688,-1.9805655,-2.5649095,-3.3620608,4.0428576,-1.7870665,3.679263,3.5361867,3.6213396,4.1955304,-2.2205727,-2.8200374,-2.4279237,-0.9380465,-1.7234766,-1.2929995,-1.7287956,-1.1483737,-1.7615396,-1.2905517,-3.9817617,-2.0228481,-6.307265,-7.242678,-7.105737,-6.3196516,-6.4339275,-6.2307296,-6.275896,-5.6644382,-5.998119,-5.763127,-5.5628877,-5.2644,-4.994543,-3.9948742,-4.690197,-6.3795495,-6.268248,-5.9867716,-3.0801663,3.786089,4.1611776,4.155381,3.6457424,4.2930293,-1.8659143,-3.027976,-2.960663,-1.2570665,-6.255547,-6.2811975,-6.153541,-6.9170356,-6.1942034,-5.804491,-5.5686007,-5.1068015,-2.9725008,-3.2139697,-4.911038,-4.858529,-3.551365,1.3307184,-2.979573,-3.4052517,-2.9214754,-2.5185478,-1.7370764,-0.8405652,-1.7319672,-1.2668782,-1.175903,-0.43453106,-1.38034,-1.1114188,-0.049562566,-0.68061465,0.29686898,-1.2650311,-0.8892104,-0.38229692,-0.33361328,1.4290928,-0.59572387,0.88363576,-0.28365824,1.6852494,-0.8419972,0.10734835,3.4783583,1.6446108,0.3054194,-0.3537235,-2.5152214,-4.919316,-3.3863478,1.9317557,-3.8071868,-4.6649,-5.0137715,-0.5787796,-0.5289749,-0.63986737,-0.6265905,-3.067465,-1.9824873,6.5798235,-1.9515747,-1.455187,-2.0480893,-1.5975169,-1.9040152,6.5463037,-2.0149581,-1.6856346,-0.5801058,-1.3828912,-0.69222474,0.8493491,-1.1222609,-2.1375744,-0.029816985,-1.1732726,-1.284183,-0.81302726,-0.24978909,0.05647364,0.09590516,0.032779768,-0.0039343955,0.16620979,-0.32150176,-0.5024502,1.1936044,0.037395835,0.8491481,-0.54685766,-0.68045306,3.5703661,1.476753,0.42912084,0.2945804,-1.3227564,-0.41945648,5.0333195,1.7562329,1.1214173,2.311275,4.9329095,-2.790791,5.0074697,4.276683,4.97379,-3.6301343,-3.6161377,-3.4663327,-3.5062766,-3.5264015,-3.4891622,-3.4322839,-3.4264414,-3.4913583,-3.4515812,-3.397733,-3.4289644,-3.5149832,2.9078588,4.3899856,4.331384,4.318792,5.191426,4.3278155,4.6829386,4.1895976,1.5931756,4.075228,4.252867,4.0544167,4.286436,3.994176,4.3074617,5.131773,2.6259613,3.4185872,3.4606097,1.611056,3.6311352,3.525057,-3.119126,-3.688867,-3.6866925,-2.5635233,-3.2791893,-2.4205565,-2.8060308,-3.564469,-1.232712,-2.0911849,-1.7548296,-3.4800684,0.8382542,1.1259922,-2.526239,-2.4580545,-2.1187234,-3.179438,-0.7526393,-1.1556481,-0.039215676,-0.6596918,-0.5336019,-1.0525582,-1.0658509,-0.38452315,-4.6832786,-2.6450727,-2.335009,-2.3758812,-2.2198176,-2.1820943,-0.7705945,-1.9778506,-2.5415835,-1.9634799,-2.1963503,-2.0403526,-3.3716178,-1.9353471,11.649663,-5.0318365,-5.806108,-2.8918204,-2.080035,-1.5693475,-1.6438196,-1.8891122,-1.9984791,-1.9359196,-1.5671082,-1.281952,-2.371441,11.404591,12.600718,11.2078705,-0.5467514,11.301299,-3.5876539,-3.6837626,-3.461926,-3.5417254,-3.6684847,-3.5479164,-2.9159536,-1.8069459,-3.3531697,-3.3383238,-2.241112,-2.0659122,0.26763868,0.047437932,-0.115911946,0.09471353,0.52036506,-3.6581297,-3.668686,-3.547198,-3.1146514,-3.4961927,-3.4554389,11.5217905,7.415771,6.5652623,11.273356,11.031061,0.80725694,-0.13972573,11.184671,-3.2961051,-3.9765582,-0.037575606,-4.7541256,-3.2604754,-2.02623,-1.8470458,-1.4930754,-0.9211067,-1.0451694,-1.2805488,-2.083903,-5.0094914,-2.2360687,-2.0425918,-2.14754,-1.4525992,-1.3658237,-0.73555285,-1.4396622,-3.1750124,-3.6440697,-3.633552,-3.3656514,-3.362602,-1.528956,-2.1070797,-2.151255,-0.78773075,-2.0220275,-2.2690868,-1.6436396,-0.48939332,-1.7667168,-1.6124167,-0.9658187,-2.7671196,-0.60363686,-0.7498521,-0.50587785,0.36371985,0.05552289,-0.17004998,0.1688184,0.3848784,0.29902884,0.61612016,-5.0857935,-4.164137,-0.43826008,-3.5555634,-2.4661243,-2.7451062,-3.4975195,-3.8762958,-3.6864755,-2.6957715,-3.5605104,-0.5611098,-0.9550998,0.4126306,-0.86550003,-1.1279199,-2.465629,-1.8666971,-2.1634536,-2.0459816,-3.3299572,-2.1128867,4.3155746,4.285658,3.6037903,4.4776692,4.5481563,4.453159,4.643392,3.6671977,3.3861814,2.8246794,-2.6362047,-3.4938695,0.037276022,-3.8294477,-3.1850352,-1.8458847,-1.3492267,-1.6042129,-2.0844707,-0.8541969,-5.7259097,-2.5030882,-1.3053756,-0.81959265,-1.779727,2.2874582,-3.7224996,-3.7885048,-3.5022132,-3.61382,-3.4536238,-3.5874515,-3.9820063,5.8403854,-3.6317341,-3.609046,-3.62835,-3.2311764,-3.3490279,-3.746186,-3.7072487,-3.5841138,-3.463317,-3.057288,-2.9486876,-3.1928337,-3.4758615,-3.4431267,-3.2662094,-3.466238,-3.3994155,-3.160266,-3.1474943,-3.7577033,-3.5778196,-3.858876,-3.5973277,-3.3778188,-3.69106,-3.7143512,4.354398,4.547091,4.762906,5.082051,4.4397507,5.4290137,-5.5614448,-5.883703,-5.8668356,-6.6146445,-5.4658775,-3.5493317,-2.0848281,2.2279263,1.6030282,3.8548295,3.8608954,4.5272512,4.6622715,-6.9099827,-6.6467943,-6.8511596,-6.8966823,-6.578235,11.660715,11.434516,11.373459,11.437581,9.968329,11.336495,11.255294,11.230821,-4.211907,11.365855,11.04408,11.826797,11.8496065,11.370766,12.031793,7.7279654,2.790684,0.752874,1.021102,1.3225461,4.1786947,2.1508904,1.075671,1.562222,3.3031507,-4.201938,-0.8663365,1.9842838,-2.89196,-3.5635383,0.26218483,2.1571755,1.2625988,-3.4900084,-3.542527,-3.38195,-3.3438513,-3.3344643,-3.182676,-2.3596325,-2.5931592,-1.8280233,-2.6898558,-1.6269602,-2.2785208,-1.4147635,-2.2930176,-2.5117447,-6.7702584,11.500558,-3.0092776,-3.221339,-2.8817797,-2.897249,-3.4785385,-2.9542594,1.2378001,-3.615435,-3.5985773,0.20000173,-3.4384289,-0.74069285,-2.2582006,-1.9035792,-0.73222536,-2.5987377,-0.5837829,-0.24242419,-1.9473454,-2.9382231,-0.7551722,-2.277714,-3.6036692,-3.4552207,-3.5083456,-3.6155977,-3.482738,-3.489503,-3.5625184,-3.3146105,-2.9979784,-2.7270942,-3.4550476,-3.4048586,-3.696593,-5.273991,-1.933779,0.8351641,6.4707956,-1.2063228,-0.6854139,6.1648717,-1.4985342,-1.2741649,-5.8726206,-5.9087725,-5.7423086,-5.725553,-2.829526,-3.3646705,-2.9028988,-1.9047844,-1.2872766,-0.9690712,-0.31332877,0.16457039,-0.6900122,-0.9766794,-1.7265104,-1.8371947,2.0450509,0.5853157,-0.9777178,-0.09380256,-1.6424003,-1.4172525,-0.9780262,0.80720437,2.5946922,-0.14030236,-1.3550471,-0.68034,-1.095942,0.051587045,0.19086537,2.2512262,1.2605838,-0.90369505,1.4562339,0.29838753,0.21692947,-1.9297132,-2.1096964,-2.029549,-2.3222303,-2.9113228,-1.2998035,-1.3920543,-0.6489136,-3.3512354,-3.4972854,-3.3110857,-3.2781653,-3.6265109,-2.7983074,-3.7659647,-2.7458527,-2.3906002,-1.9012252,-2.3208766,-2.3647318,-1.7727728,-4.6084423,-2.5606346,-2.240265,-2.2739084,-2.1941543,-2.1349916,-0.7055579,-4.0765796,5.6289997,-3.8505988,-3.8217463,-3.4328368,-2.310941,-3.2655787,-2.2667327,-2.567123,-1.3222995,-1.3628883,-1.6602569,-3.1235318,-2.3489332,-1.6685791,-0.36032197,-2.3440137,-1.8719302,-0.45650882,-0.640345,-0.38129392,-1.2589253,-1.4973767,-1.1603601,-1.1660968,-0.51484525,-0.61122537,0.10574845,6.3238792,6.54645,6.7672243,6.535974,-1.1354396,-1.3867459,-1.4202265,-1.0550226,-1.6551402,-0.68914443,-1.0441422,0.026986765,-1.5156007,-1.7232792,-1.5431838,-1.1000799,-0.7026742,-0.60854745,-0.7645539,0.1639315,-0.8128473,-0.83489114,-0.6739323,-0.31766394,-1.1236392,0.17681721,-0.27360806,-0.27425477,-0.68685955,-0.23244558,0.06384444,-0.76735705,-0.23529762,0.32640013,-0.058171246,-1.792901,-1.6620283,-0.07392952,-1.5476098,-2.190125,-5.5110745,-2.7636898,-4.374697,-6.606718,-6.6414204,-6.6530547,-6.593537,-6.520811,-6.3587894,-6.3871226,-6.59254,-6.5324492,-6.30949,-6.266249,-6.795508,-3.8610291,-3.7788618,-2.3586957,-2.6674895,-2.6014166,-2.6054091,-0.9553837,-1.1642252,-1.1886572,-1.4465084,-1.0117747,-1.0593603,-1.0466872,-0.5273128,-0.77602655,-3.5301774,-3.291328,-3.035453,-2.8917544,-3.4082365,-3.4985793,-3.0258253,-2.226372,-1.127511,-1.3004574,-1.9220253,-2.8714364,-3.4631085,-3.3147337,2.1945453,2.4477472,2.2583766,2.252886,2.334445,1.5092014,1.2267181,-3.3782783,-3.6404111,3.729768,4.3233395,3.0922275,2.7545161,2.6712005,-5.051616,-2.2546926,-2.2971904,-0.006731892,-0.22883798,-2.4186552,-2.8703747,-1.6838646,-0.8648664,-1.371538,-1.5418802,-1.707754,-1.3594102,-1.4905517,-0.28000814,-0.63972515,-1.200282,-1.300562,-5.340711,-5.3580456,-4.438096,-5.497744,-5.8371034,-5.958531,-6.173036,-6.3672934,6.278135,-4.768715,-4.9924364,-5.0014205,-5.2923813,-5.699296,9.06346,-4.3245378,-4.412864,2.4989026,2.1002207,2.2688203,1.9529886,2.1114008,0.6625471,-0.6137532,0.77048194,1.6920975,2.4811597,2.2682316,-6.119235,-4.6790967,-2.2127118,-2.0845854,-2.2646868,-0.26547462,6.9833293,8.240834,-3.4769769,-3.0876043,-3.490227,-2.655164,-5.240143,-4.763473,-3.9309752,-3.8303335,-6.01247,2.902458,2.96658,2.8040998,2.6782525,2.2615767,2.7532845,2.83836,-2.4091039,-1.6311134,-1.0869465,-0.63128674,-1.4697315,-1.5085163,-3.0744927,-3.5883923,-2.2382731,-3.156386,-2.2822108,-2.2920437,-1.4227467,-0.6638966,-3.8347883,-3.0877254,-3.6599584,-3.752333,-3.8793983,-5.051896,-3.7109313,-3.3927379,-3.4263475,-3.7575579,-3.6103272,-2.6956608,-3.4040406,-3.6256428,11.514176,-3.5411456,-3.4396217,2.3701878,0.70209444,-1.2329528,-1.980263,-3.3104374,-3.3295343,-2.8599863,-3.1848335,-3.1958337,0.07677769,-3.1990154,-3.1272647,3.202513,-3.1687658,-3.1628702,-2.5485895,-3.5186636,-3.7999005,-2.4021974,-2.5657902,-2.6475759,-2.4625134,-2.271472,-0.5843048,-1.3465091,-1.9621536,-1.3546613,-0.588969,-0.37018013,-0.024018213,-1.5701016,-0.52409226,-1.9306554,-1.4825431,-1.4137223,5.505743,4.235703,-1.7191879,6.8586044,6.3588223,1.0888393,5.5035777,4.4025774,5.8176346,5.4380555,5.6723557,-1.6458769,-1.2259477,-0.80518943,-1.1283605,-0.28153262,-0.90465486,-0.26068166,-0.991829,-0.22018705,-5.984453,-3.1481419,-4.748076,-4.907338,0.062286153,0.43212005,0.9710581,-2.3558776,-3.281114,-2.9071963,-2.570312,-2.0310528,-1.8268216,-0.5043459,-0.74803144,-0.95834005,-0.71840155,-0.77050453,1.2188647,-0.46797448,-0.83159673,1.6042639,-0.037324395,0.29334232,0.18183017,-0.4079611,-0.5156955,0.5321251,-0.17746103,1.6644067,1.5692853,-0.6312035,1.0840687,0.6265204,0.6081638,-0.27283314,-0.99381495,-0.03431095,-1.4732221,-1.3112617,-1.7770507,-1.3286653,-0.7517991,-0.83019114,-0.9082265,1.5717638,1.8222632,2.4436777,6.042375,0.47698513,1.0522655,0.6770893,-0.37191695,0.36408156,-0.48506775,0.75161064,-2.098292,-2.593089,-0.17540173,-1.8421638,-1.4040065,-0.31759444,0.4028901,-0.012797902,0.39535102,5.8356795,0.8279874,0.8234274,1.0401895,1.1052252,0.9076373,2.9955122,2.2340589,2.1595192,-4.1321087,-0.09858995,3.1046205,1.1543357,1.226194,2.4137013,2.0957346,-3.2378247,-3.4642205,-3.1911223,-2.7224822,-2.627683,-3.0711694,-5.7148466,0.14014156,-0.14905444,-0.5687844,4.9415517,-1.986933,-2.003054,-2.3231819,-4.213661,-5.08943,-3.1699255,10.912772,11.234349,10.530048,11.052669,11.014211,-1.5917711,-1.050515,0.17846808,-2.3118908,0.09321163,0.24292874,0.13921277,-0.7011523,-3.2620285,-3.285082,-3.6099417,-3.6160455,-3.62152,-3.5066988,-2.2608278,-3.2692904,-2.9634044,-3.115723,3.4089398,-1.9616265,1.1431311,4.856152,-3.5587838,-3.4695716,-3.2355049,-3.3016849,-3.4820104,-3.1686497,-2.2610316,-1.2387658,-1.5905875,-1.5701753,-1.9334589,-3.2246625,-3.0599818,-7.104025,-5.347725,-0.5839781,-0.7613751,1.9116954,-0.24603067,-1.0688038,1.8625036,2.62969,2.1054392,2.1528037,4.65902,0.23835759,-5.4263544,-3.0490897,-4.244328,1.5948302,0.8938244,1.0749192,2.8428059,4.800967,4.2199416,1.8212655,0.63952774,0.09827105,-4.0020113,-2.0196574,-1.7587299,-1.6779144,-0.99890876,0.18644817,2.8785052,2.7734663,2.8688478,-1.6357065,4.2306542,1.7506964,1.8726869,1.4819688,1.6328735,1.5109931,1.5788928,-3.6549885,-3.0084503,-3.6908557,-3.5461195,-3.8333087,-3.8252292,-3.6473074,-2.884512,-3.4124148,11.262343,0.7201527,1.1922934,2.0199833,-1.7730007,-1.7259113,1.3898449,-0.32572854,-0.7372768,0.96559787,-2.3666806,-1.8213179,-0.9075626,-0.4707043,-1.5182425,-0.2833884,0.3274162,0.40931994,-1.346614,-0.31199273,-0.30649632,-0.6868136,-1.4923102,-1.0328667,-0.2539595,0.35217875,0.56937003,2.3263836,-0.23815598,0.37937987,-1.7783674,6.026477,4.719567,-4.373742,-3.8910732,-4.706141],\"xaxis\":\"x\",\"y\":[-4.3159165,-4.3355203,-4.310713,-4.2510424,-3.7473168,2.9460535,-4.621832,-4.6133423,-4.4645095,-4.6115184,-4.6161747,-4.397206,-4.4881773,-3.3675897,-4.065716,-4.439969,-4.586733,-4.58635,-3.2377179,-5.59093,-5.652073,-5.661507,-5.4914675,-5.418911,-5.3749967,-5.4379883,-5.3910766,-5.5171556,-5.3633847,-5.5438724,-5.4155335,-4.300669,-2.4145834,-2.0410452,-4.6124463,-5.324846,-4.766633,1.4535329,-4.7313547,8.139935,8.180212,8.225105,8.135572,-2.0469863,-2.060313,-2.1284323,-1.8248394,-1.9596965,-1.970033,-0.9424648,-1.7069411,-1.5313588,-1.0696542,-1.3722494,-1.2618223,-0.7551037,-0.75755733,-1.9188809,-0.8424211,-2.5274975,-0.6996847,-0.7645039,-0.42099273,-1.3861941,-1.2219062,-3.2304463,-2.4031467,-1.7801383,-3.4513445,-2.5935595,-1.624484,-3.696663,-2.8420217,-0.41363278,-0.9874703,-0.52878433,-1.1851774,-1.0826026,-0.21437739,1.5646994,-3.5958047,-4.2193146,-4.0029206,-4.3285794,-4.4343104,2.5818553,0.71289676,1.7069103,1.6388595,1.9151819,2.7283568,-4.556614,-4.7997026,-5.4198766,-4.8052473,-4.895826,-1.9239676,-2.1054466,-0.63276273,-1.5695325,-0.2850095,-0.1943537,0.33020478,0.1017365,-0.2959999,-0.5132266,-0.6971172,-0.82609695,-0.77089983,-1.1612763,-0.9781358,-0.54401404,-0.6865084,-0.5643907,-0.5251442,-1.0129924,-0.836043,-0.84657663,-1.019533,-1.44088,-5.2608614,-4.7015233,-0.7997994,-1.0827236,-0.7298342,-1.0288353,-0.84027916,-1.4572715,1.8262812,-1.8477411,-3.0844443,-2.3722887,-4.889206,-5.332253,-5.1461167,-2.613529,-4.6542525,-4.3633595,-4.228525,-4.9561567,-4.4521084,-4.5914917,-3.6999807,-4.253271,-3.0778933,-3.8915641,-3.977921,-5.1850586,-3.2788787,-1.9181116,-0.0799359,0.11735969,-0.21453422,0.60279435,-4.4684854,-5.2129364,-1.5905029,-2.692608,-3.6319914,0.40971467,0.84790546,-1.0048126,-1.7250992,-0.80586267,-5.6323557,-0.39037538,-1.2024621,-4.5935774,-5.351624,-5.4466877,-4.8538437,-5.30907,-4.4362726,-5.165597,-4.763961,-5.5387154,-5.5103416,-5.841292,-6.0053773,-6.0305567,-5.747472,-5.876429,2.5278885,-2.0047438,-1.6028608,-2.5103593,-1.6979414,-2.3666844,-1.8749188,-1.3760837,-1.6351725,-2.4049537,-1.6518395,-0.24984182,0.6949245,-2.802125,-1.5107455,-0.6707472,0.11354104,-1.8336766,-3.178815,-4.1133695,-4.4273562,-4.2413673,-4.347986,-1.3916206,-3.39837,-4.171834,0.38490945,-3.8023803,-4.389918,-3.8638372,-3.6728039,-3.4398549,-0.04528968,-1.2420489,-3.5961938,-2.7242827,-1.223634,-0.50159854,1.9564272,0.39187413,-5.5911245,-4.304283,-4.1930246,-4.2062674,-4.3188934,-4.2570257,-4.2704244,-3.4256814,-3.1408374,-3.2135482,-3.2148604,-2.9757907,-3.1681454,-3.124461,-3.1013327,-3.3276753,-3.3355527,-0.52210164,-3.4109519,-3.3847728,-4.433674,-6.4324884,-5.8167734,-6.0464225,-6.515695,-5.7852473,-5.660024,-5.7984295,-5.773139,-5.8002653,-5.919313,-5.7484984,-5.280902,-5.047535,-5.519248,-4.982154,-4.4950266,-5.2751427,-5.2209916,-1.289224,-1.216683,-1.630818,-1.8870181,-2.0072389,-1.3847368,-0.6892138,-0.12073445,-1.4590766,-1.7591984,-0.2979356,-0.7285715,-0.06095955,-0.46790254,-3.4958465,-2.3949363,0.29460233,0.14584804,1.8943834,2.7577758,2.7466204,2.1101973,1.830546,2.1526115,3.0447056,-1.1970385,-1.5446812,-1.381475,-0.54490286,0.49882567,0.17091736,0.111581676,0.039421726,-1.8689182,-3.0748608,-2.9266639,-0.961376,-2.7952108,-2.594543,-2.4441886,-2.9704313,-2.640555,-5.7736397,-4.798223,-3.8423975,-6.925932,-3.040208,-2.8720489,-2.8231807,-3.0599008,-2.9345212,-2.2797081,-2.2699606,-2.7506588,-4.13276,-2.0106442,-1.3978592,-0.36853018,-0.9110374,-4.224866,-3.4745703,-3.2610111,-2.2018156,-4.193244,-3.9210112,-0.46769866,-1.5165418,-2.1953552,-3.6385727,-4.053926,-1.4802765,-7.5321918,-6.971813,-7.670007,-6.5923586,-3.788655,-4.225234,-4.9267125,-0.6084806,0.22666211,-0.4888768,0.49569422,-0.6215809,-0.021872776,-0.08396585,1.1918476,-0.03940088,0.030804945,-0.23462744,-4.1838555,-1.6341118,-1.5962864,-4.672771,-3.7403924,-1.7089278,-0.67794937,-1.5409728,-4.4544125,-4.3505197,-4.5948524,-5.1136694,-3.2794917,-4.02041,-5.227732,-4.3689637,-4.398803,-4.135084,-4.4659047,-4.607232,-5.058102,-4.3771253,-4.702414,-4.1041145,-3.4445744,-4.558997,-4.328712,-1.3728396,-4.220286,-4.0242777,-2.9638536,-2.3691895,-4.496567,-4.755731,-7.4281335,-6.3629346,-6.926734,-0.9776353,0.068180636,-0.77409893,-0.17415473,0.1807027,-2.4315927,-2.7585068,-0.04251567,0.6965801,0.056564223,0.22548172,0.91764116,0.5427867,0.011535894,-4.077762,-0.8252202,-1.7405567,-1.4643984,-2.3834805,-2.3119147,-5.032147,-2.5644386,-0.84171975,-0.5913214,-5.6400824,-5.291734,-5.515257,-5.7080464,-5.697364,-5.713726,-5.779331,-5.7445006,-5.8952703,-5.8392005,-5.671716,-5.7541914,-5.6832128,-3.5354352,-2.8826342,-2.8488646,-2.79703,-2.6205683,-2.8958604,-2.5285354,-2.667077,-4.0321913,-2.9423673,-2.9071574,-3.2928298,-3.0048118,-2.8985124,-2.7880104,-2.2206488,-3.9392185,-3.4112787,-2.8020084,-4.398263,-2.9598954,-3.284238,-3.2811768,-4.319462,-3.9381344,-3.8838336,-4.1810694,-3.2905605,-3.539783,-3.955361,-1.6464733,-2.576416,-2.9768853,-2.323991,-4.298263,-4.228591,-3.9715862,-4.73397,-4.7149835,-3.9775474,-4.1228013,-4.419401,-4.1151514,-4.1966543,-3.494285,-3.9155078,-3.7501826,-2.850217,-0.71239233,-1.6952497,-0.9479255,-1.2584316,-0.93959296,-0.9535055,-0.35184076,-3.2314167,-0.5601321,-0.69659305,-0.73033535,-0.6495542,-1.2166308,-0.7433359,-4.5855823,-1.2054012,1.1343064,-2.201692,-3.57197,-2.854185,-2.811302,-3.277434,-3.874819,-3.7436934,-3.2895942,-2.9718313,-4.179888,-4.4987707,-5.7232327,-4.631748,-2.1508074,-4.7966666,-5.2363544,-5.435979,-5.541381,-5.3576617,-5.080475,-5.132325,-5.0565014,-4.6682086,-5.226067,-5.3974986,0.31673208,-3.5919304,0.3623859,-4.862406,-5.773955,-5.9702044,0.38774586,-4.7437572,-5.211703,-5.362155,-4.464536,-5.1536026,-4.681288,-4.5231137,-3.4258723,-3.653322,-4.583315,-4.1332355,-1.362783,-2.5603304,-4.6112823,-2.3087854,-1.8975846,-4.871893,-1.966164,-0.24926548,-2.6169434,-3.2724724,-3.317901,-3.0408907,-3.6396344,-3.4997144,-4.399895,-1.0962582,-4.555194,-4.492128,-4.312771,-3.202942,-3.4920342,-4.440458,-4.595156,-4.6537204,-5.2675805,-5.5211844,-5.386485,-5.3724537,-4.5982213,-4.717746,-4.569993,-1.8322295,-4.23099,-4.4364395,-2.7477474,-2.519834,-4.374572,-4.558641,-4.367022,-2.5044339,-3.674792,-2.8989182,0.25334197,1.0670882,-5.460929,2.159086,1.3812195,1.7030457,0.9934051,2.3960915,-0.34664035,-0.33874574,-5.5859604,-2.796215,-2.881969,-3.2872283,-1.52245,-2.9592628,-2.944553,-3.793125,-2.842214,-1.887822,-2.039727,-1.3488746,-2.2985919,-2.6702948,-0.7937757,-0.59512866,-0.6797794,-0.7022254,-1.152141,-0.84210265,-0.7410918,-2.1019182,-1.2914985,-0.89001215,-0.8279311,-0.9356807,-0.9345836,-1.5136548,-1.5844992,-1.8596522,-1.4557012,-2.8998578,-6.016605,-3.9195154,-3.5010808,-2.9478474,-3.2693832,-2.8579416,-1.617892,-2.6151438,-2.3026464,-1.8766983,-2.328716,-3.0526109,-3.891084,-3.761216,-5.376438,-5.07494,-5.4636517,-5.4476247,-5.003259,-5.5045524,-5.294849,-4.263452,-5.4735184,-5.5922084,-5.6922355,-4.7062607,-4.773728,-5.2105145,-5.5505576,-5.730909,-5.004589,-5.1593485,-5.12133,-4.5595884,-4.8548203,-5.0316987,-5.0174627,-5.2228312,-4.8328695,-5.380336,-4.898743,-3.5464468,-4.4008904,-3.9565525,-4.335256,-3.8440342,-3.9722126,-4.2883453,-1.180162,-1.1760195,-1.7394433,-1.8602185,-0.86936367,-1.6450616,-2.5627391,-2.5857825,-2.651313,-2.943239,-2.8256042,-2.3206587,-3.2184188,-1.7948612,-0.9181056,-1.9591073,-2.9071841,-2.5269973,-1.7223412,-2.786934,-3.2811034,-3.2525775,-3.3978534,-3.3910098,-4.620045,-4.914463,-4.6461678,-4.8134747,-4.9044256,-4.6326084,-4.9718666,-4.848916,-2.6916966,-4.71011,-4.3920856,-4.503106,-4.595861,-4.3540683,-4.600708,-2.9427812,0.006969301,-0.021346465,0.19328032,0.16604498,-0.46854398,0.034273103,0.4182601,0.16247515,-0.6859948,-2.3774383,-2.249252,-3.5898788,-4.470805,-3.487161,0.09919732,-0.43041486,0.26615384,-5.452685,-5.6619463,-5.651836,-5.4707823,-4.9275656,-2.4064436,-2.7890558,-3.9182143,-2.9339025,-4.07483,-2.6631398,-0.21444699,-0.667998,-0.5179079,-0.5053841,-0.97004914,-4.5289235,-1.8294535,-5.268959,-5.2162623,-5.0750422,-5.1193957,-5.201731,-2.379615,-3.8417292,-3.0299456,-5.6119432,-3.0030496,-5.51305,-2.37127,-2.765365,-3.3052313,-4.594121,-3.5224202,-5.4293575,-4.8855257,-5.2233133,-4.8090773,-4.8083816,-5.659254,-5.547454,-5.701443,-5.5441623,-5.6104164,-5.654894,-5.6696205,-5.8493567,-5.8829236,-6.3008494,-5.656133,-5.974641,-2.2054806,-1.3019888,-5.0702324,-4.5359306,-4.6666007,-4.6270466,-4.7174788,-5.028435,-4.7139144,-4.7115765,-2.29507,-1.8315309,-1.303456,-1.8778343,-3.8208652,-4.0535274,-4.447914,-4.464066,-5.7482915,-7.2197266,-7.317059,-7.176689,-7.1431856,-7.0435543,-4.8363395,-2.3706203,-0.893474,0.30533925,-1.8717613,-6.7494297,-3.762759,-3.304862,-1.3091341,-0.5164628,-0.52514136,0.059579484,-1.5500392,-4.2899475,-3.297919,1.4621205,0.5119349,0.2631152,0.079248376,-4.2968273,0.28293857,-5.2014027,-0.8352902,-4.7552795,-4.510752,-4.7200828,-4.7704077,-5.083717,-3.2685702,-3.811517,-1.4501334,-5.183181,-5.202146,-5.378258,-5.2404737,-4.627821,-4.8208675,-2.4163668,-3.4113576,-4.5499597,-4.4855137,-4.6921077,-4.7335715,-4.640735,-1.9914505,-3.3799698,-4.234813,-4.3524575,-4.4261036,-4.693975,-2.7539892,-1.2389917,-4.181068,-3.037344,-4.0296984,-3.2328858,-4.115435,-4.353336,-1.7155766,-2.1627886,-3.8788133,-3.7441025,-4.4133306,-5.182503,-4.5156355,-4.0412016,-1.7584012,-0.44833758,-1.5148921,-1.7889757,-2.4289198,-2.5557213,-1.5554233,-1.0396997,-1.674289,-1.5968027,-2.4165485,-2.8586974,-4.851267,-5.1797657,-4.625244,-4.07786,-4.5874295,-2.1718788,-0.8659393,-0.056354363,-0.069444545,1.333082,1.8167783,-2.1109014,-4.053172,-1.4040229,-0.5528009,-0.8384847,-0.6451271,-1.2843363,-2.0022619,-2.2033036,-6.1540446,-2.6620817,-2.122289,-2.4169858,-2.3575988,-4.294639,-6.1403475,-1.9740232,-0.42655468,-2.4102445,-4.2756467,0.6692872,1.3104132,-2.0455797,-6.4010863,-2.5536737,-0.8869336,-0.9857171,-5.19254,-1.2756108,-0.46216398,-0.5740953,-2.913261,-3.1719704,-3.3427305,-3.4785342,-3.5096402,-3.406292,-3.4058285,-3.289583,-3.3906047,-3.4421182,-3.414166,-3.166932,-3.2681074,-3.6571548,-2.857391,-2.8929093,-2.7011068,-4.796957,-4.9152937,-4.888181,-7.2317486,-7.1346784,-7.2745705,-7.195232,-7.2934103,-7.287525,-7.137903,-7.0983496,-7.2601504,-5.6226068,-6.130718,-6.061025,-5.992635,-6.0408463,-5.713417,-4.6978893,-5.490036,-5.232186,-5.347723,-5.465931,-5.4295444,-5.2223797,-5.7073655,-4.4278283,-4.3264947,-4.424043,-4.4356756,-4.5113754,-4.485062,-4.4087353,-0.009804878,-3.5604904,-0.13800503,-0.9037261,0.18969913,0.5851598,0.1915401,-1.853513,-4.641037,-4.7717743,-4.488175,-4.765715,-3.793513,-3.9467053,-3.0146296,-7.466767,-7.2692957,-7.0289836,-6.967997,-7.2634816,-7.2394443,-6.982579,-7.3784437,-7.451019,-7.3149247,1.6929861,-0.5961564,-0.12461832,-0.22391367,-0.4256799,2.5689323,2.8543541,4.1024585,-1.4626504,-1.5401653,-1.643349,-1.3051939,-1.8289394,-1.5157628,-4.6283236,-1.5358607,-1.8644568,-4.166369,-4.3204107,-4.446766,-4.3891573,-4.5397635,-4.307255,-4.4016747,-4.8434095,-4.688082,-4.624882,-4.38083,-6.7640686,-2.4488878,-4.627881,-4.8233886,-4.720921,-2.6380746,-2.3817296,-3.1310468,-2.7833683,-3.002316,-2.7003522,-1.5264962,-1.7980448,-2.177021,-2.591504,-2.7849863,-3.0177686,-3.9210882,-3.9164727,-4.1501837,-4.310191,-4.3776274,-4.2847714,-4.070173,-0.23017153,0.45274022,0.028748685,0.42216012,-0.20320937,-0.15450214,1.0311376,-5.2670584,-3.8110754,-4.211517,-2.122969,-2.3429532,-3.8823566,-3.80855,-2.6268466,-3.3562014,-4.0300097,-4.110826,-4.0171695,0.3464753,-5.161954,-4.8956347,-4.3168616,-5.399729,-5.3388176,-4.861375,-4.791658,-6.570788,-4.4475813,-4.8882165,-4.550816,-4.1459317,-4.2038527,-4.433146,-4.292629,-5.1152263,-5.338864,-4.4624557,-4.606019,-5.363394,-2.9035292,-5.2670035,-5.20018,-3.6213713,-4.5737906,-4.214745,-4.1895022,-3.9349303,-3.426082,-0.5014172,-4.763528,-4.8712664,-4.90703,-1.3426472,-3.8200598,-1.1584136,-1.1662349,-0.34428474,0.113137,-0.23142008,-6.1108556,-2.789741,-3.570394,-4.488821,-4.416927,-4.3676805,-1.5313106,-2.0797975,-2.0652492,-2.684132,-3.4445548,-3.1899276,-2.065343,-2.5321674,-2.364767,-2.3685107,-1.6427702,-0.6738256,-1.7225121,-2.1670895,-2.7975523,1.873224,2.4720654,0.6710585,-2.0793297,0.21716978,-2.5815356,-2.2680357,-1.9762989,-0.6358694,-4.2989335,-3.999956,-4.21448,-4.682869,-4.66432,-3.4759438,-4.953015,-4.786739,-4.861885,-4.03613,-0.8223982,-3.6599338,-3.6112683,-1.6244856,-1.0363693,-0.037437107,-0.994458,0.25366288,-6.328309,-7.476504,-7.1274614,-4.980094,-3.9194217,-0.44414684,0.29301482,0.44189486,0.19107231,0.11514537,0.5644988,0.31941846,0.61783886,-4.062009,0.17077497,-5.372267,-1.0416684,-2.0241387,-2.9461672,-2.4425354,-2.722703,-0.57160246,0.30599385,-3.9701612,-3.957122,-4.2581162,-5.0372047,-4.5801153,-4.447695,-4.039448,-4.784559,-4.697375,-4.9670587,-4.355899,-5.2653885,-5.377826,-4.363564,-4.9069157,-4.2493377,-4.3606443,-4.497532,-4.4701333,-3.7831864,-4.7606297,-4.324687,-4.138918,-4.452567,-4.0832224,-4.590301,-3.2027333,-4.353077,-4.363534,-1.304536,-0.24417734,-3.6917217,-4.685979,-4.340633,-4.186109,-4.1784635,-2.439452,-2.4819388,-5.3068495,-5.270214,-5.145326,-5.2752957,0.33130425,-3.7846744,-3.3750527,-2.3836298,-3.675466,-4.282007,-4.4302516,-4.1592035,-3.1024299,-2.141985,-4.455238,-4.182077,-5.1057286,-4.183488,-4.4287167,-4.267208,-0.6398896,-4.1583314,0.29173157,-1.2324357,-5.858854,-6.5963154,-6.4759674,-0.040566392,-3.5780902,-5.09839,-5.54286,-5.683104,-5.6911902,-4.2506924,-2.596055,-4.0505595,-4.6763153,-4.980103,-1.9339151,0.1808692,-5.0039387,-2.034798,-5.4603076,-5.8320317,-6.066296,-5.9795322,-5.779897,-5.005205,-5.338486,-5.415653,-5.311731,-5.381221,-5.35038,-5.2204976,-4.898407,5.0937486,0.3163263,-2.3409111,-2.4090283,-1.511979,-2.4714673,-2.6397505,-1.713325,-1.9343572,-1.5678793,-1.9841807,-2.9775991,-1.4096478,-0.5113728,-1.8142143,-2.1414077,-4.4185076,-4.621218,-4.4292307,-3.698424,-2.696821,-3.1134071,-4.2883143,-4.528684,-4.46374,-2.4891636,-2.5143073,-3.0058174,-3.335049,-3.783819,-4.6784463,-3.8927643,-4.070821,-3.8657825,-3.6623383,-0.33874908,0.18792853,0.09449306,0.12377996,-0.12954873,-0.1615044,-0.05211313,-3.8008747,-4.9728312,-4.359298,-4.2606735,-4.356384,-4.167504,-4.5649114,-1.7160798,-1.5753547,-4.8141017,-4.3536396,-4.3544965,-4.3135023,-1.261948,-4.1904902,-2.6907914,-4.347826,-4.597502,-4.451215,-4.1231604,-2.3372684,-0.52798873,-2.2282498,-3.3512561,-2.52267,0.6440463,-0.10490006,1.2238101,-4.0379057,-4.451393,-0.48394573,-4.204096,-3.3573322,0.061687514,-0.058525812,0.60399914,-0.10359585,0.024680864,0.6711986,-0.80509746,-1.3210028,-2.399072,0.13843127,-0.17549853,-0.29888722],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"Its architecture includes 3 main components:\\n1. [FLAN-UL2](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fflan-ul2), ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nLoRA is very versatile and supported for [DreamBooth](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffuser...\"],[\"```\\n\\nNavigate to the example folder with the training script and install the required dependencies f...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMany of the basic and important parameters are described in the [Text-to-image](text2image#scri...\"],[\"```py\\nlora_attn_procs = {}\\nfor name in unet.attn_processors.keys():\\n    cross_attention_dim = None i...\"],[\"```\\n\\nThe [optimizer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fdd9a5caf61f04d11c0fa9f3947b69ab00...\"],[\"```\\n\\nAside from setting up the LoRA layers, the training script is more or less the same as train_te...\"],[\"accelerate launch --mixed_precision=\\\"fp16\\\"  train_text_to_image_lora.py \\\\\\n  --pretrained_model_name_...\"],[\"```\\n\\nOnce training has been completed, you can use your model for inference:\\n\\n```py\\nfrom diffusers i...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Text-to-audio (TTA) system has recently gained attention for its a...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"For a more technical overview of LCMs, refer to [the paper](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2310.04378...\"],[\"Before going through this guide, we'll take a look at the general workflow for performing inference ...\"],[\"```\\n\\n## Text-to-image\\n\\nYou'll use the [`StableDiffusionXLPipeline`] with the scheduler: [`LCMSchedul...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npi...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# Combine LoRAs\\npipe.set_adapters([\\\"lcm\\\", \\\"papercut\\\"], adapter_weights=[1.0, 0.8])\\n\\nprompt = \\\"paperc...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npi...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# load adapter\\nadapter = T2IAdapter.from_pretrained(\\\"TencentARC\\u002ft2i-adapter-canny-sdxl-1.0\\\", torch_d...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"pipe.set_adapters([\\\"lcm\\\", \\\"motion-lora\\\"], adapter_weights=[0.55, 1.2])\\n\\nprompt = \\\"best quality, mast...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"Latent Consistency Distillation Example:\\n\\n[Latent Consistency Models (LCMs)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2...\"],[\"```\\n\\nWhen running `accelerate config`, if we specify torch compile mode to True there can be dramati...\"],[\"```bash\\nexport MODEL_NAME=\\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsave...\"],[\"```\\n\\n## LCM-LoRA\\n\\nInstead of fine-tuning the full model, we can also just train a LoRA that can be i...\"],[\"```bash\\nexport MODEL_NAME=\\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsave...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*There is large consent that successful training of deep networks r...\"],[\"Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](http...\"],[\"## Available Pipelines:\\n\\n| Pipeline | Tasks | Colab\\n|---|---|:---:|\\n| [pipeline_stable_diffusion.py]...\"],[\"## Examples:\\n\\n### Using Stable Diffusion without being logged into the Hub.\\n\\nIf you want to download...\"],[\"```\\n\\nThis however can make it difficult to build applications on top of `diffusers` as you will alwa...\"],[\"```\\n\\n### Text-to-Image with K-LMS scheduler\\n\\n```python\\n# make sure you're logged in with `huggingfac...\"],[\"```\\n\\n### CycleDiffusion using Stable Diffusion and DDIM scheduler\\n\\n```python\\nimport requests\\nimport ...\"],[\"image.save(\\\"horse_to_elephant.png\\\")\\n\\n# let's try another example\\n# See more samples at the original ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"The original codebase can be found at [ai-forever\\u002fKandinsky-2](https:\\u002f\\u002fgithub.com\\u002fai-forever\\u002fKandins...\"],[\"## KandinskyV22Img2ImgPipeline\\n\\n[[autodoc]] KandinskyV22Img2ImgPipeline\\n\\t- all\\n\\t- __call__\\n\\n## Kandi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about MultiDiffusion on the [project page](https:\\u002f\\u002fmultidiffusio...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"The original codebase can be found at [openai\\u002fshap-e](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002fshap-e).\\n\\n\\u003cTip\\u003e\\n\\nSee...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"- **Self-contained**: An example script shall only depend on \\\"pip-install-able\\\" Python packages that...\"],[\"We provide **official** examples that cover the most popular tasks of diffusion models.\\n*Official* e...\"],[\"Training examples show how to pretrain or fine-tune diffusion models for a variety of tasks. Current...\"],[\"## Community\\n\\nIn addition, we provide **community** examples, which are examples added and maintaine...\"],[\"```\\nThen cd in the example folder of your choice and run\\n```bash\\npip install -r requirements.txt\\n```...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Creating noise from data is easy; creating data from noise is gene...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe `apply_patch` function exposes a number of [arguments](https:\\u002f\\u002fgithub.com\\u002fdbolya\\u002ftomesd#usa...\"],[\"```bash\\n- `diffusers` version: 0.15.1\\n- Python version: 3.8.16\\n- PyTorch version (GPU?): 1.13.1+cu11...\"],[\"```\\n\\nTo reproduce this benchmark, feel free to use this [script](https:\\u002f\\u002fgist.github.com\\u002fsayakpaul\\u002f2...\"],[\"| **GPU**  | **Resolution** | **Batch size** | **Vanilla** | **ToMe**       | **ToMe + xFormers** |\\n...\"],[\"| **V100** |            512 |             10 |         OOM |          10.03 |                9.29 |\\n...\"],[\"As seen in the tables above, the speed-up from `tomesd` becomes more pronounced for larger image res...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about InstructPix2Pix on the [project page](https:\\u002f\\u002fwww.timothyb...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThis tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline cla...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"response = requests.get(url)\\nimage = Image.open(BytesIO(response.content)).convert(\\\"RGB\\\")\\nimage.thum...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n## Use multiple pipelines\\n\\nFor some workflows or if you're loading many pipelines, it is more m...\"],[\"```\\n\\nIf you passed an optional argument - like disabling the safety checker - to the original pipeli...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Free-form inpainting is the task of adding new content to an image...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelin...\"],[\"```\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002foptimum\\u002fdocument...\"],[\"--\\n{{ card_data }}\\n---\\n\\n\\u003c!-- This model card has been generated automatically according to the infor...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about Attend-and-Excite on the [project page](https:\\u002f\\u002fattendande...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*The incredible generative ability of large-scale text-...\"],[\"## Usage example with the base model of StableDiffusion-1.4\\u002f1.5\\n\\nIn the following we give a simple e...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002ft2i-adapter\\u002fcolor_ref...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002ft2i-adapter\\u002fcolor_out...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fAdapter\\u002ft2iadapter\\u002fresolve\\u002fmain\\u002fsketch.png)\\n\\nThen, create the ada...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fAdapter\\u002ft2iadapter\\u002fresolve\\u002fmain\\u002fsketch_output.png)\\n\\n## Available ...\"],[\"| Model Name | Control Image Overview| Control Image Example | Generated Image Example |\\n|---|---|--...\"],[\"|[TencentARC\\u002ft2iadapter_canny_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_canny_sd14v1)\\u003cbr\\u002f...\"],[\"|[TencentARC\\u002ft2iadapter_sketch_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_sketch_sd14v1)\\u003cb...\"],[\"|[TencentARC\\u002ft2iadapter_depth_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_depth_sd14v1)\\u003cbr\\u002f...\"],[\"|[TencentARC\\u002ft2iadapter_openpose_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_openpose_sd14v...\"],[\"|[TencentARC\\u002ft2iadapter_keypose_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_keypose_sd14v1)...\"],[\"|[TencentARC\\u002ft2iadapter_seg_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_seg_sd14v1)\\u003cbr\\u002f\\u003e*Tr...\"],[\"|[TencentARC\\u002ft2iadapter_zoedepth_sd15v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_zoedepth_sd15v...\"],[\"## Combining multiple adapters\\n\\n[`MultiAdapter`] can be used for applying multiple conditionings at ...\"],[\"```\\n\\nThe two control images look as such:\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-ima...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002ft2i-adapter\\u002fkeypose_d...\"],[\"Adapt a model to a new task\\n\\nMany diffusion systems share the same components, allowing you to adapt...\"],[\"```\\n\\nTo adapt your text-to-image model for inpainting, you'll need to change the number of `in_chann...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## StableDiffusionLatentUpscalePipeline\\n\\n[[autodoc]] StableDiffusionLatentUpscalePipeline\\n\\t-...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Text-to-3D\\n\\nTo generate a gif of a 3D object, pass a text prompt to the [`ShapEPipeline`]. T...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"prompt = \\\"A cheeseburger, white background\\\"\\n\\nimage_embeds, negative_image_embeds = prior_pipeline(pr...\"],[\"```\\n\\nPass the cheeseburger to the [`ShapEImg2ImgPipeline`] to generate a 3D representation of it.\\n\\n`...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nUse the [`~utils.export_to_ply`] function to save the mesh output as a `ply` file:\\n\\n\\u003cTip\\u003e\\n\\nYou ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*With the advance of text-to-image models (e.g., Stable...\"],[\"## Usage example\\n\\nAnimateDiff works with a MotionAdapter checkpoint and a Stable Diffusion model che...\"],[\"```\\n\\nHere are some sample outputs:\\n\\n\\u003ctable\\u003e\\n    \\u003ctr\\u003e\\n        \\u003ctd\\u003e\\u003ccenter\\u003e\\n        masterpiece, bestq...\"],[\"scheduler = DDIMScheduler.from_pretrained(\\n    model_id, subfolder=\\\"scheduler\\\", clip_sample=False, t...\"],[\"```\\n\\n\\u003ctable\\u003e\\n    \\u003ctr\\u003e\\n        \\u003ctd\\u003e\\u003ccenter\\u003e\\n        masterpiece, bestquality, sunset.\\n        \\u003cbr\\u003e\\n  ...\"],[\"```\\n\\nThen you can use the following code to combine Motion LoRAs.\\n\\n```python\\nimport torch\\nfrom diffu...\"],[\"```\\n\\n\\u003ctable\\u003e\\n    \\u003ctr\\u003e\\n        \\u003ctd\\u003e\\u003ccenter\\u003e\\n        masterpiece, bestquality, sunset.\\n        \\u003cbr\\u003e\\n  ...\"],[\"!--Copyright 2023 The GLIGEN Authors and The HuggingFace Team. All rights reserved.\\n\\nLicensed under ...\"],[\"The abstract from the [paper](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2301.07093) is:\\n\\n*Large-scale text-to-im...\"],[\"## StableDiffusionGLIGENPipeline\\n\\n[[autodoc]] StableDiffusionGLIGENPipeline\\n\\t- all\\n\\t- __call__\\n\\t- en...\"],[\"Würstchen text-to-image fine-tuning\\n\\n## Running locally with PyTorch\\n\\nBefore running the scripts, ma...\"],[\"```\\n\\n## Prior training\\n\\nYou can fine-tune the Würstchen prior model with the `train_text_to_image_pr...\"],[\"```\\n\\u003c!-- accelerate_snippet_end --\\u003e\\n\\n## Training with LoRA\\n\\nLow-Rank Adaption of Large Language Mode...\"],[\"```bash\\nexport DATASET_NAME=\\\"lambdalabs\\u002fpokemon-blip-captions\\\"\\n\\naccelerate launch train_text_to_imag...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about Self-Attention Guidance on the [project page](https:\\u002f\\u002fku-c...\"],[\"# [Deprecated] Multi Token Textual Inversion\\n\\n**IMPORTART: This research project is deprecated. Mult...\"],[\"Colab for inference\\n[![Open In Colab](https:\\u002f\\u002fcolab.research.google.com\\u002fassets\\u002fcolab-badge.svg)](htt...\"],[\"```\\n\\nThen cd in the example folder  and run\\n```bash\\npip install -r requirements.txt\\n```\\n\\nAnd initial...\"],[\"```\\n\\nIf you have already cloned the repo, then you won't need to go through these steps.\\n\\n\\u003cbr\\u003e\\n\\nNow ...\"],[\"```\\n\\nA full training run takes ~1 hour on one V100 GPU.\\n\\n### Inference\\n\\nOnce you have trained a mode...\"],[\"```\\nIt should be at least 70% faster than the PyTorch script with the same configuration.\\n\\n### Train...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase can be found at [CompVis\\u002flatent-diffusion](https:\\u002f\\u002fgithub.com\\u002fCompVis\\u002flatent-d...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\nNext, load a LoRA checkpoint with the [`~diffusers.loaders.StableDiffusionXLLoraLoaderMixin.lo...\"],[\"```\\n\\nLet's now generate an image with the second adapter and check the result:\\n\\n```python\\nprompt = \\\"...\"],[\"```\\n\\nNow that we have set these two adapters, let's generate an image from the combined adapters!\\n\\n\\u003c...\"],[\"```\\n\\n![toy-face-pixel-art](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002f...\"],[\"```\\n\\n![no-lora](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffus...\"],[\"```\\n\\n## Fusing adapters into the model\\n\\nYou can use PEFT to easily fuse\\u002funfuse multiple adapters dir...\"],[\"Consistency Decoder\\n\\nConsistency decoder can be used to decode the latents from the denoising UNet i...\"],[\"# Amused training\\n\\nAmused can be finetuned on simple datasets relatively cheaply and quickly. Using ...\"],[\"#### Full finetuning\\n\\nBatch size: 8, Learning rate: 1e-4, Gives decent results in 750-1000 steps\\n\\n| ...\"],[\"```\\n\\n#### Full finetuning + 8 bit adam\\n\\nNote that this training config keeps the batch size low and ...\"],[\"```sh\\naccelerate launch train_amused.py \\\\\\n    --output_dir \\u003coutput path\\u003e \\\\\\n    --train_batch_size \\u003cb...\"],[\"```\\n\\n#### Full finetuning + lora\\n\\nBatch size: 16, Learning rate: 8e-4, Gives decent results in 1000-...\"],[\"```\\n\\n### Finetuning the 512 checkpoint\\n\\nThese examples finetune on this [minecraft](https:\\u002f\\u002fhuggingf...\"],[\"```sh\\naccelerate launch train_amused.py \\\\\\n    --output_dir \\u003coutput path\\u003e \\\\\\n    --train_batch_size \\u003cb...\"],[\"```\\n\\n#### Full finetuning + 8 bit adam\\n\\nBatch size: 8, Learning rate: 5e-6, Gives decent results in ...\"],[\"```\\n\\n#### Full finetuning + lora \\n\\nBatch size: 8, Learning rate: 1e-4, Gives decent results in 500-1...\"],[\"```\\n\\n### Styledrop\\n\\n[Styledrop](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2306.00983) is an efficient finetuning method ...\"],[\"```\\n\\n#### 256\\n\\nExample results:\\n\\n![glowing_256_1](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-ima...\"],[\"```\\n\\n#### 512\\n\\nExample results:\\n\\n![glowing_512_1](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-ima...\"],[\"# Diffusers examples with Intel optimizations\\n\\n**This research project is not actively maintained by...\"],[\"🧨 Diffusers Experimental\\n\\nWe are adding experimental code to support novel applications and usages o...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\n```bash\\ncd examples\\u002fdreambooth\\npip install -r requirements_fl...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nSome basic and important parameters to know and specify are:\\n\\n- `--pretrained_model_name_or_pat...\"],[\"```\\n\\n### Prior preservation loss\\n\\nPrior preservation loss is a method that uses a model's own genera...\"],[\"```\\n\\n## Training script\\n\\nDreamBooth comes with its own dataset classes:\\n\\n- [`DreamBoothDataset`](htt...\"],[\"```\\n\\nNext is the [`main()`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f072e00897a7cf4302c347a63ec...\"],[\"if model_has_vae(args):\\n    vae = AutoencoderKL.from_pretrained(\\n        args.pretrained_model_name_...\"],[\"```\\n\\nThen, it's time to [create the training dataset](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f...\"],[\"```\\n\\nLastly, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f072e00897a7cf4302c347...\"],[\"```\\n\\nOne more thing before you launch the script! Depending on the GPU you have, you may need to ena...\"],[\"```\\n\\nDuring configuration, confirm that you want to use DeepSpeed. Now it should be possible to trai...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\n```bash\\nexport MODEL_NAME=\\\"duongna\\u002fstable-diffusion-v1-4-flax...\"],[\"```\\n\\n\\u003c\\u002fTip\\u003e\\n\\n\\u003chfoptions id=\\\"training-inference\\\"\\u003e\\n\\u003chfoption id=\\\"PyTorch\\\"\\u003e\\n\\n```py\\nfrom diffusers impor...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## LoRA\\n\\nLoRA is a training technique for significantly reducing the ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n2. Set the number of timesteps to run the denoising process for:\\n\\n```py\\n\\u003e\\u003e\\u003e scheduler.set_times...\"],[\"```\\n\\n5. Now write a loop to iterate over the timesteps. At each timestep, the model does a [`UNet2DM...\"],[\"```\\n\\nIn the next section, you'll put your skills to the test and breakdown the more complex Stable D...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nNow that you know what you need for the Stable Diffusion pipeline, load all these components...\"],[\"```\\n\\nInstead of the default [`PNDMScheduler`], exchange it for the [`UniPCMultistepScheduler`] to se...\"],[\"```\\n\\nTokenize the text and generate the embeddings from the prompt:\\n\\n```py\\n\\u003e\\u003e\\u003e text_input = tokenize...\"],[\"```\\n\\n### Create random noise\\n\\nNext, generate some initial random noise as a starting point for the d...\"],[\"```\\n\\nThe last step is to create the denoising loop that'll progressively transform the pure noise in...\"],[\"```\\n\\nLastly, convert the image to a `PIL.Image` to see your generated image!\\n\\n```py\\n\\u003e\\u003e\\u003e image = (ima...\"],[\"Overview\\n\\nThese examples show how to run [Diffuser](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2205.09991) in Diffusers. ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"There are two options for converting a `.ckpt` file: use a Space to convert the checkpoint or conver...\"],[\"```\\n\\nTo use the script:\\n\\n1. Git clone the repository containing the `.ckpt` file you want to convert...\"],[\"```\\n\\n## Keras .pb or .h5\\n\\n\\u003cTip warning={true}\\u003e\\n\\n🧪 This is an experimental feature. Only Stable Diffu...\"],[\"The Convert KerasCV Space allows you to input the following:\\n\\n* Your Hugging Face token.\\n* Paths to ...\"],[\"```\\n\\nThen, you can generate an image like:\\n\\n```py\\nfrom diffusers import DiffusionPipeline\\n\\npipeline ...\"],[\"```\\n\\nLoad the LoRA checkpoint into the pipeline with the [`~loaders.LoraLoaderMixin.load_lora_weight...\"],[\"InstructPix2Pix SDXL training example\\n\\n***This is based on the original InstructPix2Pix training exa...\"],[\"### Toy example\\n\\nAs mentioned before, we'll use a [small toy dataset](https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\nNow, we can launch training:\\n\\n```bash\\naccelerate launch train_instruct_pix2pix_sdxl.py \\\\\\n    --...\"],[\"```\\n\\nAdditionally, we support performing validation inference to monitor training progress\\nwith Weig...\"],[\"```\\n\\n We recommend this type of validation as it can be useful for model debugging. Note that you ne...\"],[\"```bash \\naccelerate launch --mixed_precision=\\\"fp16\\\" --multi_gpu train_instruct_pix2pix_sdxl.py \\\\\\n   ...\"],[\"```\\n\\n ## Inference\\n\\n Once training is complete, we can perform inference:\\n\\n ```python\\nimport PIL\\nimp...\"],[\"```\\n\\nWe encourage you to play with the following three parameters to control\\nspeed and quality durin...\"],[\"accelerate launch train_instruct_pix2pix.py \\\\\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\\\\n    ...\"],[\"```\\n\\nWe discovered that compared to training with SD-1.5 as the pretrained model, SDXL-0.9 results i...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Here's the overview from the [project page](https:\\u002f\\u002fvislearn.github.io\\u002fControlNet-XS\\u002f):\\n\\n*With incre...\"],[\"## StableDiffusionXLControlNetXSPipeline\\n[[autodoc]] StableDiffusionXLControlNetXSPipeline\\n\\t- all\\n\\t-...\"],[\"ControlNet training example\\n\\n[Adding Conditional Control to Text-to-Image Diffusion Models](https:\\u002f\\u002f...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell e.g. a notebook\\n\\n```python\\nfrom acc...\"],[\"```\\n\\n\\n```bash\\nexport MODEL_DIR=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport OUTPUT_DIR=\\\"path to save mode...\"],[\"```\\n\\n## Training with multiple GPUs\\n\\n`accelerate` allows for seamless multi-GPU training. Follow the...\"],[\"```\\n\\n## Example results\\n\\n#### After 300 steps with batch size 8\\n\\n| |  | \\n|-------------------|:-----...\"],[\"#### After 6000 steps with batch size 8:\\n\\n| |  | \\n|-------------------|:-------------------------:|\\n...\"],[\"```bash\\nexport MODEL_DIR=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport OUTPUT_DIR=\\\"path to save model\\\"\\n\\nac...\"],[\"```\\n\\n## Training on a 12 GB GPU\\n\\nOptimizations:\\n- Gradient checkpointing\\n- bitsandbyte's 8-bit optim...\"],[\"```\\n\\nWhen using `enable_xformers_memory_efficient_attention`, please make sure to install `xformers`...\"],[\"```\\n\\nSee [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002faccelerate\\u002fusage_guides\\u002fdeepspeed) for more Dee...\"],[\"```\\n\\n## Performing inference with the trained ControlNet\\n\\nThe trained model can be run the same as t...\"],[\"```\\n\\n## Training with Flax\\u002fJAX\\n\\nFor faster training on TPUs and GPUs you can leverage the flax train...\"],[\"```\\n\\nIf you want to use Weights and Biases logging, you should also install `wandb` now\\n\\n```bash\\npip...\"],[\"```\\n\\nAnd finally start the training\\n\\n```bash\\npython3 train_controlnet_flax.py \\\\\\n --pretrained_model_...\"],[\"```\\n\\nSince we passed the `--push_to_hub` flag, it will automatically create a model repo under your ...\"],[\"```\\n\\nNote, however, that the performance of the TPUs might get bottlenecked as streaming with `datas...\"],[\"```\\n\\nWe support training with the Min-SNR weighting strategy proposed in [Efficient Diffusion Traini...\"],[\"```\\n\\nThe profile can then be inspected at http:\\u002f\\u002flocalhost:6006\\u002f#profile\\n\\nSometimes you'll get versi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n2. Pass a prompt to the pipeline to generate an image:\\n\\n```py\\nimage = pipeline(\\n\\t\\\"stained glass...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\t\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocume...\"],[\"```\\n\\n### Stable Diffusion XL\\n\\nSDXL is a much larger version of the previous Stable Diffusion models,...\"],[\"```\\n\\n### ControlNet\\n\\nControlNet models are auxiliary models or adapters that are finetuned on top of...\"],[\"```\\n\\nPass the `controlnet` to the [`AutoPipelineForText2Image`], and provide the prompt and pose est...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"## Configure pipeline parameters\\n\\nThere are a number of parameters that can be configured in the pip...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\t\\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline = AutoPipelineForText2I...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline = AutoPipelineForText2I...\"],[\"```\\n\\n## Control image generation\\n\\nThere are several ways to exert more control over how an image is ...\"],[\"```\\n\\n### ControlNet\\n\\nAs you saw in the [ControlNet](#controlnet) section, these models offer a more ...\"],[\"```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline = AutoPipelineForText2I...\"],[\"```\\n\\nFor more tips on how to optimize your code to save memory and speed up inference, read the [Mem...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"For more details about how Stable Diffusion 2 works and how it differs from the original Stable Diff...\"],[\"Stable Diffusion 2 is available for tasks like text-to-image, inpainting, super-resolution, and dept...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Stable Diffusion [Tips](overview#tips) section to learn how to exp...\"],[\"```\\n\\n## Inpainting\\n\\n```py\\nimport torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepSc...\"],[\"```\\n\\n## Super-resolution\\n\\n```py\\nfrom diffusers import StableDiffusionUpscalePipeline\\nfrom diffusers....\"],[\"```\\n\\n## Depth-to-image\\n\\n```py\\nimport torch\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\nfr...\"],[\"# Textual Inversion fine-tuning example\\n\\n[Textual inversion](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.01618) is a ...\"],[\"```\\n\\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. Y...\"],[\"```\\nThe above is a simple distributed training usage on 2 nodes with 2 processes on each node. Add t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have 🤗 Datasets installed to load and preprocess image datas...\"],[\"```\\n\\nWe encourage you to share your model with the community, and in order to do that, you'll need t...\"],[\"```\\n\\n## Training configuration\\n\\nFor convenience, create a `TrainingConfig` class containing the trai...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n💡 You can find additional datasets from the [HugGan Community Event](https:\\u002f\\u002fhuggingface...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nFeel free to visualize the images again to confirm that they've been resized. Now you're ready ...\"],[\"```\\n\\n## Create a UNet2DModel\\n\\nPretrained models in 🧨 Diffusers are easily created from their model c...\"],[\"```\\n\\nIt is often a good idea to quickly check the sample image shape matches the model output shape:...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nThen, you'll need a way to evaluate the model. For evaluation, you can use the [`DDPMPipeline`]...\"],[\"```\\n\\nNow you can wrap all these components together in a training loop with 🤗 Accelerate for easy Te...\"],[\"...     # Prepare everything\\n...     # There is no specific order to remember, you just need to unpa...\"],[\"...                 accelerator.clip_grad_norm_(model.parameters(), 1.0)\\n...                 optimiz...\"],[\"```\\n\\nPhew, that was quite a bit of code! But you're finally ready to launch the training with 🤗 Acce...\"],[\"ConsistencyDecoderScheduler\\n\\nThis scheduler is a part of the [`ConsistencyDecoderPipeline`] and was ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The callback function should have the following arguments:\\n\\n* `pipe` (or the pipeline instance) prov...\"],[\"```\\n\\nNow, you can pass the callback function to the `callback_on_step_end` parameter and the `prompt...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to...\"],[\"- **Encouraging safety in deployment**\\n\\n  - [**Safe Stable Diffusion**](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002f...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nNow pass your prompt to the pipeline. You can also pass a `negative_prompt` to prevent certain ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Stable Diffusion text-to-image fine-tuning\\n\\nThe `train_text_to_image.py` script shows how to fine-tu...\"],[\"```\\n\\nAnd initialize an [🤗Accelerate](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002faccelerate\\u002f) environment with:\\n\\n...\"],[\"```\\n\\nIf you have already cloned the repo, then you won't need to go through these steps.\\n\\n\\u003cbr\\u003e\\n\\n####...\"],[\"```\\n\\u003c!-- accelerate_snippet_end --\\u003e\\n\\n\\nTo run on your own training files prepare the dataset accordin...\"],[\"```\\n\\nCheckpoints only save the unet, so to run inference from a checkpoint, just load the unet\\n\\n```p...\"],[\"```\\n\\n\\n#### Training with Min-SNR weighting\\n\\nWe support training with the Min-SNR weighting strategy ...\"],[\"In a nutshell, LoRA allows adapting pretrained models by adding pairs of rank-decomposition matrices...\"],[\"**___Note: It is quite useful to monitor the training progress by regularly generating sample images...\"],[\"```\\n\\nFor this example we want to directly store the trained LoRA embeddings on the Hub, so\\nwe need t...\"],[\"```\\n\\nThe above command will also run inference as fine-tuning progresses and log the results to Weig...\"],[\"```python\\nfrom diffusers import StableDiffusionPipeline\\nimport torch\\n\\nmodel_path = \\\"sayakpaul\\u002fsd-mod...\"],[\"```\\n\\nIf you are loading the LoRA parameters from the Hub and if the Hub repository has\\na `base_model...\"],[\"```\\n\\n```bash\\nexport MODEL_NAME=\\\"duongna\\u002fstable-diffusion-v1-4-flax\\\"\\nexport DATASET_NAME=\\\"lambdalabs\\u002f...\"],[\"```\\n\\n### Training with xFormers:\\n\\nYou can enable memory efficient attention by [installing xFormers]...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Diffusers examples with ONNXRuntime optimizations\\n\\n**This research project is not actively maintai...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The Intel Labs Team Authors and HuggingFace Team. All rights reserved.\\n\\nLicensed u...\"],[\"The abstract from the paper is:\\n\\n*This research paper proposes a Latent Diffusion Model for 3D (LDM3...\"],[\"# Upscaler\\n\\n[LDM3D-VR](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2311.03226.pdf) is an extended version of LDM3D. \\n\\nThe ...\"],[\"# Training examples\\n\\nCreating a training image set is [described in a different document](https:\\u002f\\u002fhu...\"],[\"Models\\n\\nFor more detail on the models, please refer to the [docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffus...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Diffusion models have significantly advanced the fields of image, ...\"],[\"!--Copyright 2023 Custom Diffusion authors The HuggingFace Team. All rights reserved.\\n\\nLicensed unde...\"],[\"```\\n\\nNavigate to the example folder with the training script and install the required dependencies:\\n...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMany of the basic parameters are described in the [DreamBooth](dreambooth#script-parameters) tr...\"],[\"```\\n\\nTo enable regularization, add the following parameters:\\n\\n- `--with_prior_preservation`: whether...\"],[\"```\\n\\n## Training script\\n\\n\\u003cTip\\u003e\\n\\nA lot of the code in the Custom Diffusion training script is similar...\"],[\"```py\\nparams_to_freeze = itertools.chain(\\n    text_encoder.text_model.encoder.parameters(),\\n    text...\"],[\"```\\n\\nNow you'll need to add the [Custom Diffusion weights](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002f...\"],[\"```py\\nst = unet.state_dict()\\nfor name, _ in unet.attn_processors.items():\\n    cross_attention_dim = ...\"],[\"weights[\\\"to_out_custom_diffusion.0.bias\\\"] = st[layer_name + \\\".to_out.0.bias\\\"]\\n    if cross_attention...\"],[\"```\\n\\nThe [optimizer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f84cd9e8d01adb47f046b1ee449fc76a0c...\"],[\"```\\n\\nIn the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f84cd9e8d01adb47f046b1ee449...\"],[\"```\\n\\n## Launch the script\\n\\nOnce you’ve made all your changes or you’re okay with the default configu...\"],[\"```bash\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\"\\nexport OUTPUT_DIR=\\\"path-to-save-model\\\"\\nexp...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"multiple concepts\\\"\\u003e\\n\\nCustom Diffusion can also learn multiple concept...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\nOnce training is finished, you can use your new Custom Diffusion mode...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## Next steps\\n\\nCongratulations on training a model with Custom Diffus...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"The original codebase can be found at [ai-forever\\u002fKandinsky-2](https:\\u002f\\u002fgithub.com\\u002fai-forever\\u002fKandins...\"],[\"[[autodoc]] KandinskyInpaintPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyInpaintCombinedPipeline\\n\\n[[auto...\"],[\"Stable Diffusion text-to-image fine-tuning\\n\\nThe `train_text_to_image.py` script shows how to fine-tu...\"],[\"```\\n\\nIf you have already cloned the repo, then you won't need to go through these steps.\\n\\n\\u003cbr\\u003e\\n\\n## U...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Prompt weighting works by increasing or decreasing the scale of the text embedding vector that corre...\"],[\"```\\n\\nFor this guide, let's generate an image with the prompt `\\\"a red cat playing with a ball\\\"` using...\"],[\"```\\n\\ncompel uses `+` or `-` to increase or decrease the weight of a word in the prompt. To increase ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\nIncorporate the concept to condition a prompt with using the `\\u003cconcept\\u003e` syntax:\\n\\n```py\\nprompt_...\"],[\"```\\n\\nCreate a `Compel` class with a tokenizer and text encoder, and pass your prompt to it. Dependin...\"],[\"```\\n\\nThis time, let's upweight \\\"ball\\\" by a factor of 1.5 for the first prompt, and downweight \\\"ball\\\"...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nNow you can load a pipeline, and pass the pre-learned concept to it:\\n\\n```py\\npipeline = StableDi...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nThere are two tensors, `\\\"clip_g\\\"` and `\\\"clip_l\\\"`.\\n`\\\"clip_g\\\"` corresponds to the bigger text enc...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find the original codebase for Stable Diffusion v1.0 at [CompVis\\u002fstable-diffusion](https:\\u002f\\u002fg...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cdiv class=\\\"rounded-xl border border-gray-200\\\"\\u003e\\n    \\u003ctable cla...\"],[\"\\u003c\\u002ftd\\u003e\\n        \\u003c\\u002ftr\\u003e\\n        \\u003ctr\\u003e\\n            \\u003ctd class=\\\"px-4 py-2 text-gray-700\\\"\\u003e\\n            \\u003ca hre...\"],[\"\\u003ca href=\\\".\\u002fdepth2img\\\"\\u003eStableDiffusionDepth2Img\\u003c\\u002fa\\u003e\\n            \\u003c\\u002ftd\\u003e\\n            \\u003ctd class=\\\"px-4 py-...\"],[\"\\u003c\\u002ftd\\u003e\\n            \\u003ctd class=\\\"px-4 py-2 text-gray-700\\\"\\u003efiltered text-to-image\\u003c\\u002ftd\\u003e\\n            \\u003ctd cl...\"],[\"\\u003c\\u002ftd\\u003e\\n            \\u003ctd class=\\\"px-4 py-2 text-gray-700\\\"\\u003etext-to-image, image-to-image\\u003c\\u002ftd\\u003e\\n           ...\"],[\"\\u003c\\u002ftr\\u003e\\n        \\u003ctr\\u003e\\n            \\u003ctd class=\\\"px-4 py-2 text-gray-700\\\"\\u003e\\n            \\u003ca href=\\\".\\u002fldm3d_dif...\"],[\"## Tips\\n\\nTo help you get the most out of the Stable Diffusion pipelines, here are a few tips for imp...\"],[\"```\\n\\n### Reuse pipeline components to save memory\\n\\nTo save memory and use the same components across...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nIf a community pipeline doesn't work as expected, please open a GitHub issue and mention the au...\"],[\"diffuser_pipeline = DiffusionPipeline.from_pretrained(\\n    \\\"CompVis\\u002fstable-diffusion-v1-4\\\",\\n    cust...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f43138...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fuser-images.githubuse...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe example prompt you'll use is a portrait of an old warrior chief, but feel free to use your ...\"],[\"```\\n\\nNow you can generate an image:\\n\\n```python\\nimage = pipeline(prompt, generator=generator).images[...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-...\"],[\"```python\\npipeline.scheduler.compatibles\\n[\\n    diffusers.schedulers.scheduling_lms_discrete.LMSDiscr...\"],[\"```\\n\\nThe Stable Diffusion model uses the [`PNDMScheduler`] by default which usually requires ~50 inf...\"],[\"```\\n\\nStart with `batch_size=4` and see how much memory you've consumed:\\n\\n```python\\nfrom diffusers.ut...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-...\"],[\"```python\\nfrom diffusers import AutoencoderKL\\n\\nvae = AutoencoderKL.from_pretrained(\\\"stabilityai\\u002fsd-v...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## VQModel\\n\\n[[autodoc]] VQModel\\n\\n## VQEncoderOutput\\n\\n[[autodoc]] models.vq_model.VQEncoderOutput...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional smaller Stable Diffusion XL (SDXL) ControlNet checkpoints from the 🤗 [Diffus...\"],[\"## StableDiffusionXLControlNetInpaintPipeline\\n[[autodoc]] StableDiffusionXLControlNetInpaintPipeline...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Inspired by [Stable Diffusion](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002fapi\\u002fpipelines\\u002fstable_diffusion\\u002f...\"],[\"The abstract of the paper is the following:\\n\\n*Although audio generation shares commonalities across ...\"],[\"All checkpoints share the same model size for the text encoders and VAE. They differ in the size and...\"],[\"### Controlling inference\\n\\n* The _quality_ of the predicted audio sample can be controlled by the `n...\"],[\"!---\\nCopyright 2022 - The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License,...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fraw.githubusercontent.com\\u002fhuggingface\\u002fdiffusers\\u002fma...\"],[\"🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating ima...\"],[\"```\\n\\nWith `conda` (maintained by the community):\\n\\n```sh\\nconda install -c conda-forge diffusers\\n```\\n\\n...\"],[\"```\\n\\nYou can also dig into the models and schedulers toolbox to build your own diffusion system:\\n\\n``...\"],[\"```\\n\\nCheck out the [Quickstart](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002fquicktour) to launch your diff...\"],[\"| **Documentation**                                                   | **What can I learn?**       ...\"],[\"| [Optimization](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002foptimization\\u002fopt_overview)                   ...\"],[\"We ❤️  contributions from the open-source community!\\nIf you want to contribute to this library, plea...\"],[\"\\u003ctable\\u003e\\n  \\u003ctr\\u003e\\n    \\u003cth\\u003eTask\\u003c\\u002fth\\u003e\\n    \\u003cth\\u003ePipeline\\u003c\\u002fth\\u003e\\n    \\u003cth\\u003e🤗 Hub\\u003c\\u002fth\\u003e\\n  \\u003c\\u002ftr\\u003e\\n  \\u003ctr style=\\\"borde...\"],[\"\\u003c\\u002ftr\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd\\u003eText-to-Image\\u003c\\u002ftd\\u003e\\n    \\u003ctd\\u003e\\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002fapi\\u002fp...\"],[\"\\u003c\\u002ftr\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd\\u003eText-guided Image-to-Image\\u003c\\u002ftd\\u003e\\n    \\u003ctd\\u003e\\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdi...\"],[\"\\u003c\\u002ftr\\u003e\\n  \\u003ctr style=\\\"border-top: 2px solid black\\\"\\u003e\\n    \\u003ctd\\u003eImage Variation\\u003c\\u002ftd\\u003e\\n    \\u003ctd\\u003e\\u003ca href=\\\"https...\"],[\"## Popular libraries using 🧨 Diffusers\\n\\n- https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fTaskMatrix\\n- https:\\u002f\\u002fgithub.c...\"],[\"We also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffus...\"],[\"Stable Diffusion text-to-image fine-tuning\\nThis extended LoRA training script was authored by [haofa...\"],[\"With LoRA, it's possible to fine-tune Stable Diffusion on a custom image-caption pair dataset\\non con...\"],[\"```\\n\\nFor this example we want to directly store the trained LoRA embeddings on the Hub, so \\nwe need ...\"],[\"```\\n\\nThe above command will also run inference as fine-tuning progresses and log the results to Weig...\"],[\"```python\\nfrom diffusers import StableDiffusionPipeline\\nimport torch\\n\\nmodel_path = \\\"sayakpaul\\u002fsd-mod...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Depending on the use case, one should choose a technique accordingly. In many cases, these technique...\"],[\"|                     **Method**                      | **Inference only** | **Requires training \\u002f\\u003cb...\"],[\"|              [DreamBooth](#dreambooth)              |         ❌         |                   ✅     ...\"],[\"[Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2211.09800)\\n\\n[InstructPix2Pix](..\\u002fapi\\u002fpipelines\\u002fpix2pix) is fine-tuned...\"],[\"Pix2Pix Zero can be used both to edit synthetic images as well as real images.\\n\\n- To edit synthetic ...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Attend and Excite\\n\\n[Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2301.13826)\\n\\n[Attend and Excite](..\\u002fapi\\u002f...\"],[\"## Self-attention Guidance (SAG)\\n\\n[Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2210.00939)\\n\\n[Self-attention Guidanc...\"],[\"## Textual Inversion\\n\\n[Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.01618)\\n\\n[Textual Inversion](..\\u002ftraining\\u002ftex...\"],[\"## DiffEdit\\n\\n[Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2210.11427)\\n\\n[DiffEdit](..\\u002fapi\\u002fpipelines\\u002fdiffedit) allows...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Sliced VAE\\n\\nSliced VAE enables decoding large batches of images with limited VRAM or batches with...\"],[\"```\\n\\nYou may see a small performance boost in VAE decoding on multi-image batches, and there should ...\"],[\"```\\n\\nThe output image has some tile-to-tile tone variation because the tiles are decoded separately,...\"],[\"```\\n\\nCPU offloading works on submodules rather than whole models. This is the best way to minimize m...\"],[\"During model offloading, only one of the main components of the pipeline (typically the text encoder...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nIn order to properly offload models after they're called, it is required ...\"],[\"```\\n\\n## Tracing\\n\\nTracing runs an example input tensor through the model and captures the operations ...\"],[\"# trace\\nprint(\\\"tracing..\\\")\\nunet_traced = torch.jit.trace(unet, inputs)\\nunet_traced.eval()\\nprint(\\\"don...\"],[\"```\\n\\nReplace the `unet` attribute of the pipeline with the traced model:\\n\\n```python\\nfrom diffusers i...\"],[\"```\\n\\n## Memory-efficient attention\\n\\nRecent work on optimizing bandwidth in the attention block has g...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Tips\\n\\n- SDXL Turbo uses the exact same architecture as [SDXL](.\\u002fstable_diffusion_xl), which means...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Loading from the original format\\n\\nBy default the [`AutoencoderKL`] should be loaded with [`~Model...\"],[\"```\\n\\n## AutoencoderKL\\n\\n[[autodoc]] AutoencoderKL\\n\\n## AutoencoderKLOutput\\n\\n[[autodoc]] models.autoenc...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"We enormously value feedback from the community, so please do not be afraid to speak up if you belie...\"],[\"* 1. Asking and answering questions on [the Diffusers discussion forum](https:\\u002f\\u002fdiscuss.huggingface....\"],[\"* 9. Add a new pipeline, model, or scheduler, see [\\\"New Pipeline\\u002fModel\\\"](https:\\u002f\\u002fgithub.com\\u002fhuggingf...\"],[\"As said before, **all contributions are valuable to the community**.\\nIn the following, we will expla...\"],[\"**NOTE about channels**:\\n[*The forum*](https:\\u002f\\u002fdiscuss.huggingface.co\\u002fc\\u002fdiscussion-related-to-httpsg...\"],[\"**Please consider the following guidelines when opening a new issue**:\\n- Make sure you have searched...\"],[\"New issues usually include the following.\\n\\n#### 2.1. Reproducible, minimal bug reports\\n\\nA bug report...\"],[\"#### 2.2. Feature requests\\n\\nA world-class feature request addresses the following points:\\n\\n1. Motiva...\"],[\"#### 2.4 Technical questions\\n\\nTechnical questions are mainly about why certain code of the library w...\"],[\"### 3. Answering issues on the GitHub issues tab\\n\\nAnswering issues on GitHub might require some tech...\"],[\"### 4. Fixing a \\\"Good first issue\\\"\\n\\n*Good first issues* are marked by the [Good first issue](https:\\u002f...\"],[\"Contributing to the library can have many forms:\\n\\n- Correcting spelling or grammatical errors.\\n- Cor...\"],[\"- Official Pipelines\\n- Community Pipelines\\n\\nBoth official and community pipelines follow the same de...\"],[\"An example can be seen [here](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fpull\\u002f2400).\\n\\nCommunity pipeli...\"],[\"```\\n\\nas well as to install all additional dependencies required for training:\\n\\n```bash\\npip install -...\"],[\"```\\n\\nTherefore when adding an example, the `requirements.txt` file shall define all pip dependencies...\"],[\"To contribute an example, it is highly recommended to look at already existing examples such as [dre...\"],[\"### 8. Fixing a \\\"Good second issue\\\"\\n\\n*Good second issues* are marked by the [Good second issue](http...\"],[\"By adding a new model, pipeline, or scheduler you might enable a new powerful use case for any of th...\"],[\"If you are unsure or stuck in the PR, don't hesitate to leave a message to ask for a first review or...\"],[\"```\\n\\nTo learn more, read this section of the [~Don't~ Repeat Yourself*](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002f...\"],[\"1. Make sure that you've used the correct template for your issue. You can pick between *Bug Report*...\"],[\"4. **Minimalistic**: Try to help the reader as much as you can to understand the issue as quickly as...\"],[\"## How to write a good PR...\"],[\"1. Be a chameleon. Understand existing design patterns and syntax and make sure your code additions ...\"],[\"11. Due to the rapidly growing repository, it is important to make sure that no files that would sig...\"],[\"## How to open a PR\\n\\nBefore writing code, we strongly advise you to search through the existing PRs ...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n ```bash\\n $ git checkout -b a-descrip...\"],[\"```\\n\\nIt is a good idea to sync your copy of the code with the original\\nrepository regularly. This wa...\"],[\"```\\n\\n### Syncing forked main with upstream (HuggingFace) main\\n\\nTo avoid pinging the upstream reposit...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"| Training | SDXL-support | LoRA-support | Flax-support |\\n|---|---|---|---|\\n| [unconditional image g...\"],[\"| [ControlNet](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002ftree\\u002fmain\\u002fexamples\\u002fcontrolnet) | 👍 |  | 👍 |\\n...\"],[\"These examples are **actively** maintained, so please feel free to open an issue if they aren't work...\"],[\"```\\n\\nThen navigate to the folder of the training script (for example, [DreamBooth](https:\\u002f\\u002fgithub.co...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Recent text-to-video generation approaches rely on computationally...\"],[\"prompt = \\\"A panda is playing guitar on times square\\\"\\nresult = pipe(prompt=prompt).images\\nresult = [(...\"],[\"```\\nYou can change these parameters in the pipeline call:\\n* Motion field strength (see the [paper](h...\"],[\"# Generate the video chunk-by-chunk\\nresult = []\\nchunk_ids = np.arange(0, video_length, chunk_size - ...\"],[\"```\\n\\n\\n- #### SDXL Support\\nIn order to use the SDXL model when generating a video from prompt, use th...\"],[\"```\\n    To extract pose from actual video, read [ControlNet documentation](controlnet).\\n\\n3. Run `Sta...\"],[\"```\\n- #### SDXL Support\\n\\t\\n\\tSince our attention processor also works with SDXL, it can be utilized to...\"],[\"```\\n\\n### Text-To-Video with Edge Control\\n\\nTo generate a video from prompt with additional Canny edge...\"],[\"```\\n\\n3. Run `StableDiffusionInstructPix2PixPipeline` with our custom attention processor\\n    ```pyth...\"],[\"```\\n\\n2. Read video from path\\n    ```python\\n    from PIL import Image\\n    import imageio\\n\\n    reader ...\"],[\"```\\n\\nYou can filter out some available DreamBooth-trained models with [this link](https:\\u002f\\u002fhuggingfac...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nA ControlNet model has two sets of weights (or blocks) connected by a zero-convolution layer...\"],[\"```\\n\\n## Text-to-image\\n\\nFor text-to-image, you normally pass a text prompt to the model. But with Con...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nNow pass your prompt and canny image to the pipeline:\\n\\n```py\\noutput = pipe(\\n    \\\"the mona lisa\\\"...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocum...\"],[\"```\\n\\nNext, load a ControlNet model conditioned on depth maps and pass it to the [`StableDiffusionCon...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nCreate a function to prepare the control image from the initial and mask images. This'll create...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nNow pass your prompt, initial image, mask image, and control image to the pipeline:\\n\\n```py\\noutp...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocum...\"],[\"original_image = load_image(\\\"https:\\u002f\\u002fhuggingface.co\\u002ftakuma104\\u002fcontrolnet_dev\\u002fresolve\\u002fmain\\u002fbird_512x5...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002ftakuma...\"],[\"original_image = load_image(\\n    \\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhf-internal-testing\\u002fdiffusers-imag...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nNow pass your prompt (and optionally a negative prompt if you're using one) and canny image to ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdiffu...\"],[\"image = np.array(original_image)\\nimage = cv2.Canny(image, 100, 200)\\nimage = image[:, :, None]\\nimage ...\"],[\"```\\n\\n### MultiControlNet\\n\\n\\u003cTip\\u003e\\n\\nReplace the SDXL model with a model like [runwayml\\u002fstable-diffusion...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"vae = AutoencoderKL.from_pretrained(\\\"madebyollin\\u002fsdxl-vae-fp16-fix\\\", torch_dtype=torch.float16, use_...\"],[\"```\\n\\nNow you can pass your prompt (an optional negative prompt if you're using one), canny image, an...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"As said before, **all contributions are valuable to the community**.\\nIn the following, we will expla...\"],[\"#### 2.4 Technical questions\\n\\nTechnical questions are mainly about why certain code of the library w...\"],[\"Contributing to the library can have many forms:\\n\\n- Correcting spelling or grammatical errors.\\n- Cor...\"],[\"To contribute an example, it is highly recommended to look at already existing examples such as [dre...\"],[\"By adding a new model, pipeline, or scheduler you might enable a new powerful use case for any of th...\"],[\"## How to open a PR\\n\\nBefore writing code, we strongly advise you to search through the existing PRs ...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n ```bash\\n $ git checkout -b a-descrip...\"],[\"```\\n\\nIt is a good idea to sync your copy of the code with the original\\nrepository regularly. This wa...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Tips\\n\\nIt is recommended to set `solver_order` to 2 for guide sampling, and `solver_order=3` for u...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"To load any community pipeline on the Hub, pass the repository id of the community pipeline to the `...\"],[\"```\\n\\nLoading an official community pipeline is similar, but you can mix loading weights from an offi...\"],[\"```\\n\\nFor more information about community pipelines, take a look at the [Community pipelines](custom...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nIn steps 4 and 5, the custom [UNet](https:\\u002f\\u002fgithub.com\\u002fshowlab\\u002fShow-1\\u002fblo...\"],[\"```\\n\\n5. Finally, you'll load the custom pipeline code. For this example, it has already been created...\"],[\"```\\n\\nAfter the pipeline is successfully pushed, you need a couple of changes:\\n\\n1. Change the `_class...\"],[\"```\\n\\nAs an additional reference example, you can refer to the repository structure of [stabilityai\\u002fj...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*We present the vector quantized diffusion (VQ-Diffusion) model for...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nTo generate images with Stable Diffusion 1 and 2 on Gaudi, you need to instantiate two instance...\"],[\"```\\n\\nFor more information, check out 🤗 Optimum Habana's [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002f...\"],[\"For [Stable Diffusion v2.1](https:\\u002f\\u002fhuggingface.co\\u002fstabilityai\\u002fstable-diffusion-2-1) on 768x768 imag...\"],[\"DreamBooth training example for Stable Diffusion XL (SDXL)\\n\\n[DreamBooth](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208....\"],[\"```\\n\\nOr for a default accelerate configuration without answering questions about your environment\\n\\n`...\"],[\"```\\n\\nThis will also allow us to push the trained LoRA parameters to the Hugging Face Hub platform. \\n...\"],[\"```\\n\\nTo better track our training experiments, we're using the following flags in the command above:...\"],[\"```\\n\\nand making sure that you have the following libraries installed:\\n\\n```\\nbitsandbytes\\u003e=0.40.0\\nxfor...\"],[\"```\\n\\nWe can further refine the outputs with the [Refiner](https:\\u002f\\u002fhuggingface.co\\u002fstabilityai\\u002fstable-...\"],[\"```\\n\\nHere's a side-by-side comparison of the with and without Refiner pipeline outputs:\\n\\n| Without R...\"],[\"## Results\\n\\nYou can explore the results from a couple of our internal experiments by checking out th...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nIf you look at the [`runwayml\\u002fstable-diffusion-v1-5`](https:\\u002f\\u002fhuggingface.co\\u002frunwayml\\u002fstable-di...\"],[\"```\\n\\n## Convert to safetensors\\n\\nNot all weights on the Hub are available in the `.safetensors` forma...\"],[\"```\\n\\n## Why use safetensors?\\n\\nThere are several reasons for using safetensors:\\n\\n- Safety is the numb...\"],[\"Deprecated Pipelines\\n\\nThis folder contains pipelines that have very low usage as measured by model d...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is as follows:\\n\\n*Latent Diffusion models (LDMs) have achieved remarkable r...\"],[\"## LatentConsistencyModelPipeline\\n\\n[[autodoc]] LatentConsistencyModelPipeline\\n    - all\\n    - __call...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This guide will explore the [train_text_to_image_sdxl.py](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fb...\"],[\"```\\n\\nThen navigate to the example folder containing the training script and install the required dep...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMost of the parameters are identical to the parameters in the [Text-to-image](text2image#script...\"],[\"```\\n\\n## Training script\\n\\nThe training script is also similar to the [Text-to-image](text2image#train...\"],[\"Within the [`main()`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002faab6de22c33cc01fb7bc81c0807d6109...\"],[\"```\\n\\nThe [prompt and image embeddings](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002faab6de22c33cc01...\"],[\"```\\n\\nFinally, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002faab6de22c33cc01fb7bc...\"],[\"```\\n\\nIf you want to learn more about how the training loop works, check out the [Understanding pipel...\"],[\"accelerate launch train_text_to_image_sdxl.py \\\\\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\\\\n  --...\"],[\"```\\n\\nAfter you've finished training, you can use your newly trained SDXL model for inference!\\n\\n\\u003chfop...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## Next steps\\n\\nCongratulations on training a SDXL model! To learn mor...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## FromOriginalVAEMixin\\n\\n[[autodoc]] loaders.single_file.FromOriginalVAEMixin\\n\\n## FromOriginalContro...\"],[\"Schedulers\\n\\nFor more information on the schedulers, please refer to the [docs](https:\\u002f\\u002fhuggingface.c...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage\\n\\nBefore you can use IF, you need to accept its usage conditions. To do so:\\n1. Make sure to ...\"],[\"```\\n\\nrun the login function in a Python shell:\\n\\n```py\\nfrom huggingface_hub import login\\n\\nlogin()\\n```...\"],[\"```\\n\\nThe following sections give more in-detail examples of how to use IF. Specifically:\\n\\n- [Text-to...\"],[\"- *Stage-3*\\n  - [stabilityai\\u002fstable-diffusion-x4-upscaler](https:\\u002f\\u002fhuggingface.co\\u002fstabilityai\\u002fstable...\"],[\"prompt = 'a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of th...\"],[\"```\\n\\n### Text Guided Image-to-Image Generation\\n\\nThe same IF model weights can be used for text-guide...\"],[\"# stage 3\\nsafety_modules = {\\n    \\\"feature_extractor\\\": stage_1.feature_extractor,\\n    \\\"safety_checker...\"],[\"```\\n\\n### Text Guided Inpainting Generation\\n\\nThe same IF model weights can be used for text-guided im...\"],[\"# stage 3\\nsafety_modules = {\\n    \\\"feature_extractor\\\": stage_1.feature_extractor,\\n    \\\"safety_checker...\"],[\"# stage 3\\nstage_3_output = stage_3(prompt=prompt, image=stage_2_output, generator=generator, noise_l...\"],[\"```\\n\\n### Converting between different pipelines\\n\\nIn addition to being loaded with `from_pretrained`,...\"],[\"```\\n\\nWhen doing image variation or inpainting, you can also decrease the number of timesteps\\nwith th...\"],[\"```\\n\\nor the more aggressive layer based CPU offloading.\\n\\n```py\\npipe = DiffusionPipeline.from_pretrai...\"],[\"```\\n\\nFor CPU RAM constrained machines like Google Colab free tier where we can't load all model comp...\"],[\"#pt_to_pil(stage_1_output)[0].save(\\\".\\u002fif_stage_I.png\\\")\\n\\n# Remove the pipeline so we can load the sup...\"],[\"```\\n\\n## Available Pipelines:...\"],[\"## Available Pipelines:\\n\\n| Pipeline | Tasks | Colab\\n|---|---|:---:|\\n| [pipeline_if.py](https:\\u002f\\u002fgithu...\"],[\"## IFPipeline\\n[[autodoc]] IFPipeline\\n\\t- all\\n\\t- __call__\\n\\n## IFSuperResolutionPipeline\\n[[autodoc]] IF...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThen enable the FreeU mechanism with the FreeU-specific hyperparameters. These values are scali...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002ffre...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002ffre...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThen navigate to the example folder containing the training script and install the required dep...\"],[\"```\\n\\nTo setup a default 🤗 Accelerate environment without choosing any configurations:\\n\\n```bash\\naccel...\"],[\"```\\n\\nMany of the basic and important parameters are described in the [Text-to-image](text2image#scri...\"],[\"```\\n\\n## Training script\\n\\nAs with the script parameters, a general walkthrough of the training script...\"],[\"```\\n\\nWithin the [`main()`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f64603389da01082055a901f2883...\"],[\"```\\n\\nFinally, in the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f64603389da0108205...\"],[\"```\\n\\nOne more thing before you launch the script! Depending on the GPU you have, you may need to ena...\"],[\"```\\n\\nDuring configuration, confirm that you want to use DeepSpeed stage 2. Now it should be possible...\"],[\"```\\n\\nYou should also change the default Adam optimizer to DeepSpeed’s optimized version of Adam [`de...\"],[\"```\\n\\nThen you can inspect the profile at [http:\\u002f\\u002flocalhost:6006\\u002f#profile](http:\\u002f\\u002flocalhost:6006\\u002f#pro...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\nOnce training is complete, you can use your newly trained model for i...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usability over Performance\\n\\n- While Diffusers has many built-in performance-enhancing features (s...\"],[\"## Simple over easy\\n\\nAs PyTorch states, **explicit is better than implicit** and **simple is better ...\"],[\"## Tweakable, contributor-friendly over abstraction\\n\\nFor large parts of the library, Diffusers adopt...\"],[\"In Diffusers, we follow this philosophy for both pipelines and schedulers, but only partly for diffu...\"],[\"### Pipelines\\n\\nPipelines are designed to be easy to use (therefore do not follow [*Simple over easy*...\"],[\"The following design principles are followed:\\n- Pipelines follow the single-file policy. All pipelin...\"],[\"- Pipelines should be used **only** for inference.\\n- Pipelines should be very readable, self-explana...\"],[\"### Models\\n\\nModels are designed as configurable toolboxes that are natural extensions of [PyTorch's ...\"],[\"The following design principles are followed:\\n- Models correspond to **a type of model architecture*...\"],[\"- Models intend to expose complexity, just like PyTorch's `Module` class, and give clear error messa...\"],[\"### Schedulers\\n\\nSchedulers are responsible to guide the denoising process for inference as well as t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Stable Diffusion weights (or checkpoints) are stored in the PyTorch format, so you need to convert t...\"],[\"## Selecting the Core ML Variant to Use\\n\\nStable Diffusion models can be converted to different Core ...\"],[\"The official Core ML Stable Diffusion [models](https:\\u002f\\u002fhuggingface.co\\u002fapple\\u002fcoreml-stable-diffusion-...\"],[\"```\\ncoreml-stable-diffusion-v1-4\\n├── README.md\\n├── original\\n│   ├── compiled\\n│   └── packages\\n└── sp...\"],[\"```\\n\\nPass the path of the downloaded checkpoint with `-i` flag to the script. `--compute-unit` indic...\"],[\"```\\n\\n## Core ML inference in Swift\\n\\nRunning inference in Swift is slightly faster than in Python bec...\"],[\"```\\n\\nYou have to specify in `--resource-path` one of the checkpoints downloaded in the previous step...\"],[\"If you feel strongly about any missing features, please feel free to open a feature request or, bett...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n🤗 Accelerate is a library for helping you train on multiple GPUs\\u002fTPUs or with mixed-prec...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMost of the parameters are identical to the parameters in the [Text-to-image](text2image#script...\"],[\"```\\n\\nYou'll also load the [`WuerstchenPrior`] model for optimization.\\n\\n```py\\nprior = WuerstchenPrior...\"],[\"```\\n\\nFinally, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f65ef7a0c5c594b4f8409...\"],[\"```\\n\\nIf you want to learn more about how the training loop works, check out the [Understanding pipel...\"],[\"```\\n\\nOnce training is complete, you can use your newly trained model for inference!\\n\\n```py\\nimport to...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*There is large consent that successful training of deep networks r...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*There is large consent that successful training of deep networks r...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This guide explores the [train_text_to_image_prior.py](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob...\"],[\"```\\n\\nThen navigate to the example folder containing the training script and install the required dep...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMost of the parameters are identical to the parameters in the [Text-to-image](text2image#script...\"],[\"```\\n\\n## Training script\\n\\nThe training script is also similar to the [Text-to-image](text2image#train...\"],[\"with ContextManagers(deepspeed_zero_init_disabled_context_manager()):\\n    image_encoder = CLIPVision...\"],[\"```\\n\\nKandinsky uses a [`PriorTransformer`] to generate the image embeddings, so you'll want to setup...\"],[\"```\\n\\nFinally, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f6e68c71503682c8693cb...\"],[\"```\\n\\nIf you want to learn more about how the training loop works, check out the [Understanding pipel...\"],[\"```\\n\\nNext, the script includes several image transforms and a [preprocessing](https:\\u002f\\u002fgithub.com\\u002fhug...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## Launch the script\\n\\nOnce you’ve made all your changes or you’re oka...\"],[\"```bash\\nexport DATASET_NAME=\\\"lambdalabs\\u002fpokemon-blip-captions\\\"\\n\\naccelerate launch --mixed_precision=...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"decoder model\\\"\\u003e\\n\\n```bash\\nexport DATASET_NAME=\\\"lambdalabs\\u002fpokemon-blip...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nFeel free to replace `kandinsky-community\\u002fkandinsky-2-2-decoder` with your own trained d...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## Next steps\\n\\nCongratulations on training a Kandinsky 2.2 model! To ...\"],[\"InstructPix2Pix training example\\n\\n[InstructPix2Pix](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2211.09800) is a method to...\"],[\"## Running locally with PyTorch\\n\\n### Installing the dependencies\\n\\nBefore running the scripts, make s...\"],[\"```\\n\\nThen cd in the example folder and run\\n```bash\\npip install -r requirements.txt\\n```\\n\\nAnd initiali...\"],[\"```\\n\\nNow, we can launch training:\\n\\n```bash\\naccelerate launch --mixed_precision=\\\"fp16\\\" train_instruct...\"],[\"```\\n\\nAdditionally, we support performing validation inference to monitor training progress\\nwith Weig...\"],[\"```\\n\\n We recommend this type of validation as it can be useful for model debugging. Note that you ne...\"],[\"```\\n\\n ## Inference\\n\\n Once training is complete, we can perform inference:\\n\\n ```python\\nimport PIL\\nimp...\"],[\"```\\n\\nAn example model repo obtained using this training script can be found\\nhere - [sayakpaul\\u002finstru...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nFor example, let's use the [`ptx0\\u002fpseudo-journey-v2`](https:\\u002f\\u002fhuggingface.co\\u002fptx0\\u002fpseudo-journe...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The SDE variant of DPMSolver and DPM-Solver++ is also supported, but only for the first and second-o...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Tips\\n\\n- Using SDXL with a DPM++ scheduler for less than 50 steps is known to produce [visual arti...\"],[\"Check out the [Stability AI](https:\\u002f\\u002fhuggingface.co\\u002fstabilityai) Hub organization for the official b...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\nAll methods of the logging module are documented below. The main methods are\\n[`logging.get_ver...\"],[\"[[autodoc]] utils.logging.set_verbosity_debug\\n\\n## Other functions\\n\\n[[autodoc]] utils.logging.get_ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Training an unconditional diffusion model\\n\\nCreating a training image set is [described in a differ...\"],[\"```\\nAn example trained model: https:\\u002f\\u002fhuggingface.co\\u002fanton-l\\u002fddpm-ema-flowers-64\\n\\nA full training ru...\"],[\"```\\nAn example trained model: https:\\u002f\\u002fhuggingface.co\\u002fanton-l\\u002fddpm-ema-pokemon-64\\n\\nA full training ru...\"],[\"```\\n\\nTo be able to use Weights and Biases (`wandb`) as a logger you need to install the library: `pi...\"],[\"```\\n\\nInternally, the script will use the [`ImageFolder`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fv2.0.0...\"],[\"```\\n\\n`ImageFolder` will create an `image` column containing the PIL-encoded images.\\n\\nNext, push it t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*The most advanced text-to-image (T2I) models require significant t...\"],[\"You can find the original codebase at [PixArt-alpha\\u002fPixArt-alpha](https:\\u002f\\u002fgithub.com\\u002fPixArt-alpha\\u002fPi...\"],[\"Run the [`PixArtAlphaPipeline`] with under 8GB GPU VRAM by loading the text encoder in 8-bit precisi...\"],[\"```\\n\\nThen load the text encoder in 8-bit:\\n\\n```python\\nfrom transformers import T5EncoderModel\\nfrom di...\"],[\"```\\n\\nThen compute the latents with the prompt embeddings as inputs:\\n\\n```python\\npipe = PixArtAlphaPip...\"],[\"```\\n\\nBy deleting components you aren't using and flushing the GPU VRAM, you should be able to run [`...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Scenarios\\n\\nWe cover Diffusion models with the following pipelines:\\n\\n- Text-guided image generatio...\"],[\"![parti-prompts](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002fevaluation_diffu...\"],[\"# Fixing these sample prompts in the interest of reproducibility.\\nsample_prompts = [\\n    \\\"a corgi\\\",\\n...\"],[\"```\\n\\nNow we can use these prompts to generate some images using Stable Diffusion ([v1-4 checkpoint](...\"],[\"```\\n\\n![parti-prompts-14](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002fevaluati...\"],[\"Let's first load a [`StableDiffusionPipeline`]:\\n\\n```python\\nfrom diffusers import StableDiffusionPipe...\"],[\"```\\n\\nGenerate some images with multiple prompts:\\n\\n```python\\nprompts = [\\n    \\\"a photo of an astronaut...\"],[\"```\\n\\nIn the above example, we generated one image per prompt. If we generated multiple images per pr...\"],[\"```\\n\\nIt seems like the [v1-5](https:\\u002f\\u002fhuggingface.co\\u002frunwayml\\u002fstable-diffusion-v1-5) checkpoint perf...\"],[\"Following is a pictorial overview:\\n\\n![edit-consistency](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdo...\"],[\"```\\n\\n```bash\\n{'input': Value(dtype='string', id=None),\\n 'edit': Value(dtype='string', id=None),\\n 'ou...\"],[\"```\\n\\nAnd here is the image:\\n\\n```python\\ndataset[idx][\\\"image\\\"]\\n```\\n\\n![edit-dataset](https:\\u002f\\u002fhuggingfac...\"],[\"```\\n\\nTo measure the directional similarity, we first load CLIP's image and text encoders:\\n\\n```python...\"],[\"```\\n\\nNotice that we are using a particular CLIP checkpoint, i.e., `openai\\u002fclip-vit-large-patch14`. T...\"],[\"def encode_text(self, text):\\n        tokenized_text = self.tokenize_text(text)\\n        text_features...\"],[\"```\\n\\nLet's put `DirectionalSimilarity` to use now.\\n\\n```python\\ndir_similarity = DirectionalSimilarity...\"],[\"```\\n\\nLike the CLIP Score, the higher the CLIP directional similarity, the better it is.\\n\\nIt should b...\"],[\"***Using the above metrics helps evaluate models that are class-conditioned. For example, [DiT](http...\"],[\"Let's first download a few images from the ImageNet-1k training set:\\n\\n```python\\nfrom zipfile import ...\"],[\"```\\n\\n```python\\nfrom PIL import Image\\nimport os\\n\\ndataset_path = \\\"sample-imagenet-images\\\"\\nimage_paths ...\"],[\"```\\n\\nWe now load the [`DiTPipeline`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002fapi\\u002fpipelines\\u002fdit) to gen...\"],[\"```\\n\\nThe lower the FID, the better it is. Several things can influence FID here:\\n\\n- Number of images...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*This paper proposes a unified diffusion framework (dubbed UniDiffu...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThis pipeline was contributed by [dg845](https:\\u002f\\u002fgithub.com\\u002fdg845). ❤️\\n\\n## Usage Examples\\n\\nB...\"],[\"```\\n\\nThis is also called \\\"joint\\\" generation in the UniDiffuser paper, since we are sampling from the...\"],[\"```\\n\\n### Text-to-Image Generation\\n\\nUniDiffuser is also capable of sampling from conditional distribu...\"],[\"```\\n\\nThe `text2img` mode requires that either an input `prompt` or `prompt_embeds` be supplied. You ...\"],[\"```\\n\\nThe `img2text` mode requires that an input `image` be supplied. You can set the `img2text` mode...\"],[\"```\\n\\n### Text Variation\\n\\nSimilarly, text variation can be performed on an input prompt with a text-t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMany of the basic and important parameters are described in the [Text-to-image](text2image#scri...\"],[\"```py\\nin_channels = 8\\nout_channels = unet.conv_in.out_channels\\nunet.register_to_config(in_channels=i...\"],[\"```\\n\\nThese UNet parameters are [updated](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f64603389da010...\"],[\"```\\n\\nNext, the edited images and and edit instructions are [preprocessed](https:\\u002f\\u002fgithub.com\\u002fhugging...\"],[\"```\\n\\nFinally, in the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f64603389da0108205...\"],[\"```\\n\\nThen, the script applies dropout to the original image and edit instruction embeddings to suppo...\"],[\"```\\n\\nThat's pretty much it! Aside from the differences described here, the rest of the script is ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nIf you’re training on more than one GPU, add the `--multi_gpu` parameter to the `accelerate ...\"],[\"```\\n\\nAfter training is finished, you can use your new InstructPix2Pix for inference:\\n\\n```py\\nimport P...\"],[\"```\\n\\nYou should experiment with different `num_inference_steps`, `image_guidance_scale`, and `guidan...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nYou can also try experimenting with the `num_inference_steps` parameter, which controls the num...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nYou can also use the [`~StableDiffusionXLPipeline.from_single_file`] method to load a model che...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase can be found at [Xiang-cd\\u002fDiffEdit-stable-diffusion](https:\\u002f\\u002fgithub.com\\u002fXiang-...\"],[\"* The pipeline can generate masks that can be fed into other inpainting pipelines.\\n* In order to gen...\"],[\"* Swap the `source_prompt` and `target_prompt` in the arguments to `generate_mask`.\\n    * Change the...\"],[\"## StableDiffusionDiffEditPipeline\\n[[autodoc]] StableDiffusionDiffEditPipeline\\n    - all\\n    - gener...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\nimport torch\\nfrom accelerate import PartialState\\nfrom diffusers import DiffusionPipeline\\n\\npipe...\"],[\"```\\n\\nUse the `--num_processes` argument to specify the number of GPUs to use, and call `accelerate l...\"],[\"```\\n\\nYou'll want to create a function to run inference; [`init_process_group`](https:\\u002f\\u002fpytorch.org\\u002fd...\"],[\"T2I-Adapter training example for Stable Diffusion XL (SDXL)\\n\\nThe `train_t2i_adapter_sdxl.py` script ...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell (e.g., a notebook)\\n\\n```python\\nfrom ...\"],[\"```\\n\\nThen run `huggingface-cli login` to log into your Hugging Face account. This is needed to be ab...\"],[\"```\\n\\nTo better track our training experiments, we're using the following flags in the command above:...\"],[\"```\\n\\n## Notes\\n\\n### Specifying a better VAE\\n\\nSDXL's VAE is known to suffer from numerical instability...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Scaled dot product attention\\n\\n[`torch.nn.functional.scaled_dot_product_attention`](https:\\u002f\\u002fp...\"],[\"```\\n\\nSDPA should be as fast and memory efficient as `xFormers`; check the [benchmark](#benchmark) fo...\"],[\"```\\n\\nDepending on GPU type, `torch.compile` can provide an *additional speed-up* of **5-300x** on to...\"],[\"if run_compile:\\n    print(\\\"Run torch compile\\\")\\n    pipe.unet = torch.compile(pipe.unet, mode=\\\"reduce...\"],[\"```\\n\\n### Stable Diffusion image-to-image\\n\\n```python\\nfrom diffusers import StableDiffusionImg2ImgPipe...\"],[\"```\\n\\n### Stable Diffusion inpainting\\n\\n```python\\nfrom diffusers import StableDiffusionInpaintPipeline...\"],[\"```\\n\\n### ControlNet\\n\\n```python\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetMo...\"],[\"```\\n\\n### DeepFloyd IF text-to-image + upscaling\\n\\n```python\\nfrom diffusers import DiffusionPipeline\\ni...\"],[\"prompt = \\\"the blue hulk\\\"\\n\\nprompt_embeds = torch.randn((1, 2, 4096), dtype=torch.float16)\\nneg_prompt_...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\nThe graph below highlights the relative speed-ups for the [`StableDiffusionPipeline`...\"],[\"In the following tables, we report our findings in terms of the *number of iterations\\u002fsecond*.\\n\\n### ...\"],[\"### A100 (batch size: 4)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cbr\\u003eno...\"],[\"### V100 (batch size: 1)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cbr\\u003eno...\"],[\"### V100 (batch size: 16)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cbr\\u003en...\"],[\"### T4 (batch size: 1)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cbr\\u003eno c...\"],[\"### T4 (batch size: 4)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cbr\\u003eno c...\"],[\"### T4 (batch size: 16)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cbr\\u003eno ...\"],[\"### RTX 3090 (batch size: 1)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cb...\"],[\"### RTX 3090 (batch size: 16)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003c...\"],[\"### RTX 4090 (batch size: 1)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cb...\"],[\"### RTX 4090 (batch size: 4)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cb...\"],[\"## Notes\\n\\n* Follow this [PR](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fpull\\u002f3313) for more details on...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n**📋 Copy-paste the English version with a new language code**\\n\\nThe documentation files are in o...\"],[\"```\\n\\nHere, `\\u003cLANG-ID\\u003e` should be one of the ISO 639-1 or ISO 639-2 language codes -- see [here](http...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## StableDiffusionInpaintPipeline\\n\\n[[autodoc]] StableDiffusionInpaintPipeline\\n\\t- all\\n\\t- __ca...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"💡 Skip to the [DiffusionPipeline explained](#diffusionpipeline-explained) section if you are interes...\"],[\"```\\n\\nYou can also load a checkpoint with its specific pipeline class. The example above loaded a Sta...\"],[\"```\\n\\nThen pass the local path to [`~DiffusionPipeline.from_pretrained`]:\\n\\n```python\\nfrom diffusers i...\"],[\"```\\n\\nLet's use the [`SchedulerMixin.from_pretrained`] method to replace the default [`PNDMScheduler`...\"],[\"```\\n\\n### Safety checker\\n\\nDiffusion models like Stable Diffusion can generate harmful content, which ...\"],[\"```\\n\\nThen you can pass the `components` to another pipeline without reloading the weights into RAM:\\n...\"],[\"```\\n\\n## Checkpoint variants\\n\\nA checkpoint variant is usually a checkpoint whose weights are:\\n\\n- Stor...\"],[\"There are two important arguments to know for loading variants:\\n\\n- `torch_dtype` defines the floatin...\"],[\"```\\n\\nTo save a checkpoint stored in a different floating-point type or as a non-EMA variant, use the...\"],[\"```\\n\\nHowever, this behavior is now deprecated since the \\\"revision\\\" argument should (just as it's don...\"],[\"```\\n\\nYou can also load and save model variants by specifying the `variant` argument in [`ModelMixin....\"],[\"```\\n\\n## Schedulers\\n\\nSchedulers are loaded from the [`SchedulerMixin.from_pretrained`] method, and un...\"],[\"# replace `dpm` with any of `ddpm`, `ddim`, `pndm`, `lms`, `euler_anc`, `euler`\\npipeline = StableDif...\"],[\"```\\n\\n## DiffusionPipeline explained\\n\\nAs a class method, [`DiffusionPipeline.from_pretrained`] is res...\"],[\"```\\n\\nYou'll see pipeline is an instance of [`StableDiffusionPipeline`], which consists of seven comp...\"],[\"```\\n\\nCompare the components of the pipeline instance to the [`runwayml\\u002fstable-diffusion-v1-5`](https...\"],[\"```\\n\\nYou can access each of the components of the pipeline as an attribute to view its configuration...\"],[\"```\\n\\nEvery pipeline expects a [`model_index.json`](https:\\u002f\\u002fhuggingface.co\\u002frunwayml\\u002fstable-diffusion-...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Creating noise from data is easy; creating data from noise is gene...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase can be found at [facebookresearch\\u002fdit](https:\\u002f\\u002fgithub.com\\u002ffacebookresearch\\u002fdit...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nWe recommend installing the [invisible-watermark](https:\\u002f\\u002fpypi.org\\u002fprojec...\"],[\"```\\n\\nYou can also use the [`~StableDiffusionXLPipeline.from_single_file`] method to load a model che...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"refiner = DiffusionPipeline.from_pretrained(\\n    \\\"stabilityai\\u002fstable-diffusion-xl-refiner-1.0\\\",\\n    ...\"],[\"```\\n\\nTo use this approach, you need to define the number of timesteps for each model to run through ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"img_url = \\\"https:\\u002f\\u002fraw.githubusercontent.com\\u002fCompVis\\u002flatent-diffusion\\u002fmain\\u002fdata\\u002finpainting_examples\\u002f...\"],[\"```\\n\\nThis ensemble of expert denoisers method works well for all available schedulers!\\n\\n### Base to ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Size conditioning\\n\\nThere are two types of size conditioning:\\n\\n- [`original_size`](https:...\"],[\"🤗 Diffusers also lets you specify negative conditions about an image's size to steer generation away...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-col justify-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffuser...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n## Use a different prompt for each text-encoder\\n\\nSDXL uses two text-encoders, so it is possible...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n## Other resources\\n\\nIf you're interested in experimenting with a minimal version of the [`UNet2...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Tips\\n\\nUse the `safety_concept` property of [`StableDiffusionPipelineSafe`] to check and edit the ...\"],[\"```\\nFor each image generation the active concept is also contained in [`StableDiffusionSafePipelineO...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nGreat, now you can import the rest of the dependencies you'll need:\\n\\n```python\\nimport jax.numpy...\"],[\"```\\n\\nModel parameters and inputs have to be replicated across the 8 parallel devices. The parameters...\"],[\"```\\n\\nTo take advantage of JAX's optimized speed on a TPU, pass `jit=True` to the pipeline to compile...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fYiYiXu\\u002ftest-doc-assets\\u002fresolve\\u002fmain\\u002fstable_diffusion_jax...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fYiYiXu\\u002ftest-doc-assets\\u002fresolve\\u002fmain\\u002fstable_diffusion_jax...\"],[\"```\\n\\nAfter calling `pmap`, the prepared function `p_generate` will:\\n\\n1. Make a copy of the underlyin...\"],[\"# Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pled...\"],[\"**Consequence**: A permanent ban from any sort of public interaction within\\nthe community.\\n\\n## Attri...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*There is large consent that successful training of deep networks r...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThen navigate to the example folder containing the training script and install the required dep...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMany of the basic and important parameters are described in the [Text-to-image](text2image#scri...\"],[\"```py\\nconditioning_image_transforms = transforms.Compose(\\n    [\\n        transforms.Resize(args.resol...\"],[\"```\\n\\nWithin the [`main()`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002faab6de22c33cc01fb7bc81c0807...\"],[\"```\\n\\nLastly, in the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002faab6de22c33cc01fb7...\"],[\"```\\n\\nIf you want to learn more about how the training loop works, check out the [Understanding pipel...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nTo monitor training progress with Weights & Biases, add the `--report_to=wandb` paramete...\"],[\"```\\n\\nOnce training is complete, you can use your T2I-Adapter for inference:\\n\\n```py\\nfrom diffusers im...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Stable Diffusion XL for JAX + TPUv5e\\n\\n[TPU v5e](https:\\u002f\\u002fcloud.google.com\\u002fblog\\u002fproducts\\u002fcompute\\u002fhow-c...\"],[\"👉 Try it out for yourself:\\n\\n[![Hugging Face Spaces](https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002f%F0%9F%A4%97%20Hugg...\"],[\"```\\npip install jax[tpu] -f https:\\u002f\\u002fstorage.googleapis.com\\u002fjax-releases\\u002flibtpu_releases.html\\n```\\n\\nNe...\"],[\"```\\nHere, a pre-trained model `stable-diffusion-xl-base-1.0` from the namespace `stabilityai` is loa...\"],[\"```\\nTo utilize JAX's parallel capabilities, the parameters and input tensors are duplicated across d...\"],[\"```\\nNow that the function is compiled, this section shows how to use it for fast inference. It measu...\"],[\"Once the function is compiled, these parameters are omitted from future calls and\\ncannot be changed ...\"],[\"```\\n\\nNext we can compile the generate function by executing `aot_compile`.\\n\\n```python\\nstart = time.t...\"],[\"```\\n\\nFrom this point forward, any calls to generate should result in a faster inference\\ntime and it ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"For a more technical overview of LCMs, refer to [the paper](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2310.04378...\"],[\"```python\\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler\\nimport...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"pipe = AutoPipelineForImage2Image.from_pretrained(\\n    \\\"Lykon\\u002fdreamshaper-7\\\",\\n    unet=unet,\\n    tor...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\ngenerator = torch....\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# load adapter\\nadapter = T2IAdapter.from_pretrained(\\\"TencentARC\\u002ft2i-adapter-canny-sdxl-1.0\\\", torch_d...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Diffusion probabilistic models (DPMs) have demonstrated a very pro...\"],[\"## UniPCMultistepScheduler\\n[[autodoc]] UniPCMultistepScheduler\\n\\n## SchedulerOutput\\n[[autodoc]] sched...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Take a look at the tensor values in the [`DDIMPipeline`] after two inference steps:\\n\\n```python\\nfrom ...\"],[\"```\\n\\nRunning the code above prints one value, but if you run it again you get a different value. Wha...\"],[\"```\\n\\nNow when you run the code above, it always prints a value of `1491.1711` no matter what because...\"],[\"```\\n\\nThe result is not the same even though you're using an identical seed because the GPU uses a di...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n💡 If reproducibility is important, we recommend always passing a CPU generator.\\nThe perf...\"],[\"```\\n\\nNow when you run the same pipeline twice, you'll get identical results.\\n\\n```py\\nimport torch\\nfro...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Guided image synthesis enables everyday users to create and edit p...\"],[\"## StableDiffusionPipelineOutput\\n\\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutp...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nNext, we move it to GPU:\\n\\n```python\\npipeline.to(\\\"cuda\\\")\\n```\\n\\n## Access the scheduler\\n\\nThe sched...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpatrickvonplaten\\u002fimag...\"],[\"```\\n\\n**Output**:\\n```\\n[diffusers.utils.dummy_torch_and_torchsde_objects.DPMSolverSDEScheduler,\\n diffu...\"],[\"```\\n\\nCool, lots of schedulers to look at. Feel free to have a look at their respective class definit...\"],[\"```\\n\\nreturns a dictionary of the configuration of the scheduler:\\n\\n**Output**:\\n```py\\nFrozenDict([('nu...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpatrickvonplaten\\u002fimag...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpatrickvonplaten\\u002fimag...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpatrickvonplaten\\u002fimag...\"],[\"prng_seed = jax.random.PRNGKey(0)\\nnum_inference_steps = 25\\n\\n# shard inputs and rng\\nparams = replicat...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nThe following Flax schedulers are _not yet compatible_ with the Flax Stab...\"],[\"# Textual Inversion fine-tuning example\\n\\n[Textual inversion](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.01618) is a ...\"],[\"```\\n\\n### Cat toy example\\n\\nFirst, let's login so that we can upload the checkpoint to the Hub during ...\"],[\"```\\n\\nA full training run takes ~1 hour on one V100 GPU.\\n\\n**Note**: As described in [the official pap...\"],[\"```\\n\\n```bash\\nexport MODEL_NAME=\\\"duongna\\u002fstable-diffusion-v1-4-flax\\\"\\nexport DATA_DIR=\\\"path-to-dir-con...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nThe quicktour is a simplified version of the introductory 🧨 Diffusers [notebook](https:\\u002f\\u002fcola...\"],[\"```\\n\\n- [🤗 Accelerate](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002faccelerate\\u002findex) speeds up model loading for infe...\"],[\"| **Task**                     | **Description**                                                    ...\"],[\"Start by creating an instance of a [`DiffusionPipeline`] and specify which pipeline checkpoint you w...\"],[\"```\\n\\nThe [`DiffusionPipeline`] downloads and caches all modeling, tokenization, and scheduling compo...\"],[\"```\\n\\n### Local pipeline\\n\\nYou can also use the pipeline locally. The only difference is you need to d...\"],[\"```\\n\\nTry generating an image with the new scheduler and see if you notice a difference!\\n\\nIn the next...\"],[\"```\\n\\nTo access the model parameters, call `model.config`:\\n\\n```py\\n\\u003e\\u003e\\u003e model.config\\n```\\n\\nThe model con...\"],[\"```\\n\\nFor inference, pass the noisy image and a `timestep` to the model. The `timestep` indicates how...\"],[\"```\\n\\nTo generate actual examples though, you'll need a scheduler to guide the denoising process. In ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n💡 Unlike a model, a scheduler does not have trainable weights and is parameter-free!\\n\\n\\u003c\\u002f...\"],[\"```\\n\\nTo speed up the denoising process, move the input and model to a GPU:\\n\\n```py\\n\\u003e\\u003e\\u003e model.to(\\\"cuda...\"],[\"```\\n\\nSit back and watch as a cat is generated from nothing but noise! 😻\\n\\n\\u003cdiv class=\\\"flex justify-ce...\"],[\"🧨 Diffusers Pipelines\\n\\nPipelines provide a simple way to run state-of-the-art diffusion models in in...\"],[\"To that end, we strive to offer all open-sourced, state-of-the-art diffusion system under a unified ...\"],[\"| Pipeline                                                                                          ...\"],[\"| [ddim](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmain\\u002fsrc\\u002fdiffusers\\u002fpipelines\\u002fddim)           ...\"],[\"| [score_sde_ve](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmain\\u002fsrc\\u002fdiffusers\\u002fpipelines\\u002fscore_sd...\"],[\"| [stable_diffusion](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmain\\u002fsrc\\u002fdiffusers\\u002fpipelines\\u002fstab...\"],[\"**Note**: Pipelines are simple examples of how to play around with the diffusion systems as describe...\"],[\"- [`from_pretrained` method](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f5cbed8e0d157f65d3ddc2420d...\"],[\"In addition, a `model_index.json` file is created at the root of the local path, *e.g.* `.\\u002fstable_di...\"],[\"**Note**: All pipelines have PyTorch's autograd disabled by decorating the `__call__` method with a ...\"],[\"- **Self-contained**: A pipeline shall be as self-contained as possible. More specifically, this mea...\"],[\"- **One-purpose-only**: Pipelines should be used for one task and one task only. Even if two tasks a...\"],[\"## Examples\\n\\n### Text-to-Image generation with Stable Diffusion\\n\\n```python\\n# make sure you're logged...\"],[\"```\\n\\n### Image-to-Image text-guided generation with Stable Diffusion\\n\\nThe `StableDiffusionImg2ImgPip...\"],[\"```\\nYou can also run this example on colab [![Open In Colab](https:\\u002f\\u002fcolab.research.google.com\\u002fasset...\"],[\"def download_image(url):\\n    response = requests.get(url)\\n    return PIL.Image.open(BytesIO(response...\"],[\"```\\n\\nYou can also run this example on colab [![Open In Colab](https:\\u002f\\u002fcolab.research.google.com\\u002fasse...\"],[\"Distillation for quantization on Textual Inversion models to personalize text2image\\n\\n[Textual invers...\"],[\"```\\n\\n## Prepare Datasets\\n\\nOne picture which is from the huggingface datasets [sd-concepts-library\\u002fdi...\"],[\"```\\n\\n## Do distillation for quantization\\n\\nDistillation for quantization is a method that combines [i...\"],[\"```\\n\\nAfter the distillation for quantization process, the quantized UNet would be 4 times smaller (3...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cbr\\u003e\\n\\nPipelines do not offer any training functionality. You'll notice PyTorch's autograd is disable...\"],[\"| Pipeline | Tasks |\\n|---|---|\\n| [AltDiffusion](alt_diffusion) | image2image |\\n| [AnimateDiff](anima...\"],[\"| [DiT](dit) | text2image |\\n| [GLIGEN](stable_diffusion\\u002fgligen) | text2image |\\n| [InstructPix2Pix](p...\"],[\"| [Self-Attention Guidance](self_attention_guidance) | text2image |\\n| [Semantic Guidance](semantic_s...\"],[\"| [Value-guided planning](value_guided_sampling) | value guided sampling |\\n| [Versatile Diffusion](v...\"],[\"## DiffusionPipeline\\n\\n[[autodoc]] DiffusionPipeline\\n\\t- all\\n\\t- __call__\\n\\t- device\\n\\t- to\\n\\t- components...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The library has three main components:\\n\\n- State-of-the-art diffusion pipelines for inference with ju...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003c\\u002fa\\u003e\\n    \\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" ...\"],[\"Latent Consistency Distillation Example:\\n\\n[Latent Consistency Models (LCMs)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2...\"],[\"```\\n\\nWhen running `accelerate config`, if we specify torch compile mode to True there can be dramati...\"],[\"```bash\\nexport MODEL_NAME=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsaved\\u002fmodel\\\"\\n\\n...\"],[\"```\\n\\n## LCM-LoRA\\n\\nInstead of fine-tuning the full model, we can also just train a LoRA that can be i...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Loading from the original format\\n\\nBy default the [`ControlNetModel`] should be loaded with [`~Mod...\"],[\"```\\n\\n## ControlNetModel\\n\\n[[autodoc]] ControlNetModel\\n\\n## ControlNetOutput\\n\\n[[autodoc]] models.contro...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[Kandinsky 3](..\\u002fapi\\u002fpipelines\\u002fkandinsky3) simplifies the architecture and shifts away from the two-...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nKandinsky 2.1 and 2.2 usage is very similar! The only difference is Kandi...\"],[\"```\\n\\nNow pass all the prompts and embeddings to the [`KandinskyPipeline`] to generate an image:\\n\\n```...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatas...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n🤗 Diffusers also provides an end-to-end API with the [`KandinskyCombi...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Kandinsky 2.2\\\"\\u003e\\n\\n```py\\nfrom diffusers import AutoPipelineForText2Imag...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Kandinsky 2.2\\\"\\u003e\\n\\n```py\\nimport torch\\nfrom diffusers import KandinskyV2...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fraw.githubuserconten...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatas...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n🤗 Diffusers also provides an end-to-end API with the [`KandinskyImg2I...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Kandinsky 2.2\\\"\\u003e\\n\\n```py\\nfrom diffusers import AutoPipelineForImage2Ima...\"],[\"```\\n\\n\\u003c\\u002fTip\\u003e\\n\\nFor inpainting, you'll need the original image, a mask of the area to replace in the or...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\nLoad an initial image and create a mask:\\n\\n```py\\ninit_image = load_ima...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatas...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatas...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Kandinsky 2.2\\\"\\u003e\\n\\n```py\\nimport torch\\nimport numpy as np\\nfrom PIL impor...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## Interpolation\\n\\nInterpolation allows you to explore the latent spac...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Kandinsky 2.2\\\"\\u003e\\n\\n```py\\nfrom diffusers import KandinskyV22PriorPipelin...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"htt...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatas...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatas...\"],[\"```\\n\\nGenerate the image embeddings from a prompt and negative prompt:\\n\\n```py\\nprompt = \\\"A robot, 4k p...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatas...\"],[\"```\\n\\nLoad the prior pipeline and the [`KandinskyV22ControlnetImg2ImgPipeline`]:\\n\\n```py\\nprior_pipelin...\"],[\"```\\n\\nNow you can run the [`KandinskyV22ControlnetImg2ImgPipeline`] to generate an image from the ini...\"],[\"```\\n\\n2. Enable `torch.compile` if you're using PyTorch \\u003e= 2.0 to automatically use scaled dot-produc...\"],[\"# Diffusers examples with ONNXRuntime optimizations\\n\\n**This research project is not actively maintai...\"],[\"Multi Subject DreamBooth training\\n\\n[DreamBooth](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.12242) is a method to per...\"],[\"```\\n\\n### Multi Subject Training Example\\nIn order to have your model learn multiple concepts at once,...\"],[\"accelerate launch train_multi_subject_dreambooth.py \\\\\\n  --pretrained_model_name_or_path=$MODEL_NAME ...\"],[\"```\\n\\nThis example shows training for 2 subjects, but please note that the model can be trained on an...\"],[\"An example of how to generate the file:\\n```python\\nimport json\\n\\n# here we are using parameters for pr...\"],[\"```\\nAnd then just point to the file when executing the script:\\n\\n```bash\\n# exports...\\naccelerate laun...\"],[\"```\\n\\n### Inference from a training checkpoint\\n\\nYou can also perform inference from one of the checkp...\"],[\"accelerate launch train_dreambooth.py \\\\\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\\\\n  --instanc...\"],[\"```\\n\\n### Training with prior-preservation loss\\n\\nPrior-preservation is used to avoid overfitting and ...\"],[\"```\\n\\n\\n### Training on a 16GB GPU:\\n\\nWith the help of gradient checkpointing and the 8-bit optimizer f...\"],[\"```\\n\\n### Training on a 8 GB GPU:\\n\\nBy using [DeepSpeed](https:\\u002f\\u002fwww.deepspeed.ai\\u002f) it's possible to o...\"],[\"accelerate launch --mixed_precision=\\\"fp16\\\" train_dreambooth.py \\\\\\n  --pretrained_model_name_or_path=$...\"],[\"```\\n\\n### Fine-tune text encoder with the UNet.\\n\\nThe script also allows to fine-tune the `text_encode...\"],[\"```\\n\\n### Using DreamBooth for other pipelines than Stable Diffusion\\n\\nAltdiffusion also support dream...\"],[\"Inference Examples\\n\\n**The inference examples folder is deprecated and will be removed in a future ve...\"],[\"ControlNet training example for Stable Diffusion XL (SDXL)\\n\\nThe `train_controlnet_sdxl.py` script sh...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell (e.g., a notebook)\\n\\n```python\\nfrom ...\"],[\"```\\n\\nThen run `huggingface-cli login` to log into your Hugging Face account. This is needed to be ab...\"],[\"```\\n\\nTo better track our training experiments, we're using the following flags in the command above:...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nYou'll notice throughout the guide, we use [`~DiffusionPipeline.enable_model_cpu_offload...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```py\\nimport torch\\nfrom diffusers import AutoPipelineForImage2Image\\nfrom diffusers.utils import make...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"# prepare image\\nurl = \\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"# prepare image\\nurl = \\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```py\\nimport torch\\nfrom diffusers import AutoPipelineForImage2Image\\nfrom diffusers.utils import make...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```py\\nimport torch\\nfrom diffusers import AutoPipelineForImage2Image\\nfrom diffusers.utils import make...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```py\\nimport torch\\nfrom diffusers import AutoPipelineForImage2Image\\nfrom diffusers.utils import make...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```py\\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\\nimport torch\\nfrom ...\"],[\"```\\n\\nNow you can pass this generated image to the image-to-image pipeline:\\n\\n```py\\npipeline = AutoPip...\"],[\"```\\n\\n### Image-to-image-to-image\\n\\nYou can also chain multiple image-to-image pipelines together to c...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIt is important to specify `output_type=\\\"latent\\\"` in the pipeline to keep all the output...\"],[\"```\\n\\n### Image-to-upscaler-to-super-resolution\\n\\nAnother way you can chain your image-to-image pipeli...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIt is important to specify `output_type=\\\"latent\\\"` in the pipeline to keep all the output...\"],[\"```\\n\\n## Control image generation\\n\\nTrying to generate an image that looks exactly the way you want ca...\"],[\"```\\n\\n### ControlNet\\n\\nControlNets provide a more flexible and accurate way to control image generatio...\"],[\"```\\n\\nLoad a ControlNet model conditioned on depth maps and the [`AutoPipelineForImage2Image`]:\\n\\n```p...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"prompt = \\\"elden ring style astronaut in a jungle\\\" # include the token \\\"elden ring style\\\" in the prom...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocum...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nGenerating multiple prompts in a batch seems to take too much memory. Whi...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002foptimum\\u002fdocumen...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## StableDiffusionPipelineOutput\\n\\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutp...\"],[\"Stable Diffusion XL text-to-image fine-tuning\\n\\nThe `train_text_to_image_sdxl.py` script shows how to...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell (e.g., a notebook)\\n\\n```python\\nfrom ...\"],[\"```\\n\\n**Notes**:\\n\\n*  The `train_text_to_image_sdxl.py` script pre-computes text embeddings and the VA...\"],[\"```\\n\\n### Inference in Pytorch XLA\\n```python\\nfrom diffusers import DiffusionPipeline\\nimport torch\\nimp...\"],[\"```\\n\\nNote: There is a warmup step in PyTorch XLA. This takes longer because of\\ncompilation and optim...\"],[\"With LoRA, it's possible to fine-tune Stable Diffusion on a custom image-caption pair dataset\\non con...\"],[\"```\\n\\nFor this example we want to directly store the trained LoRA embeddings on the Hub, so\\nwe need t...\"],[\"```\\n\\nThe above command will also run inference as fine-tuning progresses and log the results to Weig...\"],[\"```\\n\\n### Inference\\n\\nOnce you have trained a model using above command, the inference can be done sim...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nGenerating multiple prompts in a batch can [crash](https:\\u002f\\u002fgithub.com\\u002fhug...\"],[\"```\\n\\n## Troubleshoot\\n\\nM1\\u002fM2 performance is very sensitive to memory pressure. When this occurs, the ...\"],[\"\\u003c!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ve...\"],[\"```\\n\\nNavigate to the example folder with the training script and install the required dependencies f...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nSome other basic and important parameters to specify include:\\n\\n- `--pretrained_model_name_or_pa...\"],[\"Next, you'll find the dataset preprocessing code and training loop in the [`main()`](https:\\u002f\\u002fgithub....\"],[\"# Load scheduler and models\\nnoise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_na...\"],[\"```\\n\\nThe special [placeholder token](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fb81c69e489aad3a0b...\"],[\"```\\n\\nFinally, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fb81c69e489aad3a0ba73...\"],[\"```\\n\\nSet the environment variable `MODEL_NAME` to a model id on the Hub or a path to a local model, ...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\n```bash\\nexport MODEL_NAME=\\\"duongna\\u002fstable-diffusion-v1-4-flax...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\nFlax doesn't support the [`~loaders.TextualInversionLoaderMix...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## Next steps\\n\\nCongratulations on training your own Textual Inversion...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase can be found at [openai\\u002fconsistency_models](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002fconsiste...\"],[\"```\\n\\n\\n## ConsistencyModelPipeline\\n[[autodoc]] ConsistencyModelPipeline\\n    - all\\n    - __call__\\n\\n## ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [AutoPipeline](..\\u002f..\\u002ftutorials\\u002fautopipeline) tutorial to learn how to use ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nTo use with Stable Diffusion XL 1.0\\n\\n```python\\nimport torch\\nfrom diffusers import DiffusionPipe...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This guide will explore the [train_lcm_distill_sd_wds.py](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fb...\"],[\"```\\n\\nThen navigate to the example folder containing the training script and install the required dep...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMost of the parameters are identical to the parameters in the [Text-to-image](text2image#script...\"],[\"```py\\ndef transform(example):\\n    image = example[\\\"image\\\"]\\n    image = TF.resize(image, resolution, ...\"],[\"```\\n\\nFor improved performance on reading and writing large datasets stored in the cloud, this script...\"],[\"```\\n\\nNow you can create the [optimizer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f3b37488fa3280a...\"],[\"```\\n\\nNext, you're ready to setup the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f3...\"],[\"```\\n\\nIt gets the [teacher model predictions](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f3b37488fa...\"],[\"```\\n\\nIf you want to learn more about how the training loop works, check out the [Understanding pipel...\"],[\"```bash\\nexport MODEL_DIR=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsaved\\u002fmodel\\\"\\n\\na...\"],[\"```\\n\\nOnce training is complete, you can use your new LCM for inference.\\n\\n```py\\nfrom diffusers import...\"],[\"```\\n\\n## LoRA\\n\\nLoRA is a training technique for significantly reducing the number of trainable parame...\"],[\"## Next steps\\n\\nCongratulations on distilling a LCM model! To learn more about LCM, the following may...\"],[\"Custom Diffusion training example \\n\\n[Custom Diffusion](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2212.04488) is a method...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell e.g. a notebook\\n\\n```python\\nfrom acc...\"],[\"```\\n\\n**___Note: Change the `resolution` to 768 if you are using the [stable-diffusion-2](https:\\u002f\\u002fhug...\"],[\"```\\n\\n**Use `--enable_xformers_memory_efficient_attention` for faster training with lower VRAM requir...\"],[\"```\\n\\nHere is an example [Weights and Biases page](https:\\u002f\\u002fwandb.ai\\u002fsayakpaul\\u002fcustom-diffusion\\u002fruns\\u002f2...\"],[\"```\\n\\nHere is an example [Weights and Biases page](https:\\u002f\\u002fwandb.ai\\u002fsayakpaul\\u002fcustom-diffusion\\u002fruns\\u002f3...\"],[\"```\\n\\n## Inference\\n\\nOnce you have trained a model using the above command, you can run inference usin...\"],[\"```\\n\\nHere is an example of performing inference with multiple concepts:\\n\\n```python\\nimport torch\\nfrom...\"],[\"```\\n\\nHere, `cat` and `wooden pot` refer to the multiple concepts.\\n\\n### Inference from a training che...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*There is large consent that successful training of deep networks r...\"],[\"!---\\nCopyright 2023- The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, ...\"],[\"```\\n\\nFor example:\\n\\n```bash\\ndoc-builder preview diffusers docs\\u002fsource\\u002fen\\n```\\n\\nThe docs will be viewab...\"],[\"```\\nand of course, if you moved it to another file, then:\\n\\n```md\\nSections that were moved:\\n\\n[ \\u003ca hre...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\nFo...\"],[\"```\\n[[autodoc]] XXXPipeline\\n    - all\\n\\t- __call__\\n```\\n\\nThis will include every public method of the ...\"],[\"```\\n\\nYou can follow the same process to create a new scheduler under the `docs\\u002fsource\\u002f\\u003clanguageCode\\u003e...\"],[\"```\\n\\nIf the description is too long to fit in one line, another indentation is necessary before writ...\"],[\"```\\n\\n#### Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Tips\\n\\nThe paper [Common Diffusion Noise Schedules and Sample Steps are Flawed](https:\\u002f\\u002fhuggingfac...\"],[\"```\\n\\n2. train a model with `v_prediction` (add the following argument to the [train_text_to_image.py...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThen navigate to the example folder containing the training script and install the required dep...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nSome basic and important parameters include:\\n\\n- `--pretrained_model_name_or_path`: the name of ...\"],[\"```\\n\\nYou can compare the loss surfaces for different `snr_gamma` values in this [Weights and Biases]...\"],[\"```\\n\\nThen the script [loads the UNet](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f8959c5b9dec1c94d...\"],[\"```\\n\\nLastly, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f8959c5b9dec1c94d6ba48...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n```bash\\nexport MODEL_NAME=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport dataset_name=\\\"lambdalabs\\u002fp...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\nTraining with Flax can be faster on TPUs and GPUs thanks to [...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\n```py\\nimport jax\\nimport numpy as np\\nfrom flax.jax_utils impor...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*StableDiffusion is a revolutionary text-to-image generator that is...\"],[\"Evaluation results can be found in section 4.1 of the original paper.\\n\\n## Available checkpoints\\n\\n* [...\"],[\"image = pipe(prompt=prompt, image=original_image, mask_image=mask_image).images[0]\\nmake_image_grid([...\"],[\"```\\n\\n## AsymmetricAutoencoderKL\\n\\n[[autodoc]] models.autoencoders.autoencoder_asym_kl.AsymmetricAutoe...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*We introduce Würstchen, a novel architecture for text-to-image syn...\"],[\"## Würstchen Overview\\nWürstchen is a diffusion model, whose text-conditional model works in a highly...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fgithub.com\\u002fdome272\\u002fWuerstchen\\u002fassets\\u002f61938694\\u002f2914830f-cbd3-461c-be64-d50734f4b49d...\"],[\"```\\n\\nFor explanation purposes, we can also initialize the two main pipelines of Würstchen individual...\"],[\"caption = \\\"Anthropomorphic cat dressed as a fire fighter\\\"\\nnegative_prompt = \\\"\\\"\\n\\nprior_output = prior...\"],[\"```\\n\\n## Speed-Up Inference\\nYou can make use of `torch.compile` function and gain a speed-up of about...\"],[\"```\\n\\n## Limitations\\n\\n- Due to the high compression employed by Würstchen, generations can lack a goo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*By decomposing the image formation process into a sequential appli...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## StableDiffusionPipeline\\n\\n[[autodoc]] StableDiffusionPipeline\\n\\t- all\\n\\t- __call__\\n\\t- enable...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Simple over easy\\n\\nAs PyTorch states, **explicit is better than implicit** and **simple is better ...\"],[\"In Diffusers, we follow this philosophy for both pipelines and schedulers, but only partly for diffu...\"],[\"- Models intend to expose complexity, just like PyTorch's `Module` class, and give clear error messa...\"],[\"### Schedulers\\n\\nSchedulers are responsible to guide the denoising process for inference as well as t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003cjax\\u003e\\n```bash\\npip install diffusers[\\\"flax\\\"] transformers\\n```\\n\\u003c\\u002fjax\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n#...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003cjax\\u003e\\n```bash\\npip install -e \\\".[flax]\\\"\\n```\\n\\u003c\\u002fjax\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nThese commands will...\"],[\"```\\n\\nFor more details about managing and cleaning the cache, take a look at the [caching](https:\\u002f\\u002fhu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"| A1111\\u002fk-diffusion    | 🤗 Diffusers                         | Usage                                ...\"],[\"| DPM++ SDE           | [`DPMSolverSinglestepScheduler`]    |                                       ...\"],[\"All schedulers are built from the base [`SchedulerMixin`] class which implements low level utilities...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*The past few years have witnessed the great success of Diffusion m...\"],[\"## DEISMultistepScheduler\\n[[autodoc]] DEISMultistepScheduler\\n\\n## SchedulerOutput\\n[[autodoc]] schedul...\"],[\"# Textual Inversion fine-tuning example\\n\\n[Textual inversion](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.01618) is a ...\"],[\"```\\n\\nAnd initialize an [🤗Accelerate](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002faccelerate\\u002f) environment with:\\n\\n...\"],[\"```\\n\\nThis will be our training data.\\nNow we can launch the training using\\n\\n## Use ONNXRuntime to acc...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase can be found at [hohonathanho\\u002fdiffusion](https:\\u002f\\u002fgithub.com\\u002fhojonathanho\\u002fdiffu...\"],[\"Kandinsky2.2 text-to-image fine-tuning\\n\\nKandinsky 2.2 includes a prior pipeline that generates image...\"],[\"```\\n\\nAnd initialize an [🤗Accelerate](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002faccelerate\\u002f) environment with:\\n\\n...\"],[\"```\\n\\nTo disable wandb logging, remove the `--report_to==\\\"wandb\\\"` and `--validation_prompts=\\\"A robot ...\"],[\"```\\n\\u003c!-- accelerate_snippet_end --\\u003e\\n\\n\\nTo train on your own training files, prepare the dataset accor...\"],[\"```\\n\\nCheckpoints only save the unet, so to run inference from a checkpoint, just load the unet\\n```py...\"],[\"```\\n\\u003c!-- accelerate_snippet_end --\\u003e\\n\\n\\nTo perform inference with the fine-tuned prior model, you will...\"],[\"```\\n\\nIf you want to use a fine-tuned decoder checkpoint along with your fine-tuned prior checkpoint,...\"],[\"```\\n\\n\\n#### Training with Min-SNR weighting\\n\\nWe support training with the Min-SNR weighting strategy ...\"],[\"With LoRA, it's possible to fine-tune Kandinsky 2.2 on a custom image-caption pair dataset\\non consum...\"],[\"```\\n\\n#### Train prior\\n\\n```bash\\nexport DATASET_NAME=\\\"lambdalabs\\u002fpokemon-blip-captions\\\"\\n\\naccelerate la...\"],[\"```\\n\\n**___Note: When using LoRA we can use a much higher learning rate compared to non-LoRA fine-tun...\"],[\"```\\n\\n### Training with xFormers:\\n\\nYou can enable memory efficient attention by [installing xFormers]...\"],[\"e don't yet support training T2I-Adapters on Stable Diffusion yet. For training T2I-Adapters on Stab...\"],[\"Research projects\\n\\nThis folder contains various research projects using 🧨 Diffusers.\\nThey are not re...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Here's the overview from the [project page](https:\\u002f\\u002fvislearn.github.io\\u002fControlNet-XS\\u002f):\\n\\n*With incre...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Use TensorFloat-32\\n\\nOn Ampere and later CUDA devices, matrix multiplications and convolutions can...\"],[\"```\\n\\nYou can learn more about TF32 in the [Mixed precision training](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftra...\"],[\"[DreamBooth](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002ftree\\u002fmain\\u002fexamples\\u002fdreambooth) by [colossalai]...\"],[\"```\\n\\n## Dataset for Teyvat BLIP captions\\nDataset used to train [Teyvat characters text to image mode...\"],[\"torchrun --nproc_per_node 2 train_dreambooth_colossalai.py \\\\\\n  --pretrained_model_name_or_path=$MODE...\"],[\"```\\n\\n\\n### Training with prior-preservation loss\\n\\nPrior-preservation is used to avoid overfitting and...\"],[\"```\\n\\n## Inference\\n\\nOnce you have trained a model using above command, the inference can be done simp...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe partially inverted latents are generated from the [`~StableDiffusionDiffEditPipeline.invert...\"],[\"```\\n\\nUse the [`~StableDiffusionDiffEditPipeline.generate_mask`] function to generate the image mask....\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fgithub.com\\u002fXiang-cd\\u002fD...\"],[\"```\\n\\nNext, create a utility function to generate the prompts:\\n\\n```py\\n@torch.no_grad()\\ndef generate_p...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [generation strategy](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fgen...\"],[\"```\\n\\nFinally, pass the embeddings to the [`~StableDiffusionDiffEditPipeline.generate_mask`] and [`~S...\"],[\"```\\n\\n## Generate a caption for inversion\\n\\nWhile you can use the `source_prompt` as a caption to help...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cfigure\\u003e\\n        \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fgit...\"],[\"Community Examples\\n\\n\\u003e **For more information about community pipelines, please have a look at [this ...\"],[\"| Example                                                                                           ...\"],[\"--------------------------------------------|:------------------------------------------------------...\"],[\"---------------------------------------------------------------------------|:-----------------------...\"],[\"------------|--------------------------------------------------------------:|...\"],[\"| LLM-grounded Diffusion (LMD+)                                                                     ...\"],[\"| Stable Diffusion Interpolation                                                                    ...\"],[\"| Wild Card Stable Diffusion                                                                        ...\"],[\"| Image to Image Inpainting Stable Diffusion                                                        ...\"],[\"MagicMix                                                                                            ...\"],[\"| CLIP Guided Img2Img Stable Diffusion Pipeline                                                     ...\"],[\"| CLIP Guided Images Mixing Stable Diffusion Pipeline | Сombine images using usual diffusion models....\"],[\"FABRIC - Stable Diffusion with feedback Pipeline | pipeline supports feedback from liked and dislike...\"],[\"|   Latent Consistency Interpolation Pipeline                                                       ...\"],[\"| AnimateDiff ControlNet Pipeline                                                                   ...\"],[\"To load a custom pipeline you just need to pass the `custom_pipeline` argument to `DiffusionPipeline...\"],[\"```\\n\\n## Example usages\\n\\n### LLM-grounded Diffusion\\n\\nLMD and LMD+ greatly improves the prompt underst...\"],[\"#### Use this pipeline with an LLM\\n```python\\nimport torch\\nfrom diffusers import DiffusionPipeline\\n\\np...\"],[\"```\\n\\n#### Use this pipeline on its own for layout generation\\n```python\\nimport torch\\nfrom diffusers i...\"],[\"```\\n\\n### CLIP Guided Stable Diffusion\\n\\nCLIP guided stable diffusion can help to generate more realis...\"],[\"```\\n\\nThe `images` list contains a list of PIL images that can be saved locally or displayed directly...\"],[\"```\\n\\nThe output of the `walk(...)` function returns a list of images saved under the folder as defin...\"],[\"images = pipe.img2img(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images\\n\\n##...\"],[\"```\\n\\nAs shown above this one pipeline can run all both \\\"text-to-image\\\", \\\"image-to-image\\\", and \\\"inpai...\"],[\"torch_dtype=torch.float16\\n)\\npipe=pipe.to(\\\"cuda\\\")\\n\\nprompt = \\\"best_quality (1girl:1.3) bow bride brown...\"],[\"```\\n\\n#### onnxruntime\\n\\n```python\\nfrom diffusers import DiffusionPipeline\\nimport torch\\n\\npipe = Diffus...\"],[\"```\\n\\nif you see `Token indices sequence length is longer than the specified maximum sequence length ...\"],[\"```\\nThis example produces the following image:\\n\\n![image](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f4...\"],[\"```\\ndog\\ncat\\nmouse\\n```\\n\\ncreate `object.txt`, with contents like:\\n\\n```\\nchair\\nsofa\\nbench\\n```\\n\\n```python...\"],[\"```\\n\\n### Composable Stable diffusion\\n\\n[Composable Stable Diffusion](https:\\u002f\\u002fenergy-based-model.githu...\"],[\"pipe.safety_checker = None\\n\\nimages = []\\ngenerator = th.Generator(\\\"cuda\\\").manual_seed(args.seed)\\nfor ...\"],[\"```\\n\\n### Imagic Stable Diffusion\\nAllows you to edit an image using stable diffusion....\"],[\"```python\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nimport torch\\nimport os\\nfrom d...\"],[\"os.makedirs(\\\"imagic\\\", exist_ok=True)\\nimage = res.images[0]\\nimage.save('.\\u002fimagic\\u002fimagic_image_alpha_1...\"],[\"```\\n\\n### Seed Resizing\\nTest seed resizing. Originally generate an image in 512 by 512, then generate...\"],[\"width = 512\\nheight = 592\\n\\nres = pipe(\\n    prompt,\\n    guidance_scale=7.5,\\n    num_inference_steps=50...\"],[\"```\\n\\n### Multilingual Stable Diffusion Pipeline\\n\\nThe following code can generate an images from text...\"],[\"diffuser_pipeline = DiffusionPipeline.from_pretrained(\\n    \\\"CompVis\\u002fstable-diffusion-v1-4\\\",\\n    cust...\"],[\"```\\n\\nThis example produces the following images:\\n![image](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f...\"],[\"```\\n\\n![2 by 2 grid demonstrating image to image inpainting.](https:\\u002f\\u002fuser-images.githubusercontent.c...\"],[\"```\\n\\n### Bit Diffusion\\nBased https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.04202, this is used for diffusion on discret...\"],[\"```\\n\\nTo make sure that K Diffusion and `diffusers` yield the same results:\\n\\n**Diffusers**:\\n```python...\"],[\"```\\n\\n![diffusers_euler](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpatrickvonplaten\\u002fimages\\u002fresolve\\u002fmain\\u002fk_diffu...\"],[\"#Three checkpoint merging. Only \\\"add_difference\\\" method actually works on all three checkpoints. Usi...\"],[\"```\\nSome examples along with the merge details:\\n\\n1. \\\"CompVis\\u002fstable-diffusion-v1-4\\\" + \\\"hakurei\\u002fwaifu...\"],[\"### Stable Diffusion Comparisons\\n\\nThis Community Pipeline enables the comparison between the 4 check...\"],[\"```\\n\\nAs a result, you can look at a grid of all 4 generated images being shown together, that captur...\"],[\"```\\nThe `mix_img` is a PIL image that can be saved locally or displayed directly in a google colab. ...\"],[\"```python\\nimport torch\\nfrom diffusers import DiffusionPipeline\\n\\ndevice = torch.device(\\\"cpu\\\" if not t...\"],[\"# this pipeline only use prior module in \\\"kakaobrain\\u002fkarlo-v1-alpha\\\"\\n# It is used to convert clip te...\"],[\"```\\n\\n\\n`shiba-inu.jpg`\\n\\n\\n![shiba-inu](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f16448529\\u002f209185639-6e...\"],[\"```\\n\\nThe resulting images in order:-\\n\\n![result_0](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fNagaSaiAbhinay\\u002fUnC...\"],[\"```python\\nimport torch\\nfrom diffusers import DiffusionPipeline\\nfrom PIL import Image\\n\\ndevice = torch...\"],[\"```\\nThe original images:-\\n\\n![starry](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fNagaSaiAbhinay\\u002fUnCLIPImageInter...\"],[\"### DDIM Noise Comparative Analysis Pipeline\\n#### **Research question: What visual concepts do the d...\"],[\"for strength in np.linspace(0.1, 1, 25):\\n    denoised_image, latent_timestep = pipe(\\n        image_p...\"],[\"```\\n\\nHere is the result of this pipeline (which is DDIM) on CelebA-HQ dataset.\\n\\n![noise-comparative-...\"],[\"The following code requires roughly 12GB of GPU RAM.\\n\\n```python\\nfrom io import BytesIO\\nimport reques...\"],[\"```\\n\\nInit Image\\n\\n![img2img_init_clip_guidance](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fnjindal\\u002fimages\\u002fresolv...\"],[\"```\\n\\n### EDICT Image Editing Pipeline\\n\\nThis pipeline implements the text-guided image editing approa...\"],[\"# initialize pipeline\\npipeline = DiffusionPipeline.from_pretrained(\\n    pretrained_model_name_or_pat...\"],[\"```\\n\\nInit Image\\n\\n![img2img_init_edict_text_editing](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fJoqsan\\u002fimages\\u002fre...\"],[\"Disclaimer: The mask gets transferred into latent space, this may lead to unexpected changes on the ...\"],[\"```\\n\\n### TensorRT Image2Image Stable Diffusion Pipeline\\n\\nThe TensorRT Pipeline can be used to accele...\"],[\"```\\n\\n### Stable Diffusion Reference\\n\\nThis pipeline uses the Reference Control. Refer to the [sd-webu...\"],[\"```\\n\\nReference Image\\n\\n![reference_image](https:\\u002f\\u002fhf.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fres...\"],[\"```py\\nimport cv2\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nfrom diffusers import UniPCMu...\"],[\"```\\n\\nReference Image\\n\\n![reference_image](https:\\u002f\\u002fhf.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fres...\"],[\"```\\n**Note:** To install a specific version, run with the following command:\\n```\\npython -m pip insta...\"],[\"```\\n\\nThe following code compares the performance of the original stable diffusion pipeline with the ...\"],[\"##############     fp32 inference performance    ###############\\n\\n# 1. IPEX Pipeline initialization\\n...\"],[\"```\\n\\n### CLIP Guided Images Mixing With Stable Diffusion\\n\\n![clip_guided_images_mixing_examples](http...\"],[\"# text2img\\nt2i_images = pipe(\\n    prompt=prompt,\\n    negative_prompt=neg_prompt,\\n).images # alternat...\"],[\"```\\n\\nIn the above code, the `prompt2` is appended to the `prompt`, which is more than 77 tokens. \\\"bi...\"],[\"# Pipeline creating\\nmixing_pipeline = DiffusionPipeline.from_pretrained(\\n    \\\"CompVis\\u002fstable-diffusi...\"],[\"```\\n\\n![image_mixing_result](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fTheDenk\\u002fimages_mixing\\u002fresolve\\u002fmain\\u002fborom...\"],[\"```\\n![mixture_tiling_results](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fkadirnar\\u002fdiffusers_readme_images\\u002fresol...\"],[\"mask_url = \\\"https:\\u002f\\u002fraw.githubusercontent.com\\u002fCompVis\\u002flatent-diffusion\\u002fmain\\u002fdata\\u002finpainting_examples...\"],[\"```\\n\\n### Stable Diffusion Mixture Canvas\\n\\nThis pipeline uses the Mixture. Refer to the [Mixture](htt...\"],[\"```\\n![Input_Image](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fkadirnar\\u002fdiffusers_readme_images\\u002fresolve\\u002fmain\\u002finp...\"],[\"```\\n\\nThe training loop is also straightforward:\\n\\n```python\\n\\n# Training loop\\nwhile True:\\n    x0 = sam...\"],[\"```\\n\\n### Zero1to3 pipeline\\n\\nThis pipeline is the implementation of the [Zero-1-to-3: Zero-shot One I...\"],[\"# load image\\n# H, W = (256, 256) # H, W = (512, 512)   # zero123 training is 256,256\\n\\n# for batch in...\"],[\"# better do preprocessing\\nfrom gradio_new import preprocess_image, create_carvekit_interface\\nimport ...\"],[\"```\\n\\n### Stable Diffusion XL Reference\\n\\nThis pipeline uses the Reference . Refer to the [stable_diff...\"],[\"```\\n\\nReference Image\\n\\n![reference_image](https:\\u002f\\u002fhf.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fres...\"],[\"### Stable diffusion fabric pipeline\\n\\nFABRIC approach applicable to a wide range of popular diffusio...\"],[\"```\\n\\n*With enough feedbacks you can create very similar high quality images.*\\n\\nThe original codebase...\"],[\"```python\\nimg = PIL.Image.open(\\\".\\u002fmech.png\\\")\\n# read image with mask painted over\\nimg_paint = PIL.Ima...\"],[\"```\\n\\noriginal image mech.png\\n\\n\\u003cimg src=https:\\u002f\\u002fgithub.com\\u002fnoskill\\u002fdiffusers\\u002fassets\\u002f733626\\u002f10ad972d-d...\"],[\"```\\n\\nAnd abbreviated examples for the other edits:\\n\\n`ReplaceEdit with local blend`\\n```python\\nprompts...\"],[\"```\\n\\nSide note: See [this GitHub gist](https:\\u002f\\u002fgist.github.com\\u002fUmerHA\\u002fb65bb5fb9626c9c73f3ade2869e361...\"],[\"The model can be used with `diffusers` as follows:\\n\\n - *1. Load the model from the community pipelin...\"],[\"```\\n\\n- 2. Run inference with as little as 4 steps:\\n\\n```py\\nprompt = \\\"Self-portrait oil painting, a be...\"],[\"```\\n\\n- 2. Run inference with as little as 4 steps:\\n\\n```py\\nprompt = \\\"Self-portrait oil painting, a be...\"],[\"```\\n\\n\\n\\n### Latent Consistency Interpolation Pipeline\\n\\nThis pipeline extends the Latent Consistency P...\"],[\"torch.manual_seed(seed)\\nnp.random.seed(seed)\\n\\nimages = pipe(\\n    prompt=prompts,\\n    height=512,\\n   ...\"],[\"```\\n\\n###  StableDiffusionUpscaleLDM3D Pipeline\\n[LDM3D-VR](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2311.03226.pdf) is a...\"],[\"prompt =f\\\"A picture of some lemons on a table\\\"\\noutput = pipe_ldm3d(prompt)\\nrgb_image, depth_image = ...\"],[\"### ControlNet + T2I Adapter Pipeline\\nThis pipelines combines both ControlNet and T2IAdapter into a ...\"],[\"pipe = StableDiffusionXLControlNetAdapterPipeline.from_pretrained(\\n    \\\"stabilityai\\u002fstable-diffusion...\"],[\"```\\n\\n### ControlNet + T2I Adapter + Inpainting Pipeline\\n```py\\nimport cv2\\nimport numpy as np\\nimport t...\"],[\"pipe = StableDiffusionXLControlNetAdapterInpaintPipeline.from_pretrained(\\n    \\\"diffusers\\u002fstable-diff...\"],[\"depth_image = midas_depth(\\n  image, detect_resolution=512, image_resolution=1024\\n)\\n\\nstrength = 0.4\\n\\n...\"],[\"```\\n\\n### Regional Prompting Pipeline\\nThis pipeline is a port of the [Regional Prompter extension](ht...\"],[\"```\\n### Cols, Rows mode\\nIn the Cols, Rows mode, you can split the screen vertically and horizontally...\"],[\"```\\n![sample](https:\\u002f\\u002fgithub.com\\u002fhako-mikan\\u002fsd-webui-regional-prompter\\u002fblob\\u002fimgs\\u002frp_pipeline4.png)\\n\\n...\"],[\"```\\n![sample](https:\\u002f\\u002fgithub.com\\u002fhako-mikan\\u002fsd-webui-regional-prompter\\u002fblob\\u002fimgs\\u002frp_pipeline3.png)\\n#...\"],[\"```\\nbest quality, 3persons in garden, ADDCOMM\\na girl white dress BREAK\\na boy blue shirt BREAK\\nan old...\"],[\"```\\n    @article{chung2022diffusion,\\n    title={Diffusion posterior sampling for general noisy inver...\"],[\"```\\n* This pipeline allows zero-shot conditional sampling from the posterior distribution $p(x|y)$, ...\"],[\"def forward(self, x):\\n                    return self.seq(x)\\n\\n                def weights_init(self)...\"],[\"```\\n* Next, you should obtain the corrupted image $y$ by the operator. In this example, we generate ...\"],[\"```\\n* We provide an example pair of saved source and corrupted images, using the Gaussian blur opera...\"],[\"```\\n* The zeta is a hyperparameter that is in range of $[0,1]$. It need to be tuned for best effect....\"],[\"motion_id = \\\"guoyww\\u002fanimatediff-motion-adapter-v1-5-2\\\"\\nadapter = MotionAdapter.from_pretrained(motio...\"],[\"\\u003ctable\\u003e\\n  \\u003ctr\\u003e\\u003ctd colspan=\\\"2\\\" align=center\\u003e\\u003cb\\u003eConditioning Frames\\u003c\\u002fb\\u003e\\u003c\\u002ftd\\u003e\\u003c\\u002ftr\\u003e\\n  \\u003ctr align=center\\u003e\\n...\"],[\"\\u003ctr\\u003e\\u003ctd colspan=\\\"2\\\" align=center\\u003e\\u003cb\\u003eAnimateDiff model: CardosAnime\\u003c\\u002fb\\u003e\\u003c\\u002ftd\\u003e\\u003c\\u002ftr\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd alig...\"],[\"- `stride` (`int`, defaults to 64):\\n  The stride of moving local patches. A smaller stride is better...\"],[\"images = pipe(\\n    prompt,\\n    negative_prompt=negative_prompt,\\n    height=3072,\\n    width=3072,\\n   ...\"],[\"```\\nYou can display and save the generated images as:\\n```py\\ndef image_grid(imgs, save_path=None):\\n\\n ...\"],[\"```\\n ![output_example](https:\\u002f\\u002fgithub.com\\u002fPRIS-CV\\u002fDemoFusion\\u002fblob\\u002fmain\\u002foutput_example.png)\\n\\n### SDE ...\"],[\"# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\\n# If not train...\"],[\"DreamBooth training example\\n\\n[DreamBooth](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.12242) is a method to personali...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell e.g. a notebook\\n\\n```python\\nfrom acc...\"],[\"```\\n\\n### Training with prior-preservation loss\\n\\nPrior-preservation is used to avoid overfitting and ...\"],[\"```\\n\\n\\n### Training on a 16GB GPU:\\n\\nWith the help of gradient checkpointing and the 8-bit optimizer f...\"],[\"```\\n\\n\\n### Training on a 12GB GPU:\\n\\nIt is possible to run dreambooth on a 12GB GPU by using the follo...\"],[\"```\\n\\n\\n### Training on a 8 GB GPU:\\n\\nBy using [DeepSpeed](https:\\u002f\\u002fwww.deepspeed.ai\\u002f) it's possible to ...\"],[\"accelerate launch --mixed_precision=\\\"fp16\\\" train_dreambooth.py \\\\\\n  --pretrained_model_name_or_path=$...\"],[\"```\\n\\n### Fine-tune text encoder with the UNet.\\n\\nThe script also allows to fine-tune the `text_encode...\"],[\"```\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\" --\\u003e export MODEL_NAME=\\\"BAAI\\u002fAltDiffusion-m9\\\"\\no...\"],[\"```\\n\\n### Inference from a training checkpoint\\n\\nYou can also perform inference from one of the checkp...\"],[\"### Training\\n\\nLet's get started with a simple example. We will re-use the dog example of the [previo...\"],[\"```\\n\\nFor this example we want to directly store the trained LoRA embeddings on the Hub, so \\nwe need ...\"],[\"```\\n\\n**___Note: When using LoRA we can use a much higher learning rate compared to vanilla dreamboot...\"],[\"### Inference\\n\\nAfter training, LoRA weights can be loaded very easily into the original pipeline. Fi...\"],[\"```\\n\\nNext, we can load the adapter layers into the UNet with the [`load_attn_procs` function](https:...\"],[\"```\\n\\nIf you used `--train_text_encoder` during training, then use `pipe.load_lora_weights()` to load...\"],[\"```\\n\\n* LoRA parameters that have separate identifiers for the UNet and the text encoder such as: [`\\\"...\"],[\"```\\n\\n\\n### Training with prior preservation loss\\n\\n```bash\\nexport MODEL_NAME=\\\"duongna\\u002fstable-diffusion...\"],[\"```\\n\\n### Training with xformers:\\nYou can enable memory efficient attention by [installing xFormers](...\"],[\"Note that IF has a predicted variance, and our finetuning scripts only train the models predicted er...\"],[\"```\\n\\nAdditionally, a few alternative cli flags are needed for IF.\\n\\n`--resolution=64`: IF is a pixel ...\"],[\"### Stage II additional validation images\\n\\nThe stage II validation requires images to upscale, we ca...\"],[\"```\\n\\n### IF stage I LoRA Dreambooth\\nThis training configuration requires ~28 GB VRAM.\\n\\n```sh\\nexport ...\"],[\"```\\n\\n### IF stage II LoRA Dreambooth\\n\\n`--validation_images`: These images are upscaled during valida...\"],[\"```\\n\\n### IF Stage I Full Dreambooth\\n`--skip_save_text_encoder`: When training the full model, this w...\"],[\"```\\n\\n### IF Stage II Full Dreambooth\\n\\n`--learning_rate=5e-6`: With a smaller effective batch size of...\"],[\"```\\n\\n## Stable Diffusion XL\\n\\nWe support fine-tuning of the UNet shipped in [Stable Diffusion XL](htt...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## DreamBooth\\n\\n[DreamBooth](https:\\u002f\\u002fdreambooth.github.io\\u002f) finetunes an *entire diffusion mo...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n### Load multiple LoRAs\\n\\nIt can be fun to use multiple LoRAs together to create something entir...\"],[\"```\\n\\nThen fuse this pipeline with the next set of LoRA weights:\\n\\n```py\\npipeline.load_lora_weights(\\\"o...\"],[\"```\\n\\nNow use the [`~loaders.UNet2DConditionLoadersMixin.set_adapters`] to activate both LoRAs, and y...\"],[\"```\\n\\nLoad the LoRA checkpoint with the [`~loaders.LoraLoaderMixin.load_lora_weights`] method, and sp...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nSome limitations of using Kohya LoRAs with 🤗 Diffusers include:\\n\\n- Images...\"],[\"```\\n\\n## IP-Adapter \\n\\n[IP-Adapter](https:\\u002f\\u002fip-adapter.github.io\\u002f) is an effective and lightweight ada...\"],[\"```\\n\\n\\u003cTip\\u003e\\nIP-Adapter relies on an image encoder to generate the image features, if your IP-Adapter ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fYiYiXu\\u002ftesting-...\"],[\"pipeline.load_ip_adapter(\\\"h94\\u002fIP-Adapter\\\", subfolder=\\\"models\\\", weight_name=\\\"ip-adapter_sd15.bin\\\")\\nge...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"inpaint\\\"\\u003e\\n\\n```py\\nfrom diffusers import AutoPipelineForInpaint\\nimport ...\"],[\"```\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n\\nIP-Adapters can also be used with [SDXL](..\\u002fapi\\u002fpipelines\\u002fstable_diff...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIt is recommended to use `DDIMScheduler` and `EulerDiscreteScheduler` for face model. \\n\\n...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"prompt = \\\"best quality, high quality\\\"\\nimage = load_image(\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f...\"],[\"```\\n\\n### Other pipelines\\n\\nIP-Adapter is compatible with any pipeline that (1) uses a text prompt and...\"],[\"```\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\\nimport torch\\nfrom diffu...\"],[\"```\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:...\"],[\"# enable memory savings\\npipe.enable_vae_slicing()\\npipe.enable_model_cpu_offload()\\n\\n# load ip_adapter...\"],[\"export_to_gif(output_frames, \\\"test_out_animation.gif\\\")...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nTo ensure your pipeline and its components (`unet` and `scheduler`) can be saved with [`~Diffus...\"],[\"```\\n\\nThat's it! 🚀 You can now run this pipeline by passing a `unet` and `scheduler` to it:\\n\\n```pytho...\"],[\"```\\n\\nAnother way to share your community pipeline is to upload the `one_step_unet.py` file directly ...\"],[\"```\\n\\nTake a look at the following table to compare the two sharing workflows to help you decide the ...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## How do community pipelines work?\\n\\nA community pipeline is a class that inherits from [`Di...\"],[\"```\\n\\nThe magic behind community pipelines is contained in the following code. It allows the communit...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Transformer2DModel\\n\\n[[autodoc]] Transformer2DModel\\n\\n## Transformer2DModelOutput\\n\\n[[autodoc]] mode...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Image to Video Generation\\n\\nThe are two variants of SVD. [SVD](https:\\u002f\\u002fhuggingface.co\\u002fstabili...\"],[\"```\\n\\n\\u003cvideo controls width=\\\"1024\\\" height=\\\"576\\\"\\u003e\\n  \\u003csource src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggi...\"],[\"```\\n\\n### Low-memory\\n\\nVideo generation is very memory intensive as we have to essentially generate `n...\"],[\"```\\n\\n\\nIncluding all these tricks should lower the memory requirement to less than 8GB VRAM.\\n\\n### Mic...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"RealFill\\n\\n[RealFill](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2309.16668) is a method to personalize text2image inpaint...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell e.g. a notebook\\n\\n```python\\nfrom acc...\"],[\"```\\n\\n### Training on a low-memory GPU:\\n\\nIt is possible to run realfill on a low-memory GPU by using ...\"],[\"```\\n\\n### Training with gradient checkpointing and 8-bit optimizers:\\n\\nWith the help of gradient check...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nTo setup a default 🤗 Accelerate environment without choosing any configurations:\\n\\n```bash\\naccel...\"],[\"```\\n\\nSome basic and important parameters to specify include:\\n\\n- `--dataset_name`: the name of the da...\"],[\"```py\\nmodel = UNet2DModel(\\n    sample_size=args.resolution,\\n    in_channels=3,\\n    out_channels=3,\\n ...\"],[\"```\\n\\nNext, the script initializes a [scheduler](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f096f84...\"],[\"```\\n\\nThen it [loads a dataset](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f096f84b05f9514fae9f185c...\"],[\"```\\n\\nFinally, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f096f84b05f9514fae9f1...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\nThe training script creates and saves a checkpoint file in your repos...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Tips\\n\\nStable unCLIP takes  `noise_level` as input during inference which determines how much nois...\"],[\"stable_unclip_model_id = \\\"stabilityai\\u002fstable-diffusion-2-1-unclip-small\\\"\\n\\npipe = StableUnCLIPPipelin...\"],[\"```\\n\\u003cTip warning={true}\\u003e\\n\\nFor text-to-image we use `stabilityai\\u002fstable-diffusion-2-1-unclip-small` a...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*Diffusion models have shown promising results in cross...\"],[\"During inference:\\n\\n* The _quality_ of the generated audio sample can be controlled by the `num_infer...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nTo check a specific pipeline or model output, refer to its corresponding API documentati...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase can be found at [salesforce\\u002fLAVIS](https:\\u002f\\u002fgithub.com\\u002fsalesforce\\u002fLAVIS\\u002ftree\\u002fma...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [takuma104](https:\\u002f\\u002fhuggingface.co\\u002ftakuma104). ❤️\\n\\nThe original codeba...\"],[\"## StableDiffusionControlNetInpaintPipeline\\n[[autodoc]] StableDiffusionControlNetInpaintPipeline\\n\\t- ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nYou'll notice throughout the guide, we use [`~DiffusionPipeline.enable_model_cpu_offload...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"## Popular models\\n\\n[Stable Diffusion Inpainting](https:\\u002f\\u002fhuggingface.co\\u002frunwayml\\u002fstable-diffusion-in...\"],[\"generator = torch.Generator(\\\"cuda\\\").manual_seed(92)\\nprompt = \\\"concept art digital painting of an elv...\"],[\"```\\n\\n### Stable Diffusion XL (SDXL) Inpainting\\n\\nSDXL is a larger and more powerful version of Stable...\"],[\"```\\n\\n### Kandinsky 2.2 Inpainting\\n\\nThe Kandinsky model family is similar to SDXL because it uses two...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"## Non-inpaint specific checkpoints\\n\\nSo far, this guide has used inpaint specific checkpoints such a...\"],[\"generator = torch.Generator(\\\"cuda\\\").manual_seed(92)\\nprompt = \\\"concept art digital painting of an elv...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"runwayml\\u002fstable-diffusion-inpainting\\\"\\u003e\\n\\n```py\\nimport torch\\nfrom diffu...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"htt...\"],[\"# load base and mask image\\ninit_image = load_image(\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocu...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"runwayml\\u002fstable-diffusion-inpaint\\\"\\u003e\\n\\n```py\\nimport torch\\nfrom diffuser...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"htt...\"],[\"```py\\nimport PIL\\nimport numpy as np\\nimport torch\\n\\nfrom diffusers import AutoPipelineForInpainting\\nfr...\"],[\"# Convert mask to grayscale NumPy array\\nmask_image_arr = np.array(mask_image.convert(\\\"L\\\"))\\n# Add a c...\"],[\"```\\n\\n## Configure pipeline parameters\\n\\nImage features - like quality and \\\"creativity\\\" - are dependen...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"You can use `strength` and `guidance_scale` together for more control over how expressive the model ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"```py\\nimport torch\\nfrom diffusers import AutoPipelineForInpainting\\nfrom diffusers.utils import load_...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cfigure\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingfa...\"],[\"```\\n\\nLoad the mask image of the output from above:\\n\\n```py\\nmask_image = load_image(\\\"https:\\u002f\\u002fhuggingfa...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"# load base and mask image\\ninit_image = load_image(\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocu...\"],[\"```\\n\\nNow let's pass the image to another inpainting pipeline with SDXL's refiner model to enhance th...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIt is important to specify `output_type=\\\"latent\\\"` in the pipeline to keep all the output...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"### Prompt weighting\\n\\nPrompt weighting provides a quantifiable way to scale the representation of co...\"],[\"```\\n\\n### ControlNet\\n\\nControlNet models are used with other diffusion models like Stable Diffusion, a...\"],[\"# prepare control image\\ndef make_inpaint_condition(init_image, mask_image):\\n    init_image = np.arra...\"],[\"```\\n\\nNow generate an image from the base, mask and control images. You'll notice features of the bas...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https...\"],[\"You can also offload the model to the CPU to save even more memory:\\n\\n```diff\\n+ pipeline.enable_xform...\"],[\"```\\n\\nTo speed-up your inference code even more, use [`torch_compile`](..\\u002foptimization\\u002ftorch2.0#torch...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about Text-to-Video on the [project page](https:\\u002f\\u002fmodelscope.cn\\u002f...\"],[\"```\\n\\nDiffusers supports different optimization techniques to improve the latency\\nand memory footprin...\"],[\"```\\n\\nHere are some sample outputs:\\n\\n\\u003ctable\\u003e\\n    \\u003ctr\\u003e\\n        \\u003ctd\\u003e\\u003ccenter\\u003e\\n        An astronaut ridin...\"],[\"# memory optimization\\npipe.unet.enable_forward_chunking(chunk_size=1, dim=1)\\npipe.enable_vae_slicing...\"],[\"```\\n\\nNow the video can be upscaled:\\n\\n```py\\npipe = DiffusionPipeline.from_pretrained(\\\"cerspense\\u002fzeros...\"],[\"```\\n\\nHere are some sample outputs:\\n\\n\\u003ctable\\u003e\\n    \\u003ctr\\u003e\\n        \\u003ctd \\u003e\\u003ccenter\\u003e\\n        Darth vader surfi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## AttnAddedKVProcessor2_0\\n[[autodoc]] models.attention_processor.AttnAddedKVProcessor2_0\\n\\n## LoRAAt...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## DPMSolverMultistepInverseScheduler\\n[[autodoc]] DPMSolverMultistepInverseScheduler\\n\\n## SchedulerOu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Models\\n\\nTo push a model to the Hub, call [`~diffusers.utils.PushToHubMixin.push_to_hub`] and...\"],[\"```\\n\\nThe [`~diffusers.utils.PushToHubMixin.push_to_hub`] function saves the scheduler's `scheduler_c...\"],[\"```\\n\\n## Pipeline\\n\\nYou can also push an entire pipeline with all it's components to the Hub. For exam...\"],[\"```\\n\\nPass all of the components to the [`StableDiffusionPipeline`] and call [`~diffusers.utils.PushT...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase can be found at [Fantasy-Studio\\u002fPaint-by-Example](https:\\u002f\\u002fgithub.com\\u002fFantasy-S...\"],[\"Dreambooth for the inpainting model\\n\\nThis script was added by @thedarkzeno .\\n\\nPlease note that this ...\"],[\"```\\n\\n### Training with prior-preservation loss\\n\\nPrior-preservation is used to avoid overfitting and ...\"],[\"```\\n\\n\\n### Training with gradient checkpointing and 8-bit optimizer:\\n\\nWith the help of gradient check...\"],[\"```\\n\\n### Fine-tune text encoder with the UNet.\\n\\nThe script also allows to fine-tune the `text_encode...\"],[\"Create a dataset for training\\n\\nThere are many datasets on the [Hub](https:\\u002f\\u002fhuggingface.co\\u002fdatasets?...\"],[\"```\\n\\n## Upload your data to the Hub\\n\\n\\u003cTip\\u003e\\n\\n💡 For more details and context about creating and upload...\"],[\"```\\n\\nThen use the [`~datasets.Dataset.push_to_hub`] method to upload the dataset to the Hub:\\n\\n```pyt...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nNow, define four different `Generator`s and assign each `Generator` a seed (`0` to `3`) so you ...\"],[\"```\\n\\nCreate four generators with seed `0`, and generate another batch of images, all of which should...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nGiven a prompt, get the inference time for the original model:\\n\\n```py\\nimport time\\n\\nseed = 2023\\n...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nTime the distilled model and distilled VAE inference:\\n\\n```py\\nstart = time.time_ns()\\nfor _ in ra...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about the model on the [project page](https:\\u002f\\u002fdiffusion-planning...\"]],\"hovertemplate\":\"source=diffusers\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"diffusers, circle\",\"marker\":{\"color\":\"#FECB52\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"diffusers, circle\",\"showlegend\":true,\"x\":[-1.8963416,-8.9722185,-1.2804643,-0.9806386,-0.26288047,1.0837384,9.817565,-0.46017212,-0.23286551,0.18324555,0.41828424,0.6925916,0.50224787,-8.068781,-8.162428,1.8012619,-1.6094166,-1.112969,-0.49233398,-0.41986397,-0.6003634,1.1022643,-0.37230492,-0.5274952,0.90871567,-0.008638744,0.7065257,-0.34672517,0.77690893,0.28690207,-0.50714684,0.7457898,-0.69653535,-1.137925,0.28387186,0.5725113,-0.9165645,0.59213144,-1.3532531,-9.669501,-1.7495844,0.16315825,0.91960245,0.66775674,0.54748625,0.6272217,0.63611656,-2.1107745,1.4946125,1.9629534,-1.853604,-1.5682899,1.7129343,-2.921095,1.7358589,-0.022947043,0.6364571,-0.43974456,-1.0145285,0.9269662,2.8510332,-1.2708277,-1.9878702,0.1273554,-1.0742977,0.3772446,-1.9498924,-2.24053,5.723435,-1.1622152,-1.5899528,1.6808785,-0.04224788,0.86605096,-0.04184313,0.803664,0.14587796,0.7778029,0.9538736,1.065717,-1.1324414,-1.7847531,-2.6009007,0.12547123,-0.35764983,-0.7081454,-2.093366,1.4149642,-1.6761322,-1.9564795,1.6306586,-1.9028254,-2.0211773,0.055409376,0.07907969,0.15555412,0.5751571,-0.29676303,-0.47864828,-0.35132235,-0.79499793,-0.50145084,-0.6053089,-0.5083381,-9.131638,6.6392612,-0.09237036,0.57408583,-0.22238997,-0.07418539,-0.20234996,-0.29327565,-1.2736565,-1.3318012,1.4314238,-3.9771037,0.6024478,0.33130166,0.87967217,0.48259243,0.4383951,0.13338467,-2.2967277,-2.1710668,0.36305973,-0.03979364,1.0877588,-1.388926,0.4916041,0.55427295,-1.8347534,-1.9921685,1.3819562,0.31127214,0.03996334,-1.1319374,0.7516437,0.6484824,-1.2618098,-2.0483851,1.360973,-1.1111392,0.5324906,3.0544736,0.4703323,-0.07186016,-0.5468978,-1.9361486,1.6306806,-0.15516946,0.48356226,0.46488464,0.07322215,0.40917444,0.21737228,0.7090844,-7.426391,-2.10872,-2.0780604,-1.9443598,0.6616362,-1.8742648,-2.10754,0.67168015,-2.035341,-1.9989142,-1.6614742,-0.6894969,-0.8668871,-1.057414,-0.21491754,-0.6095568,1.1506468,9.818543,0.22295259,-0.4377232,0.24750802,1.2224965,-0.3716518,0.06383964,0.14909212,-0.54362166,-0.6017335,0.5316534,0.61241424,-0.8502428,-0.22960883,0.35703954,-0.3894202,-0.6772738,-0.62563044,-1.1611001,0.5585105,0.3662462,-2.2512794,-0.81199497,-0.6743364,-0.28148004,0.18202606,-0.6056243,0.80710137,3.9670787,-0.6959155,-0.694472,0.16454807,0.4557231,-1.0082192,-0.08706908,0.5441376,0.18077913,-0.5333327,0.5694081,0.50598514,-1.2545234,0.64789313,-1.4195033,-0.8183196,-0.9067589,-1.160703,-0.6903762,1.3976645,-0.36531392,0.34011403,0.52917737,-0.036078397,-1.0700189,-0.84270215,0.5527141,-0.11201663,-0.7604481,-0.50800306,0.47586438,-0.038237035,2.2841742,0.8204796,3.0207937,-0.47991994,-0.6392635,1.1191807,-1.3249018,-0.8438514,0.78557974,-0.78679353,-0.45132533,-0.4823393,0.6072848,-0.123887494,0.47576654,-0.14201057,0.07995302,0.8734601,0.046463758,0.9559986,0.5295487,-0.15269037,0.92213756,-1.1072226,-1.4910097,-1.2577835,-1.5084968,0.7459202,0.6889474,0.63032585,0.8964451,-1.1810609,-0.48813933,-1.2825594,-0.93628615,-0.12667559,3.9366052,0.46253386,2.0285914,1.157176,0.8788523,-0.13345037,-0.614129,-0.2138174,-0.0901012,1.5408937,-0.19793312,0.46791023,-0.36637005,0.06549346,-1.3539757,-0.43030298,0.13857783,0.4315353,0.17197901,0.54106706,0.29469964,-1.1934801,-0.7227284,0.8355302,-1.6023659,-0.9133119,1.8164817,-0.12747215,0.38241243,0.21950492,-1.5302206,-1.0067941,-0.0959951,3.299047,0.110462435,0.8767506,0.5031861,0.59171003,-0.3456018,-1.2153244,-0.5916452,-1.3459924,-2.0858455,-2.4843466,-2.2712796,0.1634352,-0.013368898,-0.95810825,-1.9710841,-1.0066717,1.1100477,9.8175125,-0.7764678,-0.32348135,-0.52008283,0.38784948,-0.5150132,-0.22591993,-0.38572592,-0.4072895,-0.1550147,0.24757826,0.6418094,-0.8605726,0.4427989,-1.4199203,-1.8709503,1.4984018,1.7666764,-0.8002629,0.2257846,-1.020248,-0.12770358,0.48614958,0.40083948,0.113556385,0.17884558,-0.8329346,0.3047609,0.2920657,0.088228576,-0.9694514,-0.9560654,0.59463084,-0.83259344,0.6617124,-1.4805769,-0.4762781,-0.6206334,-0.94757104,-1.1128672,-1.1521986,-1.0794349,-1.1134396,1.0813451,0.9601243,0.8822857,-0.20702675,0.7080218,-0.14302333,7.485957,-0.35322237,0.5678587,-0.21763457,-0.8126074,1.0642064,-0.46898767,0.1406727,-0.85122246,0.8773427,0.161114,-0.38232043,-0.8996756,-1.991468,1.6590933,-8.446755,-10.744349,-1.8216022,-0.02967406,1.447352,-8.090892,-8.107148,-8.025815,-7.5924244,-7.710001,6.7164636,0.008272262,-0.16708834,0.018396618,-0.11737891,-0.36033145,0.3580245,-0.5803841,0.38951734,-0.024105925,-0.42623323,-0.7523923,-1.0245966,-0.845119,-0.5775062,-1.1928687,-0.97311157,3.3254337,0.01622599,0.9150641,-1.4757662,-1.5611686,-6.8218927,-1.5609057,-1.7385228,-1.3602004,-1.6036471,-1.9114351,-1.5531493,-1.3685747,-1.458896,-0.104732774,-0.8909948,0.35285023,-1.2128514,-0.60064197,-0.4277522,-0.28700465,-0.44675115,0.814713,-0.9037842,-1.9297943,-0.57850575,-8.38078,-0.04500322,-10.629862,0.09814821,5.602587,1.272799,0.98185736,0.98386645,0.784442,1.3545344,1.6074543,1.2363393,0.8122711,1.369373,1.3759979,1.3521065,1.197009,1.2765588,2.7009456,1.0258127,0.86611784,1.28431,0.7199096,1.1446695,5.464682,1.4167625,3.7527947,6.077598,4.9326653,5.1209755,1.3312774,4.5652246,4.371068,5.0669127,-0.35358518,-0.68131363,-0.5090901,0.8641483,0.3598369,-2.44931,-2.4256208,1.1452676,0.45119533,0.598652,0.1417256,0.31070983,0.35801637,-0.5205509,0.4309407,0.43396053,0.592809,-1.2527006,-0.8614303,0.18211241,0.04764821,0.8475493,-0.14387624,0.4165043,-0.122342154,0.6093851,-0.032425586,0.63168794,-0.19498439,0.55228215,-0.19202486,0.4080313,-0.14079581,0.38744384,0.06967058,0.8273008,-0.5332165,-0.20111594,-0.071749195,0.8837822,0.6473749,0.20350228,1.1032428,0.8545265,1.4805683,0.9655423,0.6975806,1.3701468,4.6495857,4.5423183,-1.2765934,-1.2893385,-1.9645704,0.067946635,0.40699697,1.5173678,1.2723877,1.4767864,0.89452046,0.36582538,0.7610895,0.9811592,-1.7528409,-2.0949326,0.31096318,-0.087423585,-1.2759835,-1.6459587,-0.61560947,0.51676774,0.7339626,-0.31073204,0.37815306,0.41153505,-0.88808614,-0.6435284,0.1921923,1.2419091,1.214941,0.07085796,1.727598,-1.9767642,-2.0543046,1.376179,-1.1228057,-1.4324472,-0.037024155,1.0821921,9.818188,-0.18801765,-0.18304081,-1.6362426,-0.11268721,-0.21697493,0.12408883,0.6003414,0.48710376,-0.74260724,0.83462775,4.820979,0.36783043,-2.0197158,3.8212838,4.297055,-0.6536063,-0.08169158,0.8608981,0.06089543,0.6222259,-0.002125808,0.7285715,0.7653186,0.8600166,-0.29854348,-0.03349894,-0.12296918,0.64521897,1.7191204,0.6962378,1.5975461,-0.9496691,0.026772343,0.06555,-0.48984945,-0.75257105,0.7718661,9.817531,-0.1686382,-0.20565015,1.1119591,-0.05006571,-0.3675061,-0.33039656,-0.33807066,0.2914286,0.29275253,-0.20791891,-0.14369223,-0.35531032,0.07122282,0.6390981,1.5194677,0.82998765,1.7404339,-1.0427346,0.15085852,-1.4459616,-0.87290883,-1.0678244,-1.2795255,-1.3210608,-1.0443927,-0.78222144,0.14213856,-1.1787996,-1.1605082,-1.0873935,-0.26836112,-1.0243077,0.69550973,9.8182,0.06287997,-0.29025677,-0.24009037,0.18758959,0.5469384,-1.4120173,-9.74092,-1.2425734,-9.623256,-1.4573989,0.12619808,1.133447,9.817671,-0.10921794,-0.17415571,-0.040147156,-0.4915496,-0.21487266,-0.010420699,-0.1445641,0.7588861,0.73413855,0.4721363,0.3254596,-0.8414069,-1.1597651,1.114913,0.9050893,0.6032723,0.14362493,-0.1419251,0.6023097,-1.1030308,-0.8362971,0.8052389,-1.5023361,-0.74683094,-1.8538246,-0.95342803,-0.7042432,0.91684866,1.6914583,1.7026713,-3.3461933,-0.00066320173,-0.12246736,-0.7425541,1.0541842,2.8624732,3.0123413,-1.5411822,-2.056151,-0.99483836,-1.1161042,0.3711204,0.69479334,-1.6744967,-1.1423959,-1.2756661,-1.220449,5.9774942,0.10549302,-1.1717178,0.82227266,0.6364391,0.34940708,-1.3638242,-0.4688795,1.5417299,0.17742592,-8.634288,-8.862826,-5.217157,0.42121685,-1.4241464,-9.021682,2.1112924,0.83584696,0.5975383,-1.1910615,-1.5934792,-1.7828583,0.5302895,0.02535663,0.25420913,0.23385847,0.27259943,0.3613008,-0.7473939,9.818374,-0.45706186,0.08487541,-0.27340996,-0.12691823,-0.13420208,0.22974387,-0.16264278,-0.24728453,0.56265825,-1.3370506,-0.62454,-0.35607183,-0.541249,0.3852671,0.06359026,-0.3294936,-0.746059,-1.9817916,-0.23587713,-0.049037814,0.26958078,1.3791734,-1.863557,0.03162861,0.92299426,0.14453778,0.13752192,0.042221062,0.19172186,2.9490652,0.1654502,-0.29926014,-0.6903519,-0.43716848,0.080479614,-1.1513207,1.0094088,0.722352,0.63572454,0.8001537,0.7464902,0.9515343,-1.1146026,-1.5605667,-1.6902634,-1.6450845,-1.6203802,-1.669434,-1.5974946,-1.6434957,-1.6428652,-1.6324475,-1.6233593,-1.6220831,-1.3867613,0.40769035,6.005045,6.382952,-1.0513577,1.343833,0.23903705,0.78139645,0.8329908,1.204946,1.0726635,0.97647417,0.8699033,-0.7017,0.9791306,0.93986505,1.2364916,-0.54255474,0.9818361,1.0846477,1.3391263,1.3584204,1.2576005,1.0634779,1.3393372,-1.3788397,-1.9494008,-1.7843186,1.7305067,-1.0859615,0.69000024,0.6257844,-0.09757087,0.042669967,-0.5561162,0.75104964,-0.5164577,-0.103146695,-0.07468631,-0.61218715,-0.6036446,-0.7196194,0.39581883,-0.2654177,0.018687692,0.65379703,-0.995341,0.3778308,-2.0227337,0.8100499,0.5553247,0.44217792,-0.08877384,0.13649483,0.049604733,-0.011073008,0.37855375,-0.0017353349,-0.06812878,6.7336473,6.8022547,-0.008409643,-1.5961767,-1.1170344,-1.3570896,-9.614629,-0.4806558,1.0810043,9.818498,-0.644809,1.1598648,0.5467648,-0.25540373,0.1264231,0.22915366,0.3253956,-1.2982078,-0.27665618,-0.0858505,0.008905718,0.12196248,0.008718646,-0.04170497,0.7731003,0.06050564,-0.26745844,-1.8468997,-1.0510236,0.85478723,-0.7008551,0.72054577,-0.6684592,-0.29191965,0.94349205,-0.35862,0.66976166,-0.5754165,-1.5808004,-1.8585764,-10.687829,-1.0091538,-0.016016746,0.3116547,0.2265642,0.11224261,-0.6065731,0.54244167,-1.676117,-2.0051973,1.4295326,0.19690917,0.7488254,0.73084027,-0.037142437,1.0506587,0.73516613,0.3950708,0.41923025,0.2766757,0.17985268,0.68546736,-1.0379349,0.7554949,-0.5467408,0.3780089,-0.84355617,0.0015955974,-0.021094428,-0.30613613,0.37463602,0.16553755,0.9639496,-0.120096125,-0.8463445,-0.69651586,-0.5772392,-0.684247,-0.69379604,-0.78012204,0.51451534,0.07398034,-0.6790577,-0.88334185,-1.0578294,-1.0103496,0.6978289,1.4056845,0.38004616,0.288286,1.0881829,1.7532994,-0.04782429,0.08665409,0.02178109,0.44796714,-0.49450496,-1.3954823,0.5360967,-3.4005466,-2.9175441,0.5418034,1.1426235,-1.532085,-1.555237,-1.6778325,-1.396814,1.3756698,-0.4516546,1.0383098,-0.49229673,6.9103107,-1.096492,0.15767246,0.5575428,-0.61729157,-1.9612089,1.2868571,-10.760541,-1.9923481,-1.2802358,0.3870521,0.66581374,0.3053177,0.53667116,0.5780646,0.6374369,0.47857735,0.039372876,0.40752083,0.39432144,0.56194514,0.39721203,0.08457497,0.08732811,0.5074469,0.2592021,0.4661515,0.26508138,0.039762236,0.16102763,0.73099107,0.21897289,0.67721766,0.19632602,-0.5155581,-0.6172461,-0.55468273,0.6229742,0.580736,-0.10105951,2.4726949,0.6743471,0.111971654,0.600652,-0.5522308,-0.48950145,-0.7041826,0.4977355,-0.09165788,0.0962642,-0.03047609,0.14822978,0.36883214,3.0971549,0.2537634,-0.6945515,-0.06659142,-0.95661145,0.7648355,-0.28352666,0.5935616,-0.25692314,0.6028806,-0.26501128,0.79915977,-0.12295796,0.8079522,0.03176173,0.8599762,0.13456446,0.9524007,0.6366559,0.32803857,0.4951607,0.31870395,0.31773704,0.45612842,-0.049674977,0.6146028,-0.108163506,0.93052334,-0.7113575,-1.2630186,-0.17234363,-0.4503773,-0.38721648,-1.2734964,1.3066808,-0.70755184,0.23656659,-0.12874642,0.6391198,-1.090298,-0.96760666,3.3976874,-0.076785944,0.52515393,-0.27902418,0.31923348,-0.48119986,-0.9601302,1.2178249,9.818277,0.51631045,0.4732373,1.2252088,-1.4244676,-0.17761193,0.5818806,0.5867317,0.39579257,-1.2202004,-1.9077272,0.6766706,1.3503203,0.8313898,-0.6728955,-1.0248805,-0.65003425,0.51756954,-1.5453573,0.3035539,1.0834113,9.818154,-0.81724346,1.066123,-0.42528653,-0.02109893,-0.42861184,-0.5181363,0.1976363,0.62036496,0.53940994,-0.9186552,-0.893017,-0.61333394,0.40078992,0.44374493,-0.05263542,0.1294418,-0.5793479,0.44786066,0.59568757,0.08953982,-1.2068465,-9.603419,5.5918097,5.9149227,5.728225,5.961659,1.534666,1.4827625,-0.3922924,5.2178974,0.54464066,-1.9545615,-1.0635078,0.2365283,-0.6721137,1.13151,9.818354,0.34891686,-0.90938175,-0.09315932,-0.061762057,0.6467371,-0.22653669,0.30694595,-8.846756,-2.0833695,-0.80897003,0.75169456,-10.164416,-1.8208292,-2.0415437,-1.8755021,0.46301225,0.61657137,0.95303434,-0.84508395,-1.9026124,-1.7479591,-1.9542153,1.3805496,-1.1645122,-0.38719854,0.59000975,-1.7778186,-0.96080697,0.3023709,2.7777925,1.5955303,5.203148,-1.0095674,-1.0185941,-0.8192904,-0.76005167,-1.3648916,-1.8441538,-10.689968,-1.0858214,2.4800768,0.39649656,-1.7577118,1.7023749,-0.68885636,3.2043777,1.6081859,0.5306179,0.33495063,0.28907695,0.26438257,-1.274616,-0.5470078,0.90189147,0.2555778,-0.8938626,-0.71368825,1.5385485,-1.1857132,-0.7987063,-1.4101266,-0.99933916,-0.8809889,-1.0346711,-0.8878252,0.24111165,-0.67245114,0.5460807,-0.8828773,-0.19027832,0.38876748,0.20281927,-1.9334767,-4.2808185,0.23806278,0.3124303,-0.93230927,1.2075202,5.599054,19.535559,19.563593,19.522163,-1.8170496,-1.1606936,-1.1516359,-1.2195652,-1.3275229,-1.1265998,-1.1573031,-1.2828138,-1.2780399,-2.305572,1.3388104,-1.3908049,0.42100435,0.4856902,-0.4041324,-0.13583775,-0.097848594,0.60287553,0.14926682,1.16253,0.74139225,-6.7366867,0.21223678,0.51028705,0.07853415,0.7574377,-1.6350247,0.8266307,0.6811313,0.3563706,0.6391124,-0.85427564,0.645694,0.27887097,0.04372644,0.056907028,0.45379227,0.44007826,0.80400103,-0.8500873,-0.7210594,-0.9660138,0.30026066,0.76597476,1.0811992,0.26938578,-0.05958652,0.66549027,6.351592,-1.2535673,-0.7668087,-1.1508173,-0.015055844,-0.68057394,0.23372795,0.5786346,-0.98526824,0.25832635,-0.4489877,0.6331319,-0.3804967,0.5067509,-0.5795032,0.21529031,-0.1566535,-0.73770684,-0.33456674,0.53028923,0.9593949,0.33111078,0.334304,-0.8810398,0.48685735,0.30072436,-0.21862194,0.0025750592,0.45336267,0.4519629,11.105634,0.5258176,-0.4149321,-0.52933127,-1.0125821,0.52786696,0.47408304,-0.24267575,-1.9611971,0.16896902,0.16170293,0.33334938,-0.1108667,0.5664555,-1.9692175,0.82719874,0.72196466,0.5981889,0.6723663,0.5436661,0.6840784,0.17575097,7.9694595,-0.29226038,-0.5233515,-0.0870709,-1.6341541,-0.28316244,-0.19769695,-0.5813311,-0.6674328,-0.19641,0.6497984,-0.60070974,-1.038706,-0.23781283,0.9395375,0.6266525,0.3393484,0.5010788,-0.51185864,0.37896597,-0.59409493,-0.55789435,-0.5863889,-0.66694355,0.6464444,-0.297477,0.51416516,-0.73935825,-0.045530885,3.2114093,-0.7127308,0.4304495,0.5032938,0.37141412,0.075610176,0.102018125,0.18100335,0.7064935,-1.1572932,0.10029308,-0.86560047,-0.2810573,-0.6398583,-1.3295028,-1.161484,0.16314162,-1.0550354,-0.3298048,-0.24069759,-0.21393132,-0.51579356,0.30304042,0.29169127,0.50514686,0.6195068,0.54107153,0.629015,0.5120851,-0.09905301,0.3916236,-0.121460825,0.86939317,0.7151108,0.4435197,-0.1126674,0.53889126,0.08990121,0.9223581,0.4438947,0.8966733,0.4199465,0.76562697,0.37603027,5.388074,1.1400682,0.80916804,1.1437229,1.4371209,1.6741221,1.4076717,1.521667,-1.9486201,-9.279966,-6.6799574,-0.6141613,0.15486258,-1.3483348,-1.6842135,-0.6826096,-1.1863828,-0.5025286,0.30314752,-0.54757845,-0.93154967,0.20034105,9.818133,0.5513092,-0.33305925,0.58186525,0.19196208,-0.014767462,0.42475,-6.5137215,-1.9497354,-0.8364438,0.63359874,0.05670952,1.5820788,-8.014006,-8.08777,-7.5511165,-0.8642235,-1.1724688,-1.3840163,0.8719946,0.8859616,-2.068475,1.4677212,-1.6908865,1.4895741,1.449034,0.15052564,0.047722816,-0.19514008,-0.15208507,-0.7771544,0.72517556,-0.04426474,0.1501852,-0.3105405,-0.19883564,0.7582401,0.74787027,-0.026098609,0.3789375,0.6847115,-0.014515551,0.77571803,0.3891801,0.410179,-0.39052188,0.13087213,0.034547076,0.68305355,-0.06684261,0.3882735,0.023361972,0.2546767,0.36432093,-0.009096413,-0.05382314,0.45297214,-0.08977484,0.44735372,0.5009665,-0.39538547,-1.061482,-0.7004825,-2.5188432,-0.42666492,-0.078523986,0.015548647,0.8185833,0.35536945,0.7784326,-8.438209,-10.46941,-1.4956038,-10.682404,0.8435988,2.433875,3.8702316,1.2261295,1.4201087,-1.7772604,0.25729114,0.44236147,-0.53599197,-0.5856951,-0.016721249,2.5335321,3.0364487,3.3999033,-0.36531946,0.3300774,0.21914478,-1.192056,-0.45758724,-0.6788645,-0.5228256,-8.218699,0.71815985],\"xaxis\":\"x\",\"y\":[7.519988,-0.91652936,7.4762344,4.94311,6.2536354,3.4954467,-19.720436,5.150314,-1.9382882,5.633056,4.104992,3.5059695,6.0039086,-6.618835,-6.5419297,7.2088013,6.986795,6.2678585,6.102498,7.1208067,7.1248116,8.107837,7.112315,7.1402473,8.13788,7.4156876,8.021362,7.221533,7.9154315,8.127079,7.4308796,8.327391,7.30658,6.5322447,3.5458264,4.515331,5.968841,4.453138,7.887066,-0.25181848,7.6856823,7.552012,6.78928,7.702842,8.098452,7.476278,8.522144,7.5658336,7.1125565,-0.7296154,7.8743005,7.471715,7.21542,7.559626,7.062846,7.1504965,6.171737,7.159924,6.686176,6.7177434,0.199605,7.9782596,8.058602,7.113304,6.6330857,6.5118513,3.255336,3.1121075,-6.3320956,4.4842386,7.176477,7.2030377,7.7027483,7.508931,8.103821,8.607922,8.47363,6.99135,7.3881726,7.2482824,7.8258033,8.002286,2.2793422,7.021879,7.4827027,7.377461,8.064923,7.255304,8.111178,7.839252,7.2129073,7.809209,7.861693,7.342004,8.133281,7.3992624,7.928233,7.4066987,7.424967,7.3231463,7.3812633,7.3918056,7.3347144,7.291364,0.14640424,-2.1383758,7.601999,7.8934107,7.1137238,7.4946923,7.3419037,5.38758,8.080822,7.82655,8.089572,7.222856,8.429034,8.489613,8.724628,8.532003,8.505765,8.485872,7.799365,7.7707534,7.687633,7.284449,7.628319,7.201121,8.165254,7.069894,7.6481853,7.824033,8.107807,6.006515,4.910825,5.020476,3.4058201,7.030521,8.062396,7.8465347,7.2732935,6.991185,6.3076773,0.80274534,4.3872447,4.5488305,4.17966,7.8975463,7.2272687,6.8150907,5.9873314,6.4201627,6.1744366,6.2945657,6.1102147,5.5164976,-4.244028,3.6435437,3.4893763,3.1275144,3.4922712,3.9022682,3.5090306,3.3578851,3.3622515,3.600205,7.2711706,5.9496455,5.82856,6.0606036,6.896719,6.090761,3.5096402,-19.71891,3.9340234,5.2481213,6.7432323,6.581665,-0.6237322,4.611169,5.555241,3.5008302,3.473837,4.81786,5.709028,5.829409,7.0630827,7.0145087,7.26316,7.515645,7.716172,7.836869,7.2376027,7.1876802,-2.9875906,7.669189,7.632077,7.638455,7.029989,6.9873414,6.2787685,-0.41318548,7.0267744,6.517646,7.572168,6.277388,6.5250034,5.5663023,3.1403463,4.4016986,3.2036393,3.372796,6.2096353,6.5055733,3.5435085,6.522009,7.0753736,7.9172263,7.6656084,7.3005347,8.111781,6.6722426,4.6329775,4.5209913,3.3058803,6.7686987,6.696582,4.537738,3.9082386,3.2128654,3.2477968,6.5781364,3.8091054,1.2616371,2.727723,0.07345466,0.9142343,0.8947076,-0.18118368,7.857801,7.7204466,8.635268,7.459261,7.960798,7.826703,8.142067,7.913222,8.190277,8.367329,8.6944475,8.445668,8.589963,8.467723,8.357875,7.7263274,8.298233,3.985209,7.699176,7.76825,8.072665,7.2509775,8.478371,8.2406025,8.430986,7.0369396,2.9775884,4.9761,7.5522656,3.618017,-0.47171253,2.6757991,-3.2617936,-2.3891003,-2.8814697,7.1355352,7.6493125,6.0895114,7.752706,0.27537856,6.93145,0.7591313,5.6107807,7.2695637,8.163856,-0.33589882,-0.44541,7.726794,7.0524473,6.807111,6.776996,8.169362,7.8753486,8.803262,7.0823154,7.0574346,2.3361561,3.6916447,4.4769254,4.404796,4.667955,5.749061,5.2302094,0.115351446,5.9086986,8.164237,3.5628686,4.2380376,5.4281063,5.330437,6.882769,8.197853,7.8107047,7.7077575,7.753668,5.5308323,7.013735,7.6500363,8.091554,7.0589204,3.638398,-19.718782,6.2948823,5.43117,6.641767,-1.6330566,6.4408407,-1.6494241,-1.5413059,6.022787,6.332899,4.9999485,4.5645695,6.6819353,6.0843487,7.2355447,7.6414194,7.0594654,-0.7285622,6.8897977,3.6789804,8.271169,7.67622,8.268527,8.420409,8.259841,8.323271,7.304755,8.164994,8.306449,7.8893213,7.3086066,7.3131137,7.750907,7.3390064,7.0059514,7.7359858,7.270868,7.9677587,7.864359,7.813803,7.7959332,7.8714194,7.8577843,7.4668927,7.307272,7.1057897,7.8710213,8.183453,8.293149,-4.7198286,7.679765,8.1966715,7.9931617,7.306154,7.169473,7.2103024,6.8799033,7.32501,8.365342,8.495467,7.8360114,5.8946266,7.898582,7.3740916,-1.5218891,-3.8925395,7.9212227,7.2270794,8.067534,-6.5849338,-6.5974402,-6.562749,-5.94806,-7.1442256,0.7127804,6.9356356,7.061303,7.078331,7.5621815,7.082644,6.82997,6.8324614,6.9510636,7.721435,7.3831124,7.8521233,7.8237214,7.2668858,7.059867,5.727015,6.0796337,0.18548365,5.8969507,7.902086,7.6311464,6.885806,0.2890638,7.1889277,7.540671,7.6245193,7.642299,7.7970004,7.4075336,7.658694,4.755072,6.213954,5.5083303,7.6756425,2.9323423,2.0661254,1.431127,-0.17848217,1.191574,7.660493,5.2465577,7.8883452,6.951759,-0.85712427,-0.6007388,-3.9656136,7.033066,-2.5806403,6.458266,6.7631536,6.50179,6.6107183,6.5604706,6.3574567,6.4723964,6.801005,6.4296875,6.4459977,6.430671,6.8877826,6.5057054,0.38425127,6.100103,6.129902,6.547418,6.908323,7.136373,-2.6461992,6.544568,-2.1609135,-4.125286,-1.3008653,-0.8444742,6.2936115,-0.08847108,-0.17281353,-0.41397965,7.1781664,7.3481927,7.356559,6.5029902,5.5152016,7.893819,7.990937,8.54853,7.7438264,8.208581,7.981042,8.017207,8.118863,8.104904,7.9776115,8.059706,6.8253217,7.62202,7.315079,8.333846,8.16993,8.790934,8.3909645,7.11406,8.70764,8.833534,8.02048,8.648939,7.8639636,8.553957,7.9623737,8.426413,8.206254,8.260862,8.129026,8.7493305,7.677251,8.230584,8.268051,7.613394,8.608618,6.992057,6.461153,6.738068,6.348926,6.1529145,6.699567,6.1697927,-0.1620581,0.100257084,8.065464,7.8743644,7.8781366,6.877206,7.276147,6.897885,6.931774,7.122664,6.447,8.061721,7.53717,7.6184416,7.969733,7.9800234,6.653543,7.369673,6.7853293,7.1108704,6.3132067,3.0467787,3.7661397,3.0444489,6.23229,6.692617,6.8493743,6.9172015,-0.04394215,6.6826353,6.5726485,0.34348458,6.50014,7.8177257,7.8346415,8.129426,7.634165,7.1034527,6.2110376,3.5296063,-19.720863,5.570423,6.261294,-1.898693,5.75105,6.856804,5.3265324,3.608696,6.5238643,6.3348446,6.8216133,-3.5031214,7.224658,7.8804593,0.10834167,-0.37957552,7.249105,7.4129944,8.643464,7.9994316,7.970935,7.9198575,7.689615,8.650946,7.128616,7.025598,2.506034,1.7310181,8.114892,6.5631647,7.304324,-0.55928046,7.557353,6.91393,7.421048,7.6444225,6.052081,3.1730301,-19.720127,4.5500035,6.393535,6.4799023,6.197854,3.2885323,3.3280635,3.3825843,-0.23776448,6.7647552,7.1747093,6.8348017,6.979629,6.571382,6.97622,6.6747084,6.9896297,6.6341295,-0.075715326,6.447607,0.017113473,7.9278736,6.720166,6.712749,6.619044,6.956915,5.899441,5.2318006,5.629877,6.5133576,6.838848,7.215382,7.140771,3.318579,-19.721508,6.16469,6.647533,6.5509014,5.151496,6.8156567,7.9156346,-0.28887263,7.9065876,-0.29277447,7.2394223,6.320063,3.6135743,-19.719387,5.629957,6.2323003,0.5698969,6.870163,6.449173,6.049274,6.746402,2.6896694,3.4874048,4.825092,5.71952,6.491845,6.72611,6.063497,3.9616537,2.9886541,4.038295,3.4756138,5.966682,6.6234226,7.551896,7.0132513,8.023435,7.779061,7.8241982,6.8733253,0.83499223,6.5841494,-0.33980206,-0.5831794,0.35032427,6.025552,3.4667146,3.1846085,0.20229948,-3.3904214,-2.6270149,7.692407,7.865936,7.037584,3.2939184,3.1646597,8.208445,4.2492127,7.6286616,7.698604,7.4133186,-4.6057067,7.9375563,7.5547175,7.9296637,8.41656,6.595507,7.63996,7.462682,-3.4240377,7.664564,-0.45734894,-0.27501178,-2.2852917,-4.4285574,7.6255026,-0.49132258,-2.7862878,-2.6499915,7.8471565,7.4439077,7.8868446,7.855735,7.94635,7.987384,8.159309,8.303973,8.325013,8.167209,6.9801764,-19.72086,6.8634944,-1.4085993,5.9419427,7.0868664,7.197332,8.203265,5.963008,2.9448388,6.1244507,6.6198735,7.904086,7.693036,7.385276,6.5837836,8.462997,8.210113,5.477775,7.886932,7.327403,8.200882,8.509365,8.157487,7.6534925,2.438913,8.24479,2.4461157,2.5504718,6.468226,3.541622,0.361463,4.900289,-0.6071165,5.860252,5.8422585,6.7142997,4.9473715,8.286193,8.290809,8.360416,8.063912,8.187011,8.651862,4.6431155,4.5999513,4.9898634,4.989451,5.063812,4.9881525,5.059078,5.0119524,4.984361,5.112513,5.00772,5.015683,5.1104274,6.9325805,-1.2997861,-1.9000374,7.9745593,8.125622,7.005676,7.235801,6.961488,7.0139966,7.137828,6.993364,7.2319193,-0.23609035,6.2168703,6.7727637,6.5661197,-0.40051168,7.032381,7.0555124,6.9473777,7.0211306,6.9767265,-0.96852565,6.9195604,7.918448,8.052574,7.9482136,7.2687554,7.6850705,7.2013783,6.55105,8.540919,8.700716,7.694542,7.6606746,7.4805217,8.155262,8.241275,7.5835567,7.182239,7.683034,8.152273,8.240354,8.480874,8.19277,6.7964053,6.5015326,7.9017673,7.5873494,7.5910935,7.1041036,5.240414,5.0168624,4.818518,4.583798,8.4232025,5.287752,4.7454553,-3.510952,-3.8411653,7.052546,4.5130424,8.082666,7.816683,-0.20214026,6.9805255,3.4161856,-19.717815,6.4345474,-2.4670775,6.2491884,6.4881005,5.7125087,4.5392838,6.6500697,7.8606563,5.4503965,5.4511533,5.3426824,4.7197695,4.911632,4.9251885,-1.2046317,5.489723,5.1710143,7.597396,6.6310306,8.15925,7.0891643,8.4937315,7.2194834,7.4493876,8.12638,7.423087,7.8221393,7.2423863,8.100895,8.112841,-3.4431314,7.598023,-0.91093034,4.993871,3.8708777,5.0025764,3.22412,6.2600923,7.827236,8.016506,8.026366,7.253134,7.102733,7.2751822,-3.9843688,7.2219415,6.738642,6.4789147,7.3799906,6.150651,4.855814,6.799931,6.952343,4.215423,3.3795831,4.514254,7.504025,6.847647,6.993805,7.933193,7.1929593,7.4388566,7.162157,7.219399,-0.75253993,7.6809173,7.684485,7.586281,7.58729,7.384235,7.058305,7.1772504,7.583451,7.690195,7.568015,7.7715106,7.058638,6.756617,7.3472424,6.4294324,7.0133653,6.560139,7.750953,8.101474,7.756345,8.480713,7.346477,6.7888966,4.5740247,2.128806,2.9395587,7.046734,6.878501,7.9429493,7.898501,7.84134,7.288831,7.8728876,7.3980994,7.010603,7.191229,-4.438507,6.4961543,3.3845224,4.578186,5.4566894,7.8642764,6.468332,-3.8231182,7.4446244,7.286416,7.959336,8.693189,8.562318,8.183619,8.363809,8.307629,8.569573,8.580425,8.112255,8.3601055,8.366705,8.694186,8.555251,8.607425,8.510141,8.188143,8.412889,8.459581,8.365556,8.462874,8.724905,8.512571,8.346813,8.474594,5.880129,7.1820607,6.418052,4.5095086,3.5929487,6.982563,-3.3099372,5.3574524,5.147039,3.61809,5.3150315,3.7017305,3.225964,3.5185194,5.61766,5.9652953,7.0919566,5.6966534,3.4978385,0.3699983,4.5100656,7.6685834,6.255999,7.7849293,8.5679865,8.1400385,8.611298,8.281521,8.565005,8.281977,8.580242,8.646828,8.6115675,8.716086,8.681568,8.607392,8.655574,8.399762,8.26992,7.8315463,8.059111,7.968787,8.507907,8.141277,7.914742,8.425676,8.8235855,6.42362,8.120885,7.193418,1.2605708,7.3473372,7.726561,8.02234,6.3384137,3.1723306,5.4661803,6.7620025,5.6007442,6.2447925,0.1928084,5.288654,6.273421,6.7240696,5.773885,5.8527737,6.6966677,3.5186455,-19.720013,3.9837298,6.142115,6.9071774,-3.347097,6.309522,4.061165,4.7920504,4.889539,6.8179564,7.9276624,6.609321,-0.61040854,7.170771,8.067328,8.005643,7.3659825,7.9377236,6.665139,6.39364,3.4855669,-19.720985,5.849332,-2.7099802,-3.436587,5.862117,6.5485196,-2.7163515,5.446873,4.4278264,6.3085427,5.899538,5.9352756,6.676286,3.854339,4.711895,4.4448156,5.2529635,5.485041,6.071628,6.0452867,5.621629,7.893744,-0.24445274,0.082726195,-1.7894301,-2.0492604,-1.3059067,-0.7761408,7.198357,-3.6820965,-0.8528996,-1.3029855,8.092177,7.3647356,6.4558,6.0921845,3.4464269,-19.721348,4.288094,5.965939,6.749482,5.5365834,4.289262,3.8974414,5.1717787,-1.1008811,7.9005675,7.2248797,8.690255,-4.0375514,7.687949,7.8478928,7.4691987,8.056262,7.7775445,8.651662,4.171784,7.591183,7.834102,7.798445,8.075226,6.7633567,7.0416064,7.020297,-0.11802478,7.845573,6.5090065,1.1823161,6.2839146,-1.8900486,7.985778,7.8057313,7.6548953,7.7048483,7.9251842,8.106796,-3.3103263,6.9154778,1.4397812,4.148499,8.024057,7.1816473,6.49093,0.6434418,-0.5588063,3.8518934,4.8670344,5.6338496,5.1745987,5.2941933,5.754074,3.5671718,5.8008227,4.950944,6.8653407,6.2255573,7.5939555,7.4292755,4.713732,2.2084353,2.9346251,6.3353233,6.4323044,2.675206,5.5603213,6.0384016,7.862491,7.7451067,8.593086,8.398191,-2.8675702,-2.4083884,8.342475,8.522976,7.706103,6.776757,-4.030398,8.159189,8.198126,8.140533,7.6228724,7.831198,7.8830695,7.8692408,7.852062,7.8379917,8.016757,7.9922757,7.7255554,7.400672,7.1098104,7.2197633,7.694239,8.049196,7.6885853,7.8222747,7.868237,8.569602,8.097356,8.687333,8.293401,-6.577783,7.9689655,8.099442,7.8235774,8.2897215,8.042969,8.285958,8.692594,7.9992347,8.3087845,7.7587376,8.200077,8.424655,8.326853,7.620487,7.1056013,7.2570524,6.2683496,7.3263073,7.3669004,7.4567227,8.4968605,8.186934,7.1926303,8.099682,8.224889,8.292,-4.0233703,7.5352426,7.5735397,7.4557886,6.843015,7.396751,8.121735,8.30788,8.106977,8.228054,7.1359386,7.570111,7.559594,8.123617,6.8954473,7.8057976,6.203133,5.144501,7.9394274,8.499142,8.3923235,7.8782883,7.6427917,7.75524,8.506932,7.787762,7.771536,-0.49263877,7.6794543,7.97296,-5.179428,7.629182,7.493414,7.809681,7.93933,8.498034,8.192442,-3.9965117,7.7572784,7.0987754,6.5045757,5.577693,7.8817153,8.065845,7.7162485,8.487796,7.7804127,7.785371,8.170079,7.7707133,8.527738,7.998575,-4.6282234,8.18963,7.9812775,8.211618,7.94814,7.9700804,-1.8422282,8.165598,7.6782374,7.552497,8.104525,7.2282662,7.1676774,7.665926,8.718008,8.798717,7.7342215,8.178985,6.323539,3.3763227,5.3735843,3.6593225,3.7129009,3.319243,3.5405939,5.616384,6.260586,5.2483025,5.4606366,0.16724883,5.3696384,6.139485,6.109151,5.8593454,4.382093,4.710698,5.103964,6.3912144,6.374546,6.951843,3.64619,5.0350738,4.276675,3.8931007,6.731132,7.0947475,7.1930165,6.972343,7.913617,8.149556,5.4877563,6.0591474,5.496111,5.7772884,6.1035647,6.2823763,6.223598,6.2091103,7.467021,7.939224,8.419531,8.291417,8.552492,7.6596503,8.308332,7.48807,8.212147,8.697579,7.069064,8.387104,7.9531603,7.984658,8.676675,-4.451324,7.031193,7.3265343,7.2395716,6.5599647,6.7086353,7.0356894,6.8201733,8.014259,-0.8203602,-0.44779307,7.5725136,7.216086,5.5923505,4.504158,6.213952,7.8958597,7.2659383,3.2582448,3.6933682,3.1704729,4.5382967,-19.721647,4.1450634,-1.9020406,6.7107515,7.663817,4.898119,5.4103327,-0.73300916,8.005989,7.7338166,7.9642363,8.161925,7.201456,-6.6437273,-6.641632,-7.0068913,7.4766426,8.007386,8.211857,-1.7529517,-0.9199443,7.929949,7.225436,7.805009,7.1940465,8.173285,6.92232,8.49432,6.311726,8.713896,8.006597,8.498821,7.9198627,8.123803,8.088592,7.799405,8.511092,8.605725,8.227031,8.684983,8.6786175,8.6565,8.624645,8.564603,8.296119,8.368426,7.698332,8.6263,8.658432,8.669043,8.698424,8.46845,8.702605,7.876168,7.946539,8.754989,8.415193,7.6411643,8.675055,8.522093,7.490517,2.8253944,3.877144,7.911201,8.174327,6.7732115,7.928581,7.9204154,7.795441,6.90472,-2.0881069,-3.2331815,8.002258,-3.4557867,6.9018555,0.06092594,-0.92291504,7.258314,6.680997,7.868215,7.5851426,4.711166,5.2496076,3.7827923,5.4961224,-3.1515539,-2.8624914,-2.3844671,7.97223,8.16421,8.441107,7.243517,4.054035,7.6326184,6.4491324,5.7982216,6.9922514],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Datasets server - worker\\n\\n\\u003e Workers that pre-compute and cache the response to \\u002fsplits, \\u002ffirst-rows,...\"],[\"- `WORKER_CONTENT_MAX_BYTES`: the maximum size in bytes of the response content computed by a worker...\"],[\"- `WORKER_MAX_LOAD_PCT`: maximum load of the machine (in percentage: the max between the 1m load and...\"],[\"Also, it's possible to force the parent directory in which the temporary files (as the current job s...\"],[\"- `NUMBA_CACHE_DIR`: directory where the `numba` decorators (used by `librosa`) can write cache.\\n\\nNo...\"],[\"### First rows worker\\n\\nSet environment variables to configure the `first-rows` worker (`FIRST_ROWS_`...\"],[\"- `PARQUET_AND_INFO_COMMIT_MESSAGE`: the git commit message when the worker uploads the parquet file...\"],[\"- `PARQUET_AND_INFO_SOURCE_REVISION`: the git revision of the dataset to use to prepare the parquet ...\"],[\"### Duckdb Index worker\\n\\nSet environment variables to configure the `duckdb-index` worker (`DUCKDB_I...\"],[\"### Descriptive statistics worker\\n\\nSet environment variables to configure the `descriptive-statistic...\"],[\"`column_statistics` content depends on the feature type, see examples below.\\n##### class_label\\n\\n\\u003cdet...\"],[\"```\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e \\n\\n##### float\\n\\nBin size for histogram is counted as `(max_value - min_value) \\u002f D...\"],[\"```\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e \\n\\n##### int\\n\\nAs bin edges for integer values also must be integers, bin size is ...\"],[\"```python\\n{\\n    \\\"column_name\\\": \\\"direction\\\",\\n    \\\"column_type\\\": \\\"int\\\",\\n    \\\"column_statistics\\\": {\\n   ...\"],[\"],\\n            \\\"bin_edges\\\": [\\n                0,\\n                3,\\n                6,\\n             ...\"],[\"\\\"nan_count\\\": 0,\\n        \\\"nan_proportion\\\": 0.0,\\n        \\\"min\\\": 0,\\n        \\\"max\\\": 6,\\n        \\\"mean\\\": 3...\"],[\"```\\n\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e\\n\\n##### string_label\\n\\nIf the number of unique values in a column (within request...\"],[\"```\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e\\n\\n##### bool\\n\\n\\u003cdetails\\u003e\\u003csummary\\u003eexample: \\u003c\\u002fsummary\\u003e\\n\\u003cp\\u003e\\n\\n```python\\n{\\n    'column_...\"],[\"--\\ntitle: Datasets Server Admin UI\\nemoji: 📊\\ncolorFrom: gray\\ncolorTo: purple\\nsdk: gradio\\nsdk_version:...\"],[\"Filter rows in a dataset\\n\\nDatasets Server provides a `\\u002ffilter` endpoint for filtering rows in a data...\"],[\"```\\nwhere=age\\u003e30 AND (name='Simone' OR children=0)\\n```\\nwill filter the data to select only those row...\"],[\"List Parquet files\\n\\nDatasets can be published in any format (CSV, JSONL, directories of images, etc....\"],[\"The `\\u002fparquet` endpoint accepts the dataset name as its query parameter:\\n\\n\\u003cinferencesnippet\\u003e\\n\\u003cpython...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe endpoint response is a JSON containing a list of the dataset's ...\"],[\"```json\\n{\\n  \\\"parquet_files\\\": [\\n    {\\n      \\\"dataset\\\": \\\"duorc\\\",\\n      \\\"config\\\": \\\"ParaphraseRC\\\",\\n     ...\"],[\"},\\n    {\\n      \\\"dataset\\\": \\\"duorc\\\",\\n      \\\"config\\\": \\\"SelfRC\\\",\\n      \\\"split\\\": \\\"test\\\",\\n      \\\"url\\\": \\\"ht...\"],[\"```\\n\\n## Sharded Parquet files\\n\\nBig datasets are partitioned into Parquet files (shards) of about 500...\"],[\"```json\\n{\\n  \\\"parquet_files\\\": [\\n    {\\n      \\\"dataset\\\": \\\"amazon_polarity\\\",\\n      \\\"config\\\": \\\"amazon_pol...\"],[\"{\\n      \\\"dataset\\\": \\\"amazon_polarity\\\",\\n      \\\"config\\\": \\\"amazon_polarity\\\",\\n      \\\"split\\\": \\\"train\\\",\\n   ...\"],[\"```\\n\\nTo read and query the Parquet files, take a look at the [Query datasets from Datasets Server](p...\"],[\"```\\n\\u003c\\u002fjs\\u003e\\n\\u003ccurl\\u003e\\n```curl\\ncurl https:\\u002f\\u002fhuggingface.co\\u002fapi\\u002fdatasets\\u002fduorc\\u002fparquet \\\\\\n        -X GET \\\\\\n ...\"],[\"```\\n\\nOptionally you can specify which configuration name to return, as well as which split:\\n\\n\\u003cinfere...\"],[\"datasets-server Helm chart\\n\\nThe `datasets-server` Helm [chart](https:\\u002f\\u002fhelm.sh\\u002fdocs\\u002ftopics\\u002fcharts\\u002f) ...\"],[\"How to contribute to the Datasets Server?\\n\\n[![Contributor Covenant](https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002fCon...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n   ```bash\\n   git checkout -b a-descr...\"],[\"DuckDB\\n\\n[DuckDB](https:\\u002f\\u002fduckdb.org\\u002fdocs\\u002f) is a database that supports reading and querying Parquet ...\"],[\"```\\n\\u003c\\u002fjs\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nNow you can write and execute your SQL query on the Parquet file:\\n\\n\\u003ci...\"],[\"```\\n\\u003c\\u002fjs\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nTo query multiple files - for example, if the dataset is sharded:\\n\\n\\u003ci...\"],[\"Overview\\n\\nDatasets Server automatically converts and publishes public datasets less than 5GB on the ...\"],[\"Pandas\\n\\n[Pandas](https:\\u002f\\u002fpandas.pydata.org\\u002fdocs\\u002findex.html) is a popular DataFrame library for data ...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nYou can adapt the `BUILD_DIR` environment variable to set any temporary folder that you prefer....\"],[\"Check dataset validity\\n\\nBefore you download a dataset from the Hub, it is helpful to know if a speci...\"],[\"## Check if a dataset is valid\\n\\n`\\u002fis-valid` checks whether a specific dataset loads without any erro...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"Security Policy\\n\\n## Supported Versions\\n\\n\\u003c!--\\nUse this section to tell people about which versions of...\"],[\"Datasets server admin machine\\n\\n\\u003e Admin endpoints\\n\\n## Configuration\\n\\nThe worker can be configured usi...\"],[\"### Prometheus\\n\\n- `PROMETHEUS_MULTIPROC_DIR`: the directory where the uvicorn workers share their pr...\"],[\"Get dataset information\\n\\nDatasets Server provides an `\\u002finfo` endpoint for exploring the general info...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe endpoint response is a JSON with the `dataset_info` key. Its st...\"],[\"```json\\n{\\n    \\\"dataset_info\\\": {\\n        \\\"description\\\": \\\"DuoRC contains 186,089 unique question-answe...\"],[\"\\\"answers\\\": {\\n                \\\"feature\\\": {\\n                    \\\"dtype\\\": \\\"string\\\",\\n                   ...\"],[\"\\\"num_bytes\\\": 24388192,\\n                \\\"checksum\\\": null\\n            },\\n            \\\"https:\\u002f\\u002fraw.gith...\"],[\"Datasets server\\n\\n\\u003e Integrate into your apps over 10,000 datasets via simple HTTP requests, with pre-...\"],[\"You can also report bugs and propose enhancements on the code, or the documentation, in the [GitHub ...\"],[\"Download slices of rows\\n\\nDatasets Server provides a `\\u002frows` endpoint for visualizing any slice of ro...\"],[\"The `\\u002frows` endpoint accepts five query parameters:\\n\\n- `dataset`: the dataset name, for example `glu...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe endpoint response is a JSON containing two keys:\\n\\n- The [`featu...\"],[\"```json\\n\\u002f\\u002f https:\\u002f\\u002fdatasets-server.huggingface.co\\u002frows?dataset=duorc&config=SelfRC&split=train&offse...\"],[\"\\\"_type\\\": \\\"Sequence\\\"\\n      }\\n    },\\n    {\\n      \\\"feature_idx\\\": 6,\\n      \\\"name\\\": \\\"no_answer\\\",\\n      \\\"t...\"],[\"\\\"plot\\\": \\\"The film is centered on Mortal Kombat, a fighting tournament between the representatives of...\"],[\"thunder and defender of the Earth realm, to overcome their powerful adversaries in order to prevent ...\"],[\"daughter. Aware that Kitana is a dangerous adversary because she is the rightful heir to Outworld an...\"],[\"freezing abilities, until Liu recalls Kitana's advice and uses it to kill Sub-Zero.\\\\nPrince Goro ent...\"],[\"accepts on the condition that he be allowed to challenge any opponent of his choosing, anytime and a...\"],[\"to Outworld, intending to fight her as his opponent. Knowing that his powers are ineffective there a...\"],[\"castle tower, Shang Tsung challenges Sonya to fight him, claiming that her refusal to accept will re...\"],[\"tournaments. In a last-ditch attempt to take advantage, Tsung morphs into Chan. Seeing through the c...\"],[\"at the Shaolin temple. The jubilation abruptly stops, however, when Shao Kahn's giant figure suddenl...\"],[\"\\\"title\\\": \\\"Mortal Kombat\\\",\\n        \\\"question_id\\\": \\\"40c1866a-b214-11ba-be57-8979d2cefa90\\\",\\n        \\\"qu...\"],[\"\\\"title\\\": \\\"Mortal Kombat\\\",\\n        \\\"question_id\\\": \\\"f1fdefcf-1191-b5f9-4cae-4ce4d0a59da7\\\",\\n        \\\"qu...\"],[\"```\\n\\n## Image and audio samples\\n\\nImage and audio are represented by a URL that points to the file.\\n\\n...\"],[\"Quickstart\\n\\n[[open-in-colab]]\\n\\nIn this quickstart, you'll learn how to use the Datasets Server's RES...\"],[\"| Endpoint                    | Method | Description                                             | Q...\"],[\"| [\\u002frows](.\\u002frows)             | GET    | Get a slice of rows of a dataset split.                 | -...\"],[\"There is no installation or setup required to use Datasets Server.\\n\\n\\u003cTip\\u003e\\n  Sign up for a \\u003ca href=\\\"h...\"],[\"```\\nhttps:\\u002f\\u002fdatasets-server.huggingface.co\\n```\\n\\n## Gated datasets\\n\\nFor gated datasets, you'll need t...\"],[\"```\\n\\n## Check dataset validity\\n\\nTo check whether a specific dataset is valid, for example, [Rotten T...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThis returns the first 100 rows of the dataset:\\n\\n```json\\n{\\n  \\\"datas...\"],[\"```\\n\\n## Download slices of a dataset\\n\\nThe `\\u002frows` endpoint returns a JSON list of a slice of rows of...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nYou can download slices of 100 rows maximum at a time.\\n\\nThe respons...\"],[\"```\\n\\n## Search text in a dataset\\n\\nThe `\\u002fsearch` endpoint returns a JSON list of a slice of rows of a...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nYou can get slices of 100 rows maximum at a time, and you can ask f...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fjs\\u003e\\n\\u003ccurl\\u003e\\n```curl\\ncurl https:\\u002f\\u002fdatasets-server.huggingface.co\\u002fparquet?dataset=rotten_tomatoes...\"],[\"```\\n\\n## Get the size of the dataset\\n\\nThe `\\u002fsize` endpoint returns a JSON with the size (number of ro...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThis returns a URL to the Parquet file for each split:\\n\\n```json\\n{\\n ...\"],[\"Datasets server SSE API\\n\\n\\u003e Server-sent events API for the Datasets server. It's used to update the H...\"],[\"libapi\\n\\nA Python library for the API services\\n\\n## Configuration\\n\\nThe APIs can be configured using en...\"],[\"- `API_HF_AUTH_PATH`: the path of the external authentication service, on the hub (see `HF_ENDPOINT`...\"],[\"- `API_MAX_AGE_LONG`: number of seconds to set in the `max-age` header on data endpoints. Defaults t...\"],[\"### Uvicorn\\n\\nThe following environment variables are used to configure the Uvicorn server (`API_UVIC...\"],[\"ClickHouse\\n\\n[ClickHouse](https:\\u002f\\u002fclickhouse.com\\u002fdocs\\u002fen\\u002fintro) is a fast and efficient column-orient...\"],[\"```\\n\\n## Aggregate functions\\n\\nNow you can begin to analyze the dataset. Use the `-q` argument to spec...\"],[\"```\\n\\nClickHouse also provides functions for visualizing your queries. For example, you can use the [...\"],[\"```\\n\\nRemember to set `enable_url_encoding` to 0 and `max_https_get_redirects` to 1 to redirect to th...\"],[\"```\\n\\nYou can make this even easier by creating another function that calls `hugging_paths` and outpu...\"],[\"```\\n\\nNow use the `hf` function to query any dataset by passing the dataset name:\\n\\n```bash\\nSELECT hor...\"],[\"Analyze a dataset on the Hub\\n\\n[[open-in-colab]]\\n\\nIn the Quickstart, you were introduced to various e...\"],[\"Use the `\\u002fparquet` endpoint to convert the dataset to a Parquet file and return the URL to it:\\n\\n```p...\"],[\"```\\n\\n## Read dataset with Pandas\\n\\nWith the URL, you can read the Parquet file into a Pandas DataFram...\"],[\"```\\n\\n|                                               src | complexity |                         prob...\"],[\"Polars \\n\\n[Polars](https:\\u002f\\u002fpola-rs.github.io\\u002fpolars-book\\u002fuser-guide\\u002f) is a fast DataFrame library wri...\"],[\"```\\n\\nTo read from a single Parquet file, use the [`read_parquet`](https:\\u002f\\u002fpola-rs.github.io\\u002fpolars\\u002fp...\"],[\"```\\n\\nTo read multiple Parquet files - for example, if the dataset is sharded - you'll need to use th...\"],[\"```\\n\\n## Lazy API\\n\\nPolars offers a [lazy API](https:\\u002f\\u002fpola-rs.github.io\\u002fpolars-book\\u002fuser-guide\\u002flazy\\u002fu...\"],[\"Explore statistics over split data\\n\\nDatasets Server provides a `\\u002fstatistics` endpoint for fetching s...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe response JSON contains two keys:\\n* `num_examples` - number of s...\"],[\"```json\\n{\\n  \\\"num_examples\\\": 8551,\\n  \\\"statistics\\\": [\\n    {\\n      \\\"column_name\\\": \\\"idx\\\",\\n      \\\"column_...\"],[\"```\\n\\n## Response structure by data type\\n\\nCurrently, statistics are supported for strings, float and ...\"],[\"```\\n\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e\\n\\n### float\\n\\nThe following measures are returned for float data types:\\n\\n* minimu...\"],[\"```\\n\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e\\n\\n### bool\\n\\nThe following measures are returned for bool data type:\\n\\n* number an...\"],[\"```\\n\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e\\n\\n### string_text\\n\\nIf string column has more than 30 unique values within the re...\"],[\"libcommon\\n\\nA Python library with common code (cache, queue, workers logic, processing steps, configu...\"],[\"## Cached Assets configuration\\n\\nSet the cached-assets (images and audio files) environment variables...\"],[\"## Common configuration\\n\\nSet the common environment variables to configure the following aspects:\\n\\n-...\"],[\"## Cache configuration\\n\\nSet environment variables to configure the storage of precomputed API respon...\"],[\"Splits and configurations\\n\\nMachine learning datasets are commonly organized in *splits* and they may...\"],[\"Configurations are flexible, and can be used to organize a dataset along whatever objective you'd li...\"],[\"e2e\\n\\nEnd to end tests, written in Python...\"],[\"Preview a dataset\\n\\nDatasets Server provides a `\\u002ffirst-rows` endpoint for visualizing the first 100 r...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe endpoint response is a JSON containing two keys:\\n\\n- The [`featu...\"],[\"```json\\n{\\n  \\\"dataset\\\": \\\"duorc\\\",\\n  \\\"config\\\": \\\"SelfRC\\\",\\n  \\\"split\\\": \\\"train\\\",\\n  \\\"features\\\": [\\n    {\\n    ...\"],[\"}\\n    },\\n    {\\n      \\\"feature_idx\\\": 6,\\n      \\\"name\\\": \\\"no_answer\\\",\\n      \\\"type\\\": { \\\"dtype\\\": \\\"bool\\\", \\\"...\"],[\"\\\"title\\\": \\\"Ghosts of Mars\\\",\\n        \\\"question_id\\\": \\\"b440de7d-9c3f-841c-eaec-a14bdff950d1\\\",\\n        \\\"q...\"],[\"\\\"title\\\": \\\"Ghosts of Mars\\\",\\n        \\\"question_id\\\": \\\"a9f95c0d-121f-3ca9-1595-d497dc8bc56c\\\",\\n        \\\"q...\"],[\"```\\n\\n## Truncated responses\\n\\nFor some datasets, the response size from `\\u002ffirst-rows` may exceed 1MB,...\"],[\"```json\\n  ...\\n  \\\"rows\\\": [\\n    {\\n      \\\"row_idx\\\": 0,\\n      \\\"row\\\": {\\n        \\\"start\\\": \\\"2016-07-01T00:0...\"],[\"Search text in a dataset\\n\\nDatasets Server provides a `\\u002fsearch` endpoint for searching words in a dat...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe endpoint response is a JSON containing two keys (same format as...\"],[\"```json\\n{\\n  \\\"features\\\": [\\n    {\\n      \\\"feature_idx\\\": 0,\\n      \\\"name\\\": \\\"plot_id\\\",\\n      \\\"type\\\": { \\\"dt...\"],[\"\\\"name\\\": \\\"no_answer\\\",\\n      \\\"type\\\": { \\\"dtype\\\": \\\"bool\\\", \\\"_type\\\": \\\"Value\\\" }\\n    }\\n  ],\\n  \\\"rows\\\": [\\n    ...\"],[\"\\\"plot\\\": \\\"The film begins with clips that track a telephone call between London and Geneva, where a u...\"],[\"the Criminal Code opened at random, and concentrates on that passage. As she drives back to her apar...\"],[\"Valentine is walking Rita the next day the dog runs away and Valentine eventually finds her back at ...\"],[\"a contented nuclear family, causing her to change her mind about exposing their secrets. She returns...\"],[\"bowling. Valentine covers her ears but from the very little she hears she concludes that they love e...\"],[\"books. Auguste says yes. Karin gives him a fancy fountain pen as a gift and he wonders what the firs...\"],[\"the news about a retired judge who spied on his neighbours and rushes to Kern's house to tell him th...\"],[\"on selfish grounds and from fear than about obeying the law or being decent. It is his birthday and ...\"],[\"condemned might have seen a different life had he decided otherwise. Valentine tells Kern about her ...\"],[\"when she rushes outside, he hides from her. In a temper, he ties his dog by a quayside and abandons ...\"],[\"Kern to a fashion show where she is modeling. After the show they speak about the dream Kern had abo...\"],[\"and he accidentally dropped one of his books. When he picked it up, Kern studied the chapter where t...\"],[\"the man guilty. He tells Valentine the judgment was entirely legal but also that he subsequently req...\"],[\"the trilogy, Julie and Olivier from Blue, Karol and Dominique from White, and Valentine and Auguste,...\"],[\"\\\"title\\\": \\\"Three Colors: Red\\\",\\n        \\\"question_id\\\": \\\"7c583513-0b7f-ddb3-be43-64befc7e90cc\\\",\\n       ...\"],[\"\\\"title\\\": \\\"Three Colors: Red\\\",\\n        \\\"question_id\\\": \\\"80becb22-908d-84bc-3a5f-00b620d551bc\\\",\\n       ...\"],[\"```\\n\\nIf the result has `partial: true` it means that the search couldn't be run on the full dataset ...\"],[\"Developer guide\\n\\nThis document is intended for developers who want to install, test or contribute to...\"],[\"```\\n\\nIt will create a virtual environment in a `.\\u002f.venv\\u002f` subdirectory.\\n\\nIf you use VSCode, it might...\"],[\"If you have access to the internal HF notion, see https:\\u002f\\u002fwww.notion.so\\u002fhuggingface2\\u002fDatasets-server...\"],[\"Note that every worker has its own job queue:\\n\\n- `\\u002fsplits`: the job is to refresh a dataset, namely ...\"],[\"The following environments contain all the modules: reverse proxy, API server, admin API server, wor...\"],[\"```\\n\\nTo check the quality (which includes checking the style, but also security vulnerabilities):\\n\\n`...\"],[\"```\\n\\n## Set up development environment\\n\\n### Linux\\n\\nInstall pyenv:\\n\\n```bash\\n$ curl https:\\u002f\\u002fpyenv.run ...\"],[\"```\\n\\n#### Then: as a normal user\\n\\nAdd ICU to the path:\\n\\n```bash\\n$ echo 'export PATH=\\\"\\u002fopt\\u002fhomebrew\\u002fo...\"],[\"```\\n\\nSet the python version to use with poetry:\\n\\n```bash\\npoetry env use 3.9.18\\n```\\n\\nAvoid an issue w...\"],[\"Server infrastructure\\n\\nThe [Datasets Server](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets-server) has two...\"],[\"You might've noticed the `\\u002frows` and `\\u002fsearch` endpoints don't have a job in the queue. The response...\"],[\"Datasets server maintenance job\\n\\n\\u003e Job to run maintenance actions on the datasets-server\\n\\nAvailable ...\"],[\"Datasets server - storage admin\\n\\n\\u003e A Ubuntu machine to log into and manage the storage manually...\"],[\"🤗 Datasets Server\\n\\nDatasets Server is a lightweight web API for visualizing and exploring all types ...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg\\n    style=\\\"margin-bottom: 0;\\\"\\n    class=\\\"block dark:hidden\\\"...\"],[\"Get the number of rows and the size in bytes\\n\\nThis guide shows you how to use Datasets Server's `\\u002fsi...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe endpoint response is a JSON containing the size of the dataset,...\"],[\"```json\\n{\\n  \\\"size\\\": {\\n    \\\"dataset\\\": {\\n      \\\"dataset\\\": \\\"duorc\\\",\\n      \\\"num_bytes_original_files\\\": 9...\"],[\"\\\"num_bytes_parquet_files\\\": 26005668,\\n        \\\"num_bytes_memory\\\": 496682909,\\n        \\\"num_rows\\\": 6952...\"],[\"\\\"config\\\": \\\"SelfRC\\\",\\n        \\\"split\\\": \\\"validation\\\",\\n        \\\"num_bytes_parquet_files\\\": 3114390,\\n     ...\"],[\"```\\n\\nIf the size has `partial: true` it means that the actual size of the dataset couldn't been dete...\"],[\"Datasets server API - rows endpoint\\n\\n\\u003e \\u002frows endpoint\\n\\n## Configuration\\n\\nThe service can be configur...\"],[\"Datasets server - reverse proxy\\n\\n\\u003e Reverse-proxy in front of the API\\n\\nSee [docker-compose-datasets-s...\"],[\"It takes various environment variables, all of them are mandatory:\\n\\n- `ASSETS_DIRECTORY`: the direct...\"],[\"Data types\\n\\nDatasets supported by Datasets Server have a tabular format, meaning a data point is rep...\"],[\"```\\n\\nThis dataset has two columns, `text` and `label`:\\n\\n- The `text` column has a type of `Value`. T...\"],[\"List splits and configurations\\n\\nDatasets typically have splits and may also have configurations. A _...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"Datasets server API\\n\\n\\u003e API on 🤗 datasets\\n\\n## Configuration\\n\\nThe service can be configured using envi...\"],[\"Datasets server API - search service\\n\\n\\u003e \\u002fsearch endpoint\\n\\u003e \\u002ffilter endpoint\\n\\n## Configuration\\n\\nThe s...\"],[\"Datasets server databases migrations\\n\\n\\u003e Scripts to migrate the datasets server databases\\n\\n## Configu...\"]],\"hovertemplate\":\"source=datasets-server\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"datasets-server, circle\",\"marker\":{\"color\":\"#636efa\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"datasets-server, circle\",\"showlegend\":true,\"x\":[3.7391973,2.9479566,0.3148262,4.0034046,4.9554024,2.6909983,4.528073,4.413421,4.1863565,2.0267437,1.5868243,2.112991,2.029531,1.9080994,4.7715583,2.7722943,1.7519411,2.3034382,13.01566,2.9059863,2.1362703,3.6157541,3.5833185,4.098691,3.6258087,4.161318,6.652165,3.0755398,4.0589375,4.5229044,3.576634,3.9959185,3.8398397,3.7078056,4.4233036,4.8948135,3.423829,3.245172,3.425537,3.4388654,3.1761487,5.870074,5.429831,3.7079272,3.4659767,4.061666,5.8733974,4.094393,3.658174,3.599559,3.6193347,3.922892,3.545484,5.0433517,3.5826693,4.245769,3.5008974,3.351969,4.262997,3.301473,4.380311,5.6623983,6.6266894,6.6432586,6.63055,6.69417,6.56592,6.660427,6.619236,6.659147,6.617161,6.286322,6.391044,2.7469938,3.5816226,3.2769766,2.2404757,3.900616,3.9475894,3.7237408,3.843199,4.0298758,3.0496385,3.6458974,3.3979197,3.5073376,3.4047515,4.1373143,3.796515,3.6186404,3.8141484,3.7784579,4.795369,5.3505645,3.1718085,4.069153,3.5448377,3.353414,3.3776278,3.6270106,2.7010908,2.764231,3.4390335,3.4286954,3.1209016,2.6581953,3.37612,3.1941094,3.1947746,3.2400868,3.597328,3.0296872,3.6645615,1.8569579,2.396122,2.4689684,2.1996474,4.4333363,4.730848,4.0777855,3.8876936,2.6396382,3.2368197,1.0156707,3.3211763,4.2618914,3.3643057,4.1945024,6.593152,6.3259196,6.171257,2.16743,3.5964546,3.1170125,4.1683764,3.4365208,4.338638,4.178021,6.659432,6.670019,6.7016335,6.6683064,6.64875,6.6571198,6.663028,6.697251,6.712386,6.71321,6.669327,6.763121,6.665162,6.630005,6.2375083,6.280548,1.9993023,4.0154653,4.2096004,3.8955128,4.0956197,4.2714276,4.1280947,2.539602,2.79194,2.5277388,3.6873214,3.6283748,3.8681927,4.182105,3.5615108,4.593995,3.3168445,3.4492877,4.0198874,3.1967826,3.3367534,1.6881398,3.6115556,4.150818,4.881996,2.3140178,1.7190995,2.9007075,3.9847827,3.6826012,3.5354297,3.7822125],\"xaxis\":\"x\",\"y\":[-3.0474648,-3.843913,1.6065153,-1.8973447,-1.8586007,-4.2380376,-1.4458067,-1.5469298,-2.6729827,-4.788577,-4.6133537,-5.13909,-4.973693,-5.127107,-5.533521,-5.2664576,-5.141303,-4.728254,-5.213021,-4.4333405,-4.5692215,-4.0388083,-4.463551,-4.5555253,-4.668972,-4.7703857,-4.280247,-4.4055986,-4.7680497,-4.018851,-4.3566976,-4.587168,-4.4643316,-3.4512882,-2.2346592,-0.20569882,-4.3238773,-4.4860992,-4.521082,-4.2939954,-4.4379673,0.137748,-1.2463989,-3.2836359,-4.2158227,-4.604788,-1.816905,-2.75447,-2.67623,-4.347334,-4.536536,-4.9500313,-3.6728199,-3.8037121,-3.2937076,-2.7716186,-4.001897,-4.5772953,-4.578593,-4.5645733,-4.6548157,-4.75766,-5.522946,-5.536553,-5.5375924,-5.5040493,-5.584193,-5.605705,-5.5743723,-5.4731994,-5.475456,-5.126112,-5.209847,-3.3229,-3.9155207,-4.2604012,-4.4606924,-3.3631477,-4.0893135,-4.532622,-4.7191105,-4.677885,-4.8328314,-4.6321077,-4.871065,-4.6381483,-4.7058287,-4.6975837,-4.646642,-4.715823,-4.7859273,-3.679411,-2.3540666,-1.7102278,-3.4774745,-2.2016613,-4.179396,-4.6626735,-4.3301783,-4.5269732,-3.775986,-4.43872,-4.168813,-4.284187,-4.1157994,-4.8729067,-4.3942943,-4.5012155,-4.3942833,-4.5502834,-4.381381,-4.862184,-4.9291315,-4.9851837,-5.203601,-5.094619,-4.9831576,-2.26581,-2.2688043,-2.762329,-2.9582684,-3.8224516,-3.6031551,-2.7939904,-4.17926,-4.598503,-4.743298,-4.811164,-5.7154446,-5.260261,-5.2076254,-4.578514,-4.990537,-4.5091066,-4.626251,-4.62654,-4.794999,-4.924448,-5.9518356,-5.9649873,-5.93574,-5.985247,-5.7980046,-5.875497,-5.8657994,-5.93723,-5.935056,-5.8990855,-5.809161,-5.8477516,-5.881988,-5.6448946,-5.109466,-5.1851244,-4.6053104,-0.55513185,-1.6013387,-3.4189978,-2.9974833,-1.7578129,-0.61635417,0.3678812,0.13457386,0.31616327,-3.6048045,-3.578392,-3.101012,-2.7847707,-3.6915994,-2.6016958,-4.6384044,-4.7333617,-4.818453,-4.479553,-4.525883,-4.5588603,-4.025549,-3.2656355,-2.0803783,-4.3796372,-4.4167085,-4.2544546,-4.630918,-3.782239,-3.9075582,-2.9557831],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Differences between Dataset and IterableDataset\\n\\nThere are two types of dataset objects, a [`Dataset...\"],[\"```\\n\\nStreaming can read online data without writing any file to disk.\\nFor example, you can stream da...\"],[\"```\\n\\n## Loading local files entirely and progressively\\n\\nIt is possible to convert local or remote da...\"],[\"```\\n\\nOn the other hand, due to the \\\"lazy\\\" nature of an `IterableDataset`, calling [`IterableDataset....\"],[\"```\\n\\nSince we don't have random access to the rows in the case of an `IterableDataset`, we can't use...\"],[\"```\\n\\nBut using a shuffle buffer is not enough to provide a satisfactory shuffling for machine learni...\"],[\"```\\n\\n## Speed differences\\n\\nRegular [`Dataset`] objects are based on Arrow which provides fast random...\"],[\"```\\n\\n\\nIn this case, we recommend switching to an [`IterableDataset`] and leveraging its fast approxi...\"],[\"```\\n\\nIf you want to shuffle your dataset or [use it with a PyTorch DataLoader](.\\u002fuse_with_pytorch#st...\"],[\"Metric Card for F1\\n\\n\\n## Metric Description\\n\\nThe F1 score is the harmonic mean of the precision and r...\"],[\"```\\n\\n\\n### Inputs\\n- **predictions** (`list` of `int`): Predicted labels.\\n- **references** (`list` of ...\"],[\"### Output Values\\n- **f1**(`float` or `array` of `float`): F1 score or list of f1 scores, depending ...\"],[\"```\\n```python\\n{'f1': array([0.8, 0.0, 0.0])}\\n```\\n\\nThis metric outputs a dictionary, with either a si...\"],[\"```\\n\\nExample 4-A multiclass example, with different values for the `average` input.\\n```python\\n\\u003e\\u003e\\u003e pr...\"],[\"Cache management\\n\\nWhen you download a dataset, the processing scripts and data are stored locally on...\"],[\"```\\n\\nRefer to [`DownloadMode`] for a full list of download modes.\\n\\n## Cache files\\n\\nClean up the cach...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nKeeping the predictions in-memory is not possible in a distributed settin...\"],[\"Metric Card for MSE\\n\\n\\n## Metric Description\\n\\nMean Squared Error(MSE) represents the average of the s...\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'mse': array([0.41666667, 1. ])}\\n```\\n\\n#### Values fro...\"],[\"```\\n\\n## Limitations and Bias\\nMSE has the disadvantage of heavily weighting outliers -- given that it...\"],[\"All about metrics\\n\\n\\u003cTip warning={true}\\u003e\\n\\nMetrics is deprecated in 🤗 Datasets. To learn more about ho...\"],[\"## Distributed evaluation\\n\\nComputing metrics in a distributed environment can be tricky. Metric eval...\"],[\"Metric Card for METEOR\\n\\n## Metric description\\n\\nMETEOR (Metric for Evaluation of Translation with Exp...\"],[\"```\\n\\n## Output values\\n\\nThe metric outputs a dictionary containing the METEOR score. Its values range...\"],[\"```\\n\\n## Limitations and bias\\n\\nWhile the correlation between METEOR and human judgments was measured ...\"],[\"Preprocess\\n\\nIn addition to loading datasets, 🤗 Datasets other main goal is to offer a diverse set of...\"],[\"```\\n\\nGrab a dataset of your choice and follow along!\\n\\n## Tokenize text\\n\\nModels cannot process raw te...\"],[\"```\\n\\n**2**. Call your tokenizer on the first row of `text` in the dataset:\\n\\n```py\\n\\u003e\\u003e\\u003e tokenizer(data...\"],[\"```\\n\\nThe tokenizer returns a dictionary with three items:\\n\\n- `input_ids`: the numbers representing t...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nUse the [`~Dataset.to_tf_dataset`] function to set the dataset format to be compatibl...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n**5**. The dataset is now ready for training with your machine learni...\"],[\"```\\n\\n**2**. Index into the first row of the dataset. When you call the `audio` column of the dataset...\"],[\"```\\n\\n**3**. Reading a dataset card is incredibly useful and can give you a lot of information about ...\"],[\"```\\n\\n**4**. Use the [`~Dataset.map`] function to resample the entire dataset to 16kHz. This function...\"],[\"```\\n\\n**2**. Index into the first row of the dataset. When you call the `image` column of the dataset...\"],[\"Image classification\\n\\nImage classification datasets are used to train a model to classify an entire ...\"],[\"```\\n\\nThe dataset has three fields:\\n\\n* `image`: a PIL image object.\\n* `image_file_path`: the path to ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.c...\"],[\"Beam Datasets\\n\\nSome datasets are too large to be processed on a single machine. Instead, you can pro...\"],[\"```\\n\\n4. Run the pipeline:\\n\\n```\\ndatasets-cli run_beam datasets\\u002f$DATASET_NAME \\\\\\n--name $CONFIG_NAME \\\\\\n...\"],[\"Load image data\\n\\nImage datasets have [`Image`] type columns, which contain PIL objects. \\n\\n\\u003cTip\\u003e\\n\\nTo ...\"],[\"```\\n\\nIf you only want to load the underlying path to the image dataset without decoding the image ob...\"],[\"```\\n\\nLoad your dataset by specifying `imagefolder` and the directory of your dataset in `data_dir`:\\n...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nFor more information about creating your own `ImageFolder` dataset, take a look at the [...\"],[\"Metric Card for Recall\\n\\n\\n## Metric Description\\n\\nRecall is the fraction of the positive examples that...\"],[\"### Inputs\\n- **predictions** (`list` of `int`): The predicted labels.\\n- **references** (`list` of `i...\"],[\"- `'samples'`: Calculate metrics for each instance, and find their average (only meaningful for mult...\"],[\"### Output Values\\n- **recall**(`float`, or `array` of `float`, for multiclass targets): Either the g...\"],[\"```\\n```python\\n{'recall': array([1., 0., 0.])}\\n```\\n\\nThis metric outputs a dictionary with one entry, ...\"],[\"```\\n\\nExample 4-A multiclass example, using different averages.\\n```python\\n\\u003e\\u003e\\u003e recall_metric = dataset...\"],[\"Dataset features\\n\\n[`Features`] defines the internal structure of a dataset. It is used to specify th...\"],[\"```\\n\\nThe [`Value`] feature tells 🤗 Datasets:\\n\\n- The `idx` data type is `int32`.\\n- The `sentence1` an...\"],[\"```\\n\\nThe `answers` field is constructed using the [`Sequence`] feature because it contains two subfi...\"],[\"```\\n\\n## Audio feature\\n\\nAudio datasets have a column with type [`Audio`], which contains three import...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nIndex into an audio dataset using the row index first and then the `audio...\"],[\"```\\n\\n## Image feature\\n\\nImage datasets have a column with type [`Image`], which loads `PIL.Image` obj...\"],[\"```\\n\\nDepending on the dataset, you may get the path to the local downloaded image, or the content of...\"],[\"Metric Card for GLUE\\n\\n## Metric description\\nThis metric is used to compute the GLUE evaluation metri...\"],[\"```\\n## Output values\\n\\nThe output of the metric depends on the GLUE subset chosen, consisting of a di...\"],[\"### Values from popular papers\\nThe [original GLUE paper](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fglue) repor...\"],[\"```\\n\\nMinimal values for the STSB subset (which outputs `pearson` and `spearmanr`):\\n\\n```python\\nfrom d...\"],[\"```\\n    \\n## Further References \\n\\n- [GLUE benchmark homepage](https:\\u002f\\u002fgluebenchmark.com\\u002f)\\n- [Fine-tun...\"],[\"Metric Card for Matthews Correlation Coefficient\\n\\n## Metric Description\\nThe Matthews correlation coe...\"],[\"```\\n\\nThe same example as above, but also including sample weights:\\n```python\\n\\u003e\\u003e\\u003e matthews_metric = d...\"],[\"```\\n\\n## Further References\\n\\n- This Hugging Face implementation uses [this scikit-learn implementatio...\"],[\"Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pledge...\"],[\"**Consequence**: A permanent ban from any sort of public interaction within\\nthe community.\\n\\n## Attri...\"],[\"Table Classes\\n\\nEach `Dataset` object is backed by a PyArrow Table.\\nA Table can be loaded from either...\"],[\"## ConcatenationTable\\n\\n[[autodoc]] datasets.table.ConcatenationTable\\n    - validate\\n    - equals\\n   ...\"],[\"Metric Card for Precision\\n\\n\\n## Metric Description\\n\\nPrecision is the fraction of correctly labeled po...\"],[\"### Inputs\\n- **predictions** (`list` of `int`): Predicted class labels.\\n- **references** (`list` of ...\"],[\"- 'samples': Calculate metrics for each instance, and find their average (only meaningful for multil...\"],[\"### Output Values\\n- **precision**(`float` or `array` of `float`): Precision score or list of precisi...\"],[\"```\\n```python\\n{'precision': array([0.66666667, 0.0, 0.0])}\\n```\\n\\n\\n\\n\\n#### Values from Popular Papers\\n\\n...\"],[\"```\\n\\nExample 4-A multiclass example, with different values for the `average` input.\\n```python\\n\\u003e\\u003e\\u003e pr...\"],[\"```\\n\\n\\n## Limitations and Bias\\n\\n[Precision](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002fprecision) and [recall](ht...\"],[\"Load tabular data\\n\\nA tabular dataset is a generic dataset used to describe any data stored in rows a...\"],[\"```\\n\\nTo load zipped CSV files:\\n\\n```py\\n\\u003e\\u003e\\u003e url = \\\"https:\\u002f\\u002fdomain.org\\u002ftrain_data.zip\\\"\\n\\u003e\\u003e\\u003e data_files =...\"],[\"```\\n\\nIf the dataset doesn't look as expected, you should explicitly [specify your dataset features](...\"],[\"```\\n\\nThis creates a `states` table in the `us_covid_data.db` database which you can now load into a ...\"],[\"```\\n\\nYou can also load a dataset from a SQL query instead of an entire table, which is useful for qu...\"],[\"--\\nTODO: Add YAML tags here. Copy-paste the tags obtained with the online tagging app: https:\\u002f\\u002fhuggi...\"],[\"[More Information Needed]\\n\\n### Languages\\n\\n[More Information Needed]\\n\\n## Dataset Structure\\n\\n### Data ...\"],[\"Use with Spark\\n\\nThis document is a quick introduction to using 🤗 Datasets with Spark, with a particu...\"],[\"```\\n\\n### Caching\\n\\nWhen using [`Dataset.from_spark`], the resulting [`Dataset`] is cached; if you cal...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import Dataset, Features, Image, Value\\n\\u003e\\u003e\\u003e data = [(0, open(\\\"image.png\\\", \\\"rb...\"],[\"```\\n\\nYou can check the [`Features`] documentation to know about all the feature types available....\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cpicture\\u003e\\n    \\u003csource media=\\\"(prefers-color-scheme: dark)\\\" srcset=\\\"https:\\u002f\\u002fhuggi...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets\\u002factions\\u002fworkflows\\u002fci.yml?que...\"],[\"\\u003c\\u002fa\\u003e\\n    \\u003ca href=\\\"CODE_OF_CONDUCT.md\\\"\\u003e\\n        \\u003cimg alt=\\\"Contributor Covenant\\\" src=\\\"https:\\u002f\\u002fimg.shie...\"],[\"🤗 Datasets is a lightweight library providing **two** main features:\\n\\n- **one-line dataloaders for m...\"],[\"🤗 Datasets is designed to let the community easily add and share new datasets.\\n\\n🤗 Datasets has many ...\"],[\"```\\n\\n## With conda\\n\\n🤗 Datasets can be installed using conda as follows:\\n\\n```bash\\nconda install -c hu...\"],[\"```\\n\\nFollow the installation pages of TensorFlow and PyTorch to see how to install them with conda.\\n...\"],[\"```\\n\\nIf your dataset is bigger than your disk or if you don't want to wait to download the data, you...\"],[\"```\\n\\nFor more details on using the library, check the quick start page in the documentation: https:\\u002f...\"],[\"# Main differences between 🤗 Datasets and `tfds`\\n\\nIf you are familiar with the great TensorFlow Data...\"],[\"```bibtex\\n@inproceedings{lhoest-etal-2021-datasets,\\n    title = \\\"Datasets: A Community Library for N...\"],[\"month = nov,\\n    year = \\\"2021\\\",\\n    address = \\\"Online and Punta Cana, Dominican Republic\\\",\\n    publi...\"],[\"```\\n\\nIf you need to cite a specific version of our 🤗 Datasets library for reproducibility, you can u...\"],[\"The cache\\n\\nThe cache is one of the reasons why 🤗 Datasets is so efficient. It stores previously down...\"],[\"```\\n\\nIn order for a transform to be hashable, it needs to be picklable by [dill](https:\\u002f\\u002fdill.readth...\"],[\"```\\n\\nThe hash is computed by dumping the object using a `dill` pickler and hashing the dumped bytes....\"],[\"Metric Card for chrF(++)\\n\\n\\n## Metric Description\\nChrF and ChrF++ are two MT evaluation metrics that ...\"],[\"```\\n\\n### Inputs\\n- **`predictions`** (`list` of `str`): The predicted sentences.\\n- **`references`** (...\"],[\"The output is formatted as below:\\n```python\\n{'score': 61.576379378113785, 'char_order': 6, 'word_ord...\"],[\"```\\n\\nThe chrF(++) score can be any value between `0.0` and `100.0`, inclusive.\\n\\n#### Values from Pop...\"],[\"```\\n\\nThe same chrF++ example as above, but with `lowercase=True` to normalize all case:\\n```python\\n\\u003e\\u003e...\"],[\"```\\n\\n\\n## Limitations and Bias\\n- According to [Popović 2017](https:\\u002f\\u002fwww.statmt.org\\u002fwmt17\\u002fpdf\\u002fWMT70.p...\"],[\"## Citation\\n```bibtex\\n@inproceedings{popovic-2015-chrf,\\n    title = \\\"chr{F}: character n-gram {F}-sc...\"],[\"```\\n\\n## Further References\\n- See the [sacreBLEU README.md](https:\\u002f\\u002fgithub.com\\u002fmjpost\\u002fsacreBLEU#chrf-...\"],[\"Metric Card for BERT Score\\n\\n## Metric description\\n\\nBERTScore is an automatic evaluation metric for t...\"],[\"```\\n\\nBERTScore also accepts multiple optional arguments: \\n\\n\\n`num_layers` (int): The layer of represe...\"],[\"`recall`: The [recall](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002frecall) for each sentence from the `prediction...\"],[\"```\\n\\nPartial match with the `bert-base-uncased` model:\\n\\n```python\\nfrom datasets import load_metric\\nb...\"],[\"```\\n\\n## Limitations and bias\\n\\nThe [original BERTScore paper](https:\\u002f\\u002fopenreview.net\\u002fpdf?id=SkeHuCVFD...\"],[\"Metric Card for ROUGE\\n\\n## Metric Description\\nROUGE, or Recall-Oriented Understudy for Gisting Evalua...\"],[\"```\\n\\n### Inputs\\n- **predictions** (`list`): list of predictions to score. Each prediction\\n        sh...\"],[\"```python\\n{'rouge1': [Score(precision=1.0, recall=0.5, fmeasure=0.6666666666666666), Score(precision...\"],[\"```\\n\\nIf `rouge_types=['rouge1', 'rouge2']` and `use_aggregator=True`, the output is of the following...\"],[\"```\\n\\nThe same example, but with aggregation:\\n```python\\n\\u003e\\u003e\\u003e rouge = datasets.load_metric('rouge')\\n\\u003e\\u003e\\u003e...\"],[\"Metric Card for Exact Match\\n\\n\\n## Metric Description\\nA given predicted string's exact match score is ...\"],[\"```\\n\\n### Inputs\\n- **`predictions`** (`list` of `str`): List of predicted texts.\\n- **`references`** (...\"],[\"```\\n\\nThis metric's range is 0-100, inclusive. Here, 0.0 means no prediction\\u002freference pairs were mat...\"],[\"```\\nNote that in the example above, because the regexes are ignored before the case is normalized, \\\"...\"],[\"```\\n\\nAn example that includes sentences:\\n```python\\n\\u003e\\u003e\\u003e exact_match = datasets.load_metric(\\\"exact_mat...\"],[\"Metric Card for COMET\\n\\n## Metric description\\n\\nCrosslingual Optimized Metric for Evaluation of Transl...\"],[\"```\\n\\nIt has several configurations, named after the COMET model to be used. It will default to `wmt2...\"],[\"## Examples\\n\\nFull match:\\n\\n```python\\nfrom datasets import load_metric\\ncomet_metric = load_metric('com...\"],[\"```\\n\\nPartial match:\\n\\n```python\\nfrom datasets import load_metric\\ncomet_metric = load_metric('comet') ...\"],[\"```\\n\\n## Limitations and bias\\n\\nThe models provided for calculating the COMET metric are built on top ...\"],[\"## Citation\\n\\n```bibtex\\n@inproceedings{rei-EtAl:2020:WMT,\\n   author    = {Rei, Ricardo  and  Stewart,...\"],[\"```\\n\\n```bibtex\\n@inproceedings{rei-etal-2020-comet,\\n   title = \\\"{COMET}: A Neural Framework for {MT} ...\"],[\"Metric Card for seqeval\\n\\n## Metric description\\n\\nseqeval is a Python framework for sequence labeling ...\"],[\"```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e seqeval = load_metric('seqeval')\\n\\u003e\\u003e\\u003e predictions ...\"],[\"```\\n\\n## Output values\\n\\nThis metric returns a dictionary with a summary of scores for overall and per...\"],[\"More recently, seqeval continues being used for reporting performance on tasks such as [named entity...\"],[\"```\\n\\nMinimal values (no match):\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e seqeval = load_m...\"],[\"```\\n\\nPartial match:\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e seqeval = load_metric('seqev...\"],[\"```\\n\\n## Limitations and bias\\n\\nseqeval supports following IOB formats (short for inside, outside, beg...\"],[\"Utilities\\n\\n## Configure logging\\n\\n🤗 Datasets strives to be transparent and explicit about how it work...\"],[\"```\\n\\nAll the methods of this logging module are documented below. The main ones are:\\n\\n- [`logging.ge...\"],[\"[[autodoc]] datasets.logging.disable_propagation\\n\\n[[autodoc]] datasets.logging.enable_propagation\\n\\n#...\"],[\"Use with PyTorch\\n\\nThis document is a quick introduction to using `datasets` with PyTorch, with a par...\"],[\"```\\n\\n## N-dimensional arrays\\n\\nIf your dataset consists of N-dimensional arrays, you will see that by...\"],[\"```\\n\\n\\n## Other feature types\\n\\n[`ClassLabel`] data are properly converted to tensors:\\n\\n```py\\n\\u003e\\u003e\\u003e from...\"],[\"```\\n\\nString and binary objects are unchanged, since PyTorch only supports numbers.\\n\\nThe [`Image`] an...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nTo use the [`Audio`] feature type, you'll need to install the `audio` extra as\\n`pip inst...\"],[\"```\\n\\n## Data loading\\n\\nLike `torch.utils.data.Dataset` objects, a [`Dataset`] can be passed directly ...\"],[\"```\\n\\n### Optimize data loading\\n\\nThere are several ways you can increase the speed your data is loade...\"],[\"```\\n\\n### Stream data\\n\\nStream a dataset by loading it as an [`IterableDataset`]. This allows you to p...\"],[\"```\\n\\nIn this case each worker is given a subset of the list of shards to stream from.\\n\\n### Distribut...\"],[\"Semantic segmentation\\n\\nSemantic segmentation datasets are used to train a model to classify every pi...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"\\u003e\\u003e\\u003e visualize_seg_mask(\\n...     np.array(dataset[index][\\\"image\\\"]),\\n...     np.array(dataset[index][\\\"...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nYou can verify the transformation worked by indexing into the `pixel_values` and `label` of an ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"Metric Card for *Current Metric*\\n\\n***Metric Card Instructions:*** *Copy this file into the relevant ...\"],[\"Metric Card for WikiSplit\\n\\n## Metric description\\n\\nWikiSplit is the combination of three metrics: [SA...\"],[\"```\\n## Output values\\n\\nThis metric outputs a dictionary containing three scores:\\n\\n`sari`: the [SARI](...\"],[\"```\\n\\n### Values from popular papers\\n\\nThis metric was initially used by [Rothe et al.(2020)](https:\\u002f\\u002f...\"],[\"```\\n\\nNo match between prediction and reference:\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e ...\"],[\"Metric Card for FrugalScore\\n\\n\\n## Metric Description\\nFrugalScore is a reference-based metric for Natu...\"],[\"```\\n\\n### Values from popular papers\\nThe [original FrugalScore paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2110.0855...\"],[\"How to add one new datasets\\n\\nAdd datasets directly to the 🤗 Hugging Face Hub!\\n\\nYou can share your da...\"],[\"Metric Card for Google BLEU (GLEU)\\n\\n\\n## Metric Description\\nThe BLEU score has some undesirable prope...\"],[\"```\\n\\n### Inputs\\n- **predictions** (list of list of str): list of translations to score. Each transla...\"],[\"```\\n\\n#### Values from Popular Papers\\n\\n\\n### Examples\\nExample with one reference per sample:\\n```python...\"],[\"```\\n\\nExample with multiple references for the first sample:\\n```python\\n\\u003e\\u003e\\u003e hyp1 = ['It', 'is', 'a', '...\"],[\"\\u003e\\u003e\\u003e list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\\n\\u003e\\u003e\\u003e hypotheses = [hyp1, hyp2]\\n\\u003e\\u003e\\u003e google_b...\"],[\"```\\n\\nExample with multiple references for the first sample, and with `min_len` adjusted to `2`, inst...\"],[\"\\u003e\\u003e\\u003e hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',\\n...         'interested', 'in', 'wo...\"],[\"```\\n\\nExample with multiple references for the first sample, with `min_len` adjusted to `2`, instead ...\"],[\"\\u003e\\u003e\\u003e hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',\\n...         'interested', 'in', 'wo...\"],[\"```\\n\\n## Limitations and Bias\\n\\n\\n## Citation\\n```bibtex\\n@misc{wu2016googles,\\ntitle={Google's Neural Mac...\"],[\"# Add Dummy data test\\n\\n**Important** In order to pass the `load_dataset_\\u003cdataset_name\\u003e` test, dummy ...\"],[\"Here we have to pay close attention to the ``_split_generators(self, dl_manager)`` function of the d...\"],[\"**Note** if ``\\u003cvalue_of_dict\\u003e`` is a zipped file then the dummy data folder structure should contain...\"],[\"Main classes\\n\\n\\n## DatasetInfo\\n\\n[[autodoc]] datasets.DatasetInfo\\n\\n## Dataset\\n\\nThe base class [`Datase...\"],[\"[[autodoc]] datasets.concatenate_datasets\\n\\n[[autodoc]] datasets.interleave_datasets\\n\\n[[autodoc]] dat...\"],[\"## IterableDataset\\n\\nThe base class [`IterableDataset`] implements an iterable Dataset backed by pyth...\"],[\"[[autodoc]] datasets.Image\\n\\n## MetricInfo\\n\\n[[autodoc]] datasets.MetricInfo\\n\\n## Metric\\n\\nThe base clas...\"],[\"Metric Card for COVAL\\n\\n## Metric description\\n\\nCoVal is a coreference evaluation tool for the [CoNLL]...\"],[\"```python\\nfrom datasets import load_metric\\ncoval = load_metric('coval')\\nwords = ['bc\\u002fcctv\\u002f00\\u002fcctv_00...\"],[\"```\\nIt also has several optional arguments:\\n\\n`keep_singletons`: After extracting all mentions of key...\"],[\"### Values from popular papers\\n\\nGiven that many of the metrics returned by COVAL come from different...\"],[\"## Examples \\n\\nMaximal values\\n\\n```python\\nfrom datasets import load_metric\\ncoval = load_metric('coval'...\"],[\"```\\n\\n## Limitations and bias\\n\\nThis wrapper of CoVal currently only works with [CoNLL line format](ht...\"],[\"| Column | Type                  | Description                                                      ...\"],[\"| 5      | Part-of-Speech        |                                                                  ...\"],[\"## Citations\\n\\n```bibtex\\n@InProceedings{moosavi2019minimum,\\n  author = { Nafise Sadat Moosavi, Leo Bo...\"],[\"```\\n```bibtex\\n@inproceedings{10.3115\\u002f1072399.1072405,\\n  author = {Vilain, Marc and Burger, John and ...\"],[\"```\\n\\n```bibtex\\n@inproceedings{moosavi-strube-2016-coreference,\\n    title = \\\"Which Coreference Evalua...\"],[\"Overview\\n\\nThe how-to guides offer a more comprehensive overview of all the tools 🤗 Datasets offers a...\"],[\"If you have any questions about 🤗 Datasets, feel free to join and ask the community on our [forum](h...\"],[\"Metric Card for SacreBLEU\\n\\n\\n## Metric Description\\nSacreBLEU provides hassle-free computation of shar...\"],[\"### Inputs\\n- **`predictions`** (`list` of `str`): list of translations to score. Each translation sh...\"],[\"- `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\\n    - `'char'`: La...\"],[\"### Output Values\\n- `score`: BLEU score\\n- `counts`: Counts\\n- `totals`: Totals\\n- `precisions`: Precis...\"],[\"```\\nThe score can take any value between `0.0` and `100.0`, inclusive.\\n\\n#### Values from Popular Pap...\"],[\"Cloud storage\\n\\n🤗 Datasets supports access to cloud storage providers through a `fsspec` FileSystem i...\"],[\"```\\n\\u003e\\u003e\\u003e pip install s3fs\\n```\\n\\n2. Define your credentials\\n\\nTo use an anonymous connection, use `anon=...\"],[\"```\\n\\n3. Create your FileSystem instance\\n\\n```py\\n\\u003e\\u003e\\u003e import gcsfs\\n\\u003e\\u003e\\u003e fs = gcsfs.GCSFileSystem(**stora...\"],[\"```\\n\\n3. Create your FileSystem instance\\n\\n```py\\n\\u003e\\u003e\\u003e import ocifs\\n\\u003e\\u003e\\u003e fs = ocifs.OCIFileSystem(**stora...\"],[\"```\\n\\nUse your own data files (see [how to load local and remote files](.\\u002floading#local-and-remote-fi...\"],[\"```\\n\\n#### Dask\\n\\nDask is a parallel computing library and it has a pandas-like API for working with l...\"],[\"```\\n\\nYou can find more about dask dataframes in their [documentation](https:\\u002f\\u002fdocs.dask.org\\u002fen\\u002fstabl...\"],[\"```\\n\\n### Load serialized datasets\\n\\nWhen you are ready to use your dataset again, reload it with [`Da...\"],[\"Metric Card for BLEU\\n\\n\\n## Metric Description\\nBLEU (Bilingual Evaluation Understudy) is an algorithm ...\"],[\"```\\n\\n### Inputs\\n- **predictions** (`list`): Translations to score. Each translation should be tokeni...\"],[\"```\\n\\nBLEU's output is always a number between 0 and 1. This value indicates how similar the candidat...\"],[\"### Examples\\n\\nExample where each sample has 1 reference:\\n```python\\n\\u003e\\u003e\\u003e predictions = [\\n...     [\\\"hel...\"],[\"```\\n\\nExample where the first sample has 2 references:\\n```python\\n\\u003e\\u003e\\u003e predictions = [\\n...     [\\\"hello\\\"...\"],[\"```\\n\\n## Limitations and Bias\\nThis metric hase multiple known limitations and biases:\\n- BLEU compares...\"],[\"Create an image dataset\\n\\nThere are two methods for creating and sharing an image dataset. This guide...\"],[\"```\\n\\nYou can also use `imagefolder` to load datasets involving multiple splits. To do so, your datas...\"],[\"```\\n\\nor using `metadata.jsonl`:\\n\\n```jsonl\\n{\\\"file_name\\\": \\\"0001.png\\\", \\\"additional_feature\\\": \\\"This is a...\"],[\"```\\n\\n### Object detection\\n\\nObject detection datasets have bounding boxes and categories identifying ...\"],[\"```\\n\\n### Upload dataset to the Hub\\n\\nOnce you've created a dataset, you can share it to the Hub with ...\"],[\"```\\n\\nYou can put your images labels\\u002fcaptions\\u002fbounding boxes using JSON or text files for example.\\n\\nF...\"],[\"```\\n\\nThis guide will show you how to create a dataset loading script for image datasets, which is a ...\"],[\"```\\n\\n#### Multiple configurations\\n\\nIn some cases, a dataset may have more than one configuration. Fo...\"],[\"```\\n\\nNow you can define your subsets at the top of [`GeneratorBasedBuilder`]. Imagine you want to cr...\"],[\"```\\n\\n### Add dataset metadata\\n\\nAdding information about your dataset is useful for users to learn mo...\"],[\"```\\n\\n### Download and define the dataset splits\\n\\nNow that you've added some information about your d...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n```py\\ndef _split_generators(self, dl_manager):\\n    archive_path = dl_manager.download(_BASE_...\"],[\"```\\n\\n### Generate the dataset\\n\\nThe last method in the [`GeneratorBasedBuilder`] class actually gener...\"],[\"```\\n\\nIf your loading script passed the test, you should now have the `dataset_info` YAML fields in t...\"],[\"Create a dataset card\\n\\nEach dataset should have a dataset card to promote responsible usage and info...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n3. Click on the **Import dataset card template** link to automatically create a template wit...\"],[\"Process\\n\\n🤗 Datasets provides many tools for modifying the structure and content of a dataset. These ...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nAll processing methods in this guide return a new [`Dataset`] object. Mod...\"],[\"```\\n\\nShuffling takes the list of indices `[0:len(my_dataset)]` and shuffles it to create an indices ...\"],[\"```\\n\\n- [`~Dataset.filter`] returns rows that match a specified condition:\\n\\n```py\\n\\u003e\\u003e\\u003e start_with_ar =...\"],[\"```\\n\\nUnless the list of indices to keep is contiguous, those methods also create an indices mapping ...\"],[\"```\\n\\nAfter sharding the dataset into four chunks, the first shard will only have 6250 examples:\\n\\n```...\"],[\"```\\n\\n### Remove\\n\\nWhen you need to remove one or more columns, provide the column name to remove to t...\"],[\"```\\n\\n### Cast\\n\\nThe [`~Dataset.cast`] function transforms the feature type of one or more columns. Th...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nCasting only works if the original feature type and new feature type are compatible. For...\"],[\"```\\n\\nThe `answers` field contains two subfields: `text` and `answer_start`. Use the [`~Dataset.flatt...\"],[\"```\\n\\nNow use [`~Dataset.map`] to apply the `add_prefix` function to the entire dataset:\\n\\n```py\\n\\u003e\\u003e\\u003e u...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n🤗 Datasets also has a [`~Dataset.remove_columns`] function which is faster because it do...\"],[\"```\\n\\nThe [`~Dataset.map`] also works with the rank of the process if you set `with_rank=True`. This ...\"],[\"```\\n\\n### Batch processing\\n\\nThe [`~Dataset.map`] function supports working with batches of examples. ...\"],[\"```\\n\\nNotice how the sentences are split into shorter chunks now, and there are more rows in the data...\"],[\"```\\n\\nUse [`~Dataset.map`] to apply the function over the whole dataset:\\n\\n```py\\n\\u003e\\u003e\\u003e augmented_dataset...\"],[\"```\\n\\nFor each original sentence, RoBERTA augmented a random word with three alternatives. The origin...\"],[\"```\\n\\n### Distributed usage\\n\\nWhen you use [`~Dataset.map`] in a distributed setting, you should also ...\"],[\"```\\n\\nYou can also concatenate two datasets horizontally by setting `axis=1` as long as the datasets ...\"],[\"```\\n\\nYou can also specify the `stopping_strategy`. The default strategy, `first_exhausted`, is a sub...\"],[\"```\\n\\nThe [`~Dataset.with_format`] function also changes the format of a column, except it returns a ...\"],[\"```\\n\\n### Format transform\\n\\nThe [`~Dataset.set_transform`] function applies a custom formatting trans...\"],[\"```\\n\\nYou can also use the [`~Dataset.set_transform`] function to decode formats not supported by [`F...\"],[\"\\u003e\\u003e\\u003e audio_dataset_amr.set_transform(decode_audio_with_pydub)...\"],[\"```\\n\\n## Save\\n\\nOnce you are done processing your dataset, you can save and reuse it later with [`~Dat...\"],[\"Metric Card for SQuAD v2\\n\\n## Metric description\\nThis metric wraps the official scoring script for ve...\"],[\"The range of `total` depends on the length of predictions\\u002freferences: its minimal value is 0, and ma...\"],[\"```\\n\\nMinimal values for both exact match and F1 (no match):\\n\\n```python\\nfrom datasets import load_met...\"],[\"```\\n\\nPartial match (2 out of 3 answers correct) : \\n\\n```python\\nfrom datasets import load_metric\\nsquad...\"],[\"Metric Card for Perplexity\\n\\n## Metric Description\\nGiven a model and an input text sequence, perplexi...\"],[\"```\\n\\n### Inputs\\n- **model_id** (str): model used for calculating Perplexity. NOTE: Perplexity can on...\"],[\"```\\n\\nThis metric's range is 0 and up. A lower score is better.\\n\\n#### Values from Popular Papers\\n\\n\\n##...\"],[\"```\\n\\n## Limitations and Bias\\nNote that the output value is based heavily on what text the model was ...\"],[\"Overview\\n\\nWelcome to the 🤗 Datasets tutorials! These beginner-friendly tutorials will guide you thro...\"],[\"Metric Card for Pearson Correlation Coefficient (pearsonr)\\n\\n\\n## Metric Description\\n\\nPearson correlat...\"],[\"```\\n\\n\\n### Inputs\\n- **predictions** (`list` of `int`): Predicted class labels, as returned by a model...\"],[\"```\\n\\nExample 2-The same as Example 1, but that also returns the `p-value`.\\n```python\\n\\u003e\\u003e\\u003e pearsonr_me...\"],[\"Load audio data\\n\\nYou can load an audio dataset using the [`Audio`] feature that automatically decode...\"],[\"```\\nfolder\\u002ftrain\\u002fmetadata.csv\\nfolder\\u002ftrain\\u002ffirst_audio_file.mp3\\nfolder\\u002ftrain\\u002fsecond_audio_file.mp3\\nf...\"],[\"```\\n\\nYou can load remote datasets from their URLs with the data_files parameter:\\n\\n```py\\n\\u003e\\u003e\\u003e dataset ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nFor more information about creating your own `AudioFolder` dataset, take a look at the [...\"],[\"Search index\\n\\n[FAISS](https:\\u002f\\u002fgithub.com\\u002ffacebookresearch\\u002ffaiss) and [Elasticsearch](https:\\u002f\\u002fwww.ela...\"],[\"```\\n\\n3. Create the index with [`Dataset.add_faiss_index`]:\\n\\n```py\\n\\u003e\\u003e\\u003e ds_with_embeddings.add_faiss_i...\"],[\"```\\n\\n6. When you are done querying, save the index on disk with [`Dataset.save_faiss_index`]:\\n\\n```py...\"],[\"```\\n\\n4. If you want to reuse the index, define the `es_index_name` parameter when you build the inde...\"],[\"```\\n\\nFor more advanced Elasticsearch usage, you can specify your own configuration with custom setti...\"],[\"How to contribute to Datasets?\\n[![Contributor Covenant](https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002fContributor%20C...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n    ```bash\\n    git checkout -b a-des...\"],[\"```\\n\\n   Go the webpage of your fork on GitHub. Click on \\\"Pull request\\\" to send your to the project m...\"],[\"If you are a **dataset author**... you know what to do, it is your dataset after all ;) ! We would e...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-4\\\"\\u003e\\n   \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" href=\\\"#nl...\"],[\"\\u003cTip\\u003e\\n\\nCheck out [Chapter 5](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fchapter5\\u002f1?fw=pt) of the Hugging Face cou...\"],[\"```\\n\\n🤗 Datasets also support audio and image data formats:\\n\\n* To work with audio datasets, install t...\"],[\"```\\n\\n**2**. Next, load a pretrained [Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fwav2vec2-base) model ...\"],[\"```\\n\\n**3**. The [MInDS-14](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fPolyAI\\u002fminds14) dataset card indicates th...\"],[\"```\\n\\n**4**. Create a function to preprocess the audio `array` with the feature extractor, and trunca...\"],[\"```\\n\\n**6**. Set the dataset format according to the machine learning framework you're using.\\n\\n\\u003cframe...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n**7**. Start training with your machine learning framework! Check out...\"],[\"```\\n\\n**2**. Now you can add some data augmentations with any library ([Albumentations](https:\\u002f\\u002falbum...\"],[\"```\\n\\n**5**. Set the dataset format according to the machine learning framework you're using.\\n\\n\\u003cframe...\"],[\"```\\n\\n```py\\n\\u003e\\u003e\\u003e import albumentations\\n\\u003e\\u003e\\u003e import numpy as np\\n\\n\\u003e\\u003e\\u003e transform = albumentations.Compose(...\"],[\"```\\n\\n**2**. Next, load a pretrained [BERT](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased) model and its c...\"],[\"```\\n\\n**3**. Create a function to tokenize the dataset, and you should also truncate and pad the text...\"],[\"\\u003e\\u003e\\u003e dataset = dataset.map(encode, batched=True)\\n\\u003e\\u003e\\u003e dataset[0]\\n{'sentence1': 'Amrozi accused his bro...\"],[\"```\\n\\n**4**. Rename the `label` column to `labels`, which is the expected input name in [BertForSeque...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\nUse the [`~transformers.TFPreTrainedModel.prepare_tf_dataset`] method from 🤗 Transfo...\"],[\"Process text data\\n\\nThis guide shows specific methods for processing text datasets. Learn how to:\\n\\n- ...\"],[\"```\\n\\nSet the `batched` parameter to `True` in the [`~Dataset.map`] function to apply the tokenizer t...\"],[\"```\\n\\nThe [`~Dataset.map`] function converts the returned values to a PyArrow-supported format. But e...\"],[\"Security Policy\\n\\n## Supported Versions\\n\\u003c!--\\nUse this section to tell people about which versions of ...\"],[\"Process audio data\\n\\nThis guide shows specific methods for processing audio datasets. Learn how to:\\n\\n...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.c...\"],[\"```\\n\\n- For fine-tuned speech recognition models, you only need to load a `processor`:\\n\\n    ```py\\n   ...\"],[\"Task templates\\n\\n\\u003cTip warning={true}\\u003e\\n\\nThe Task API is deprecated in favor of [`train-eval-index`](ht...\"],[\"Object detection\\n\\nObject detection models identify something in an image, and object detection datas...\"],[\"```\\n\\nThe dataset has the following fields:\\n\\n- `image`: PIL.Image.Image object containing the image.\\n...\"],[\"\\u003e\\u003e\\u003e categories = ds['train'].features['objects'].feature['category']\\n\\n\\u003e\\u003e\\u003e boxes_xywh = torch.tensor(...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nNow when you visualize the result, the image should be flipped, but the `bboxes` should still b...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nYou can verify the transform works by visualizing the 10th example:\\n\\n```py\\n\\u003e\\u003e\\u003e example = ds['tr...\"],[\"Load\\n\\nYour data can be stored in various places; they can be on your local machine's disk, in a Gith...\"],[\"```\\n\\nSome datasets may have more than one version based on Git tags, branches, or commits. Use the `...\"],[\"```\\n\\nThe `split` parameter can also map a data file to a specific split:\\n\\n```py\\n\\u003e\\u003e\\u003e data_files = {\\\"v...\"],[\"```\\n\\n## Local and remote files\\n\\nDatasets can be loaded from local files stored on your computer and ...\"],[\"```\\n\\nAnother JSON format you may encounter is a nested field, in which case you'll need to specify t...\"],[\"```\\n\\nTo load remote Parquet files via HTTP, pass the URLs instead:\\n\\n```py\\n\\u003e\\u003e\\u003e base_url = \\\"https:\\u002f\\u002fst...\"],[\"```\\n\\nUnlike [`load_dataset`], [`Dataset.from_file`] memory maps the Arrow file without preparing the...\"],[\"```\\n\\nTo load remote WebDatasets via HTTP, pass the URLs instead:\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import...\"],[\"```\\n\\n### Python list of dictionaries\\n\\nLoad a list of Python dictionaries with [`~Dataset.from_list`]...\"],[\"```\\n\\n### Pandas DataFrame\\n\\nLoad Pandas DataFrames with [`~Dataset.from_pandas`]:\\n\\n```py\\n\\u003e\\u003e\\u003e from dat...\"],[\"```\\n\\nSelect specific rows of the `train` split:\\n\\n```py\\n\\u003e\\u003e\\u003e train_10_20_ds = datasets.load_dataset(\\\"b...\"],[\"```\\n\\nFinally, you can even create cross-validated splits. The example below creates 10-fold cross-va...\"],[\"```\\n\\n### Percent slicing and rounding\\n\\nThe default behavior is to round the boundaries to the neares...\"],[\"```\\n\\nIf you want equal sized splits, use `pct1_dropremainder` rounding instead. This treats the spec...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\n`pct1_dropremainder` rounding may truncate the last examples in a dataset...\"],[\"For example, if you try to download a configuration from the [MATINF](https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\nIf you've already downloaded a dataset from the *Hub with a loading script* to your computer, t...\"],[\"```\\n\\n## Metrics\\n\\n\\u003cTip warning={true}\\u003e\\n\\nMetrics is deprecated in 🤗 Datasets. To learn more about how ...\"],[\"```\\n\\n### Distributed setup\\n\\nWhen working in a distributed or parallel processing environment, loadin...\"],[\"Metric Card for SuperGLUE\\n\\n## Metric description\\nThis metric is used to compute the SuperGLUE evalua...\"],[\"Format of `references`:\\n- for `record`: list of question-answers dictionaries with the following key...\"],[\"```\\n## Output values\\n\\nThe output of the metric depends on the SuperGLUE subset chosen, consisting of...\"],[\"```\\n\\nMinimal values for the MultiRC subset (which outputs `pearson` and `spearmanr`):\\n\\n```python\\nfro...\"],[\"Stream\\n\\nDataset streaming lets you work with a dataset without downloading it.\\nThe data is streamed ...\"],[\"```\\n\\nDataset streaming also lets you work with a dataset made of local files without doing any conve...\"],[\"```\\n\\nLoading a dataset in streaming mode creates a new dataset type instance (instead of the classic...\"],[\"```\\n\\nThe [`~Dataset.to_iterable_dataset`] function supports sharding when the [`IterableDataset`] is...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n[`IterableDataset.shuffle`] will also shuffle the order of the shards if the dataset is ...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\n`take` and `skip` prevent future calls to `shuffle` because they lock in ...\"],[\"```\\n\\nAround 80% of the final dataset is made of the `en_dataset`, and 20% of the `fr_dataset`.\\n\\nYou ...\"],[\"```\\n\\n### Remove\\n\\nWhen you need to remove one or more columns, give [`IterableDataset.remove_columns`...\"],[\"```\\n\\n### Cast\\n\\n[`IterableDataset.cast`] changes the feature type of one or more columns. This method...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nCasting only works if the original feature type and new feature type are compatible. For...\"],[\"```\\n\\nNext, apply this function to the dataset with [`IterableDataset.map`]:\\n\\n```py\\n\\u003e\\u003e\\u003e from datasets...\"],[\"```\\n\\n### Batch processing\\n\\n[`IterableDataset.map`] also supports working with batches of examples. O...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nSee other examples of batch processing in the [batched map processing](.\\u002fprocess#batch-p...\"],[\"```\\n\\nLastly, create a simple training loop and start training:\\n\\n```py\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from torc...\"],[\"Depth estimation\\n\\nDepth estimation datasets are used to train a model to approximate the relative di...\"],[\"```\\n\\nThe dataset has two fields:\\n\\n* `image`: a PIL PNG image object with `uint8` data type.\\n* `depth...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"...    d_min = np.min(depth_map)\\n...    d_max = np.max(depth_map)\\n...    depth_map = colored_depthma...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nWith `additional_targets` defined, you can pass the target depth maps to the `depth` argument o...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"Using Datasets with TensorFlow\\n\\nThis document is a quick introduction to using `datasets` with Tenso...\"],[\"```\\n\\n## N-dimensional arrays\\n\\nIf your dataset consists of N-dimensional arrays, you will see that by...\"],[\"```\\n\\n\\n## Other feature types\\n\\n[`ClassLabel`] data are properly converted to tensors:\\n\\n```py\\n\\u003e\\u003e\\u003e from...\"],[\"```\\n\\nString and binary objects are unchanged, since PyTorch only supports numbers.\\n\\nThe [`Image`] an...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nTo use the [`Audio`] feature type, you'll need to install the `audio` extra as\\n`pip inst...\"],[\"```\\n\\n## Data loading\\n\\nAlthough you can load individual samples and batches just by indexing into you...\"],[\"Since the entire data preprocessing pipeline can be compiled in a `tf.data.Dataset`, this approach a...\"],[\"```\\n\\nThe returned `tf_ds` object here is now fully ready to train on, and can be passed directly to ...\"],[\"```\\n\\nFor a full description of the arguments, please see the [`~Dataset.to_tf_dataset`] documentatio...\"],[\"The key thing to recognize is that when you convert the whole dataset to `Tensor`s, it is static and...\"],[\"Metric Card for Mahalanobis Distance\\n\\n## Metric Description\\nMahalonobis distance is the distance bet...\"],[\"```\\n\\n## Limitations and Bias\\n\\nThe Mahalanobis distance is only able to capture linear relationships ...\"],[\"Troubleshooting\\n\\nThis guide aims to provide you the tools and knowledge required to navigate some co...\"],[\"```\\n\\nIf you encounter this issue, you need to upgrade the `datasets` library to the latest version (...\"],[\"```\\n\\nThis error can also occur when using a generator function that uses a global object that is not...\"],[\"If you are facing issues creating a custom dataset with a script on Hub, you can ask the Hugging Fac...\"],[\"```\\n\\n### GitHub Issues\\n\\nFinally, if you suspect to have found a bug related to the library itself, c...\"],[\"Process image data\\n\\nThis guide shows specific methods for processing image datasets. Learn how to:\\n\\n...\"],[\"```\\n\\nThe cache file saves time because you don't have to execute the same transform twice. The [`~Da...\"],[\"For example, if you'd like to change the color properties of an image randomly:\\n\\n```py\\n\\u003e\\u003e\\u003e from torc...\"],[\"```\\n\\nCreate a function to apply the `ColorJitter` transform:\\n\\n```py\\n\\u003e\\u003e\\u003e def transforms(examples):\\n.....\"],[\"Builder classes\\n\\n## Builders\\n\\n🤗 Datasets relies on two main classes during the dataset building proc...\"],[\"Metrics\\n\\n\\u003cTip warning={true}\\u003e\\n\\nMetrics is deprecated in 🤗 Datasets. To learn more about how to use m...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nMetrics accepts various input formats (Python lists, NumPy arrays, PyTorch tensors, etc....\"],[\"```\\n\\n\\u003ca id='metric_script'\\u003e\\u003c\\u002fa\\u003e\\n\\n## Custom metric loading script\\n\\nWrite a metric loading script to u...\"],[\"After you've filled out all these fields in the template, it should look like the following example ...\"],[\"```\\n\\n### Download metric files\\n\\nIf your metric needs to download, or retrieve local files, you will ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIf the files are stored locally, provide a dictionary of path(s) instead of URLs.\\n\\n\\u003c\\u002fTip...\"],[\"```\\n\\n### Compute score\\n\\n[`DatasetBuilder._compute`] provides the actual instructions for how to comp...\"],[\"```\\n\\n2. Create [`DatasetBuilder._compute`] with instructions for what metric to calculate for each c...\"],[\"Metric Card for MAUVE\\n\\n## Metric description\\n\\nMAUVE is a library built on PyTorch and HuggingFace Tr...\"],[\"```\\n\\nIt also has several optional arguments:\\n\\n`num_buckets`: the size of the histogram to quantize P...\"],[\"`seed`: random seed to initialize k-means cluster assignments, randomly assigned by default.\\n    \\n\\n\\n...\"],[\"```\\n\\nPartial match between prediction and reference:\\n\\n```python\\nfrom datasets import load_metric\\nmau...\"],[\"```\\n\\n## Further References \\n- [Official MAUVE implementation](https:\\u002f\\u002fgithub.com\\u002fkrishnap25\\u002fmauve)\\n-...\"],[\"Create a dataset loading script\\n\\n\\n\\u003cTip\\u003e\\n\\nThe dataset loading script is likely not needed if your dat...\"],[\"```\\n\\n```py\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\u003e\\u003e\\u003e load_dataset(\\\"path\\u002fto\\u002fmy_dataset\\\")\\n```\\n\\nThe fol...\"],[\"```\\n\\n3. `DatasetInfo.homepage` contains the URL to the dataset homepage so users can find more detai...\"],[\"```\\n\\n### Multiple configurations\\n\\nIn some cases, your dataset may have multiple configurations. For ...\"],[\"Args:\\n        features: *list[string]*, list of the features that will appear in the\\n            fea...\"],[\"```\\n\\n2. Create instances of your config to specify the values of the attributes of each configuratio...\"],[\"```\\n\\nAdditionally, users can instantiate a custom builder configuration by passing the builder confi...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nOnly use a default configuration when it makes sense. Don't set one becau...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIf the data files live in the same folder or repository of the dataset script, you can j...\"],[\"```\\n\\n## Generate samples\\n\\nAt this point, you have:\\n\\n- Added the dataset attributes.\\n- Provided instr...\"],[\"```\\n\\n## (Optional) Generate dataset metadata\\n\\nAdding dataset metadata is a great way to include info...\"],[\"```\\n\\n## Advanced features\\n\\n### Sharding\\n\\nIf your dataset is made of many big files, 🤗 Datasets autom...\"],[\"```\\n\\nTo yield Arrow tables instead of single examples, make your dataset builder inherit from [`Arro...\"],[\"Metric Card for Competition MATH\\n\\n## Metric description\\n\\nThis metric is used to assess performance o...\"],[\"```\\n\\nN.B. To be able to use Competition MATH, you need to install the `math_equivalence` dependency ...\"],[\"```\\n\\nPartial match:\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e math = load_metric(\\\"competit...\"],[\"Metric Card for SARI\\n\\n\\n## Metric description\\nSARI (***s**ystem output **a**gainst **r**eferences and...\"],[\"```\\n## Output values\\n\\nThis metric outputs a dictionary with the SARI score:\\n\\n```\\nprint(sari_score)\\n{...\"],[\"```\\n\\nPartial match between prediction and reference:\\n\\n```python\\nfrom datasets import load_metric\\nsar...\"],[\"Metric Card for Mean IoU \\n\\n\\n## Metric Description\\n\\nIoU (Intersection over Union) is the area of over...\"],[\"```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e import numpy as np\\n\\u003e\\u003e\\u003e mean_iou = load_metric(\\\"me...\"],[\"Metric Card for ROC AUC\\n\\n\\n## Metric Description\\nThis metric computes the area under the curve (AUC) ...\"],[\"```\\n\\nThe default implementation of this metric is the **binary** implementation. If employing the **...\"],[\"```\\n\\nSee the [Examples Section Below](#examples_section) for more extensive examples....\"],[\"### Inputs\\n- **`references`** (array-like of shape (n_samples,) or (n_samples, n_classes)): Ground t...\"],[\"- `None`:  No average is calculated, and scores for each class are returned. Only works with the mul...\"],[\"### Output Values\\nThis metric returns a dict containing the `roc_auc` score. The score is a `float`,...\"],[\"```\\n\\nIn contrast, though, the output takes the following format in the multilabel case when `average...\"],[\"```\\n\\nExample 3, the **multilabel** use case:\\n```python\\n\\u003e\\u003e\\u003e roc_auc_score = datasets.load_metric(\\\"roc...\"],[\"```\\n\\n```bibtex\\n@article{10.1023\\u002fA:1010920819831,\\nauthor = {Hand, David J. and Till, Robert J.},\\ntitl...\"],[\"```\\n\\n## Further References\\nThis implementation is a wrapper around the [Scikit-learn implementation]...\"],[\"Use with JAX\\n\\nThis document is a quick introduction to using `datasets` with JAX, with a particular ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nA [`Dataset`] object is a wrapper of an Arrow table, which allows fast reads from arrays...\"],[\"```\\n\\nAnother thing you'll need to take into consideration is that the formatting is not applied\\nunti...\"],[\"```\\n\\nNote that if the `device` argument is not provided to `with_format` then it will use the defaul...\"],[\"```\\n\\nString and binary objects are unchanged, since JAX only supports numbers.\\n\\nThe [`Image`] and [`...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nTo use the [`Audio`] feature type, you'll need to install the `audio` extra as\\n`pip inst...\"],[\"```\\n\\n## Data loading\\n\\nJAX doesn't have any built-in data loading capabilities, so you'll need to use...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\u003e\\u003e\\u003e ds = load_dataset(\\\"mnist\\\")\\n\\u003e\\u003e\\u003e ds = ds.with_format(\\\"...\"],[\"```\\n\\nOnce the format is set we can feed the dataset to the JAX model in batches using the `Dataset.i...\"],[\"Build and load\\n\\nNearly every deep learning workflow begins with loading a dataset, which makes it on...\"],[\"* [`datasets.packaged_modules.text.Text`] for text\\n* [`datasets.packaged_modules.csv.Csv`] for CSV a...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n   \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumenta...\"],[\"You can also set the [`DatasetBuilder.BUILDER_CONFIG_CLASS`] to any custom subclass of [`BuilderConf...\"],[\"Once the files are downloaded, [`SplitGenerator`] organizes them into splits. The [`SplitGenerator`]...\"],[\"If the dataset doesn't pass the verifications, it is likely that the original host of the dataset ma...\"],[\"Metric Card for Accuracy\\n\\n\\n## Metric Description\\n\\nAccuracy is the proportion of correct predictions ...\"],[\"```\\n\\nThis metric outputs a dictionary, containing the accuracy score.\\n\\n\\n#### Values from Popular Pap...\"],[\"```\\n\\n\\n## Limitations and Bias\\nThis metric can be easily misleading, especially in the case of unbala...\"],[\"Metric Card for CER\\n\\n## Metric description\\n\\nCharacter error rate (CER) is a common metric of the per...\"],[\"```\\n## Output values\\n\\nThis metric outputs a float representing the character error rate.\\n\\n```\\nprint(...\"],[\"```\\n\\nNo match between prediction and reference:\\n\\n```python\\nfrom datasets import load_metric\\ncer = lo...\"],[\"Metric Card for XTREME-S\\n\\n\\n## Metric Description\\n\\nThe XTREME-S metric aims to evaluate model perform...\"],[\"- `cer`:  Character error rate (CER) is similar to WER, but operates on character instead of word. T...\"],[\"```\\n\\nFor the `covost2` subset (which outputs `bleu`):\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metri...\"],[\"```\\n \\nFor the `minds14` subset (which outputs `f1` and `accuracy`):\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets imp...\"],[\"```\\n    \\n## Further References \\n\\n- [XTREME-S dataset](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fgoogle\\u002fxtreme_...\"],[\"Metric Card for WER\\n\\n## Metric description\\nWord error rate (WER) is a common metric of the performan...\"],[\"```\\n## Output values\\n\\nThis metric outputs a float representing the word error rate.\\n\\n```\\nprint(wer_s...\"],[\"```\\n\\nNo match between prediction and reference:\\n\\n```python\\nfrom datasets import load_metric\\nwer = lo...\"],[\"Share a dataset to the Hub\\n\\nThe [Hub](https:\\u002f\\u002fhuggingface.co\\u002fdatasets) is home to an extensive colle...\"],[\"Text file extensions are not tracked by Git LFS by default, and if they're greater than 10MB, they w...\"],[\"1. Click on **Create Dataset Card** to create a Dataset card. This button creates a `README.md` file...\"],[\"Once your dataset is stored on the Hub, anyone can load it with the [`load_dataset`] function:\\n\\n```p...\"],[\"```\\n\\n## Upload with Python\\n\\nUsers who prefer to upload a dataset programmatically can use the [huggi...\"],[\"```\\n\\nTo add a new configuration (or subset) to a dataset or to add a new split (train\\u002fvalidation\\u002ftes...\"],[\"Metric Card for SQuAD\\n\\n## Metric description\\nThis metric wraps the official scoring script for versi...\"],[\"```\\n{'exact_match': 100.0, 'f1': 100.0}\\n```\\n\\nThe range of `exact_match` is 0-100, where 0.0 means no...\"],[\"```\\n\\nMinimal values for both exact match and F1 (no match):\\n\\n```python\\nfrom datasets import load_met...\"],[\"```\\n\\nPartial match (2 out of 3 answers correct) : \\n\\n```python\\nfrom datasets import load_metric\\nsquad...\"],[\"Loading methods\\n\\nMethods for listing and loading datasets and metrics:\\n\\n## Datasets\\n\\n[[autodoc]] dat...\"],[\"```\\n\\n### Text\\n\\n[[autodoc]] datasets.packaged_modules.text.TextConfig\\n\\n[[autodoc]] datasets.packaged_...\"],[\"Metric Card for CUAD\\n\\n## Metric description\\n\\nThis metric wraps the official scoring script for versi...\"],[\"Note that `answer_start` values are not taken into account to compute the metric.\\n\\n```python\\nfrom da...\"],[\"```\\n## Output values\\n\\nThe output of the CUAD metric consists of a dictionary that contains one or se...\"],[\"`prec_at_90_recall`: The fraction of true examples among the predicted examples at a recall rate of ...\"],[\"```\\n\\nMinimal values:\\n\\n```python\\nfrom datasets import load_metric\\ncuad_metric = load_metric(\\\"cuad\\\")\\np...\"],[\"```\\n\\nPartial match: \\n\\n```python\\nfrom datasets import load_metric\\ncuad_metric = load_metric(\\\"cuad\\\")\\np...\"],[\"```\\n\\n## Limitations and bias\\nThis metric works only with datasets that have the same format as the [...\"],[\"Datasets 🤝 Arrow\\n\\n## What is Arrow?\\n\\n[Arrow](https:\\u002f\\u002farrow.apache.org\\u002f) enables large amounts of dat...\"],[\"```\\n\\nThis is possible because the Arrow data is actually memory-mapped from disk, and not loaded in ...\"],[\"Metric Card for IndicGLUE\\n\\n## Metric description\\nThis metric is used to compute the evaluation metri...\"],[\"```\\n    \\n## Output values\\n\\nThe output of the metric depends on the IndicGLUE subset chosen, consisti...\"],[\"```\\n\\nMinimal values for the Wiki-NER subset (which outputs `accuracy` and `f1`):\\n\\n```python\\n\\u003e\\u003e\\u003e from...\"],[\"```\\n    \\n## Further References \\n- [IndicNLP website](https:\\u002f\\u002findicnlp.ai4bharat.org\\u002fhome\\u002f)\\n-...\"],[\"Structure your repository\\n\\nTo host and share your dataset, create a dataset repository on the Huggin...\"],[\"```\\n\\n\\nYou can select multiple files per split using a list of paths:\\n\\n```\\nmy_dataset_repository\\u002f\\n├──...\"],[\"```\\n\\n## Builder parameters\\n\\nNot only `data_files`, but other builder-specific parameters can be pass...\"],[\"```\\n\\n### Filename splits\\n\\nIf you don't have any non-traditional splits, then you can place the split...\"],[\"```\\n\\n### Single split\\n\\nWhen 🤗 Datasets can't find any of the above patterns, then it'll treat all th...\"],[\"```\\n\\nFor more flexibility over how to load and generate a dataset, you can also write a [dataset loa...\"],[\"Load a dataset from the Hub\\n\\nFinding high-quality datasets that are reproducible and accessible can ...\"],[\"```\\n\\nIf you're happy with the dataset, then load it with [`load_dataset`]:\\n\\n```py\\n\\u003e\\u003e\\u003e from datasets ...\"],[\"```\\n\\n## Configurations\\n\\nSome datasets contain several sub-datasets. For example, the [MInDS-14](http...\"],[\"```\\n\\n## Remote code\\n\\nCertain datasets repositories contain a loading script with the Python code use...\"],[\"Metric Card for Code Eval\\n\\n## Metric description\\n\\nThe CodeEval metric estimates the pass@k metric fo...\"],[\"```\\n\\n## Output values\\n\\nThe Code Eval metric outputs two things:\\n\\n`pass_at_k`: a dictionary with the ...\"],[\"```\\n\\nPartial match at k=1, full match at k=2:\\n\\n```python\\nfrom datasets import load_metric\\ncode_eval ...\"],[\"Create a dataset\\n\\nSometimes, you may need to create a dataset if you're working with your own data. ...\"],[\"* [`ImageFolder`] uses the [`~datasets.Image`] feature to decode an image file. Many image extension...\"],[\"```\\npokemon\\u002ftrain\\u002fgrass\\u002fbulbasaur.png\\npokemon\\u002ftrain\\u002ffire\\u002fcharmander.png\\npokemon\\u002ftrain\\u002fwater\\u002fsquirtle...\"],[\"```\\n\\nTo learn more about each of these folder-based builders, check out the and \\u003ca href=\\\"https:\\u002f\\u002fhug...\"],[\"```\\n\\n    A generator-based [`IterableDataset`] needs to be iterated over with a `for` loop for examp...\"],[\"```\\n\\n## Next steps\\n\\nWe didn't mention this in the tutorial, but you can also create a dataset with a...\"],[\"Create an audio dataset\\n\\nYou can share a dataset with your team or with anyone in the community by c...\"],[\"```\\n\\nThen upload the dataset to the Hugging Face Hub using [`Dataset.push_to_hub`]:\\n\\n```py\\naudio_dat...\"],[\"```\\nmy_dataset\\u002f\\n├── README.md\\n├── metadata.csv\\n└── data\\u002f\\n```\\n\\nThe `data` folder can be any name you ...\"],[\"```\\nmetadata.csv\\ndata\\u002ffirst_audio_file.mp3\\ndata\\u002fsecond_audio_file.mp3\\ndata\\u002fthird_audio_file.mp3\\n\\n```...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\n    Note that if audio files are located not right next to a metadata fil...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nIf all audio files are contained in a single directory or if they are not...\"],[\"```\\n\\nThis guide will show you how to create a dataset loading script for audio datasets, which is a ...\"],[\"```\\n\\nIn addition to learning how to create a streamable dataset, you'll also learn how to:\\n\\n* Create...\"],[\"```py\\nclass VivosDataset(datasets.GeneratorBasedBuilder):\\n    \\\"\\\"\\\"VIVOS is a free Vietnamese speech c...\"],[\"```\\n\\n#### Multiple configurations\\n\\nIn some cases, a dataset may have more than one configuration. Fo...\"],[\"```\\n\\nDefine your configurations in the `BUILDER_CONFIGS` class variable inside [`GeneratorBasedBuild...\"],[\"```\\n\\n### Add dataset metadata\\n\\nAdding information about your dataset helps users to learn more about...\"],[\"```\\n\\n### Download and define the dataset splits\\n\\nNow that you've added some information about your d...\"],[\"return [\\n        datasets.SplitGenerator(\\n            name=datasets.Split.TRAIN,\\n            gen_kwa...\"],[\"```\\n\\n\\n\\u003cTip warning={true}\\u003e\\n\\nThis implementation does not extract downloaded archives. If you want to...\"],[\"```\\n\\nFinally, iterate over files in `audio_files` and yield them along with their corresponding meta...\"],[\"```\\n\\n### Upload the dataset to the Hub\\n\\nOnce your script is ready, [create a dataset card](.\\u002fdataset...\"],[\"```\\n\\n3. Use the [`~DownloadManager.iter_archive`] method to iterate over the archive at `audio_path`...\"],[\"In the `gen_kwargs` parameter, specify the file paths to `local_extracted_archive`, `audio_files`, `...\"],[\"```\\n\\n#### Generate the dataset\\n\\nHere `_generate_examples` accepts `local_extracted_archive`, `audio_...\"],[\"```\\n\\nPut both of these steps together, and the whole `_generate_examples` method should look like:\\n\\n...\"],[\"Share a dataset using the CLI\\n\\nAt Hugging Face, we are on a mission to democratize good Machine Lear...\"],[\"For more information on how to load a dataset from the Hub, take a look at the [load a dataset from ...\"],[\"```\\nhuggingface-cli login\\n```\\n\\n2. Login using your Hugging Face Hub credentials, and create a new da...\"],[\"```\\ncp \\u002fsomewhere\\u002fdata\\u002f*.json .\\ngit lfs track *.json\\ngit add .gitattributes\\ngit add *.json\\ngit commi...\"],[\"Metric Card for Spearman Correlation Coefficient Metric (spearmanr)\\n\\n\\n## Metric Description\\nThe Spea...\"],[\"```\\n\\n### Inputs\\n- **`predictions`** (`list` of `float`): Predicted labels, as returned by a model.\\n-...\"],[\"```\\n\\nThe same example, but that also returns the pvalue:\\n```python\\n\\u003e\\u003e\\u003e spearmanr_metric = datasets.l...\"],[\"Datasets\\n\\n\\u003cimg class=\\\"float-left !m-0 !border-0 !dark:border-0 !shadow-none !max-w-lg w-[150px]\\\" src...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" href=\\\".\\u002fa...\"],[\"Load text data\\n\\nThis guide shows you how to load text datasets. To learn how to load any type of dat...\"],[\"```\\n\\nTo load remote text files via HTTP, pass the URLs instead:\\n\\n```py\\n\\u003e\\u003e\\u003e dataset = load_dataset(\\\"t...\"],[\"Metric Card for TER\\n\\n## Metric Description\\nTER (Translation Edit Rate, also called Translation Error...\"],[\"```\\n\\nThe metric can take on any value `0` and above. `0` is a perfect score, meaning the predictions...\"],[\"```\\n\\nExample ignoring punctuation and capitalization, and everything matches:\\n```python\\n\\u003e\\u003e\\u003e predicti...\"],[\"Metric Card for MAE\\n\\n\\n## Metric Description\\n\\nMean Absolute Error (MAE) is the mean of the magnitude ...\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'mae': array([0.5, 1. ])}\\n```\\n\\n#### Values from Popul...\"],[\"--\\nYAML tags (full spec here: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fblob\\u002fmain\\u002fdatasetcard.md?plain...\"],[\"## Dataset Description\\n\\n- **Homepage:** [Add homepage URL here if available (unless it's a GitHub re...\"],[\"- `task-category-tag`: The dataset can be used to train a model for [TASK NAME], which consists in [...\"],[\"```\\n{\\n  'example_field': ...,\\n  ...\\n}...\"],[\"```\\n\\nProvide any additional information that is not covered in the other sections about the data her...\"],[\"### Curation Rationale\\n\\nWhat need motivated the creation of this dataset? What are some of the reaso...\"],[\"### Annotations\\n\\nIf the dataset contains annotations which are not part of the initial data collecti...\"],[\"State whether the dataset contains other data that might be considered sensitive (e.g., data that re...\"],[\"If studies of the datasets have outlined other limitations of the dataset, such as annotation artifa...\"],[\"```\\n@article{article_id,\\n  author    = {Author List},\\n  title     = {Dataset Paper Title},\\n  journal...\"],[\"Know your dataset\\n\\nThere are two types of dataset objects, a regular [`Dataset`] and then an ✨ [`Ite...\"],[\"```\\n\\nUse the `-` operator to start from the end of the dataset:\\n\\n```py\\n# Get the last row in the dat...\"],[\"```\\n\\nBut it is important to remember that indexing order matters, especially when working with large...\"],[\"```\\n\\n### Slicing\\n\\nSlicing returns a slice - or subset - of the dataset, which is useful for viewing ...\"],[\"```\\n\\nYou can also create an [`IterableDataset`] from an *existing* [`Dataset`], but it is faster tha...\"],[\"```\\n\\nYou can return a subset of the dataset with a specific number of examples in it with [`Iterable...\"],[\"Metric Card for XNLI\\n\\n## Metric description\\n\\nThe XNLI metric allows to evaluate a model's score on t...\"],[\"```\\n\\n## Output values\\n\\nThe output of the XNLI metric is simply the `accuracy`, i.e. the proportion o...\"],[\"Evaluate predictions\\n\\n\\u003cTip warning={true}\\u003e\\n\\nMetrics is deprecated in 🤗 Datasets. To learn more about...\"],[\"```\\n\\nThis will load the metric associated with the MRPC dataset from the GLUE benchmark.\\n\\n## Select ...\"],[\"```\\n\\n## Metrics object\\n\\nBefore you begin using a [`Metric`] object, you should get to know it a litt...\"],[\"```\\n\\nNotice for the MRPC configuration, the metric expects the input format to be zero or one. For a...\"],[\"Batch mapping\\n\\nCombining the utility of [`Dataset.map`] with batch mode is very powerful. It allows ...\"],[\"For example, from a dataset of 1 column and 3 rows, if you use `map` to return a new column with twi...\"],[\"```\\n\\nTo make it valid, you have to drop one of the columns:\\n\\n```py\\n\\u003e\\u003e\\u003e from datasets import Dataset\\n...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### Documentation notebooks\\n\\nYou can open any page of the documentation as a notebook in Colab (ther...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nThen run the following command:\\n\\n```bash\\ndoc-builder preview datasets docs\\u002fsource\\u002f\\n```\\n\\nThe doc...\"],[\"```\\nand of course if you moved it to another file, then:\\n\\n```\\nSections that were moved:\\n\\n[ \\u003ca href=\\\"...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\nFo...\"],[\"If you want to create a link to some internal class or function, you need to\\nprovide its path. For i...\"],[\"```\\n    Args:\\n        n_layers (`int`): The number of layers of the model.\\n```\\n\\nIf the description i...\"],[\"```\\n```\\n# first line of code\\n# second line\\n# etc\\n```\\n````\\n\\n#### Writing a return block\\n\\nThe return b...\"],[\"```\\n\\n#### Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that ...\"],[\"```\\n```\\n\\nThe docstring should give a minimal, clear example of how the respective class or function ...\"],[\"Installation\\n\\nBefore you start, you'll need to setup your environment and install the appropriate pa...\"],[\"```\\n\\nThis command downloads version 1 of the [Stanford Question Answering Dataset (SQuAD)](https:\\u002f\\u002fr...\"],[\"```\\n\\n## Audio\\n\\nTo work with audio datasets, you need to install the [`Audio`] feature as an extra de...\"]],\"hovertemplate\":\"source=datasets\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"datasets, circle\",\"marker\":{\"color\":\"#EF553B\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"datasets, circle\",\"showlegend\":true,\"x\":[2.0472007,2.1627483,2.182353,1.3078495,1.4479129,1.4361403,1.7196407,1.4731938,1.4925933,0.20238207,0.5860359,0.38846418,0.33602783,0.41146582,2.780058,2.4259803,1.7872695,0.12026395,0.7314943,0.55589116,-0.0064757215,0.32975775,0.4853489,-0.38358802,0.15863016,-1.252529,-0.30916232,-2.6816716,-1.7554309,-1.7762011,-0.08916368,-7.442706,-7.306186,-7.5255575,-7.509561,1.1550043,-9.109643,1.2556156,0.73418653,2.2707195,3.2146237,1.9996305,2.1850019,2.3141952,2.5073328,0.12123011,0.54251057,0.61410886,0.25032905,0.3329845,0.33830923,1.6854516,1.3937008,-0.7910349,-7.285996,2.457219,2.0200949,1.9948993,-0.148628,0.203151,0.035611488,-0.28053716,-2.4592223,0.21022315,0.41322923,-6.282662,6.6736665,6.8651977,2.5496197,2.0859866,0.068941176,0.49932587,0.5669032,0.32913116,0.45157447,0.46079823,0.017187642,2.6862211,2.6307309,1.668713,3.2994611,2.9392512,3.0697522,3.2818365,2.0890195,2.3541086,1.8182048,3.4369516,6.5874953,6.5877666,6.6625504,2.948229,2.4781692,3.237888,1.5241027,2.261025,2.772325,2.41447,-2.128171,-4.4489756,4.371019,1.8896953,1.0522298,1.4883837,-0.3820899,-0.22563986,0.14526178,-0.091601156,-0.098011106,-1.0186579,-1.7811973,5.32895,-0.3563364,-3.287682,-0.14911895,-0.06893871,-4.802174,-0.5720171,-0.117811404,0.32749644,0.24159013,0.25994363,0.02480518,-0.17501865,-0.0313671,-0.2346272,-0.13275349,-0.6903863,-0.56338227,0.2431788,0.20572866,-0.7945522,-1.7897176,-1.7021585,-0.061734077,0.3515013,0.1421034,0.0420395,0.27254423,0.22309658,-1.7930894,1.8895619,1.6000037,1.6685354,0.458123,0.49752864,0.5915514,1.056842,-7.323771,0.2989218,1.2531277,1.9016489,1.6609746,-9.378298,1.659255,1.4868126,1.3093203,1.2702096,1.2617384,-8.938178,0.030380381,-0.345633,0.15394983,-0.2892151,-0.33698395,-0.36882296,-0.13988619,3.7367673,-0.50894606,-0.1561121,0.2184658,-0.08904979,0.2493754,-0.57981783,0.19137877,-0.42310688,0.18059807,-5.7625995,2.6996734,2.4869006,2.5476289,2.4443817,2.1513627,2.0071917,2.8506718,-0.35884088,0.3272669,-0.2899638,-0.13187777,0.28372547,-1.8362983,3.0468194,-2.986954,-1.8252242,-1.9005398,-1.4692702,3.2595654,3.2413952,-0.55180746,-0.32759923,-3.3931968,0.23445125,-0.17413059,3.1191835,4.2599344,4.76645,2.8473623,2.677638,2.7354767,2.7006865,2.663278,-0.73513484,-0.21829152,-1.2815418,0.17748357,0.25893876,-1.0564293,2.5930784,2.4397354,2.2876709,2.1615233,3.5986054,2.4962816,2.5357049,2.8139744,2.4636142,2.2250173,2.586213,2.4823449,2.2323608,2.8665938,3.724444,3.8296816,0.13951449,1.3165505,1.4874616,0.9524834,1.3484416,0.8440173,0.87872577,1.2086821,1.2806392,-0.023953097,0.6866463,0.7394193,0.49479467,0.02393227,-1.3884915,0.93681383,-2.0175533,1.3627324,1.3784232,0.6849016,0.34947047,-0.8202921,-7.238356,-7.0982566,2.82986,-0.5141624,0.1585716,0.2388921,0.2759298,-0.6298142,-1.7177567,0.076662645,-1.3772459,3.0672386,0.18163289,0.4576171,0.44577533,2.6268625,2.5486653,2.6241486,2.5908968,-1.6984612,-1.0718893,-0.4072146,2.3120124,3.3126788,4.115036,4.846327,3.976643,3.2475808,3.2904875,-7.825769,-2.4563642,3.3466415,-7.27281,-1.2411859,-7.501549,-7.502586,0.32117918,-7.5207243,1.0567027,0.19215009,0.17675702,-1.9330254,-1.5507425,1.1782038,-0.37648222,-0.3910787,-1.2362756,-0.6824613,-0.531366,5.9145155,-7.365991,1.4076506,-7.47994,-3.7565079,-9.415096,1.2779539,0.76461357,1.4140501,0.942072,1.15811,1.1272168,2.788221,3.0081282,2.6663,2.7525983,3.2054572,2.8638258,2.4550478,2.609174,1.5503927,2.5032818,1.2835261,1.1070957,1.2830997,1.1226772,1.9523247,2.8588684,2.6342542,0.34964573,1.024292,-0.24071017,0.21582478,0.14547057,0.26060832,2.4398973,2.0762546,1.8935605,1.5107607,1.3716847,1.4946382,0.78960246,0.99099505,1.5718963,1.502598,1.0871023,-0.9386471,0.8272809,0.24204656,-9.335153,1.7114227,1.602208,1.499515,1.2914954,1.5031543,1.2990597,1.5054678,-0.016654724,0.103513435,0.3860818,1.3917989,-7.352995,-0.16591586,0.073150806,-0.32177234,-0.21392374,-0.3884122,0.0049580517,-0.03498074,5.2056127,2.4146228,2.3431523,3.7099655,4.957338,1.0491756,1.356783,1.1532289,1.0243096,3.0872352,0.28385022,0.19016716,0.7178863,1.593297,2.7434063,2.565059,0.29896677,0.37513334,-0.39108178,-1.8521341,-0.63880664,-0.34468248,-5.831515,2.8993595,2.702348,2.7141428,2.842859,2.1852896,2.757063,2.815814,2.8577037,2.5837705,1.6320466,2.827318,1.9204603,1.9295114,0.11991515,0.31412244,0.10444619,-0.46194473,0.124016725,-0.30162492,0.21738784,0.44667393,0.20615429,0.3576498,5.0353904,0.51367575,0.4575134,0.3650936,0.352208,0.44409552,0.2374828,0.37447932,1.4729062,1.4819506,1.9033039,1.3298662,1.6876493,-7.2171745,1.3977934,1.7247539,1.1068897,2.466576,2.8534768,3.117812,3.1991732,2.3202655,4.0242267,0.17889002,0.21283238,0.024062283,-0.35361788,-0.313187,-0.37209678,-0.50312525,-0.39333144,0.30341896,-0.4361689,5.1218753,-0.3421285,-0.2907803,-0.36158472,3.7281518,4.220183,3.7399554,3.073992,3.6597002,3.559073,-0.18231864,0.18805607,0.24737222,0.2758617,2.3962512,2.349177,-0.16957302,0.27373722,0.10327462,-0.0011716236,0.30951387,0.27349246,-0.3491311,2.1200256,2.1544387,-0.4070263,0.15888368,-0.48308948,5.337995,3.2095904,2.9074912,2.9361165,2.6462364,2.4222274,2.6764805,2.5746446,1.8524567,2.7082105,3.1370811,0.27094328,0.23362072,0.21538375,2.5231395,2.492348,2.6393495,2.6843293,1.9407196,2.9162416,3.0139208,3.127963,2.6687794,2.575573,2.5431294,2.7554579,2.6147165,2.7350738,-6.932567,2.5799754,2.783065,2.749437,2.5559886,2.4989104,2.680494,2.6360943,2.6454334,2.808865,2.595116,2.6578798,2.4237108,3.5546322,3.8332868,4.1607575,4.069544,0.12999572,0.34956965,0.28671876,3.1198113,6.9004817,6.489663,2.5805316,3.0560813,-0.608651,-0.09758292,-0.107285924,0.14784081,0.72924364,0.46489397,3.6060617,2.67773,1.8265214,3.9587498,2.73357,3.230573,3.1183033,2.9134598,3.7971902,4.6951175,2.1537712,1.2609698,1.3834536,1.6486783,2.0684292,1.5760187,-0.25058755,0.22435449,0.27518684,0.49240884,0.27237654,0.2610426,-0.6789988,1.2318896,1.0520036,6.37264,-3.5739179,5.653382,5.9282513,5.905105,5.9247584,1.6596472,-0.77165985,-1.6879334,4.8933825,4.028729,2.1152172,-0.80386645,2.6331604],\"xaxis\":\"x\",\"y\":[-4.5190744,-4.490292,-4.4850216,-4.653523,-5.0573688,-4.7399015,-4.63748,-4.809005,-4.683777,-7.683604,-7.009775,-7.197205,-7.7283154,-7.6280684,-3.7963305,-3.918234,-4.1518626,-7.536126,-7.4911923,-7.7849793,-7.014143,-6.3030586,-6.114991,-7.7083774,-7.4946485,-7.3580017,-3.836335,-4.107851,-4.7323833,-4.3303757,-2.6393852,-7.241282,-7.729887,-7.387401,-7.4704156,-2.9741287,-0.05914257,-2.7507489,-2.046134,-3.963981,-2.762905,-3.4194255,-3.628326,-3.688487,-3.7740061,-7.652268,-7.04742,-6.8675036,-7.210084,-7.660071,-7.6336966,-4.4493036,-4.2781425,-4.458333,-7.7569814,-4.6344037,-3.3828003,-3.3738906,-7.572144,-7.7672644,-7.591031,-7.896831,1.0540586,-7.790991,-7.4578633,0.36234856,-3.5353365,-3.940342,-4.344493,-4.2977815,-7.757214,-6.995041,-6.9020214,-7.2953396,-7.696387,-7.591396,-6.9211135,-4.214819,-4.254338,-4.393659,-3.8243988,-4.266595,-4.306446,-4.2147455,-4.4017024,-4.142119,-3.5973535,-3.1731727,-2.02142,-1.5957884,-3.6280532,-3.3735793,-3.7168183,-2.7868433,-2.3801162,-4.5087266,-3.1474757,-3.5647058,-7.0851607,-7.5369587,-3.7327213,-4.291908,-3.938091,-2.967775,-7.8677483,-7.8321724,-6.18524,-8.024604,-7.9513054,-6.4796185,-7.266829,-4.210157,-7.2616343,-0.7695323,-7.392781,-7.670117,-1.1414465,-7.448392,-7.428995,-6.794734,-7.4807925,-7.640081,-7.8561583,-7.9557133,-8.023212,-8.011044,-8.031604,-7.5936494,-7.078795,-8.091876,-8.233588,-7.451601,-7.2398324,-7.272912,-7.146428,-7.5687423,-7.6929173,-7.2472563,-7.952019,-8.271604,-4.7942066,-1.7421483,-0.53244716,-0.6619276,-2.9014974,-3.1019082,-3.504872,-3.0939293,-7.509604,-2.8302982,-4.15487,-4.4773736,-4.524806,-0.0006613839,-2.4305627,-2.482373,-3.039219,-2.6599503,-2.7720413,0.28412437,-7.6393933,-7.838985,-7.767509,-8.174294,-8.273999,-7.5963626,-7.6843224,-2.8381298,-7.792998,-7.715905,-7.5793204,-5.079044,-7.7029967,-4.616539,-7.6239977,-4.599112,-7.597489,-1.3949012,-3.9158106,-3.940648,-3.5020583,-4.232599,-4.3319817,-4.149035,-3.5827894,-7.7051544,-7.7758555,-7.6059766,-7.5238657,-7.62929,-4.802188,-4.9383855,-5.409043,-7.163147,-6.993932,-7.30424,-3.2848613,-3.5610726,-7.637515,-7.679638,-5.2683263,-7.334723,-8.130301,-3.8382683,-1.1908998,-1.690309,-3.815159,-4.0623827,-4.250321,-4.0386524,-4.10712,-7.650194,-7.7373137,-7.398169,-7.7160378,-7.771785,-7.445255,-3.4634264,-3.6493158,-3.705845,-3.6341152,-2.5840878,-3.6961923,-3.7978024,-4.0736723,-3.9440088,-3.8566942,-4.0500293,-4.2829676,-4.116257,-3.8729568,-3.057978,-2.8480833,-4.0663314,-4.7325206,-4.785451,-4.6006,-4.5559435,-4.626315,-4.6443467,-4.374395,-4.391376,-4.365054,-4.551983,-4.496738,-3.8623822,-4.5310283,-4.2184052,-4.5532722,-4.62137,-4.273339,-4.6309085,-4.0724792,-3.6458452,-3.8495347,-7.6634846,-7.608337,-4.131135,-6.8647647,-7.6768584,-8.220657,-8.239448,-6.925374,-5.9516797,-7.8012137,-6.3320932,-3.2193973,-8.053594,-7.3531127,-7.731729,-4.4670835,-4.479154,-4.3850656,-4.458767,-3.4824212,-3.670763,-3.8090825,-4.021923,-3.3288374,-2.3440824,-0.08173916,-2.5185277,-3.9270527,-2.470838,-6.0570483,-4.3554068,-2.8149083,-6.716379,-1.0270433,-7.48354,-7.3537683,-3.1806223,-6.4973116,-2.805734,-3.0945227,-3.3740327,-1.0701388,-4.2530613,-4.6275506,-3.6057415,-2.1713917,-4.5470643,-4.277769,-3.814626,-1.7358235,-7.664095,-2.450732,-7.3870654,0.68967235,-0.32889938,-2.8071492,-2.656682,-2.6451535,-2.6208255,-2.743314,-2.6423738,-3.9601548,-3.8091335,-3.9217997,-4.1414533,-4.11001,-4.1391287,-4.3533545,-4.183483,-4.4962997,-4.303918,-4.4624805,-4.3488755,-4.622682,-4.3070955,-4.113669,-3.8619285,-3.7477975,-6.3917947,-5.2879133,-7.5462,-7.7271624,-7.74177,-7.9392014,-4.324168,-4.6007094,-4.598424,-4.7243805,-4.8835773,-4.7506137,-4.3402686,-4.5190396,-4.3636537,-4.3381944,-4.5754995,-4.390655,-4.3545103,1.1693395,0.11250829,-3.1600697,-2.5778193,-3.6241148,-2.6384237,-2.5930274,-2.6231716,-2.6083777,-2.8198583,-3.3147974,-3.652191,-3.2319288,-7.5501394,-2.1812353,-2.6562161,-1.6538938,-2.6105342,-2.897032,-7.6731825,-7.034499,-1.3188215,-4.097254,-4.0352497,-2.937647,-2.3011389,-2.975214,-3.9557981,-2.6609104,-2.8682523,-3.8041549,-6.5013223,-6.96473,-6.210717,-5.196966,-2.6508095,-1.652755,-6.7612696,-6.991751,-7.676707,2.3185034,-6.9418864,-8.138951,-0.39214712,-4.102285,-3.8696923,-4.1307044,-4.0407486,-4.0959783,-3.9075449,-3.9529657,-3.9301245,-3.9707155,-4.315052,-3.857283,-4.4760723,-4.364577,-7.853943,-7.798856,-8.088732,-7.701348,-7.745145,-8.266214,-7.5543118,-7.0788107,-7.6933365,-7.002296,-4.2980485,-7.0407467,-6.974328,-7.3135796,-7.3697906,-6.9872127,-7.068741,-7.4288664,-4.036268,-4.0648594,-3.8447313,-4.088655,-3.6675367,-7.6611853,-4.0016284,-3.882519,-3.8968992,-3.9566262,-3.9395297,-3.8051443,-3.5723186,-4.192142,-3.4531815,-7.5688987,-7.6928797,-6.621415,-8.118725,-8.296046,-8.318502,-7.8658133,-8.372165,-7.723361,-8.1886,-4.4134936,-8.201171,-8.209845,-8.368762,-2.7216983,-1.7473162,-2.928343,-3.6459272,-2.3698184,-3.2856858,-7.32176,-8.087488,-8.2330675,-8.381475,-4.0847716,-4.3284993,-7.4597154,-7.9008713,-7.588305,-7.5628753,-8.152162,-8.176094,-7.2788596,-4.3605204,-4.4316235,-7.806345,-7.685284,-8.18978,-4.30044,-3.5617845,-3.8982668,-3.8738508,-4.0381265,-3.9779847,-4.104169,-3.8009446,-4.3309584,-4.1557565,-3.6200066,-6.5010843,-7.0548563,-8.222526,-3.6453786,-4.0422564,-3.777317,-3.825304,-4.196842,-3.826229,-3.8439918,-3.622179,-4.420137,-4.4621005,-4.456798,-4.3580337,-4.467384,-4.1895685,-6.8451004,-4.0621524,-3.9967906,-4.2689342,-4.4223437,-4.282842,-4.348478,-4.4441743,-4.293554,-4.2146564,-4.387333,-4.389007,-4.166049,-3.0774715,-2.7609007,-1.9572484,-2.0971007,-7.833959,-7.4124694,-7.6279182,-3.3569868,-4.616768,-4.5240216,-4.235521,-3.9240706,-7.7769866,-7.9083867,-7.89856,-7.6979365,-7.5321894,-7.95007,-3.419613,-4.299172,-4.7336063,-3.9784648,-3.8761458,-3.9282496,-4.0409193,-4.619358,-4.078489,-4.200979,-4.524511,-4.735079,-4.3665667,-4.5022388,-4.470193,-4.0173473,-7.5357585,-7.634992,-6.253284,-6.3742585,-7.1485667,-6.7036037,-4.412255,-4.4176445,-4.531202,-0.37244785,-1.2289771,-0.09832666,-1.7286793,-2.18845,-1.3648499,-1.7008222,-3.5652463,-3.899089,-0.8308115,-3.4259486,0.5629345,-3.9230697,-4.2795033],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Q-Learning Recap [[q-learning-recap]]\\n\\n\\n*Q-Learning* **is the RL algorithm that** :\\n\\n- Trains a *Q-f...\"],[\"This is the Q-Learning pseudocode:\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-co...\"],[\"Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera.org\\u002flectu...\"],[\"### Q2: Which of the following statements are true, when talking about models with bias and\\u002for varia...\"],[\"### Q4: How would you describe, with your own words, the Actor-Critic Method (A2C)?\\n\\n\\u003cdetails\\u003e\\n\\u003csumm...\"],[\"\\u003c\\u002fdetails\\u003e\\n\\nCongrats on finishing this Quiz 🥳, if you missed some elements, take time to read the ch...\"],[\"Additional Readings\\n\\nThese are **optional readings** if you want to go deeper.\\n\\n\\n## Introduction to ...\"],[\"Hands-on\\n\\nNow that you learned the basics of multi-agents, you're ready to train your first agents i...\"],[\"More precisely, AI vs. AI is three tools:\\n\\n- A *matchmaking process* defining the matches (which mod...\"],[\"### Competition Rules\\n\\nThis first AI vs. AI competition **is an experiment**: the goal is to improve...\"],[\"```\\n\\nTo be able to train our agents correctly and push to the Hub, we need to install ML-Agents\\n\\n```...\"],[\"```\\n\\nFinally, you need to install git-lfs: https:\\u002f\\u002fgit-lfs.com\\u002f\\n\\nNow that it’s installed, we need to...\"],[\"The goal in this environment **is to get the ball into the opponent's goal while preventing the ball...\"],[\"But in our case we’re 2vs2, and each team has 2 agents. How then can we **train cooperative behavior...\"],[\"The solution then is to use Self-Play with an MA-POCA trainer (called poca). The poca trainer will h...\"],[\"```\\n\\nCompared to Pyramids or SnowballTarget, we have new hyperparameters with a self-play part. How ...\"],[\"```\\n\\nThe executable contains 8 copies of SoccerTwos.\\n\\n⚠️ It’s normal if you don’t see a big increase...\"],[\"```\\n\\nThen, we need to run `mlagents-push-to-hf`.\\n\\nAnd we define four parameters:\\n\\n1. `-run-id`: the ...\"],[\"```\\n\\nIf everything worked you should see this at the end of the process (but with a different url 😆)...\"],[\"2. That you have a `SoccerTwos.onnx` file\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-dee...\"],[\"Two types of value-based methods [[two-types-value-based-methods]]\\n\\nIn value-based methods, **we lea...\"],[\"And consequently, **we don't define by hand the behavior of our policy; it's the training that will ...\"],[\"In fact, most of the time, in value-based methods, you'll use **an Epsilon-Greedy Policy** that hand...\"],[\"The value of taking action \\\\\\\\(a\\\\\\\\) in state \\\\\\\\(s\\\\\\\\) under a policy \\\\\\\\(π\\\\\\\\) is:\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhu...\"],[\"The advantages and disadvantages of policy-gradient methods\\n\\nAt this point, you might ask, \\\"but Deep...\"],[\"Our vacuum cleaner can only perceive where the walls are.\\n\\nThe problem is that the **two red (colore...\"],[\"But what if we have an infinite possibility of actions?\\n\\nFor instance, with a self-driving car, at e...\"],[\"Glossary \\n\\nThis is a community-created glossary. Contributions are welcome!\\n\\n- **Deep Q-Learning:** ...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this unit! **That was the biggest one**, and there ...\"],[\"Introduction\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fres...\"],[\"Introduction to Q-Learning [[introduction-q-learning]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhu...\"],[\"Conclusion\\n\\nCongrats on finishing this unit! You’ve just trained your first ML-Agents and shared it ...\"],[\"Language models in RL\\n## LMs encode useful knowledge for agents\\n\\n**Language models** (LMs) can exhib...\"],[\"1) Sample inefficiency\\n\\n2) Unexpected behaviors from humans’ eyes\\n\\nAs a first attempt, the paper [“G...\"],[\"## Further reading\\n\\nFor more information we recommend you check out the following resources:\\n\\n- [Goo...\"],[\"The Deep Q-Network (DQN)  [[deep-q-network]]\\nThis is the architecture of our Deep Q-Learning network...\"],[\"**Why do we stack four frames together?**\\nWe stack frames together because it helps us **handle the ...\"],[\"So, we see that Deep Q-Learning uses a neural network to approximate, given a state, the different Q...\"],[\"The certification process\\n\\n\\nThe certification process is **completely free**:\\n\\n- To get a *certifica...\"],[\"Summary [[summary]]\\n\\nThat was a lot of information! Let's summarize:\\n\\n- Reinforcement Learning is a ...\"],[\"Glossary [[glossary]]\\n\\nThis is a community-created glossary. Contributions are welcomed!\\n\\n\\n### Strat...\"],[\"### Greedy strategy:\\n\\n- Involves always choosing the action that is expected to lead to the highest ...\"],[\"The Reinforcement Learning Framework [[the-reinforcement-learning-framework]]\\n\\n## The RL Process [[t...\"],[\"This RL loop outputs a sequence of **state, action, reward and next state.**\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhugg...\"],[\"- *State s*: is **a complete description of the state of the world** (there is no hidden information...\"],[\"## Action Space [[action-space]]\\n\\nThe Action space is the set of **all possible actions in an enviro...\"],[\"Taking this information into consideration is crucial because it will **have importance when choosin...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"Introduction [[introduction]]\\n\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course...\"],[\"From Q-Learning to Deep Q-Learning [[from-q-to-dqn]]\\n\\nWe learned that **Q-Learning is an algorithm w...\"],[\"* A single frame in Atari is composed of an image of 210x160 pixels. Given that the images are in co...\"],[\"Additional Readings [[additional-readings]]\\n\\nThese are **optional readings** if you want to go deepe...\"],[\"Hands-on\\n\\n\\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\nnotebooks={[\\n  {label: \\\"Goo...\"],[\"**To start the hands-on, click on Open In Colab button** 👇 :\\n\\n[![Open In Colab](https:\\u002f\\u002fcolab.resear...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"- `Hardware Accelerator \\u003e GPU`\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course...\"],[\"```\\n\\n```bash\\n# Go inside the repository and install the package\\ncd ml-agents\\npip install -e .\\u002fml-age...\"],[\"```\\n\\nDownload the file SnowballTarget.zip from https:\\u002f\\u002fdrive.google.com\\u002ffile\\u002fd\\u002f1YHHLjyj6gaZ3Gemx1hQg...\"],[\"```\\n\\nMake sure your file is accessible\\n\\n```bash\\nchmod -R 755 .\\u002ftraining-envs-executables\\u002flinux\\u002fSnowb...\"],[\"```\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain...\"],[\"\\u003e It will fail the first time if and when you use `--resume`. Try rerunning the block to bypass the ...\"],[\"```\\n\\n### Push the agent to the Hugging Face Hub\\n\\n- Now that we've trained our agent, we’re **ready t...\"],[\"```\\n\\nIf you don't want to use Google Colab or a Jupyter Notebook, you need to use this command inste...\"],[\"```\\n\\nIt's the link to your model. It contains a model card that explains how to use it, your Tensorb...\"],[\"Now let's try a more challenging environment called Pyramids.\\n\\n## Pyramids 🏆\\n\\n### Download and move ...\"],[\"```\\n\\nUnzip it\\n\\n```python\\n%%capture\\n!unzip -d .\\u002ftraining-envs-executables\\u002flinux\\u002f .\\u002ftraining-envs-exec...\"],[\"```\\n\\n###  Modify the PyramidsRND config file\\n  \\n- Contrary to the first environment, which was a cus...\"],[\"```\\n\\n### Push the agent to the Hugging Face Hub\\n\\n- Now that we trained our agent, we’re **ready to p...\"],[\"Decision Transformers\\n\\nThe Decision Transformer model was introduced by [\\\"Decision Transformer: Rein...\"],[\"Start the tutorial here 👉 https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002ftrain-decision-transformers\\n\\n## Further readin...\"],[\"Additional Readings [[additional-readings]]\\n\\n##  An introduction to multi-agents\\n\\n- [Multi-agent rei...\"],[\"Introducing Q-Learning [[q-learning]]\\n## What is Q-Learning? [[what-is-q-learning]]\\n\\nQ-Learning is a...\"],[\"Let's go through an example of a maze.\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-r...\"],[\"If we recap, *Q-Learning* **is the RL algorithm that:**\\n\\n- Trains a *Q-function* (an **action-value ...\"],[\"## The Q-Learning algorithm [[q-learning-algo]]\\n\\nThis is the Q-Learning pseudocode; let's study each...\"],[\"At the beginning of the training, **the probability of doing exploration will be huge since ɛ is ver...\"],[\"Therefore, our \\\\\\\\(Q(S_t, A_t)\\\\\\\\) **update formula goes like this:**\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.c...\"],[\"\\u003cfigure\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002f...\"],[\"Play with Huggy [[play]]\\n\\nNow that you've trained Huggy and pushed it to the Hub. **You will be able...\"],[\"Discord 101 [[discord-101]]\\n\\nHey there! My name is Huggy, the dog 🐕, and I'm looking forward to trai...\"],[\"They are in the reinforcement learning category. **Don't forget to sign up to these channels** by cl...\"],[\"Introduction [[introduction]]\\n\\nOne of the most critical tasks in Deep Reinforcement Learning is to *...\"],[\"Designing Multi-Agents systems\\n\\nFor this section, you're going to watch this excellent introduction ...\"],[\"## Centralized approach\\n\\n\\u003cfigure\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-cour...\"],[\"Additional Readings [[additional-readings]]\\n\\nThese are **optional readings** if you want to go deepe...\"],[\"Hands-on\\n\\n\\n      \\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\n      notebooks={[\\n ...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cvideo\\n        alt=\\\"LunarLander\\\"\\n        sty...\"],[\"In this notebook, you'll learn to **code your PPO agent from scratch with PyTorch using CleanRL impl...\"],[\"## Set the GPU 💪\\n\\n- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runt...\"],[\"```\\n\\n```python\\n# Virtual display\\nfrom pyvirtualdisplay import Display\\n\\nvirtual_display = Display(vis...\"],[\"```\\n\\n- Add new argument in `parse_args()` function to define the repo-id where we want to push the m...\"],[\"```\\n\\n- Next, we add the methods needed to push the model to the Hub\\n\\n- These methods will:\\n  - `_eva...\"],[\"# Step 2: Save the model\\n        torch.save(model.state_dict(), tmpdirname \\u002f \\\"model.pt\\\")\\n\\n        # ...\"],[\"msg.info(f\\\"Your model is pushed to the Hub. You can view your model here: {repo_url}\\\")\\n    return re...\"],[\"return mean_reward, std_reward\\n\\n\\ndef record_video(env, policy, out_directory, fps=30):\\n    images = ...\"],[\"This is a trained model of a PPO agent playing {env_id}.\\n\\n  # Hyperparameters\\n  \\\"\\\"\\\"\\n    return model...\"],[\"with readme_path.open(\\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n        f.write(readme)\\n\\n    # Save our metrics t...\"],[\"```\\n\\n- Finally, we call this function at the end of the PPO training\\n\\n```python\\n# Create the evaluat...\"],[\"```\\n\\n- Here's what the final ppo.py file looks like:\\n\\n```python\\n# docs and experiment results can be...\"],[\"from wasabi import Printer\\n\\nmsg = Printer()\\n\\n\\ndef parse_args():\\n    # fmt: off\\n    parser = argparse...\"],[\"# Algorithm specific arguments\\n    parser.add_argument(\\\"--env-id\\\", type=str, default=\\\"CartPole-v1\\\",\\n...\"],[\"help=\\\"the lambda for the general advantage estimation\\\")\\n    parser.add_argument(\\\"--num-minibatches\\\",...\"],[\"# Adding HuggingFace argument\\n    parser.add_argument(\\\"--repo-id\\\", type=str, default=\\\"ThomasSimonini...\"],[\"with tempfile.TemporaryDirectory() as tmpdirname:\\n        tmpdirname = Path(tmpdirname)\\n\\n        # S...\"],[\"msg.info(f\\\"Pushing repo {repo_id} to the Hugging Face Hub\\\")\\n\\n        repo_url = upload_folder(\\n     ...\"],[\"with readme_path.open(\\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n        f.write(readme)\\n\\n    # Save our metrics t...\"],[\"class Agent(nn.Module):\\n    def __init__(self, envs):\\n        super().__init__()\\n        self.critic...\"],[\"wandb.init(\\n            project=args.wandb_project_name,\\n            entity=args.wandb_entity,\\n     ...\"],[\"agent = Agent(envs).to(device)\\n    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate,...\"],[\"# ALGO LOGIC: action logic\\n            with torch.no_grad():\\n                action, logprob, _, val...\"],[\"# bootstrap value if not done\\n        with torch.no_grad():\\n            next_value = agent.get_value...\"],[\"# Optimizing the policy and value network\\n        b_inds = np.arange(args.batch_size)\\n        clipfr...\"],[\"# Value loss\\n                newvalue = newvalue.view(-1)\\n                if args.clip_vloss:\\n      ...\"],[\"# TRY NOT TO MODIFY: record rewards for plotting purposes\\n        writer.add_scalar(\\\"charts\\u002flearning...\"],[\"```\\n\\nTo be able to share your model with the community there are three more steps to follow:\\n\\n1️⃣ (I...\"],[\"```\\n\\nIf you don't want to use Google Colab or a Jupyter Notebook, you need to use this command inste...\"],[\"The SnowballTarget Environment\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"In addition to raycasts, the agent gets a \\\"can I shoot\\\" bool as observation.\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhugg...\"],[\"What are the policy-based methods?\\n\\nThe main goal of Reinforcement learning is to **find the optimal...\"],[\"- On the other hand, in *policy-based methods*, we directly learn to approximate \\\\\\\\(\\\\pi^{*}\\\\\\\\) witho...\"],[\"## The difference between policy-based and policy-gradient methods\\n\\nPolicy-gradient methods, what we...\"],[\"Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym 🤖 [[hands-on]]\\n\\n\\n      \\u003cCours...\"],[\"# Unit 6: Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym 🤖\\n\\n### 🎮 Environmen...\"],[\"- `Hardware Accelerator \\u003e GPU`\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course...\"],[\"```\\n\\n```python\\n# Virtual display\\nfrom pyvirtualdisplay import Display\\n\\nvirtual_display = Display(vis...\"],[\"```\\n\\n## PandaReachDense-v3 🦾\\n\\nThe agent we're going to train is a robotic arm that needs to do contr...\"],[\"```\\n\\nThe observation space **is a dictionary with 3 different elements**:\\n\\n- `achieved_goal`: (x,y,z...\"],[\"```\\n\\n### Create the A2C Model 🤖\\n\\nFor more information about A2C implementation with StableBaselines3...\"],[\"```\\n\\n### Evaluate the agent 📈\\n\\n- Now that's our  agent is trained, we need to **check its performanc...\"],[\"```\\n### Publish your trained model on the Hub 🔥\\n\\nNow that we saw we got good results after the train...\"],[\"```\\nIf you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command inst...\"],[\"```\\n\\n## Some additional challenges 🏆\\n\\nThe best way to learn **is to try things by your own**! Why no...\"],[\"```\\n\\n```python\\n# 6\\nmodel_name = \\\"a2c-PandaPickAndPlace-v3\\\";\\nmodel.save(model_name)\\nenv.save(\\\"vec_nor...\"],[\"(Optional) What is Curiosity in Deep Reinforcement Learning?\\n\\nThis is an (optional) introduction to ...\"],[\"For instance, in [Vizdoom](https:\\u002f\\u002fvizdoom.cs.put.edu.pl\\u002f), a set of environments based on the game ...\"],[\"Because the idea of Curiosity is to **encourage our agent to perform actions that reduce the uncerta...\"],[\"The “Deep” in Reinforcement Learning [[deep-rl]]\\n\\n\\u003cTip\\u003e\\nWhat we've talked about so far is Reinforcem...\"],[\"What is RL? A short recap [[what-is-rl]]\\n\\nIn RL, we build an agent that can **make smart decisions**...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"Conclusion [[Conclusion]]\\n\\nThat’s all for today. Congrats on finishing this unit and the tutorial!\\n\\n...\"],[\"Diving deeper into policy-gradient methods\\n\\n## Getting the big picture\\n\\nWe just learned that policy-...\"],[\"Now that we got the big picture, let's dive deeper into policy-gradient methods.\\n\\n## Diving deeper i...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"(If you need a refresher on the difference between gradient descent and gradient ascent [check this]...\"],[\"Fortunately we're going to use a solution called the Policy Gradient Theorem that will help us to re...\"],[\"- Update the weights of the policy: \\\\\\\\(\\\\theta \\\\leftarrow \\\\theta + \\\\alpha \\\\hat{g}\\\\\\\\)\\n\\nWe can interpre...\"],[\"Additional Readings [[additional-readings]]\\n\\n## Bias-variance tradeoff in Reinforcement Learning\\n\\nIf...\"],[\"Let's train and play with Huggy 🐶 [[train]]\\n\\n\\n\\n\\n          \\u003cCourseFloatingBanner classNames=\\\"absolute...\"],[\"### The environment 🎮\\n\\n- Huggy the Dog, an environment created by [Thomas Simonini](https:\\u002f\\u002ftwitter....\"],[\"- `Hardware Accelerator \\u003e GPU`\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course...\"],[\"```\\n\\n```bash\\n# Go inside the repository and install the package (can take 3min)\\n%cd ml-agents\\npip3 i...\"],[\"```\\n\\nDownload the file Huggy.zip from https:\\u002f\\u002fdrive.google.com\\u002fuc?export=download&id=1zv3M95ZJTWHUVO...\"],[\"```\\n\\n## Let's recap how this environment works\\n\\n### The State Space: what Huggy perceives.\\n\\nHuggy do...\"],[\"Here, our goal is that Huggy **goes towards the stick but without spinning too much**. Hence, our re...\"],[\"```\\nbehaviors:\\n  Huggy:\\n    trainer_type: ppo\\n    hyperparameters:\\n      batch_size: 2048\\n      buff...\"],[\"```\\n\\n- Don't forget to save the file!\\n\\n- **In the case you want to modify the hyperparameters**, in ...\"],[\"```\\n\\n## Push the agent to the 🤗 Hub\\n\\n- Now that we trained our agent, we’re **ready to push it to th...\"],[\"```\\n\\nIf you don't want to use Google Colab or a Jupyter Notebook, you need to use this command inste...\"],[\"```\\n\\nIt’s the link to your model repository. The repository contains a model card that explains how ...\"],[\"👉 It's good **to try with different models steps to see the improvement of the agent.**\\n\\nCongrats on...\"],[\"Mid-way Quiz [[mid-way-quiz]]\\n\\nThe best way to learn and [to avoid the illusion of competence](https...\"],[\"\\u003cdetails\\u003e\\n\\u003csummary\\u003eSolution\\u003c\\u002fsummary\\u003e\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl...\"],[\"\\u003cdetails\\u003e\\n\\u003csummary\\u003eSolution\\u003c\\u002fsummary\\u003e\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl...\"],[\"Bonus: Learn to create your own environments with Unity and MLAgents\\n\\n**You can create your own rein...\"],[\"Train your first Deep Reinforcement Learning Agent 🤖 [[hands-on]]\\n\\n\\n\\n\\n      \\u003cCourseFloatingBanner cl...\"],[\"**If you don't find your model, go to the bottom of the page and click on the refresh button.**\\n\\nFor...\"],[\"### The environment 🎮\\n\\n- [LunarLander-v2](https:\\u002f\\u002fgymnasium.farama.org\\u002fenvironments\\u002fbox2d\\u002flunar_land...\"],[\"And more! \\n\\nCheck 📚 the syllabus 👉 https:\\u002f\\u002fsimoninithomas.github.io\\u002fdeep-rl-course\\n\\nDon’t forget to ...\"],[\"- Reinforcement Learning is a **computational approach to learning from actions**. We build an agent...\"],[\"To find your result, go to the [leaderboard](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fhuggingface-projects\\u002fDeep...\"],[\"```\\n\\n```bash\\npip install -r https:\\u002f\\u002fraw.githubusercontent.com\\u002fhuggingface\\u002fdeep-rl-class\\u002fmain\\u002fnoteboo...\"],[\"```\\n\\n```python\\n# Virtual display\\nfrom pyvirtualdisplay import Display\\n\\nvirtual_display = Display(vis...\"],[\"```\\n\\n## Understand Gymnasium and how it works 🤖\\n\\n🏋 The library containing our environment is called ...\"],[\"2️⃣ We reset the environment to its initial state with `observation = env.reset()`\\n\\nAt each step:\\n\\n3...\"],[\"```\\n\\n## Create the LunarLander environment 🌛 and understand how it works\\n\\n### The environment 🎮\\n\\nIn ...\"],[\"```\\n\\nThe action space (the set of possible actions the agent can take) is discrete with 4 actions av...\"],[\"```\\n\\n## Create the Model 🤖\\n\\n- We have studied our environment and we understood the problem: **being...\"],[\"```\\n# Create environment\\nenv = gym.make('LunarLander-v2')\\n\\n# Instantiate the agent\\nmodel = PPO('MlpP...\"],[\"```\\n\\n## Evaluate the agent 📈\\n\\n- Remember to wrap the environment in a [Monitor](https:\\u002f\\u002fstable-basel...\"],[\"```\\n\\n- In my case, I got a mean reward is `200.20 +\\u002f- 20.80` after training for 1 million steps, whi...\"],[\"```\\n\\nIf you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command ins...\"],[\"# TODO: Define the model architecture we used\\nmodel_architecture = \\\"\\\"\\n\\n## TODO: Define the commit me...\"],[\"```\\n\\n#### Solution\\n\\n\\n```python\\nimport gymnasium as gym\\n\\nfrom stable_baselines3 import PPO\\nfrom stabl...\"],[\"```\\n\\nCongrats 🥳 you've just trained and uploaded your first Deep Reinforcement Learning agent. The s...\"],[\"```\\n\\n```python\\nfrom huggingface_sb3 import load_from_hub\\n\\nrepo_id = \\\"Classroom-workshop\\u002fassignment2-...\"],[\"```\\n\\n## Some additional challenges 🏆\\nThe best way to learn **is to try things by your own**! As you ...\"],[\"If you’re still feel confused with all these elements...it's totally normal! **This was the same for...\"],[\"Glossary \\n\\nThis is a community-created glossary. Contributions are welcomed!\\n\\n- **Tabular Method:** ...\"],[\"- **Fixed Q-Target:** In order to calculate the **Q-Target** we need to estimate the discounted opti...\"],[\"Live 1: How the course work, Q&A, and playing with Huggy\\n\\nIn this first live stream, we explained ho...\"],[\"Quiz [[quiz]]\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera....\"],[\"\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"an action a0, action a0, state s0, state s1, reward r1\\\",\\n\\t\\t\\texpl...\"],[\"### Q3: What's the difference between a state and an observation?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext...\"],[\"### Q4: A task is an instance of a Reinforcement Learning problem. What are the two types of tasks?\\n...\"],[\"- The Policy π **is the brain of our Agent**. It’s the function that tells us what action to take gi...\"],[\"Additional Readings [[additional-readings]]\\n\\nThese are **optional readings** if you want to go deepe...\"],[\"(Optional) the Policy Gradient Theorem\\n\\nIn this optional section where we're **going to study how we...\"],[\"We can simplify further this since \\\\\\\\( \\\\frac{P(\\\\tau;\\\\theta)}{P(\\\\tau;\\\\theta)}\\\\nabla_\\\\theta P(\\\\tau;\\\\th...\"],[\"But we still have some mathematics work to do there: we need to simplify \\\\\\\\(  \\\\nabla_\\\\theta log P(\\\\t...\"],[\"We also know that the gradient of the sum is equal to the sum of gradient:\\n\\n\\\\\\\\( \\\\nabla_\\\\theta log P(...\"],[\"So, the final formula for estimating the policy gradient is:\\n\\n\\\\\\\\( \\\\nabla_{\\\\theta} J(\\\\theta) = \\\\hat{g...\"],[\"Introduction to Deep Reinforcement Learning [[introduction-to-deep-reinforcement-learning]]\\n\\n\\u003cimg sr...\"],[\"So let's get started! 🚀...\"],[\"Additional Readings [[additional-readings]]\\n\\nThese are **optional readings** if you want to go deepe...\"],[\"Advantage Actor-Critic (A2C) [[advantage-actor-critic]]\\n\\n## Reducing variance with Actor-Critic meth...\"],[\"## The Actor-Critic Process\\nNow that we have seen the Actor Critic's big picture, let's dive deeper ...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"In other words, this function calculates **the extra reward we get if we take this action at that st...\"],[\"(Automatic) Curriculum Learning for RL\\n\\nWhile most of the RL methods seen in this course work well i...\"],[\"\\u003e … a family of mechanisms that automatically adapt the distribution of training data by learning to...\"],[\"### Recent methods\\n\\n- [Evolving Curricula with Regret-Based Environment Design](https:\\u002f\\u002farxiv.org\\u002fab...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this chapter! There was a lot of information. And c...\"],[\"Hands-on [[hands-on]]\\n\\n      \\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\n      no...\"],[\"To find your result, go to the [leaderboard](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fhuggingface-projects\\u002fDeep...\"],[\"⬇️ Here is an example of what **you will achieve in just a couple of minutes.** ⬇️\\n\\n\\n\\u003cimg src=\\\"https...\"],[\"In this free course, you will:\\n\\n- 📖 Study Deep Reinforcement Learning in **theory and practice**.\\n- ...\"],[\"- When the training is done, **we have an optimal Q-Function, so an optimal Q-Table.**\\n\\n- And if we ...\"],[\"To find your result, go to the [leaderboard](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fhuggingface-projects\\u002fDeep...\"],[\"```\\n\\n```bash\\nsudo apt-get update\\nsudo apt-get install -y python3-opengl\\napt install ffmpeg xvfb\\npip3...\"],[\"```\\n\\nWe're now ready to code our Q-Learning algorithm 🔥\\n\\n# Part 1: Frozen Lake ⛄ (non slippery versi...\"],[\"```\\n\\n### Solution\\n\\n```python\\nenv = gym.make(\\\"FrozenLake-v1\\\", map_name=\\\"4x4\\\", is_slippery=False, rend...\"],[\"```\\n\\nWe see with `Observation Space Shape Discrete(16)` that the observation is an integer represent...\"],[\"```\\n\\nThe action space (the set of possible actions the agent can take) is discrete with 4 actions av...\"],[\"```\\n\\n```python\\n# Let's create our Qtable of size (state_space, action_space) and initialized each va...\"],[\"```\\n\\n## Define the epsilon-greedy policy 🤖\\n\\nEpsilon-greedy is the training policy that handles the e...\"],[\"```\\n\\n## Define the hyperparameters ⚙️\\n\\nThe exploration related hyperparamters are some of the most i...\"],[\"```\\n\\n```python\\ndef train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, ...\"],[\"```\\n\\n#### Solution\\n\\n```python\\ndef train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, e...\"],[\"```\\n\\n## Let's see what our Q-Learning table looks like now 👀\\n\\n```python\\nQtable_frozenlake\\n```\\n\\n## Th...\"],[\"```\\n\\n## Evaluate our Q-Learning agent 📈\\n\\n- Usually, you should have a mean reward of 1.0\\n- The **env...\"],[\"```\\n\\n```python\\ndef record_video(env, Qtable, out_directory, fps=1):\\n    \\\"\\\"\\\"\\n    Generate a replay vi...\"],[\"```\\n\\n```python\\ndef push_to_hub(repo_id, model, env, video_fps=1, local_repo_path=\\\"hub\\\"):\\n    \\\"\\\"\\\"\\n   ...\"],[\"# Pickle the model\\n    with open((repo_local_path) \\u002f \\\"q-learning.pkl\\\", \\\"wb\\\") as f:\\n        pickle.du...\"],[\"metadata = {}\\n    metadata[\\\"tags\\\"] = [env_name, \\\"q-learning\\\", \\\"reinforcement-learning\\\", \\\"custom-impl...\"],[\"with readme_path.open(\\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n        f.write(readme)\\n\\n    # Save our metrics t...\"],[\"```\\n\\n### .\\n\\nBy using `push_to_hub` **you evaluate, record a replay, generate a model card of your ag...\"],[\"```\\n\\nIf you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command ins...\"],[\"```\\n\\nCongrats 🥳 you've just implemented from scratch, trained, and uploaded your first Reinforcement...\"],[\"```\\n\\n```python\\naction_space = env.action_space.n\\nprint(\\\"There are \\\", action_space, \\\" possible action...\"],[\"```\\n\\n## Define the hyperparameters ⚙️\\n\\n⚠ DO NOT MODIFY EVAL_SEED: the eval_seed array **allows us to...\"],[\"```\\n\\n## Create a model dictionary 💾 and publish our trained model to the Hub 🔥\\n\\n- We create a model ...\"],[\"```\\n\\nNow that it's on the Hub, you can compare the results of your Taxi-v3 with your classmates usin...\"],[\"```\\n\\n### .\\n\\n```python\\nmodel = load_from_hub(repo_id=\\\"ThomasSimonini\\u002fq-Taxi-v3\\\", filename=\\\"q-learning...\"],[\"```\\n\\n## Some additional challenges 🏆\\n\\nThe best way to learn **is to try things on your own**! As you...\"],[\"Doom is a large environment with a huge state space (millions of different states). Creating and upd...\"],[\"An Introduction to Unreal Learning Agents\\n\\n[Learning Agents](https:\\u002f\\u002fdev.epicgames.com\\u002fcommunity\\u002flea...\"],[\"Armed with the basics, **you're now prepared to play with Learning Agents**:\\n\\n3. Get the Big Picture...\"],[\"Conclusion\\n\\nThat’s all for today. Congrats on finishing this unit and the tutorial!\\n\\nThe best way to...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this bonus unit!\\n\\nYou can now sit and enjoy playing...\"],[\"Type of tasks [[tasks]]\\n\\nA task is an **instance** of a Reinforcement Learning problem. We can have ...\"],[\"The intuition behind PPO [[the-intuition-behind-ppo]]\\n\\n\\nThe idea with Proximal Policy Optimization (...\"],[\"The Bellman Equation: simplify our value estimation [[bellman-equation]]\\n\\nThe Bellman equation **sim...\"],[\"Then, to calculate the \\\\\\\\(V(S_{t+1})\\\\\\\\), we need to calculate the return starting at that state \\\\\\\\(S...\"],[\"If we go back to our example, we can say that the value of State 1 is equal to the expected cumulati...\"],[\"Before going to the next section, think about the role of gamma in the Bellman equation. What happen...\"],[\"Conclusion\\n\\nThat's all for today. Congrats on finishing this Unit and the tutorial! ⭐️\\n\\nNow that you...\"],[\"Brief introduction to RL documentation\\n\\nIn this advanced topic, we address the question: **how shoul...\"],[\"Building on the documentation frameworks for [model cards](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1810.03993) and [da...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this unit and the tutorial. You've just trained you...\"],[\"Monte Carlo vs Temporal Difference Learning [[mc-vs-td]]\\n\\nThe last thing we need to discuss before d...\"],[\"- We always start the episode **at the same starting point.**\\n- **The agent takes actions using the ...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"\\\\\\\\(V(S_0) = 0 + 0.1 * [3 – 0]\\\\\\\\)\\n\\n\\\\\\\\(V(S_0) = 0.3\\\\\\\\)\\n\\n\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fh...\"],[\"This method is called TD(0) or **one-step TD (update the value function after any individual step).*...\"],[\"New \\\\\\\\(V(S_0) = 0 + 0.1 * [1 + 1 * 0–0]\\\\\\\\)\\n\\nNew \\\\\\\\(V(S_0) = 0.1\\\\\\\\)\\n\\nSo we just updated our value fun...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this chapter! There was a lot of information. And c...\"],[\"Glossary [[glossary]]\\n\\nThis is a community-created glossary. Contributions are welcomed!\\n\\n### Agent\\n...\"],[\"- **Episodic**: Has a starting point and an ending point.\\n- **Continuous**: Has a starting point but...\"],[\"Offline vs. Online Reinforcement Learning\\n\\nDeep Reinforcement Learning (RL) is a framework **to buil...\"],[\"- On the other hand, in *offline reinforcement learning*, the agent only **uses data collected from ...\"],[\"Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera.org\\u002flectu...\"],[\"\\u003c\\u002fdetails\\u003e\\n\\n\\n### Q3: What's the difference between policy-based methods and policy-gradient methods?...\"],[\"Hands-on: advanced Deep Reinforcement Learning. Using Sample Factory to play Doom from pixels\\n\\n\\u003cCour...\"],[\"*   [Sample Factory](https:\\u002f\\u002fwww.samplefactory.dev\\u002f) is an advanced RL framework and **only function...\"],[\"```\\n\\nTo validate this hands-on for the [certification process](https:\\u002f\\u002fhuggingface.co\\u002fdeep-rl-course...\"],[\"Sample Factory is thoroughly **tested, used by many researchers and practitioners**, and is actively...\"],[\"- Highly optimized algorithm [architecture](https:\\u002f\\u002fwww.samplefactory.dev\\u002f06-architecture\\u002foverview\\u002f)...\"],[\"- [HuggingFace 🤗 integration](https:\\u002f\\u002fwww.samplefactory.dev\\u002f10-huggingface\\u002fhuggingface\\u002f) (upload tra...\"],[\"All of the above policies are available on the 🤗 hub. Search for the tag [sample-factory](https:\\u002f\\u002fhu...\"],[\"## ViZDoom\\n\\n[ViZDoom](https:\\u002f\\u002fvizdoom.cs.put.edu.pl\\u002f) is an **open-source python interface for the D...\"],[\"```python\\n# Install ViZDoom deps from\\n# https:\\u002f\\u002fgithub.com\\u002fmwydmuch\\u002fViZDoom\\u002fblob\\u002fmaster\\u002fdoc\\u002fBuilding...\"],[\"```\\n\\n## Then we can install Sample Factory and ViZDoom\\n\\n- This can take 7min\\n\\n```bash\\npip install sa...\"],[\"```\\n\\n## Setting up the Doom Environment in sample-factory\\n\\n```python\\nimport functools\\n\\nfrom sample_f...\"],[\"def register_vizdoom_components():\\n    register_vizdoom_envs()\\n    register_vizdoom_models()\\n\\n\\n# par...\"],[\"```\\n\\nNow that the setup if complete, we can train the agent. We have chosen here to learn a ViZDoom ...\"],[\"```python\\n## Start the training, this should take around 15 minutes\\nregister_vizdoom_components()\\n\\n#...\"],[\"```\\n\\n## Let's take a look at the performance of the trained policy and output a video of the agent.\\n...\"],[\"```\\n\\nThe agent has learned something, but its performance could be better. We would clearly need to ...\"],[\"```\\n\\n## Let's load another model\\n\\n\\n\\n\\nThis agent's performance was good, but we can do better! Let's ...\"],[\"```\\n\\n## Some additional challenges 🏆: Doom Deathmatch\\n\\nTraining an agent to play a Doom deathmatch *...\"],[\"```\\n\\n\\nYou **can try to train your agent in this environment** using the code above, but not on colab...\"],[\"Introducing the Clipped Surrogate Objective Function\\n## Recap: The Policy Objective Function\\n\\nLet’s ...\"],[\"It’s the probability of taking action \\\\\\\\( a_t \\\\\\\\) at state \\\\\\\\( s_t \\\\\\\\) in the current policy, divide...\"],[\"However, without a constraint, if the action taken is much more probable in our current policy than ...\"],[\"Then, we take the minimum of the clipped and non-clipped objective, **so the final objective is a lo...\"],[\"Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera.org\\u002flectu...\"],[\"### Q4: Explain in your own words what is the `Self-Play` approach\\n\\n\\u003cdetails\\u003e\\n\\u003csummary\\u003eSolution\\u003c\\u002fsum...\"],[\"### Q6: What are the main motivations to use a ELO rating Score?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n   \\t\\t {\\n\\t\\t\\tt...\"],[\"The Problem of Variance in Reinforce [[the-problem-of-variance-in-reinforce]]\\n\\nIn Reinforce, we want...\"],[\"The solution is to mitigate the variance by **using a large number of trajectories, hoping that the ...\"],[\"An introduction to Multi-Agents Reinforcement Learning (MARL)\\n\\n## From single agent to multiple agen...\"],[\"Or a road with **several autonomous vehicles**.\\n\\n\\u003cfigure\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002f...\"],[\"- *Mixed of both adversarial and cooperative*: like in our SoccerTwos environment, two agents are pa...\"],[\"Student Works\\n\\nSince the launch of the Deep Reinforcement Learning Course, **many students have crea...\"],[\"In this project, Eric Dong recreates Bill Seiler's 1985 version of Space War in Pygame and uses rein...\"],[\"Introduction [[introduction]]\\n\\nIn this bonus unit, we'll reinforce what we learned in the first unit...\"],[\"Conclusion\\n\\n\\n**Congrats on finishing this unit**! There was a lot of information.\\nAnd congrats on fi...\"],[\"Visualize the Clipped Surrogate Objective Function\\n\\nDon't worry. **It's normal if this seems complex...\"],[\"Since the ratio is between intervals, **we can decrease the probability that our policy takes that a...\"],[\"## Case 5 and 6: the ratio is above the range\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  ...\"],[\"**You might wonder why, when the minimum is the clipped ratio, the gradient is 0.** When the ratio i...\"],[\"Welcome to the 🤗 Deep Reinforcement Learning Course [[introduction]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface....\"],[\"Let’s get started!\\n\\n## What to expect? [[expect]]\\n\\nIn this course, you will:\\n\\n- 📖 Study Deep Reinfor...\"],[\"And more!\\n\\nAt the end of this course, **you’ll get a solid foundation from the basics to the SOTA (s...\"],[\"## What's the syllabus? [[syllabus]]\\n\\nThis is the course's syllabus:\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface....\"],[\"## The Certification Process [[certification-process]]\\n\\nThe certification process is **completely fr...\"],[\"## What tools do I need? [[tools]]\\n\\nYou need only 3 things:\\n\\n- *A computer* with an internet connect...\"],[\"About the team:\\n\\n- \\u003ca href=\\\"https:\\u002f\\u002ftwitter.com\\u002fosanseviero\\\"\\u003eOmar Sanseviero\\u003c\\u002fa\\u003e is a Machine Learni...\"],[\"## I still have questions [[questions]]\\n\\nPlease ask your question in our \\u003ca href=\\\"https:\\u002f\\u002fdiscord.gg...\"],[\"Hands on\\n\\n\\n\\n      \\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\n      notebooks={[\\n...\"],[\"For more information about the certification process, check this section 👉 https:\\u002f\\u002fhuggingface.co\\u002fde...\"],[\"To test its robustness, we're going to train it in 2 different simple environments:\\n- Cartpole-v1\\n- ...\"],[\"# Let's code Reinforce algorithm from scratch 🔥\\n\\n## Some advice 💡\\n\\nIt's better to run this colab in ...\"],[\"```\\n\\n```python\\n# Virtual display\\nfrom pyvirtualdisplay import Display\\n\\nvirtual_display = Display(vis...\"],[\"```\\n\\n## Import the packages 📦\\n\\nIn addition to importing the installed libraries, we also import:\\n\\n- ...\"],[\"```\\n\\n```python\\nprint(device)\\n```\\n\\nWe're now ready to implement our Reinforce algorithm 🔥\\n\\n# First ag...\"],[\"```\\n\\n```python\\nprint(\\\"_____OBSERVATION SPACE_____ \\\\n\\\")\\nprint(\\\"The State Space is: \\\", s_size)\\nprint(\\\"...\"],[\"```\\n\\n## Let's build the Reinforce Architecture\\n\\nThis implementation is based on three implementation...\"],[\"```\\n\\n### Solution\\n\\n```python\\nclass Policy(nn.Module):\\n    def __init__(self, s_size, a_size, h_size)...\"],[\"```\\n\\n- Here we see that the error says `ValueError: The value argument to log_prob must be a Tensor`...\"],[\"```\\n\\nBy using CartPole, it was easier to debug since **we know that the bug comes from our integrati...\"],[\"The second question you may ask is **why do we minimize the loss**? Didn't we talk about Gradient As...\"],[\"# Line 6 of pseudocode: calculate the return\\n        returns = deque(maxlen=max_t)\\n        n_steps =...\"],[\"## Hence, the queue \\\"returns\\\" will hold the returns in chronological order, from t=0 to t=n_steps\\n  ...\"],[\"```\\n\\n#### Solution\\n\\n```python\\ndef reinforce(policy, optimizer, n_training_episodes, max_t, gamma, pr...\"],[\"# This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)...\"],[\"# Line 8: PyTorch prefers gradient descent\\n        optimizer.zero_grad()\\n        policy_loss.backwar...\"],[\"```\\n\\n##  Train it\\n- We're now ready to train our agent.\\n- But first, we define a variable containing...\"],[\"```\\n\\n## Define evaluation method 📝\\n- Here we define the evaluation method that we're going to use to...\"],[\"```\\n\\n```python\\ndef record_video(env, policy, out_directory, fps=30):\\n    \\\"\\\"\\\"\\n    Generate a replay v...\"],[\"```\\n\\n```python\\ndef push_to_hub(repo_id,\\n                model,\\n                hyperparameters,\\n    ...\"],[\"evaluate_data = {\\n          \\\"env_id\\\": hyperparameters[\\\"env_id\\\"],\\n          \\\"mean_reward\\\": mean_rewar...\"],[\"readme_path = local_directory \\u002f \\\"README.md\\\"\\n    readme = \\\"\\\"\\n    if readme_path.exists():\\n        wit...\"],[\"```\\n\\nBy using `push_to_hub`, **you evaluate, record a replay, generate a model card of your agent, a...\"],[\"```\\n\\nNow that we tested the robustness of our implementation, let's try a more complex environment: ...\"],[\"```\\n\\nThe observation space (7) 👀:\\n- player y position\\n- player velocity\\n- player distance to floor\\n-...\"],[\"```\\n\\n#### Solution\\n\\n```python\\nclass Policy(nn.Module):\\n    def __init__(self, s_size, a_size, h_size...\"],[\"```\\n\\n###  Train it\\n- We're now ready to train our agent 🔥.\\n\\n```python\\n# Create policy and place it t...\"],[\"```\\n\\n## Some additional challenges 🏆\\n\\nThe best way to learn **is to try things on your own**! As you...\"],[\"Sound fun? See you next time!\\n\\nFinally, we would love **to hear what you think of the course and how...\"],[\"Setup [[setup]]\\n\\nAfter all this information, it's time to get started. We're going to do two things:...\"],[\"Second Quiz [[quiz2]]\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.c...\"],[\"\\u003cdetails\\u003e\\n\\u003csummary\\u003eSolution\\u003c\\u002fsummary\\u003e\\n\\nBecause if we have an optimal Q-function, we have an optimal ...\"],[\"\\u003c\\u002fdetails\\u003e\\n\\n\\n\\n### Q6: What's the difference between on-policy and off-policy\\n\\n\\u003cdetails\\u003e\\n\\u003csummary\\u003eSol...\"],[\"Two main approaches for solving RL problems [[two-methods]]\\n\\n\\u003cTip\\u003e\\nNow that we learned the RL framew...\"],[\"This function will define a mapping from each state to the best corresponding action. Alternatively,...\"],[\"\\u003cfigure\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002f...\"],[\"Here we see that our value function **defined values for each possible state.**\\n\\n\\u003cfigure\\u003e\\n\\u003cimg src=\\\"...\"],[\"The Exploration\\u002fExploitation trade-off [[exp-exp-tradeoff]]\\n\\nFinally, before looking at the differen...\"],[\"If it’s still confusing, **think of a real problem: the choice of picking a restaurant:**\\n\\n\\n\\u003cfigure\\u003e...\"],[\"Generalization in Reinforcement Learning\\n\\nGeneralization plays a pivotal role in the realm of Reinfo...\"],[\"Godot RL Agents\\n\\n[Godot RL Agents](https:\\u002f\\u002fgithub.com\\u002fedbeeching\\u002fgodot_rl_agents) is an Open Source ...\"],[\"In this section, you will **learn how to create a custom environment in the Godot Game Engine** and ...\"],[\"While we will guide you through the steps to implement your agent, you may wish to learn more about ...\"],[\"### Loading the starter project\\n\\nWe provide two versions of the codebase:\\n- [A starter project, to d...\"],[\"The Godot RL Agents plugin is now downloaded to your machine your machine. Now click on Project → Pr...\"],[\"func get_reward() -\\u003e float:\\n\\tassert(false, \\\"the get_reward method is not implemented when extending ...\"],[\"```\\n\\nIn order to implement these methods, we will need to create a class that inherits from AIContro...\"],[\"```\\n\\nWe have now defined the agent’s observation, which is the position and velocity of the ball in ...\"],[\"```\\n\\nWe now need to synchronize between the game running in Godot and the neural network being train...\"],[\"An Introduction to Unity ML-Agents [[introduction-to-ml-agents]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"So, today, we're going to train two agents:\\n- The first one will learn to **shoot snowballs onto a s...\"],[\"Introduction [[introduction]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002f...\"],[\"\\u003cfigure\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002f...\"],[\"Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera.org\\u002flectu...\"],[\"### Q2: What of the following statements are true about Unity ML-Agents?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n...\"],[\"### Q5: Which are the differences between capturing the environment using `frames` or `raycasts`?\\n\\n\\u003c...\"],[\"Model Based Reinforcement Learning (MBRL)\\n\\nModel-based reinforcement learning only differs from its ...\"],[\"We employ sample-based model-predictive control (MPC) using the learned dynamics model, which optimi...\"],[\"Hands-on [[hands-on]]\\n\\nNow that you've learned to use Optuna, here are some ideas to apply what you'...\"],[\"[The Hugging Face Deep Reinforcement Learning Course 🤗 (v2.0)](https:\\u002f\\u002fhuggingface.co\\u002fdeep-rl-course...\"],[\"How do Unity ML-Agents work? [[how-mlagents-works]]\\n\\nBefore training our agent, we need to understan...\"],[\"- The first is the *Learning Environment*, which contains **the Unity scene (the environment) and th...\"],[\"To better understand its role, let’s remember the RL process. This can be modeled as a loop that wor...\"],[\"This RL loop outputs a sequence of **state, action, reward and next state.** The goal of the agent i...\"],[\"The Pyramid environment\\n\\nThe goal in this environment is to train our agent to **get the gold brick ...\"],[\"We also use a **boolean variable indicating the switch state** (did we turn on or off the switch to ...\"],[\"How Huggy works [[how-huggy-works]]\\n\\nHuggy is a Deep Reinforcement Learning environment made by Hugg...\"],[\"**Joint motors drive Huggy's legs**. This means that to get the target, Huggy needs to **learn to ro...\"],[\"The training loop looks like this:\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-co...\"],[\"Interesting Environments to try\\n\\nHere we provide a list of interesting environments you can try to t...\"],[\"To start using this environment, check these resources:\\n- [DonkeyCar Simulator documentation](https:...\"],[\"Self-Play: a classic technique to train competitive agents in adversarial games\\n\\nNow that we've stud...\"],[\"This solution is called *self-play*. In self-play, **the agent uses former copies of itself (of its ...\"],[\"So we need to control:\\n\\n- How **often we change opponents** with the `swap_steps` and `team_change` ...\"],[\"This ELO (starting at a specific score: frequently 1200) can decrease initially but should increase ...\"],[\"We also define a maximum adjustment rating per game: K-factor.\\n\\n- K=16 for master.\\n- K=32 for weaker...\"],[\"\\\\\\\\(ELO_B = 2300 + 16 *(1-0.151) = 2314 \\\\\\\\)\\n\\n\\n### The Advantages\\n\\nUsing the ELO score has multiple ad...\"],[\"Quiz [[quiz]]\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera....\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"\\u003c\\u002fdetails\\u003e\\n\\n### Q6: How do we use Double Deep Q-Learning?\\n\\n\\n\\u003cdetails\\u003e\\n  \\u003csummary\\u003eSolution\\u003c\\u002fsummary\\u003e\\n...\"],[\"Deep Q-Learning [[deep-q-learning]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-c...\"],[\"Apache License\\n                           Version 2.0, January 2004\\n                        http:\\u002f\\u002fw...\"],[\"\\\"Contribution\\\" shall mean any work of authorship, including\\n      the original version of the Work a...\"],[\"4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works the...\"],[\"6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, serv...\"],[\"END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To ap...\"],[\"Optuna Tutorial [[optuna]]\\n\\nThe content below comes from [Antonin's Raffin ICRA 2022 presentations](...\"],[\"Hands-on [[hands-on]]\\n\\n\\n\\n      \\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\n      ...\"],[\"**If you don't find your model, go to the bottom of the page and click on the refresh button.**\\n\\nFor...\"],[\"### 🎮 Environments:\\n\\n- [SpacesInvadersNoFrameskip-v4](https:\\u002f\\u002fgymnasium.farama.org\\u002fenvironments\\u002fatar...\"],[\"To validate this hands-on for the certification process, you need to push your trained model to the ...\"],[\"```\\n\\nIF AND ONLY IF THE VERSION ABOVE DOES NOT EXIST ANYMORE. UNCOMMENT AND INSTALL THE ONE BELOW\\n\\n`...\"],[\"```\\nSpaceInvadersNoFrameskip-v4:\\n  env_wrapper:\\n    - stable_baselines3.common.atari_wrappers.AtariW...\"],[\"```\\n\\nHere we see that:\\n- We use the `Atari Wrapper` that preprocess the input (Frame reduction ,gray...\"],[\"```\\n\\n## Let's evaluate our agent 👀\\n\\n- RL-Baselines3-Zoo provides `enjoy.py`, a python script to eval...\"],[\"```\\n\\n## Publish our trained model on the Hub 🚀\\nNow that we saw we got good results after the trainin...\"],[\"```\\n\\nIf you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command ins...\"],[\"```\\n\\n###.\\n\\nCongrats 🥳 you've just trained and uploaded your first Deep Q-Learning agent using RL-Bas...\"],[\"Let's load an agent playing Beam Rider: https:\\u002f\\u002fhuggingface.co\\u002fsb3\\u002fdqn-BeamRiderNoFrameskip-v4\\n\\n1. W...\"],[\"```\\n\\n2. Let's evaluate if for 5000 timesteps\\n\\n```bash\\npython -m rl_zoo3.enjoy --algo dqn --env BeamR...\"],[\"```\\n\\nWhy not trying to train your own **Deep Q-Learning Agent playing BeamRiderNoFrameskip-v4? 🏆.**\\n...\"],[\"If you’re still feel confused with all these elements...it's totally normal! **This was the same for...\"],[\"Introduction to PPO with Sample-Factory\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-...\"],[\"The Deep Q-Learning Algorithm [[deep-q-algorithm]]\\n\\nWe learned that Deep Q-Learning **uses a deep ne...\"],[\"To help us stabilize the training, we implement three different solutions:\\n1. *Experience Replay* to...\"],[\"The solution is to create a Replay Buffer that stores experience tuples while interacting with the e...\"],[\"However, the problem is that we are using the same parameters (weights) for estimating the TD target...\"],[\"Instead, what we see in the pseudo-code is that we:\\n- Use a **separate network with fixed parameters...\"],[\"The solution is: when we compute the Q target, we use two networks to decouple the action selection ...\"],[\"A Q-Learning example [[q-learning-example]]\\n\\nTo better understand Q-Learning, let's take a simple ex...\"],[\"## Step 1: Initialize the Q-table [[step1]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-d...\"],[\"## Step 4: Update Q(St, At) [[step4]]\\n\\nWe can now update \\\\\\\\(Q(S_t, A_t)\\\\\\\\) using our formula.\\n\\n\\u003cimg ...\"],[\"## Step 4: Update Q(St, At) [[step4-4]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-...\"],[\"Congratulations\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002f...\"],[\"What is Reinforcement Learning? [[what-is-reinforcement-learning]]\\n\\nTo understand Reinforcement Lear...\"],[\"By interacting with his environment through trial and error, your little brother understands that **...\"],[\"Introduction [[introduction]]\\n\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-cours...\"],[\"Mid-way Recap [[mid-way-recap]]\\n\\nBefore diving into Q-Learning, let's summarize what we've just lear...\"],[\"Introduction [[introduction]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002f...\"],[\"\\u003cfigure\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002f...\"],[\"RLHF\\n\\nReinforcement learning from human feedback (RLHF) is a **methodology for integrating human dat...\"],[\"## Additional readings\\n\\n*Note, this is copied from the Illustrating RLHF blog post above*.\\nHere is a...\"],[\"And here is a snapshot of the growing set of papers that show RLHF's performance for LMs:\\n- [Fine-Tu...\"],[\"## Author\\n\\nThis section was written by \\u003ca href=\\\"https:\\u002f\\u002ftwitter.com\\u002fnatolambert\\\"\\u003e Nathan Lambert \\u003c\\u002fa...\"]],\"hovertemplate\":\"source=deep-rl-class\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"deep-rl-class, circle\",\"marker\":{\"color\":\"#00cc96\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"deep-rl-class, circle\",\"showlegend\":true,\"x\":[-8.213435,-7.8578224,-7.865934,-7.777864,-7.920407,6.5170627,-7.8957243,-7.1253147,-7.015019,-7.1157804,2.4330325,1.7413211,-7.2181935,-7.3554835,-7.2106166,1.0357271,-6.7364326,3.9851716,2.3801267,-6.980919,-8.349243,-8.350532,-8.355003,-8.429638,-8.334924,-8.144039,-8.268325,-8.191248,-7.2420835,-7.1513076,-7.7176757,-7.090863,-6.2761383,-6.452249,-7.3198223,-8.117605,-7.888469,-8.12201,-6.9524283,-8.443568,-8.30957,-8.184026,-7.917958,-8.16168,-7.4589424,-7.0844107,-8.147501,-8.237301,-8.259379,-8.123363,-8.075204,-7.602434,-7.0583987,-7.0856953,-6.9020886,2.211372,2.648234,2.8699358,0.65263265,1.0894105,0.9570135,3.9543703,3.6157818,2.7867146,2.6616085,2.6810231,0.7716223,-6.8393717,-8.032565,-7.8738365,-7.5824175,-8.113115,-8.222904,-8.197434,-8.114336,-8.212664,-8.391369,-8.266255,-6.9478216,-6.9273725,-7.2201004,-6.928939,-7.63835,-7.732617,-8.025951,-6.9992747,-6.9675384,-6.972735,2.3386426,3.0619786,4.0815353,2.989364,1.5706176,1.059383,1.4386508,1.3308808,3.4854183,1.4865714,1.3050418,1.6355945,0.40681717,0.30795315,3.2349207,2.3610377,2.7635024,3.0610197,0.8033138,0.85516155,0.50898933,0.21939376,-0.031841192,-0.25712204,-0.14666475,0.45711416,4.41783,3.0417652,-7.1841865,-7.4074097,-6.891268,-8.355955,-8.386131,-8.480112,-7.435587,-7.1135373,2.7037036,2.7494438,-7.049478,-6.9198017,0.52880687,1.0108033,3.4286034,3.9562504,-7.1632485,1.082591,-8.006751,-7.912355,-8.059721,-8.009583,-8.261749,-7.843052,-7.1639166,-8.425811,-8.40811,-8.382082,-8.369592,-8.464182,-8.398672,-7.887209,-7.058168,-6.961174,2.178538,2.9923604,3.9355922,-7.0339556,-7.076284,-2.110393,1.2125379,4.072781,3.827636,2.4800954,-6.9935026,-7.965238,-7.981101,-7.784033,-6.8874974,-7.2110815,-7.1096997,-7.1898885,-7.3405147,-8.2041445,-6.822887,2.6036952,3.0173144,-7.216505,-6.854494,-6.8574834,-6.954697,-7.3157163,0.5654506,1.097319,3.3052619,3.5556712,3.1188958,2.605451,2.8938105,2.153853,-7.274214,-7.2361603,-8.191785,-8.190489,-6.8776546,-7.869771,-7.6051803,-7.67402,-7.815237,-8.33775,-7.9720693,-8.4622,-8.30949,-7.3303666,-2.8529916,-8.352111,-7.1774096,-7.037849,-7.8266797,-8.076002,-8.207607,-8.2854595,-8.172278,-7.510958,-7.154316,-7.662186,-7.383084,-7.284691,-7.044291,-7.0727797,-7.742023,-8.028401,-6.9442887,2.7454827,-7.17145,-6.560125,-6.9451847,-7.1661987,-7.8917627,-8.13127,-7.813501,-7.6755414,-7.9567285,1.0057696,-7.022575,9.239766,2.9511757,1.2792119,1.1513847,3.245092,3.681541,3.595395,-7.0897536,-6.900711,0.85235226,1.4693425,3.2922642,1.3452274,-7.48329,-8.072741,-6.984331,-7.236838,-7.159996,-7.0540314,-7.7436223,-8.64298,-8.299408,-8.401696,-8.368379,-7.0864515,-7.2662916,-7.2444983,-6.808308,-7.263895,-8.15098,-7.995799,-8.171229,-8.247799,-8.256215,-8.183667,-7.3207507,-8.023389,-8.090326,-7.9171095,-7.8497477,-8.017278,-7.9366646,-6.9410768,-1.1277094,-6.837612,-5.1830173,-2.1573038,4.432365,-7.224382,-6.4817696,2.5772634,2.777161,0.9240546,1.9158478,-6.91341,0.73506117,9.132875,3.624381,1.8807286,-6.823054,-7.238761,-8.513485,-8.762527,-8.665435,-8.521532,-7.727705,-7.670241,6.110254,-8.201014,-7.888842,-7.2886786,-7.3308177,-7.1747355,-6.9546957,-6.8276224,-7.03279,-7.2319303,-8.689507,-8.68855,-8.733247,-8.735156,-7.2638283,-7.3088193,-7.246013,-7.1548862,-7.069332,-6.88625,-6.3855147,6.3493485,-6.98105,-7.105734,-6.939765,-6.8863745,2.792733,2.3475845,-6.8495207,0.4445891,-7.437039,-0.01318159,-0.38338086,-8.290871,-0.6246012,-8.373295,-0.2387946,-0.40567803,-0.47968122,-0.14338242,0.6871201,1.4178289,9.285935,2.9025338,1.1815712,3.037848,3.527448,-6.794742,-7.0390334,-0.280999,1.0899557,-7.1444545,-7.359532,-6.993953,-7.9483705,-8.304306,-7.8479214,-8.309575,-8.4023285,-8.368561,-8.4405575,-7.901851,-7.7507634,-7.7733383,-6.7214003,-6.5673375,-6.5005207,-6.5148897,-6.1838675,-6.7919545,-6.439468,-6.850305,-6.5915184,-6.9530888,-7.1369777,-7.997573,-6.935455,-7.560317,-7.4081016,-7.585982,-8.072287,-7.9569287,-7.247663,-7.2854557,-6.938348,-6.8150578,-8.140117,-7.8400593,-7.3182197,-7.058354,-6.9165893,-6.98101,-6.8731947,-6.5624323,-6.8859997,-7.231456,-7.595363,-7.388676,-6.7420583,-6.6489472,-6.3201847,-7.9055123,-8.10088,-8.173323,-7.7478747,6.051711,6.1707277,6.182538,6.1485753,5.9865212,-6.583999,-7.089599,-7.093123,-7.0932174,2.8384683,2.6065712,-1.7323811,-1.2562472,1.6983423,3.770782,3.6503234,-7.0119286,2.457095,0.47266135,-7.289954,-7.229685,-6.837136,-8.172394,-8.098748,-8.175926,-8.235233,-8.149965,-8.172366,-8.148052,-8.155255,-8.240229,-8.053069,-7.171425,-8.017656,-8.092758,-8.151671,-8.228463,-7.2809763,-6.7518983,-6.5855823,-6.8353605,-6.3167095,6.2452984],\"xaxis\":\"x\",\"y\":[7.1089892,6.832046,6.4163747,6.321752,6.3666425,-5.028249,5.990046,5.794877,5.6951475,5.7171946,0.4169389,0.5417015,6.311474,6.114607,5.9874263,1.03688,5.565409,-0.3996386,-0.07165631,5.7627068,6.851819,6.8425317,6.9120927,6.9076943,6.6802163,6.569061,6.781797,6.81409,6.0373383,5.896176,6.649341,6.1369705,1.7611512,3.158893,5.048409,7.073741,6.8764205,7.0574226,5.538587,6.7442045,6.818843,6.6611714,6.4264846,6.6438456,6.443055,6.541827,6.622114,6.781956,6.5772977,7.122256,7.0412755,5.759899,6.0715866,6.189546,6.193516,0.7832071,0.15447514,-1.3745188,1.510529,1.0627853,1.2103924,0.11290664,0.06312701,0.12689684,-0.7379658,-1.1498528,1.0863956,6.045922,5.094991,4.797213,6.1121345,7.0049305,7.0417337,7.023446,6.9977913,6.94249,7.0165,6.705456,5.9372673,5.663127,5.1321006,5.2145147,6.2723055,6.373922,5.912812,6.112147,6.14963,6.137716,0.9268768,0.8711148,-0.6387694,-0.035698865,-0.62506765,-0.93134856,-0.58082634,-1.0363687,-1.254587,0.11828966,-1.2984343,-0.3865983,-1.8572363,-1.9985688,0.057426047,-0.35884482,-0.09310756,-1.1791773,-0.552234,0.14857964,-0.5503476,-1.3153741,-1.2126886,-1.3092754,-1.7876132,-1.5668733,-0.61447865,0.19905071,6.377313,6.528686,6.7037797,6.8147335,6.7675004,6.455422,6.271767,6.1146064,0.70044875,0.42370632,6.520195,6.767301,0.19017696,-0.63530284,0.34641704,-0.05886537,6.138797,-0.99442077,6.4504333,6.535825,6.347688,6.741569,6.7239165,6.557603,5.599838,6.752171,6.707218,6.695385,6.580578,6.447431,6.457662,6.1404085,6.018496,6.0126324,0.7804355,-0.11499491,-1.5749292,6.456469,6.4739532,2.1141274,1.3135597,0.2519804,-0.1735741,-0.23517066,5.8929634,6.4988074,6.656787,6.352257,6.2755575,6.186513,6.257318,6.212107,6.1828275,6.671412,6.1297903,0.60802513,0.25006792,6.392102,6.7739606,6.7481236,6.7604246,6.3433237,0.578164,-0.33333215,0.25076997,0.08979193,-0.06877151,0.01816839,0.12094339,-0.42145094,6.1483116,6.0105424,6.9906073,7.1095734,5.6100044,6.503512,6.2691607,6.267171,6.392697,6.84089,6.3264136,6.614803,6.2342787,-3.300968,7.976291,6.5261707,6.1035933,5.0113087,6.1855254,6.3942943,6.4931645,6.735264,6.6659083,6.157237,6.036537,6.006549,6.3203883,6.3194456,6.236566,6.2911897,6.491517,6.889437,6.2101846,0.5731786,6.5429993,6.8031864,6.90345,6.733538,6.8691187,6.67891,6.6032257,6.706265,6.844468,-0.8878451,6.3739395,-4.453206,-0.1039179,-1.0955107,-2.6587691,-0.5850277,0.29342213,0.07849634,6.6078067,6.8215246,-0.8686866,0.22113395,-0.17941856,-0.478227,6.4177847,7.037122,6.2047396,6.380048,5.9213734,5.7839546,6.550215,6.2132254,6.849168,6.917954,6.9378724,-3.1559305,6.099646,4.63237,4.0419197,5.859597,6.8331027,6.601904,6.7492943,6.7742662,6.868137,6.934246,6.123997,6.52457,6.5829988,6.299527,6.2121797,6.524163,6.291488,6.1427402,2.8299096,6.058743,2.2859893,2.359394,-0.884475,5.289035,6.3313336,0.41033778,0.19312772,0.10273214,0.35133323,6.222024,0.711672,-4.5491676,0.18955588,0.14059997,6.186424,6.1717777,6.2594576,6.259397,6.2311516,6.0289755,6.457788,6.3242464,-5.2057185,6.377116,6.033126,6.116303,6.091163,5.9751472,6.1473455,6.4446197,5.911962,6.1750293,6.271725,6.269749,6.202198,6.221129,5.8879347,6.2621255,6.2180142,5.8060665,5.464882,5.152821,3.596327,-4.6867313,6.230217,6.3078866,6.234388,6.189026,0.7288225,0.0750286,6.7101107,-3.741282,6.4118876,-1.447831,-2.1370437,6.506766,-1.1681293,6.951743,-2.6675253,-1.4879087,-1.7283325,-2.5946405,0.03588662,-0.7526172,-4.464683,-0.021320108,-1.6418587,-0.4186227,0.19885819,6.742914,6.625356,-1.3705901,0.01406037,6.0836563,5.785326,5.4775977,6.5688353,6.882421,6.4743624,6.807672,6.7117248,6.8147187,6.9350653,6.489867,6.4472594,5.807891,6.1037984,5.9929833,6.465444,6.352056,6.65773,4.3747363,6.8380766,6.8369017,6.416313,6.130133,6.210266,6.316763,6.1458654,6.1352935,6.153657,6.436445,5.9207463,5.827302,5.9234114,5.842218,6.138099,6.4089026,6.6419845,6.498166,6.445365,6.703992,6.0915704,6.308155,6.0559425,6.356392,6.1928396,6.0770125,6.3472867,5.850294,4.6856213,3.9050655,3.5831718,6.7031026,7.0404186,7.1201267,6.7798786,-3.6903627,-3.7537413,-3.6770453,-3.9823954,-3.6685128,4.832907,6.2583156,6.2589693,6.323115,-0.2207116,0.56333256,3.3699787,2.5911243,-0.49835733,0.23891534,-0.11531709,6.232212,0.28774405,-3.1448011,6.2701774,5.9310856,5.944694,7.099088,7.181007,7.0591974,7.044424,7.1140547,7.0908756,7.0126534,7.015861,6.949904,6.821372,5.89109,6.5216923,6.5007324,6.772072,6.900549,6.1071577,5.9424224,3.7693527,4.2250485,3.2753444,-4.576824],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### Onnxruntime Training\\n\\nThe following example fine-tunes a T5 large model on the wmt16 dataset.\\n\\n`...\"],[\"```\\n\\n### Performance\\n\\nWe get the following results for [t5-large](https:\\u002f\\u002fhuggingface.co\\u002ft5-large) m...\"],[\"Inference pipelines with the ONNX Runtime accelerator\\n\\nThe [`~pipelines.pipeline`] function makes it...\"],[\"```\\n\\n_Note: The default models used in the [`~pipelines.pipeline`] function are not optimized for in...\"],[\"```\\n\\nIt is also possible to load it with the `from_pretrained(model_name_or_path, export=True)`\\nmeth...\"],[\"```\\n\\nIt is also possible to load it with the `from_pretrained(model_name_or_path)`\\nmethod associated...\"],[\"```\\n\\n\\n## Optimizing and quantizing in pipelines\\n\\nThe [`~pipelines.pipeline`] function can not only r...\"],[\"\\u003e\\u003e\\u003e # Load the quantized model from a local repository\\n\\u003e\\u003e\\u003e model = ORTModelForSequenceClassification...\"],[\"```\\n\\n### Optimizing with `ORTOptimizer`\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\u003e\\u003e\\u003e fr...\"],[\"# Save and push the model to the hub\\n\\u003e\\u003e\\u003e tokenizer.save_pretrained(\\\"new_path_for_directory\\\")  # doct...\"],[\"Quantization\\n\\n## AutoGPTQ Integration\\n\\n🤗 Optimum collaborated with [AutoGPTQ library](https:\\u002f\\u002fgithub...\"],[\"- Install latest `accelerate` library:\\n`pip install --upgrade accelerate`\\n\\n### Load and quantize a m...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\nGPTQ quantization only works for text model for now. Futhermore, the quant...\"],[\"```\\n\\n### Exllama kernels for faster inference\\n\\nWith the release of exllamav2 kernels, you can get fa...\"],[\"```\\n\\nNote that only 4-bit models are supported with exllama\\u002fexllamav2 kernels for now. Furthermore, ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### ORTModelForMultipleChoice\\n\\n[[autodoc]] onnxruntime.ORTModelForMultipleChoice\\n\\n### ORTModelForQue...\"],[\"### ORTModelForVision2Seq\\n\\n[[autodoc]] onnxruntime.ORTModelForVision2Seq\\n    - forward\\n\\n### ORTModel...\"],[\"#### ORTLatentConsistencyModelPipeline\\n\\n[[autodoc]] onnxruntime.ORTLatentConsistencyModelPipeline\\n  ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"| Notebook                                                                                          ...\"],[\"|:--------------------------------------------------------------------------------------------------...\"],[\"---------------------------------------------------------------------------------------------|:-----...\"],[\"-|:-------------------------------------------------------------------------------------------------...\"],[\"-----------------------------------------------------------------------------------------|:---------...\"],[\"------------------|---------------------------------------------------------------------------------...\"],[\"-------------------------------------:|...\"],[\"| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https:\\u002f\\u002fgithub....\"],[\"## Optimum Intel\\n\\n### OpenVINO...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to run inference with OpenVINO](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-intel\\u002fblob\\u002fmain\\u002fnotebo...\"],[\"| [How to quantize a question answering model with NNCF](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-inte...\"],[\"| [Compare outputs of a quantized Stable Diffusion model with its full-precision counterpart](https:...\"],[\"### Neural Compressor...\"],[\"| [How to quantize a model with Intel Neural Compressor for text classification](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## Optimum ONNX Runtime...\"],[\"| Notebook                                                                                          ...\"],[\"----------------------------------------------------------------------------------|:----------------...\"],[\"-------------------------------------------|:-------------------------------------------------------...\"],[\"----------------------------------------------------------------|-----------------------------------...\"],[\"-----------------------------------------------------------------------------------:|...\"],[\"| [How to quantize a model with ONNX Runtime for text classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"| [How to fine-tune a model for text classification with ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingfac...\"],[\"| [How to fine-tune DeBERTa for question-answering with ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\n__Note__\\n\\u003e *To enable ONNX Runtime training, your devices need to be equipped with GPU. Install...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nLet's see now how we can apply dynamic quantization with ONNX Runtime:\\n\\n```python\\n\\u003e\\u003e\\u003e from opti...\"],[\"```\\n\\nStatic quantization relies on feeding batches of data through the model to estimate the activat...\"],[\"# Define the processing function to apply to each example after loading the dataset\\n\\u003e\\u003e\\u003e def preproce...\"],[\"```\\n\\nAs a final example, let's take a look at applying _graph optimizations_ techniques such as oper...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nYou can find more examples in the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fintel\\u002finf...\"],[\"```\\n\\nLet's see now how we can apply dynamic quantization with ONNX Runtime:\\n\\n```python\\n\\u003e\\u003e\\u003e from opti...\"],[\"```\\n\\nYou can find more examples in the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fonnxrunti...\"],[\"```\\n\\nYou can find more examples in the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fhabana\\u002fqu...\"],[\"```\\n\\nCheck out the help for more options:\\n\\n```bash\\noptimum-cli export onnx --help\\n```\\n\\nCheck out the...\"],[\"```\\n\\nCheck out the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fbettertransformer\\u002foverview) f...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"```\\n\\n\\n__Note__\\n\\u003e *To enable ONNX Runtime training, your devices need to be equipped with GPU. Instal...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eOptimize your model to speedup inference with \\u003cspan class=\\\"underline\\\" oncli...\"],[\"\\u003c\\u002fa\\u003e\\n    \\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" ...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eEnable performance optimizations for \\u003cspan class=\\\"underline\\\" onclick=\\\"event...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eApply quantization and graph optimization to accelerate Transformers models...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"```\\n\\n\\n__Note__\\n\\u003e *To enable ONNX Runtime training, your devices need to be equipped with GPU. Instal...\"],[\"**Consequence**: A permanent ban from any sort of public interaction within\\nthe community.\\n\\n## Attri...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Step 2: Set your model on your preferred device\\n\\nIf you did not used `device_map=\\\"auto\\\"` to ...\"],[\"```\\nIf you want to run a pipeline on a GPU device, run:\\n```python\\n\\u003e\\u003e\\u003e from optimum.pipelines import ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nAfter implementing it, your transformation can be used as a regular function:\\n\\n```python\\n\\u003e\\u003e\\u003e fr...\"],[\"```\\n\\n### Composing transformations together\\n\\nAs applying multiple transformations in chain is needed...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Register commands in the Optimum CLI from a subpackage\\n\\nIt is possible to register a command in the ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Quantizing a model to be used with Optimum's CLI\\n\\nThe Optimum ONNX Runtime quantization tool can ...\"],[\"```\\n\\nQuantizing an ONNX model can be done as follows:\\n\\n```bash\\n optimum-cli onnxruntime quantize --o...\"],[\"```\\n\\n\\n## Apply Dynamic Quantization\\n\\nThe [`~optimum.onnxruntime.ORTQuantizer`] class can be used to ...\"],[\"```\\n\\n## Static Quantization example\\n\\nThe [`~optimum.onnxruntime.ORTQuantizer`] class can be used to ...\"],[\"# Create the calibration dataset\\n\\u003e\\u003e\\u003e def preprocess_fn(ex, tokenizer):\\n...     return tokenizer(ex[\\\"...\"],[\"```\\n\\n## Quantize Seq2Seq models\\n\\nThe [`~optimum.onnxruntime.ORTQuantizer`] class currently doesn't s...\"],[\"```\\n\\n3. Quantize all models\\n\\n```python\\n# Define the quantization strategy by creating the appropriat...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Stable Diffusion Text-to-Image Fine-Tuning\\n\\nThis example shows how to leverage ONNX Runtime Training...\"],[\"```\\n\\nAnd initialize an [🤗Accelerate](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002faccelerate\\u002f) environment with:\\n\\n...\"],[\"```\\n\\u003c!-- accelerate_snippet_end --\\u003e\\n\\n\\nTo run on your own training files prepare the dataset accordin...\"],[\"```\\n\\n#### Training with multiple GPUs\\n\\n`accelerate` allows for seamless multi-GPU training. Follow t...\"],[\"Overview\\n\\n🤗 Optimum provides an integration with Torch FX, a library for PyTorch that allows develop...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" href=\\\".\\u002fp...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe Optimum TFLite export can be used through Optimum command-line. As only static input shapes...\"],[\"Required arguments:\\n  -m MODEL, --model MODEL\\n                        Model ID on huggingface.co or ...\"],[\"Input shapes:\\n  --batch_size BATCH_SIZE\\n                        Batch size that the TFLite exported ...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Accelerated inference on NVIDIA GPUs\\n\\nBy default, ONNX Runtime runs inference on CPU devices. Howeve...\"],[\"```\\n\\nTo avoid conflicts between `onnxruntime` and `onnxruntime-gpu`, make sure the package `onnxrunt...\"],[\"```\\nValueError: Asked to use CUDAExecutionProvider as an ONNX Runtime execution provider, but the av...\"],[\"```\\n\\nAdditionally, you can pass the session option `log_severity_level = 0` (verbose), to check whet...\"],[\"```\\n\\nIn this example, we can see that all the costly MatMul operations are placed on the CUDA execut...\"],[\"### Reduce memory footprint with IOBinding\\n\\n[IOBinding](https:\\u002f\\u002fonnxruntime.ai\\u002fdocs\\u002fapi\\u002fpython\\u002fapi_s...\"],[\"```\\n\\nFor the time being, IOBinding is supported for task-defined ORT models, if you want us to add s...\"],[\"\\u003ctable\\u003e\\u003ctr\\u003e\\n\\u003ctd\\u003e\\n  \\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg alt=\\\"GPT2\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002foptim...\"],[\"And here is a summary for the saving time with different sequence lengths (32 \\u002f 128) and generation ...\"],[\"```\\n+-----------------------------------------------------------------------------+\\n| NVIDIA-SMI 440...\"],[\"- Platform: Linux-5.4.0-1089-aws-x86_64-with-glibc2.29\\n- Python version: 3.8.10\\n- `transformers` ver...\"],[\"```\\n\\nNote that previous experiments are run with __vanilla ONNX__ models exported directly from the ...\"],[\"```\\n\\n### Checking the TensorRT installation is successful\\n\\nBefore going further, run the following s...\"],[\"```\\n\\nsomething is wrong with the TensorRT or ONNX Runtime installation.\\n\\n### TensorRT engine build a...\"],[\"```\\n\\nTensorRT builds its engine depending on specified input shapes. Unfortunately, in the [current ...\"],[\"Passing the engine cache path in the provider options, the engine can therefore be built once for al...\"],[\"```\\n\\nThe engine is stored as:\\n\\n![TensorRT engine cache folder](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002foptim...\"],[\"```\\n\\n#### Warmup\\n\\nOnce the engine is built, it is recommended to do before inference **one or a few ...\"],[\"```\\n\\nThe model can then be used with the common 🤗 Transformers API for inference and evaluation, suc...\"],[\"```\\n\\nUsing this `qconfig`, static quantization can be performed as explained in the [static quantiza...\"],[\"\\u003e\\u003e\\u003e res = ort_model(**inp)\\n\\n\\u003e\\u003e\\u003e print(res)\\n\\u003e\\u003e\\u003e print(ort_model.config.id2label[res.logits[0].argmax(...\"],[\"```\\n\\nThe model can then be used with the common 🤗 Transformers API for inference and evaluation, suc...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Base classes\\n\\n[[autodoc]] exporters.onnx.OnnxConfig\\n    - inputs\\n    - outputs\\n    - generate_dum...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nIf you'd like to use the accelerator-specific features of 🤗 Optimum, you can install the requir...\"],[\"The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the...\"],[\"```\\n\\nFor the accelerator-specific features, you can install them by appending `optimum[accelerator_t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Supported architectures from [🤗 Transformers](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002findex):\\n\\n- AS...\"],[\"Supported architectures from [🤗 Diffusers](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002findex):\\n- Stable Di...\"],[\"Optimum Inference with ONNX Runtime\\n\\nOptimum is a utility package for building and running inference...\"],[\"```\\n\\n### Loading a vanilla Transformers model\\n\\nBecause the model you want to work with might not be ...\"],[\"```\\n\\n## Sequence-to-sequence models\\n\\nSequence-to-sequence (Seq2Seq) models can also be used when run...\"],[\"```\\n\\n### Text-to-Image\\n\\nHere is an example of how you can load an ONNX Stable Diffusion model and ru...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002foptimum\\u002fdocumen...\"],[\"```\\n\\n### Inpaint\\n\\n```python\\nimport PIL\\nimport requests\\nimport torch\\nfrom io import BytesIO\\nfrom opti...\"],[\"```\\n\\n### Text-to-Image\\n\\nHere is an example of how you can load a SDXL ONNX model from [stabilityai\\u002fs...\"],[\"```\\n\\n\\n### Refining the image output\\n\\nThe image can be refined by making use of a model like [stabili...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\noptimum-cli export onnx --model gpt2 --optimize O3 gpt2_onnx\\u002f\\n```\\n\\nThe optimization levels are:\\n...\"],[\"```\\n\\n\\n### Optimization Configuration\\n\\nThe [`~onnxruntime.configuration.OptimizationConfig`] class al...\"],[\"While [`~onnxruntime.configuration.OptimizationConfig`] gives you full control on how to do optimiza...\"],[\"```\\n\\nYou can also specify custom argument that were not defined in the O2 configuration, for instanc...\"],[\"```\\n\\n\\nBelow you will find an easy end-to-end example on how to optimize a Seq2Seq model [sshleifer\\u002fd...\"],[\"```\\n\\n## Optimizing a model with Optimum CLI\\n\\nThe Optimum ONNX Runtime optimization tools can be used...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## How to convert a model into its `BetterTransformer` format?\\n\\n### Step 1: Identifying the source l...\"],[\"\\u003e\\u003e\\u003e model = AutoModel.from_pretrained(\\\"bert-base-uncased\\\")\\n\\u003e\\u003e\\u003e print(model)  # doctest: +IGNORE_RESU...\"],[\"```\\n\\nYou can clearly see that the layers that need to be replaced are the `BertLayer` modules since ...\"],[\"```\\n\\nNow, make sure to fill all the necessary attributes, the list of attributes are:\\n\\n- `in_proj_we...\"],[\"```\\n\\n\\n### Step 3: Building the forward pass\\n\\nFirst of all, start with the line `super().forward_chec...\"],[\"```\\n\\nOnce the `hidden_states` are nested, call `torch._transformer_encoder_layer_fwd` using the righ...\"],[\"```\\nMODEL_MAPPING = {\\n  ...\\n  \\\"bert\\\": (\\\"BertLayer\\\", BertLayerBetterTransformer),\\n  ...\\n}\\n```\\n\\nTry it...\"],[\"Symbolic tracer\\n\\nIn Torch FX, the symbolic tracer feeds dummy values through the code to record the ...\"],[\"Accelerated inference on AMD GPUs supported by ROCm\\n\\nBy default, ONNX Runtime runs inference on CPU ...\"],[\"```\\n\\n**Local Installation Steps:**\\n\\n##### 2.1 PyTorch with ROCm Support\\nOptimum ONNX Runtime integra...\"],[\"```\\n\\n\\u003cTip\\u003e\\nTo avoid conflicts between `onnxruntime` and `onnxruntime-rocm`, make sure the package `o...\"],[\"```\\nValueError: Asked to use ROCMExecutionProvider as an ONNX Runtime execution provider, but the av...\"],[\"```\\n\\nAdditionally, you can pass the session option `log_severity_level = 0` (verbose), to check whet...\"],[\"BetterTransformer benchmark\\n\\nPlease refer to https:\\u002f\\u002fmedium.com\\u002fpytorch\\u002fbettertransformer-out-of-the...\"],[\"# using bitsandbytes fp4\\u002ffp16 scheme\\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model meta-ll...\"],[\"```\\n\\nHere are results obtained on a single NVIDIA A100-SXM4-80GB GPU. We use a prompt length of 512,...\"],[\"Bitsandbytes uses the fp4 scheme, with the compute in fp16.\\n\\n### Batch size = 1\\n\\n|quantization |act_...\"],[\"### Batch size = 2\\n\\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency ...\"],[\"### Batch size = 4\\n\\n|quantization |act_order|bits|group_size|kernel           |Load time (s)|Per-tok...\"],[\"### Batch size = 8\\n\\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency ...\"],[\"### Batch size = 16\\n\\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency...\"],[\"# GPTQ with exllamav2 kernel (int4\\u002ffp16)\\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model The...\"],[\"```\\n\\nThe benchmark below is for a prompt length of 512, measuring only the prefill step on a single ...\"],[\"### Batch size = 2\\n\\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tok...\"],[\"### Batch size = 4\\n\\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tok...\"],[\"### Batch size = 8\\n\\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tok...\"],[\"### Batch size = 16\\n\\n|quantization |act_order|bits|group_size|kernel    |prompt_length|new_tokens|Lo...\"],[\"# GPTQ with exllamav2 kernel (int4\\u002ffp16)\\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model The...\"],[\"```\\n\\n| quantization | act_order | bits | group_size | kernel           | perplexity |\\n|-------------...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In the 2.0 version, PyTorch includes a native scaled dot-product attention operator (SDPA) as part o...\"],[\"In inference mode, the padding mask is kept for correctness and thus speedups should be expected onl...\"],[\"- [AlBERT](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1909.11942)\\n- [Bark](https:\\u002f\\u002fgithub.com\\u002fsuno-ai\\u002fbark)\\n- [BART](http...\"],[\"- [Ernie](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1904.09223)\\n- [Falcon](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2306.01116) (No need to...\"],[\"- [LayoutLM](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1912.13318)\\n- [Llama & Llama2](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2302.13971) ...\"],[\"- [ViLT](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2102.03334)\\n- [ViT](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2010.11929)\\n- [ViT-MAE](htt...\"],[\"Let us know by opening an issue in 🤗 Optimum if you want more models to be supported, or check out t...\"],[\"```\\nYou can leave `keep_original_model=False` in case you want to overwrite the current model with i...\"],[\"ONNX 🤝 ONNX Runtime\\n\\nONNX is an open standard that defines a common set of operators and a common fi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It is possible to know which tasks are supported for a model for a given backend, by doing:\\n\\n```pyth...\"],[\"```\\n\\n\\u003c\\u002fTip\\u003e\\n\\n### PyTorch\\n\\n| Task                                 | Auto Class                       ...\"],[\"### TensorFlow\\n\\n| Task                                 | Auto Class                             |\\n|-...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### CalibrationConfig\\n\\n[[autodoc]] onnxruntime.configuration.CalibrationConfig\\n\\n## ORTConfig\\n\\n[[auto...\"],[\"# How to contribute to Optimum?\\n\\nOptimum is an open source project, so all contributions and suggest...\"],[\"```\\n\\n\\t**do not** work on the `main` branch.\\n\\n4. Set up a development environment by running the foll...\"],[\"Overview\\n\\n🤗 Optimum provides an integration with ONNX Runtime, a cross-platform, high performance en...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" href=\\\".\\u002fp...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```bash\\ntorchrun --nproc_per_node=NUM_GPUS_YOU_HAVE run_classification.py \\\\\\n    --model_name_or_path...\"],[\"```\\n\\n### Performance\\n\\nWe get the following results for [meta-llama\\u002fLlama-2-7b-hf](https:\\u002f\\u002fhuggingfac...\"],[\"#### DeepSpeed\\n\\n[zero_stage_2.json](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum\\u002fblob\\u002fmain\\u002fexamples\\u002fonnxru...\"],[\"```\\n\\n### Performance\\n\\nWe get the following results for [roberta-base](https:\\u002f\\u002fhuggingface.co\\u002froberta...\"],[\"\\u003e *The inference will use PyTorch by default, if you want to use ONNX Runtime backend instead, add t...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[`ORTTrainer`] and [`ORTSeq2SeqTrainer`] APIs make it easy to compose __[ONNX Runtime (ORT)](https:\\u002f...\"],[\"Test it out to achieve __lower latency, higher throughput, and larger maximum batch size__ while tra...\"],[\"```\\nPyTorch: 1.14.0.dev20221103+cu116; ORT: 1.14.0.dev20221103001+cu116; DeepSpeed: 0.6.6; HuggingFa...\"],[\"```\\n\\n* If you want to install the dependencies beyond in a local Python environment. You can pip ins...\"],[\"```\\n\\n* If you want to install the dependencies beyond in a local Python environment. You can pip ins...\"],[\"```\\n\\nOr install from source:\\n\\n```bash\\npip install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum.git\\n```...\"],[\"```\\n\\nCheck out more detailed [example scripts](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum\\u002ftree\\u002fmain\\u002fexam...\"],[\"```\\n\\nCheck out more detailed [example scripts](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum\\u002ftree\\u002fmain\\u002fexam...\"],[\"```\\n\\n\\n\\u003cTip warning={false}\\u003e\\n\\nDeepSpeed is supported by ONNX Runtime(only ZeRO stage 1 and 2 for the ...\"],[\"```\\n\\n\\u003cTip warning={false}\\u003e\\n\\nDeepSpeed is supported by ONNX Runtime(only ZeRO stage 1 and 2 for the m...\"],[\"```\\n\\n## Other Resources\\n\\n* Blog posts\\n    * [Optimum + ONNX Runtime: Easier, Faster training for you...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### Onnx Runtime Training\\n\\nThe following example fine-tunes a BERT on the SQuAD 1.0 dataset.\\n\\n```bas...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-habana.git\\ncd optimum-habana\\nmake doc BUILD_DIR...\"],[\"```\\nSections that were moved:\\n\\n[ \\u003ca href=\\\"#section-b\\\"\\u003eSection A\\u003c\\u002fa\\u003e\\u003ca id=\\\"section-a\\\"\\u003e\\u003c\\u002fa\\u003e ]\\n```\\nand ...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs\\ncontinue to work.\\n\\nFo...\"],[\"The same works for methods so you can either use \\\\[\\\\`XXXClass.method\\\\`\\\\] or\\n\\\\[~\\\\`XXXClass.method\\\\`\\\\]...\"],[\"```\\n    Args:\\n        n_layers (`int`): The number of layers of the model.\\n```\\n\\nIf the description i...\"],[\"```\\n```\\n# first line of code\\n# second line\\n# etc\\n```\\n````\\n\\nWe follow the [doctest](https:\\u002f\\u002fdocs.pyth...\"],[\"```\\n\\n## Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that no...\"],[\"```\\n\\nThe docstring should give a minimal, clear example of how the respective model \\nis to be used i...\"],[\"# Need node to build doc HTML. Taken from https:\\u002f\\u002fstackoverflow.com\\u002fa\\u002f67491580\\nRUN apt-get update &&...\"],[\"```\\n\\nThe main thing to note here is the need to install Node in the Docker image -\\nthat's because we...\"],[\"```\\n# Add this\\n- uses: actions\\u002fcheckout@v2\\nwith:\\n    repository: 'huggingface\\u002foptimum-habana'\\n    pa...\"],[\"![ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum\\u002factions\\u002fworkflows\\u002ftest_onnxruntime.yml\\u002fbadge....\"],[\"```\\n\\nIf you'd like to use the accelerator-specific features of 🤗 Optimum, you can install the requir...\"],[\"If you'd like to use the accelerator-specific features of 🤗 Optimum, you can install the required de...\"],[\"The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the...\"],[\"```\\n\\nFor the accelerator-specific features, append `optimum[accelerator_type]` to the above command:...\"],[\"```\\n\\n## Accelerated Inference\\n\\n🤗 Optimum provides multiple tools to export and run optimized models ...\"],[\"### Features summary\\n\\n| Features                           | [ONNX Runtime](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"### OpenVINO\\n\\nBefore you begin, make sure you have all the necessary libraries installed :\\n\\n```bash\\n...\"],[\"```\\n\\nIt is possible to export 🤗 Transformers and Diffusers models to the OpenVINO format easily:\\n\\n``...\"],[\"```\\n\\nYou can find more examples in the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fintel\\u002finf...\"],[\"```\\n\\nThe model can then be quantized using `onnxruntime`:\\n\\n```bash\\noptimum-cli onnxruntime quantize ...\"],[\"```\\n\\nThese commands will export `deepset\\u002froberta-base-squad2` and perform [O2 graph optimization](ht...\"],[\"```\\n\\nMore details on how to run ONNX models with `ORTModelForXXX` classes [here](https:\\u002f\\u002fhuggingface...\"],[\"```\\n\\n```diff\\n- from transformers import Trainer, TrainingArguments\\n+ from optimum.habana import Gaud...\"],[\"```\\n\\nYou can find more examples in the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fonnxrunti...\"],[\"Quantization\\n\\nQuantization is a technique to reduce the computational and memory costs of running in...\"],[\"```\\nC = A + B\\n```\\n\\nHere the result is much bigger than the biggest representable value in `int8`, wh...\"],[\"```\\nx = S * (x_q - Z)\\n```\\n\\nwhere:\\n\\n- `x_q` is the quantized `int8` value associated to `x`\\n- `S` and...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nUsually `round(a\\u002fS + Z)` corresponds to the smallest representable value in the consider...\"],[\"### Per-tensor and per-channel quantization\\n\\nDepending on the accuracy \\u002f latency trade-off you are t...\"],[\"1. Post training **dynamic quantization**: the range for each activation is computed on the fly at *...\"],[\"For both post training static quantization and quantization aware training, it is necessary to defin...\"],[\"### Pratical steps to follow to quantize a model to `int8`\\n\\nTo effectively quantize a model to `int8...\"],[\"## Supported tools to perform quantization in 🤗 Optimum\\n\\n🤗 Optimum provides APIs to perform quantiza...\"],[\"#### Integer representation\\n\\nIntegers are usually represented with the following bit lengths: `8`, `...\"],[\"```\\n19 = 0 x 2^7 + 0 x 2^6 + 0 x 2^5 + 1 x 2^4 + 0 x 2^3 + 0 x 2^2 + 1 x 2^1 + 1 x 2^0\\n```\\n\\n2. Signe...\"],[\"```\\nx = sign x mantissa x (2^exponent)\\n```\\n\\n\\n## References\\n\\n- The\\n[Quantization and Training of Neur...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nCheck out the help for more options:\\n\\n```bash\\noptimum-cli export onnx --help\\n```\\n\\n## Why use ON...\"],[\"```\\n\\nThe Optimum ONNX export can be used through Optimum command-line:\\n\\n```bash\\noptimum-cli export o...\"],[\"Optional arguments:\\n  --task TASK           The task to export the model for. If not specified, the ...\"],[\"This is needed by some models, for some tasks. If not provided, will attempt to use the tokenizer to...\"],[\"```\\n\\nExporting a checkpoint can be done as follows:\\n\\n```bash\\noptimum-cli export onnx --model distilb...\"],[\"```\\n\\nNote that providing the `--task` argument for a model on the Hub will disable the automatic tas...\"],[\"```\\n\\nPrinting the outputs would give that:\\n\\n```bash\\nQuestionAnsweringModelOutput(loss=None, start_lo...\"],[\"```\\n\\nFor more information, check the `optimum.onnxruntime` documentation [page on this topic](\\u002fonnxr...\"],[\"```\\n\\n### Exporting a model to be used with Optimum's ORTModel\\n\\nModels exported through `optimum-cli ...\"],[\"```\\n\\nand\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\u003e\\u003e\\u003e from optimum.onnxruntime import O...\"],[\"```\\n\\nYou can then pass one of these tasks to the `--task` argument in the `optimum-cli export onnx` ...\"],[\"from optimum.exporters.onnx.base import ConfigBehavior\\nfrom typing import Dict\\n\\nclass CustomWhisperO...\"],[\"custom_whisper_onnx_config = CustomWhisperOnnxConfig(\\n        config=config,\\n        task=\\\"automatic...\"],[\"```\\n\\nFor tasks that require only a single ONNX file (e.g. encoder-only), an exported model with cust...\"],[\"class MPTDummyPastKeyValuesGenerator(DummyPastKeyValuesGenerator):\\n    \\\"\\\"\\\"\\n    MPT swaps the two las...\"],[\"def add_past_key_values(self, inputs_or_outputs: Dict[str, Dict[int, str]], direction: str):\\n       ...\"],[\"custom_onnx_configs = {\\n    \\\"decoder_model\\\": onnx_config,\\n    \\\"decoder_with_past_model\\\": onnx_config...\"],[\"```\\n\\nMoreover, the advanced argument `fn_get_submodels` to `main_export` allows to customize how the...\"],[\"Helpful tips for testing & debugging optimum\\n\\n## VSCODE\\n\\nIf you are using vscode you might have hard...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\nWhen inheriting from a middle-end class, look for the one handling the same modality \\u002f cate...\"],[\"```\\n\\nFirst let's explain what `TextEncoderOnnxConfig` is all about. While most of the features are a...\"],[\"Once you have implemented an ONNX configuration, you can instantiate it by providing the base model'...\"],[\"```\\n\\nThe resulting object has several useful properties. For example, you can view the ONNX\\noperator...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nCheck out [`BartOnnxConfig`] for an advanced example.\\n\\n\\u003c\\u002fTip\\u003e\\n\\n\\n## Registering the ONNX ...\"],[\"```\\n\\n## Exporting the model\\n\\nOnce you have implemented the ONNX configuration, the next step is to e...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIf your model is larger than 2GB, you will see that many additional files are created du...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\n### Performance\\n\\nWe get the following results for [bert-large-cased](https:\\u002f\\u002fhuggingface.co\\u002fber...\"]],\"hovertemplate\":\"source=optimum\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"optimum, circle\",\"marker\":{\"color\":\"#ab63fa\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"optimum, circle\",\"showlegend\":true,\"x\":[6.305097,6.6208596,-3.2353754,-3.6617093,-1.2893233,-1.9732506,-3.4304795,-1.9630456,-1.5009733,-1.4503359,-2.3040078,-1.4059101,-1.8829578,2.5352936,-3.6034713,-2.2570832,-2.7945511,-2.4259503,-3.214919,-4.8568892,-10.461921,0.7305943,1.6088912,6.2308393,6.2828383,19.514902,19.540785,19.523386,19.54366,19.581137,19.51799,-2.684549,-1.131342,6.145572,-2.1420515,-4.430024,-0.8646329,-6.41482,-4.726824,-1.861101,6.333746,19.538595,19.537117,19.582973,19.523056,-4.8393445,-5.8689933,-5.608785,-3.861782,-1.0220292,-2.4139042,-2.6906097,-2.889995,-0.83791816,-2.2781556,-2.6446433,-2.3475738,-2.5828018,-2.1774683,-2.2154238,-2.3420973,-2.9834492,-2.9264,-2.4406157,-0.8619094,-2.8973238,-2.185092,-2.6456933,-2.7246199,-1.7419283,-3.6886582,-4.1905107,-3.2742863,-4.067191,-0.9046477,6.7792583,-2.2947483,-1.3475342,-1.1231993,-1.7945127,-1.8796896,0.006586566,-3.0208445,1.590728,-2.9686806,-2.5520723,-2.4687457,-2.612358,-2.5604577,-2.2760675,-2.2885098,-2.4077122,-3.1649585,-3.0735533,-0.78161985,1.2571619,0.49060437,-0.14004901,-1.2988776,7.116628,7.172334,-3.1668518,-1.6524,-2.7270117,-9.0234165,-3.574789,-2.1736538,-0.90514505,-1.2747657,0.32292873,-2.4651487,-1.9255036,-3.0302296,-2.3459408,-1.8663403,-0.6915432,2.05782,-2.1335044,-1.2351736,-1.4533813,-1.5772908,-1.941652,-1.6054025,-1.8420951,-2.5066671,-2.4483597,-0.9195933,-2.6593945,-2.6383693,-10.626303,6.454316,5.7561502,-1.0924361,2.2799199,1.176604,-2.590534,-5.777151,-0.74694663,-2.3393183,-2.2422442,-1.9347517,-0.111966856,-0.29850605,0.38137773,-0.060769416,-0.19611683,-3.7939084,-2.649347,-2.2385612,-2.306582,-1.7288692,-1.3137406,-2.1092045,-2.2425258,-2.608564,-5.4769444,-2.319301,-3.000411,-1.9818752,-1.2201865,-1.8153931,-2.0108058,-2.0329657,0.29038885,-1.5968702,0.43877405,-0.4938818,-0.9991936,0.8053009,-2.6290703,-2.3589172,-2.4704242,-2.9915776,-3.1282947,-3.1116698,-3.0576172,-2.993878,-2.4490619,-1.7334458,-2.9742436,-2.9796314,-3.1208935,-3.0932608,-3.0148532,-3.558327,-2.7109957,-1.6523213,-2.495879,6.2115984,-2.900075,-3.9547198,-3.7636042,-3.4240618,-0.97040236,-2.0316405,-2.4496222,-1.5994949,-4.702432,-3.7825842,-2.765511,-10.549947,4.93262,4.7936716,-2.3244736,7.125827,7.193688,-3.9317415,0.32473758,-2.183051,-3.7617972,-2.026497,0.70940244,-3.1207945,-2.8768804,-2.4410155,-2.8020375,-0.9497429,1.9671006,1.9400147,-1.4225171,-1.9236053,-2.1772287,-0.46870047,-0.5005097,-3.1545274,-3.757439,-4.0389266,-2.978674,-2.7075312,5.931053,5.5728207,5.6930423,5.7870874,1.464212,-0.88049716,-1.8233668,5.1341276,0.15407076,4.925818,4.6836495,4.12431,-2.521706,-0.6980594,-1.0611742,2.484625,1.0904293,-2.4978855,-4.187985,2.1519616,-2.0931966,-2.493008,-2.4943044,-2.2862892,-2.6288428,-1.8253838,-2.43171,-3.4307747,-3.3660903,-3.4201837,-3.4264536,-3.0930295,-3.3402514,-3.1778293,-3.2961068,-3.1214583,-3.5170414,-3.5976799,-3.2272878,-3.8923624,6.228968,-2.994429,-2.7031941,-2.3809524,-1.647622,-2.0551004,-1.956916,-1.5461004,-1.9711285,-1.0362468,-1.8366759,-2.0426216,-1.8237677,-1.9107417,-1.658249,-1.3144957,-2.1284106,-1.6975673,-1.2610003,-1.4837338,-0.71011454,2.5910404,-3.2040176,-2.6803656,-2.0318408,-2.807045,-2.1849573,-1.9232436,-2.0322585,-2.1097503,-1.9057417,6.458468,-4.1300445,-2.11959],\"xaxis\":\"x\",\"y\":[0.71274227,0.6876064,-0.003928969,-0.48062938,1.8523237,1.979978,-1.0394106,0.3206142,0.09240685,0.11124784,1.058506,-0.15287718,0.5412234,-0.11543641,2.7720087,1.7524849,2.219522,1.9915632,2.4679308,-0.5399911,-2.9018312,7.822078,-0.66869694,0.40378338,-4.0913453,8.133507,8.166414,8.14398,8.169149,8.226574,8.13402,2.2790031,2.3032973,-4.102243,1.6535196,-1.2582879,7.013964,1.4702673,-0.047494985,1.292437,-4.2341475,8.164929,8.160556,8.22564,8.136972,-0.27882716,-0.45933828,-0.85683584,-0.24058177,1.4034729,1.1340052,1.2660097,1.6370555,-3.8220172,1.2479913,2.2584085,0.9882126,1.3381312,0.9828139,0.9061919,0.7146734,2.1429033,0.26310933,0.70894593,1.4264247,1.7576292,2.80809,2.6526458,2.8555295,2.868943,1.8509853,-0.011636012,0.8292035,-0.5985491,1.438414,-3.9632232,1.1456015,0.53156966,0.732777,-0.57296747,-1.4744853,-2.2210722,0.6102662,0.3406996,1.1554227,1.4751225,1.2030965,1.2697716,1.4441063,1.1676075,1.2533493,1.6006696,0.91650426,1.2293755,6.715033,2.6274052,4.246401,3.2558854,2.2534175,-4.5650015,-4.6815367,0.14670949,0.22911404,0.08592647,-1.6889274,0.28658336,1.6489561,0.99844706,0.70479625,0.41519758,1.806103,1.5921583,2.340211,1.2647321,1.7050742,2.6616256,1.0000852,1.1951516,0.13321629,0.9398658,0.11282831,-1.3386062,0.43050867,1.3159494,1.460342,1.5798632,-3.1828065,1.8321064,0.30144617,-3.6944773,0.657055,0.16529159,1.7320794,0.34136894,0.8476264,1.0114343,0.3430921,6.906105,1.1118468,0.3878474,0.75083464,7.20303,7.7862525,8.510509,7.174154,7.1117387,0.15234925,1.0724189,1.1748728,1.2088146,1.4257954,0.30343464,1.0132016,1.0405139,0.9549419,0.27721176,-1.0376483,-1.3993042,-1.174626,-1.0874676,-2.20993,-1.5496554,-1.4508555,-0.3338943,1.7549373,1.0056633,0.9324157,0.60947865,-0.14516197,2.4984229,2.8492,3.1117911,3.4678175,3.6890035,3.763785,3.654625,3.605554,2.6031888,2.6877704,3.6557817,3.584791,3.6328475,3.538795,2.2784107,3.1477776,2.6714752,4.169469,3.1529644,-4.753144,2.5998058,1.884787,2.3715875,-0.24332704,-0.8556693,1.0984782,0.40639243,-0.30883366,-1.3936632,-1.6659552,1.0173712,-3.7298808,-1.1151415,-0.2893059,1.2991256,-4.6749673,-4.572788,-0.4289316,1.9945548,2.3714955,-0.19832893,2.1823394,1.8213769,0.6924985,1.8081826,1.6773618,1.9935335,1.4568732,0.8092484,0.63816315,1.0653315,1.0762941,1.2242988,2.9570224,3.2744176,1.8366606,-0.038579922,-0.59191966,0.16952705,0.9552225,0.19958818,-1.1651636,-2.0934968,-1.5315828,-2.0325751,-3.5508983,-3.9084103,-0.76323986,-0.8352105,-1.2620025,-1.2009639,-1.010458,1.3775679,1.6493819,1.8770887,0.32648015,0.7375381,1.3487781,1.9821829,0.48975,0.2918031,1.2527435,1.4577448,0.76238155,1.1846081,0.12292893,1.0426077,2.957258,3.2029803,3.1499808,3.135496,2.864623,2.6629136,2.5693264,2.7015393,2.2922413,3.2908478,3.2510998,3.1529524,-0.21168427,0.58956414,0.7051854,0.6038805,0.95549494,0.64605194,0.34216806,0.675955,0.19540246,0.44243816,-1.7096422,0.4873342,0.41361746,0.23128171,0.214597,-0.28816593,-0.050944112,0.4487171,-1.2408589,-0.48973775,0.038354352,-0.32538703,-0.4652137,0.6411172,0.039112214,-0.6886791,0.08951404,0.045797322,-0.66742164,-0.04068943,0.2618671,0.3390274,0.5152338,-0.71142393,1.6907592],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Fine-tuning for image classification using LoRA and 🤗 PEFT\\n\\n## Vision Transformer model from transfo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] auto.AutoPeftModelForTokenClassification\\n\\n## AutoPeftModelForQuestionAnswering\\n\\n[[autodo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## LoHaConfig\\n\\n[[autodoc]] tuners.loha.config.LoHaConfig\\n\\n## LoHaModel\\n\\n[[autodoc]] tuners.loha.mode...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nFor example:\\n\\n```bash\\ndoc-builder preview peft docs\\u002fsource\\n```\\n\\nThe docs will be viewable at [h...\"],[\"```\\nand of course, if you moved it to another file, then:\\n\\n```\\nSections that were moved:\\n\\n[ \\u003ca href=...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\n\\n#...\"],[\"The same works for methods so you can either use \\\\[\\\\`XXXClass.method\\\\`\\\\] or \\\\[~\\\\`XXXClass.method\\\\`\\\\]...\"],[\"```\\n    Args:\\n        n_layers (`int`): The number of layers of the model.\\n```\\n\\nIf the description i...\"],[\"```\\n```python\\n# first line of code\\n# second line\\n# etc\\n```\\n````\\n\\n#### Writing a return block\\n\\nThe re...\"],[\"```\\n    Example:\\n\\n    ```python\\n    \\u003e\\u003e\\u003e import time\\n    \\u003e\\u003e\\u003e from accelerate import Accelerator\\n    \\u003e...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## Setup\\n\\nStart by defining the model and tokenizer, text and label columns, and some hyperpara...\"],[\"```\\n\\n## Load dataset\\n\\nFor this guide, you'll train on the `sentences_allagree` subset of the [`finan...\"],[\"```\\n\\n## Preprocess dataset\\n\\nInitialize a tokenizer, and create a function to pad and truncate the `m...\"],[\"```\\n\\nCreate a [`DataLoader`](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fdata.html#torch.utils.data.DataLoader) ...\"],[\"```\\n\\nSetup the optimizer and learning rate scheduler:\\n\\n```py\\noptimizer = torch.optim.AdamW(model.par...\"],[\"```\\n\\nMove the model to the GPU, and then write a training loop to begin!\\n\\n```py\\nmodel = model.to(dev...\"],[\"```\\n\\nLet's see how well the model performs on the validation set:\\n\\n```py\\ncorrect = 0\\ntotal = 0\\nfor p...\"],[\"```\\n\\nIf you check the model file size in the repository, you'll see that it is only 3.93MB! 🤏\\n\\n## In...\"],[\"Fine-tuning a multilayer perceptron using LoRA and 🤗 PEFT\\n\\n[![Open In Colab](https:\\u002f\\u002fcolab.research....\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"However, after a model is quantized it isn't typically further trained for downstream tasks because ...\"],[\"```py\\nimport torch\\nfrom transformers import BitsAndBytesConfig\\n\\nconfig = BitsAndBytesConfig(\\n    loa...\"],[\"```\\n\\nPass the `config` to the [`~transformers.AutoModelForCausalLM.from_pretrained`] method.\\n\\n```py\\n...\"],[\"```\\n\\nYou're all set for training with whichever training method you prefer!\\n\\n### LoftQ initializatio...\"],[\"```\\n\\n## Next steps\\n\\nIf you're interested in learning more about quantization, the following may be h...\"],[\"Using PEFT with timm\\n\\n`peft` allows us to train any model with LoRA as long as the layer type is sup...\"],[\"```\\n\\nThese are the transformations steps necessary to process the image.\\n\\n\\n```python\\ntransform = cre...\"],[\"```\\n\\n## Training\\n\\nThis is just a function that performs the train loop, nothing fancy happening.\\n\\n\\n`...\"],[\"```\\n\\n### Selecting which layers to fine-tune with LoRA\\n\\nLet's take a look at the layers of our model...\"],[\"```\\n\\nFinally, let's create the `peft` model, the optimizer and criterion, and we can get started. As...\"],[\"```\\n\\n\\n```python\\npeft_model.push_to_hub(model_id);\\n```\\n\\nAs we can see, the adapter size is only 4.3 M...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Below is a basic example usage of how to inject LoRA adapters into the submodule `linear` of the mod...\"],[\"```\\n\\nIf you print the model, you will notice that the adapters have been correctly injected into the...\"],[\"```\\n\\n## Pros and cons \\n\\nWhen to use this API and when to not use it? Let's discuss in this section t...\"],[\"``python\\nimport os\\n\\nimport torch\\nfrom transformers import (\\n    AutoTokenizer,\\n    default_data_coll...\"],[\"```\\n\\n\\n```python\\n# loading dataset\\ndataset = load_dataset(\\\"financial_phrasebank\\\", \\\"sentences_allagree...\"],[\"```\\n\\n\\n```python\\n# training and evaluation\\n\\n\\ndef compute_metrics(eval_preds):\\n    preds, labels = eva...\"],[\"```\\n\\n\\n```python\\nckpt = f\\\"{peft_model_id}\\u002fadapter_model.bin\\\"\\n!du -h $ckpt\\n```\\n\\n\\n```python\\nfrom peft i...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## Setup\\n\\nLet's take care of some of the setup first so you can start training faster later. Se...\"],[\"```\\n\\n## Load dataset and metric\\n\\nThe [Common Voice 11.0](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmozilla-fou...\"],[\"```\\n\\nYou'll only be training on the `sentence` and `audio` columns, so you can remove the rest of th...\"],[\"```\\n\\nIf you look at the `sampling_rate`, you'll see the audio was sampled at 48kHz. The Whisper mode...\"],[\"```\\n\\nOnce you've cleaned up the dataset, you can write a function to generate the correct model inpu...\"],[\"```\\n\\nFinally, create a `DataCollator` class to pad the labels in each batch to the maximum length, a...\"],[\"```\\n\\n## Train\\n\\nNow that the dataset is ready, you can turn your attention to the model. Start by loa...\"],[\"```\\n\\nLet's also apply LoRA to the training to make it even more efficient. Load a [`~peft.LoraConfig...\"],[\"```\\n\\nNow you're ready to define some training hyperparameters in the [`~transformers.Seq2SeqTraining...\"],[\"```\\n\\nIt is also a good idea to write a custom [`~transformers.TrainerCallback`] to save model checkp...\"],[\"```\\n\\n## Evaluate\\n\\n[Word error rate](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fevaluate-metric\\u002fwer) (WER) is a co...\"],[\"```\\n\\nWrite a loop to evaluate the model performance. Set the model to evaluation mode first, and wri...\"],[\"```\\n\\n## Share model\\n\\nOnce you're happy with your results, you can upload your model to the Hub with ...\"],[\"```\\n\\nLoad an audio sample (you can listen to it in the [Dataset Preview](https:\\u002f\\u002fhuggingface.co\\u002fdata...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Load a dataset\\n\\nTo ensure that this example runs within a reasonable time frame, here we are...\"],[\"```\\n\\n## Prepare datasets for training and evaluation\\n\\nNext, load the SegFormer image processor to pr...\"],[\"```\\n\\nFinally, combine everything in two functions that you'll use to transform training and validati...\"],[\"```\\n\\n## Create evaluation function\\n\\nIncluding a metric during training is helpful for evaluating you...\"],[\"metrics.update({f\\\"accuracy_{id2label[i]}\\\": v for i, v in enumerate(per_category_accuracy)})\\n        ...\"],[\"```\\n\\n## Load a base model \\n\\nBefore loading a base model, let's define a helper function to check the...\"],[\"```\\n\\nAt this point you can check with the `print_trainable_parameters` helper function that all 100%...\"],[\"```\\n\\nLet's review the `LoraConfig`. To enable LoRA technique, we must define the target modules with...\"],[\"When all is configured, and the base model is wrapped, the `print_trainable_parameters` helper funct...\"],[\"```\\n\\nThis confirms that only the LoRA parameters appended to the attention blocks and the `decode_he...\"],[\"```\\n\\n## Save the model and run inference\\n\\nUse the `save_pretrained()` method of the `lora_model` to ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nNext, visualize the results.  We need a color palette for this. Here, we use ade_palette(). As ...\"],[\"```\\n\\nAs you can see, the results are far from perfect, however, this example is designed to illustra...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" href=\\\".\\u002fc...\"],[\"\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fstevhliu-peft-methods.hf.space\\\"\\n\\tframeborder=\\\"0\\\"\\n\\twidth=\\\"850\\\"\\n\\theight=\\\"620\\\"\\n\\u003e\\u003c...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"[Prompt tuning](https:\\u002f\\u002fhf.co\\u002fpapers\\u002f2104.08691) was developed for text classification tasks on T5 m...\"],[\"The main difference is that the prefix parameters are inserted in **all** of the model layers, where...\"],[\"The results suggest that P-tuning is more efficient than manually crafting prompts, and it enables G...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocument...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Fine-tuning large pre-trained language models on downstream tasks ...\"],[\"Training PEFT models with new tokens being added to the embedding layers and tokenizer\\n\\nIn this exam...\"],[\"```\\n\\n## Prepare Model and Tokenizer\\n\\nNow, we will be adding 27 new tokens as well as replace the exi...\"],[\"```\\n\\nWe will be finetuning Mistral-7B model. Let's load the tokenizer and add the special tokens fol...\"],[\"```\\n\\n## Preapre Dataset\\n\\n\\n```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"smang...\"],[\"def preprocess_function(examples):\\n    batch_size = len(examples[text_column])\\n    targets = [str(x)...\"],[\"\\\"attention_mask\\\"\\n        ][i]\\n        labels[\\\"input_ids\\\"][i] = [-100] * (max_length - len(sample_inp...\"],[\"processed_datasets = dataset.map(\\n    preprocess_function,\\n    batched=True,\\n    num_proc=1,\\n    rem...\"],[\"```\\n\\n\\n```python\\ntrain_dataset\\n```\\n\\n\\n```python\\ntrain_dataloader = DataLoader(\\n    train_dataset, shuf...\"],[\"```\\n\\n# Check the model output on a sample from evaluation dataset\\n\\n\\n```python\\nimport random\\n\\ni = ran...\"],[\"```\\n\\n# Check the model loading is working as expected and generating plausible outputs.\\n\\n\\n```python\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## PromptEncoderConfig\\n\\n[[autodoc]] tuners.p_tuning.config.PromptEncoderConfig\\n\\n## PromptEncoder\\n\\n[[...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"For example, load a base model and then load the [artificialguybr\\u002f3DRedmond-V1](https:\\u002f\\u002fhuggingface....\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fybelkada\\u002fdocume...\"],[\"```\\n\\nLearn more about how PEFT supports Diffusers in the [Inference with PEFT](https:\\u002f\\u002fhuggingface.c...\"],[\"```\\n\\nIf you're interested in comparing or using more than one adapter, you can also call the [`~Peft...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nNavigate to the directory containing the training scripts for fine-tuning Dreambooth with LoRA:...\"],[\"```\\n\\nHere: \\n- `INSTANCE_DIR`: The directory containing the images that you intend to use for trainin...\"],[\"Here's what the full set of script arguments may look like:\\n\\n```bash\\naccelerate launch train_dreambo...\"],[\"```\\n\\nIf you are running this script on Windows, you may need to set the `--num_dataloader_workers` t...\"],[\"```\\n\\nNext, add a function that will create a Stable Diffusion pipeline for image generation. It will...\"],[\"```\\n\\nNow you can use the function above to create a Stable Diffusion pipeline using the LoRA weights...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\nTo switch between adapters, write a function that uses `set_adapter()` method of `PeftModel` (s...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"``python\\nfrom transformers import AutoModelForSeq2SeqLM\\nfrom peft import get_peft_config, get_peft_m...\"],[\"```\\n\\n\\n```python\\n# loading dataset\\ndataset = load_dataset(\\\"financial_phrasebank\\\", \\\"sentences_allagree...\"],[\"```\\n\\n\\n```python\\n# data preprocessing\\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\\n\\n...\"],[\"```\\n\\n\\n```python\\n# training and evaluation\\nmodel = model.to(device)\\n\\nfor epoch in range(num_epochs):\\n...\"],[\"```\\n\\n\\n```python\\n# print accuracy\\ncorrect = 0\\ntotal = 0\\nfor pred, true in zip(eval_preds, dataset[\\\"va...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ni = 107\\ninputs = tokenizer(dataset[\\\"validation\\\"][text_column][i], retur...\"],[\"``python\\nfrom transformers import AutoModelForSeq2SeqLM\\nfrom peft import get_peft_config, get_peft_m...\"],[\"```\\n\\n\\n```python\\n# data preprocessing\\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\\n\\n...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ni = 13\\ninputs = tokenizer(dataset[\\\"validation\\\"][text_column][i], return...\"],[\"``python\\nfrom transformers import AutoModelForCausalLM\\nfrom peft import get_peft_config, get_peft_mo...\"],[\"```\\n\\n\\n```python\\n# data preprocessing\\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\\ni...\"],[\"def preprocess_function(examples):\\n    batch_size = len(examples[text_column])\\n    inputs = [f\\\"{text...\"],[\"max_length - len(sample_input_ids)\\n        ) + sample_input_ids\\n        model_inputs[\\\"attention_mask...\"],[\"processed_datasets = dataset.map(\\n    preprocess_function,\\n    batched=True,\\n    num_proc=1,\\n    rem...\"],[\"```\\n\\n\\n```python\\ndef test_preprocess_function(examples):\\n    batch_size = len(examples[text_column])\\n...\"],[\"```\\n\\n\\n```python\\nlen(test_dataloader)\\n```\\n\\n\\n```python\\nnext(iter(test_dataloader))\\n```\\n\\n\\n```python\\n# c...\"],[\"```\\n\\n\\n```python\\n# training and evaluation\\nmodel = model.to(device)\\n\\nfor epoch in range(num_epochs):\\n...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ni = 16\\ninputs = tokenizer(f'{text_column} : {dataset[\\\"test\\\"][i][\\\"Tweet ...\"],[\"```\\n- Or save model locally\\n```python\\npeft_model_id = f\\\"{dataset_name}_{model_name_or_path}_{peft_co...\"],[\"```\\n\\n\\n```python\\nmodel.to(device)\\nmodel.eval()\\ni = 4\\ninputs = tokenizer(f'{text_column} : {dataset[\\\"t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"LoftQ: LoRA-fine-tuning-aware Quantization\\n\\n## Introduction\\n\\nLoftQ finds quantized LoRA initializati...\"],[\"```\\n\\n## LoftQ DIY\\n\\n### Apply LoftQ and save\\nWe provide [quantize_save_load.py](quantize_save_load.py...\"],[\"```\\n\\nThe above commands end up with creating the model directory under `$SAVE_DIR`. \\nSpecifically, t...\"],[\"```\\n\\n## LoftQ Fine-tuning\\n\\nWe also provide an example to fine-tune LoftQ on GSM8K. \\nWe load the quan...\"],[\"Finetuning Whisper-large-V2 on Colab using PEFT-Lora + BNB INT8 training\\n\\nIn this Colab, we present ...\"],[\"```\\n\\n\\n```python\\n# Select CUDA device index\\nimport os\\n\\nos.environ[\\\"CUDA_VISIBLE_DEVICES\\\"] = \\\"0\\\"\\nmodel...\"],[\"```\\n\\n### Prepare Data\\n\\n\\n```python\\nprint(common_voice[\\\"train\\\"][0])\\n```\\n\\nSince \\nour input audio is sam...\"],[\"```\\n\\nWe can apply the data preparation function to all of our training examples using dataset's `.ma...\"],[\"```\\n\\n\\n```python\\ncommon_voice[\\\"train\\\"]\\n```\\n\\n## Training and Evaluation\\n\\n### Define a Data Collator\\n\\n\\n...\"],[\"```\\n\\nLet's initialise the data collator we've just defined:\\n\\n\\n```python\\ndata_collator = DataCollator...\"],[\"```\\n\\n### Load a Pre-Trained Checkpoint\\n\\nNow let's load the pre-trained Whisper `small` checkpoint. A...\"],[\"```\\n\\n### Apply LoRA\\n\\nHere comes the magic with `peft`! Let's load a `PeftModel` and specify that we ...\"],[\"```\\n\\n**Few Important Notes:**\\n1. `remove_unused_columns=False` and `label_names=[\\\"labels\\\"]` are requ...\"],[\"```\\n\\n\\n```python\\ntrainer.train()\\n```\\n\\n\\n```python\\nmodel_name_or_path = \\\"openai\\u002fwhisper-large-v2\\\"\\npeft_...\"],[\"```\\nwithout normalizer: 'स्विच्चान नरुवित्तीची पद्दत मोठ्या प्रमाणात आमलात आणल्या बसोन या दुपन्याने ...\"],[\"```\\nPost fixing this bug, we report the 2 metrics for the top model of the leaderboard and the PEFT ...\"],[\"```\\n\\n\\n```python\\nfrom torch.utils.data import DataLoader\\nfrom tqdm import tqdm\\nimport numpy as np\\nimp...\"],[\"```\\nThe model 'PeftModel' is not supported for . Supported models are ['SpeechEncoderDecoderModel', ...\"],[\"```\\n\\n\\n```python\\nimport torch\\nimport gradio as gr\\nfrom transformers import (\\n    AutomaticSpeechRecog...\"],[\"iface = gr.Interface(\\n    fn=transcribe,\\n    inputs=gr.Audio(source=\\\"microphone\\\", type=\\\"filepath\\\"),\\n...\"],[\"``python\\nimport argparse\\nimport gc\\nimport hashlib\\nimport itertools\\nimport logging\\nimport math\\nimport...\"],[\"```\\n\\n\\n```python\\ndef get_lora_sd_pipeline(\\n    ckpt_dir, base_model_name_or_path=None, dtype=torch.fl...\"],[\"if dtype in (torch.float16, torch.bfloat16):\\n        pipe.unet.half()\\n        pipe.text_encoder.half...\"],[\"if os.path.exists(text_encoder_sub_dir):\\n        if isinstance(pipe.text_encoder, PeftModel):\\n      ...\"],[\"```\\n\\n\\n```python\\n%%time\\npipe = get_lora_sd_pipeline(os.path.join(base_path, \\\"dog_dreambooth_updated\\\")...\"],[\"```\\n\\n\\n```python\\nset_adapter(pipe, adapter_name=\\\"toy\\\")\\nprompt = \\\"superman rendered in the style of \\u003c1...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Configuration\\n\\nStart by running the following command to [create a DeepSpeed configuratio...\"],[\"```\\n\\nYou'll be asked a few questions about your setup, and configure the following arguments. In thi...\"],[\"```\\n\\nAn example [configuration file](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fpeft\\u002fblob\\u002fmain\\u002fexamples\\u002fconditio...\"],[\"```\\n\\n## The important parts\\n\\nLet's dive a little deeper into the script so you can see what's going ...\"],[\"```\\n\\nThroughout the script, you'll see the [`~accelerate.Accelerator.main_process_first`] and [`~acc...\"],[\"```\\n\\nInside the training loop, the usual `loss.backward()` is replaced by 🤗 Accelerate's [`~accelera...\"],[\"```\\n\\nYou'll see some output logs that track memory usage during training, and once it's completed, t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## MultitaskPromptTuningConfig\\n\\n[[autodoc]] tuners.multitask_prompt_tuning.config.MultitaskPromptTun...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThis should finish much quicker and allow faster iteration. Before creating the PR, however, pl...\"],[\"## Adding a new fine-tuning method\\n\\nNew parameter-efficient fine-tuning methods are developed all th...\"],[\"New features should generally be accompanied by tests and documentation or examples. Without the lat...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*In this work, we explore \\\"prompt tuning\\\", a simple yet effective m...\"],[\"``python\\nfrom transformers import AutoModelForSeq2SeqLM\\nimport peft\\nfrom peft import get_peft_config...\"],[\"```\\n\\n\\n```python\\nmodel\\n```\\n\\n\\n```python\\nmodel = get_peft_model(model, peft_config)\\nmodel.print_trainab...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*We present LLaMA-Adapter, a lightweight adaption method to efficie...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe last line is necessary if you want to activate both adapters, otherwise, only the first ada...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nYou'll be asked a few questions about your setup, and configure the following arguments. For th...\"],[\"```\\n\\nFor example, your FSDP configuration file may look like the following:\\n\\n```yaml\\ncommand_file: n...\"],[\"```\\n\\n## The important parts\\n\\nLet's dig a bit deeper into the training script to understand how it wo...\"],[\"```\\n\\nThroughout the script, you'll see the [`~accelerate.Accelerator.main_process_first`] and [`~acc...\"],[\"``python\\nfrom datasets import load_dataset\\nfrom transformers import set_seed, AutoModelForSeq2SeqLM,...\"],[\"```\\n\\n\\n```python\\ndef get_sst2(split: str):\\n    examples = load_dataset(\\\"sst2\\\")[split]\\n    result_exam...\"],[\"```\\n\\n\\n```python\\nfrom typing import Tuple\\nfrom torch.utils.data import Dataset, DataLoader\\nimport tor...\"],[\"task_ids = [i[\\\"task_id\\\"] for i in batch]\\n    task_ids = torch.tensor(task_ids)\\n\\n    return {\\n       ...\"],[\"```\\n\\n## source training\\n\\n\\n```python\\nfrom torch.optim.adamw import AdamW\\nfrom transformers import get...\"],[\"```\\n\\n\\n```python\\nPOSITIVE_TOKEN_ID = tokenizer(\\\" positive\\\", add_special_tokens=False)[\\\"input_ids\\\"][0]...\"],[\"val_loss, f1 = evaluate(model, val)\\nprint(\\n    f\\\"\\\"\\\"\\nbefore source training\\nval loss = {val_loss}\\nf1 ...\"],[\"```\\n\\n## target training\\n\\n\\n```python\\ntrain = DataLoader(MyDataset(\\\"train\\\", \\\"target\\\"), shuffle=True, b...\"],[\"```\\n\\n\\n```python\\noptimizer = AdamW(model.parameters(), lr=1e-4)\\nscheduler = get_cosine_schedule_with_...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThere is also an option to set `init_lora_weights=False` which is useful for debugging and test...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nLearn more about how PEFT works with quantization in the [Quantization](quantization) gu...\"],[\"```\\n\\nIf you need to keep a copy of the weights so you can unmerge the adapter later or delete and lo...\"],[\"```\\n\\n## Load adapters\\n\\nAdapters can be loaded onto a pretrained model with [`~PeftModel.load_adapter...\"],[\"``python\\n!pip install -q git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers.git\\n!pip install -q git+htt...\"],[\"```\\n\\n\\n```python\\nmodel\\n```\\n\\n\\n```python\\nmodel.to(\\\"cuda\\\")\\n```\\n\\n\\n```python\\nimport torch\\n\\ndevice = \\\"cuda\\\"...\"],[\"```\\n\\n\\n```python\\ninstruction = \\\"Tell me about alpacas.\\\"\\n\\nprint(evaluate(instruction))\\n```\\n\\n\\n```python...\"],[\"``python\\nfrom transformers import AutoModelForSeq2SeqLM\\nfrom peft import PeftModel, PeftConfig\\nimpor...\"],[\"```\\n\\n\\n```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"ought\\u002fraft\\\", dataset_name...\"],[\"```\\n\\n\\n```python\\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\\ntarget_max...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ni = 15\\ninputs = tokenizer(f'{text_column} : {dataset[\\\"test\\\"][i][\\\"Tweet ...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ntest_preds = []\\n\\nfor _, batch in enumerate(tqdm(test_dataloader)):\\n    ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## PEFT configurations\\n\\n\\u003cTip\\u003e\\n\\nLearn more about the parameters you can configure for each PEFT metho...\"],[\"```\\n\\nYou can create your own configuration for training by initializing a [`LoraConfig`].\\n\\n```py\\nfro...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## PEFT models\\n\\nWith a PEFT configuration in hand, you can now apply ...\"],[\"```\\n\\nTo load a [`PeftModel`] for inference, you'll need to provide the [`PeftConfig`] used to create...\"],[\"```\\n\\nTake a look at the [AutoPeftModel](package_reference\\u002fauto_class) API reference to learn more ab...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"[[autodoc]] PeftModelForFeatureExtraction\\n    - all\\n\\n## Utilities\\n\\n[[autodoc]] get_peft_model\\n\\n[[aut...\"],[\"Fine-tune FLAN-T5 using `bitsandbytes`, `peft` & `transformers` 🤗 \\n\\nIn this notebook we will see how...\"],[\"```\\n\\n## Prepare model for training\\n\\nSome pre-processing needs to be done before training such an int...\"],[\"```\\n\\nAs you can see, here we are only training 0.6% of the parameters of the model! This is a huge m...\"],[\"```\\n\\nLet's also apply some pre-processing of the input data, the labels needs to be pre-processed, t...\"],[\"```\\n\\n## Train our model! \\n\\nLet's now train our model, run the cells below.\\nNote that for T5 since so...\"],[\"```\\n\\n\\n```python\\nmodel.push_to_hub(\\\"ybelkada\\u002fflan-t5-large-financial-phrasebank-lora\\\", use_auth_token...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"```\\n\\n\\n```python\\nif any(k in model_name_or_path for k in (\\\"gpt\\\", \\\"opt\\\", \\\"bloom\\\")):\\n    padding_side =...\"],[\"```\\n\\n\\n```python\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur...\"],[\"```\\n\\n## Load adapters from the Hub\\n\\nYou can also directly load adapters from the Hub using the comma...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nInstall all the necessary required libraries with:\\n\\n```bash\\npip install -r requirements.txt\\n```...\"],[\"```\\n\\nHere's what a full set of script arguments may look like when running in Colab on a V100 GPU wi...\"],[\"```\\n\\n## Dataset for semantic similarity\\n\\nThe dataset we'll be using is a small subset of the [esci-d...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumen...\"],[\"return embeddings\\n\\n    def mean_pooling(self, model_output, attention_mask):\\n        token_embedding...\"],[\"```\\n\\nThe `get_cosine_embeddings` function computes the cosine similarity and the `get_loss` function...\"],[\"## Inference\\n\\nLet's go! Now we have the model, we need to create a search index of all the products ...\"],[\"1. Get a list of ids to products which we can call `ids_to_products_dict`:\\n\\n```bash\\n{0: 'RamPro 10\\\" ...\"],[\"```\\n\\n2. Use the trained [smangrul\\u002fpeft_lora_e5_ecommerce_semantic_search_colab](https:\\u002f\\u002fhuggingface....\"],[\"```\\n\\n3. Create a search index using HNSWlib:\\n\\n```py\\ndef construct_search_index(dim, num_elements, da...\"],[\"```\\n\\n5. Let's test it out with the query `deep learning books`:\\n\\n```py\\nquery = \\\"deep learning books\\\"...\"],[\"```\\n\\nOutput:\\n\\n```bash\\nquery='deep learning books'\\ncosine_sim_score=0.95 product='Deep Learning (The ...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"```\\n\\n\\n```python\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur...\"],[\"```\\n\\n## Load adapters from the Hub\\n\\nYou can also directly load adapters from the Hub using the comma...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## Setup\\n\\nTo get started, import 🤗 Transformers to create the base model, 🤗 Datasets to load a ...\"],[\"```\\n\\nNow you can use the `metric` to write a function that computes the accuracy and F1 scores. The ...\"],[\"```\\n\\nUse [`~datasets.Dataset.map`] to apply the `tokenize_function` to the dataset, and remove the u...\"],[\"```\\n\\nCreate the base `roberta-large` model from [`~transformers.AutoModelForSequenceClassification`]...\"],[\"```\\n\\nThen pass the model, `TrainingArguments`, datasets, tokenizer, data collator, and evaluation fu...\"],[\"```\\n\\n## Inference\\n\\nOnce the model has been uploaded to the Hub, anyone can easily use it for inferen...\"],[\"Dreambooth with OFT\\nThis Notebook assumes that you already ran the train_dreambooth.py script to cre...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Large text-to-image diffusion models have impressive capabilities ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Setup\\n\\nStart by defining the model and tokenizer, the dataset and the dataset columns to tra...\"],[\"```\\n\\n## Load dataset\\n\\nFor this guide, you'll load the `twitter_complaints` subset of the [RAFT](http...\"],[\"```\\n\\nCreate a `preprocess_function` to:\\n\\n1. Tokenize the input text and labels.\\n2. For each example ...\"],[\"```py\\ndef preprocess_function(examples):\\n    batch_size = len(examples[text_column])\\n    inputs = [f...\"],[\"model_inputs[\\\"attention_mask\\\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\\n     ...\"],[\"```\\n\\nUse the [`~datasets.Dataset.map`] function to apply the `preprocess_function` to the entire dat...\"],[\"```\\n\\n## Train\\n\\nYou're almost ready to setup your model and start training!\\n\\nInitialize a base model ...\"],[\"```\\n\\nMove the model to the GPU, then write a training loop to start training!\\n\\n```py\\nmodel = model.t...\"],[\"```\\n\\nUse the [`~transformers.PreTrainedModel.push_to_hub`] function to upload your model to a model ...\"],[\"```\\n\\nPut the model on a GPU and *generate* the predicted label:\\n\\n```py\\nmodel.to(device)\\n\\nwith torch....\"],[\"``python\\nfrom transformers import AutoModelForCausalLM\\nfrom peft import get_peft_config, get_peft_mo...\"],[\"```\\n\\n\\n```python\\nlen(test_dataloader)\\n```\\n\\n\\n```python\\nnext(iter(test_dataloader))\\n```\\n\\n\\n```python\\n# c...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ni = 33\\ninputs = tokenizer(f'{text_column} : {dataset[\\\"test\\\"][i][\\\"Tweet ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## PrefixTuningConfig\\n\\n[[autodoc]] tuners.prefix_tuning.config.PrefixTuningConfig\\n\\n## PrefixEncoder\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Authenticate to share your model\\n\\nTo share the fine-tuned model at the end of the training w...\"],[\"```\\n\\nThe `image_processor` contains useful information on which size the training and evaluation ima...\"],[\"```\\n\\n## Load and prepare a model\\n\\nBefore loading the model, let's define a helper function to check ...\"],[\"```\\n\\nBefore creating a `PeftModel`, you can check the number of trainable parameters in the original...\"],[\"```\\n\\nLet's unpack what's going on here.\\nTo use LoRA, you need to specify the target modules in `Lora...\"],[\"## Define training arguments\\n\\nFor model fine-tuning, use [`~transformers.Trainer`]. It accepts\\nsever...\"],[\"```\\n\\nCompared to non-PEFT methods, you can use a larger batch size since there are fewer parameters ...\"],[\"```\\n\\nIn just a few minutes, the fine-tuned model shows 96% validation accuracy even on this small\\nsu...\"],[\"```\\n\\nWhen calling [`~transformers.PreTrainedModel.push_to_hub`] on the `lora_model`, only the LoRA p...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fsayakpaul\\u002fsampl...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nInstalling PEFT from source is useful for keeping up with the latest developments:\\n\\n```bash\\npyt...\"],[\"```\\n\\n### Randomly initialized layers\\n\\nFor some tasks, it is important to correctly configure `module...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"As mentioned briefly earlier, [LoRA](https:\\u002f\\u002fhf.co\\u002fpapers\\u002f2106.09685) is a technique that accelerate...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocument...\"],[\"## Low-Rank Kronecker Product (LoKr)\\n\\n[LoKr](https:\\u002f\\u002fhf.co\\u002fpapers\\u002f2309.14859) is very similar to LoR...\"],[\"OFT preserves the hyperspherical energy by learning an orthogonal transformation for neurons to keep...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocument...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"```\\n\\n\\n```python\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur...\"],[\"```\\n\\n## Load adapters from the Hub\\n\\nYou can also directly load adapters from the Hub using the comma...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"```\\n\\n\\n```python\\nif any(k in model_name_or_path for k in (\\\"gpt\\\", \\\"opt\\\", \\\"bloom\\\")):\\n    padding_side =...\"],[\"```\\n\\n\\n```python\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur...\"],[\"```\\n\\n## Load adapters from the Hub\\n\\nYou can also directly load adapters from the Hub using the comma...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Few-shot in-context learning (ICL) enables pre-trained language mo...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## Setup\\n\\nLet's start by importing all the necessary libraries you'll need:\\n\\n- 🤗 Transformers f...\"],[\"```\\n\\nThe `tags` values are defined in the label ids [dictionary](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftne...\"],[\"```\\n\\nNow you can write an evaluation function to compute the metrics from the model predictions and ...\"],[\"```\\n\\nYou'll also need to write a function to:\\n\\n1. Map each token to their respective word with the [...\"],[\"```\\n\\nFinally, create a data collator to pad the examples to the longest length in a batch:\\n\\n```py\\nda...\"],[\"```\\n\\nDefine the [`LoraConfig`] with:\\n\\n- `task_type`, token classification (`TaskType.TOKEN_CLS`)\\n- `...\"],[\"```\\n\\nFrom the 🤗 Transformers library, create a [`~transformers.TrainingArguments`] class and specify...\"],[\"```\\n\\n## Inference\\n\\nTo use your model for inference, load the configuration and model:\\n\\n```py\\npeft_mo...\"],[\"```\\n\\nPass the inputs to the model, and print out the model prediction for each token:\\n\\n```py\\nwith to...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Being similar to LoRA, IA3 carries many of the same advantages: \\n\\n* IA3 makes fine-tuning more effic...\"],[\"`IA3Config` allows you to control how IA3 is applied to the base model through the following paramet...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## LoraConfig\\n\\n[[autodoc]] tuners.lora.config.LoraConfig\\n\\n## LoraModel\\n\\n[[autodoc]] tuners.lora.mode...\"],[\"``python\\nimport os\\n\\nimport torch\\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, defa...\"],[\"```\\n\\n\\n```python\\n# data preprocessing\\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\\nt...\"],[\"```\\n\\n\\n```python\\n# optimizer and lr scheduler\\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ni = 107\\ninput_ids = tokenizer(dataset[\\\"validation\\\"][text_column][i], re...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"``python\\nimport argparse\\nimport json\\nimport logging\\nimport math\\nimport os\\nimport random\\nfrom pathlib...\"],[\"```\\n\\n\\n```python\\nclass AutoModelForSentenceEmbedding(nn.Module):\\n    def __init__(self, model_name, t...\"],[\"```\\n\\n\\n```python\\nmodel_name_or_path = \\\"intfloat\\u002fe5-large-v2\\\"\\npeft_model_id = \\\"smangrul\\u002fpeft_lora_e5_s...\"],[\"```\\n\\n\\n```python\\n# base model\\nmodel = AutoModelForSentenceEmbedding(model_name_or_path, tokenizer)\\n\\n#...\"],[\"```\\n\\n\\n```python\\ndef construct_search_index(dim, num_elements, data):\\n    # Declaring index\\n    searc...\"],[\"```\\n\\n\\n```python\\nquery = \\\"NLP and ML books\\\"\\nk = 10\\nquery_embeddings = get_query_embeddings(query, mod...\"],[\"``python\\nfrom transformers import AutoModelForCausalLM\\nfrom peft import PeftModel, PeftConfig\\nimport...\"],[\"processed_datasets = dataset.map(\\n    preprocess_function,\\n    batched=True,\\n    num_proc=1,\\n    rem...\"],[\"```\\n\\n\\n```python\\ndef test_preprocess_function(examples):\\n    batch_size = len(examples[text_column])\\n...\"],[\"```\\n\\nYou can load model from hub or local\\n\\n- Load model from Hugging Face Hub, you can change to you...\"],[\"```\\n\\n\\n```python\\n# model\\n```\\n\\n\\n```python\\nmodel.hf_device_map\\n```\\n\\n\\n```python\\nmodel.eval()\\ni = 89\\ninpu...\"],[\"```\\n\\n\\n```python\\nmodel.eval()\\ntest_preds = []\\n\\nfor _, batch in enumerate(tqdm(test_dataloader)):\\n    ...\"],[\"Fine-tuning for semantic segmentation using LoRA and 🤗 PEFT\\n\\n[![Open In Colab](https:\\u002f\\u002fcolab.researc...\"],[\"Using PEFT with custom models\\n\\n`peft` allows us to fine-tune models efficiently with LoRA. In this s...\"],[\"```\\n\\n## Model\\n\\nAs a model, we use a simple multilayer perceptron (MLP). For demonstration purposes, ...\"],[\"```\\n\\n\\n```python\\ndef train(model, optimizer, criterion, train_dataloader, eval_dataloader, epochs):\\n ...\"],[\"```\\n\\n\\n```python\\n%time train(module, optimizer, criterion, train_dataloader, eval_dataloader, epochs=...\"],[\"```\\n\\nNow let's create the `peft` model by passing our initial MLP, as well as the config we just def...\"],[\"```\\n\\n\\n```python\\nparams_before = dict(module_copy.named_parameters())\\nfor name, param in peft_model.b...\"],[\"```\\n\\n\\n```python\\npeft_model.push_to_hub(model_id);\\n```\\n\\nAs we can see, the adapter size is only 211 k...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThis is a straightforward multilayer perceptron with an input layer, a hidden layer, and an out...\"],[\"```\\n\\nWith that, we can create our PEFT model and check the fraction of parameters trained:\\n\\n```pytho...\"],[\"```\\n\\nThis will print a very long list, we'll only show the first few:...\"],[\"```\\n[('', timm.models.metaformer.MetaFormer),\\n ('stem', timm.models.metaformer.Stem),\\n ('stem.conv',...\"],[\"('stages.0.blocks.0.mlp', timm.layers.mlp.Mlp),\\n ('stages.0.blocks.0.mlp.fc1', torch.nn.modules.conv...\"],[\"...\\n ('head.global_pool.flatten', torch.nn.modules.linear.Identity),\\n ('head.norm', timm.layers.norm...\"],[\"```\\n\\nUpon closer inspection, we see that the 2D conv layers have names such as `\\\"stages.0.blocks.0.m...\"],[\"```\\n\\nThis shows us that we only need to train less than 2% of all parameters, which is a huge effici...\"],[\"```\\n\\nIf that doesn't help, check the existing modules in your model architecture with the `named_mod...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\nfrom peft import LoraConfig, TaskType\\n\\npeft_config = LoraConfig(task_type=TaskType.SEQ_2_S...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nSee the [`LoraConfig`] reference for more details about other parameters you can adjust,...\"],[\"```\\n\\nOut of [bigscience\\u002fmt0-large's](https:\\u002f\\u002fhuggingface.co\\u002fbigscience\\u002fmt0-large) 1.2B parameters, y...\"],[\"```\\n\\nYou can also save your model to the Hub (make sure you're logged in to your Hugging Face accoun...\"],[\"```\\n\\nBoth methods only save the extra PEFT weights that were trained, meaning it is super efficient ...\"],[\"model = model.to(\\\"cuda\\\")\\nmodel.eval()\\ninputs = tokenizer(\\\"Preheat the oven to 350 degrees and place ...\"],[\"```\\n\\nFor other tasks that aren't explicitly supported with an `AutoPeftModelFor` class - such as aut...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"1. LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2106.09685)\\n2. P...\"],[\"10. LoftQ: [LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models](https:\\u002f\\u002farxiv.org\\u002f...\"],[\"## Getting started\\n\\n```python\\nfrom transformers import AutoModelForSeq2SeqLM\\nfrom peft import get_pe...\"],[\"```\\n\\n## Use Cases\\n\\n### Get comparable performance to full finetuning by adapting LLMs to downstream ...\"],[\"Performance of PEFT-LoRA tuned [`bigscience\\u002fT0_3B`](https:\\u002f\\u002fhuggingface.co\\u002fbigscience\\u002fT0_3B) on [`ou...\"],[\"GPU memory required by different settings during training is given below. The final checkpoint size ...\"],[\"accelerate launch train_dreambooth.py \\\\\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\\\\n  --instanc...\"],[\"```\\n\\nTry out the 🤗 Gradio Space which should run seamlessly on a T4 instance:\\n[smangrul\\u002fpeft-lora-sd...\"],[\"### INT8 training of large models in Colab using PEFT LoRA and bitsandbytes\\n\\n- Here is now a demo on...\"],[\"An example of using LoRA for the task of adapting `LayoutLMForTokenClassification` on `FUNSD` datase...\"],[\"### Example of PEFT model training using 🤗 Accelerate's DeepSpeed integration\\n\\nDeepSpeed version req...\"],[\"```\\n  b. run the below command to launch the example script\\n  ```bash\\n  accelerate launch --config_f...\"],[\"```\\n\\n### Example of PEFT model inference using 🤗 Accelerate's Big Model Inferencing capabilities\\nAn ...\"],[\"### Sequence Classification\\n|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | ...\"],[\"### Image Classification\\n\\n|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | IA...\"],[\"## Caveats:\\n\\n1. Below is an example of using PyTorch FSDP for training. However, it doesn't lead to ...\"],[\"```\\n\\n  Example of parameter efficient tuning with [`mt0-xxl`](https:\\u002f\\u002fhuggingface.co\\u002fbigscience\\u002fmt0-...\"],[\"```\\n\\n2. When using ZeRO3 with zero3_init_flag=True, if you find the gpu memory increase with trainin...\"],[\"```\\n\\nLearn more about the [low level API in the docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fpeft\\u002fdeveloper_gui...\"],[\"```\\n\\n## Contributing\\n\\nIf you would like to contribute to PEFT, please check out our [contributing gu...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"```\\n\\n\\n```python\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur...\"],[\"```\\n\\n## Load adapters from the Hub\\n\\nYou can also directly load adapters from the Hub using the comma...\"]],\"hovertemplate\":\"source=peft\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"peft, circle\",\"marker\":{\"color\":\"#FFA15A\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"peft, circle\",\"showlegend\":true,\"x\":[1.2047689,-1.0977739,1.2600243,-10.989114,-5.5166965,-10.597703,5.6593223,5.9772496,5.8089957,5.8222995,1.4275138,0.28249538,-1.804193,0.64309365,-4.4260283,1.132476,-0.1915197,-0.7563392,0.3719413,0.35049126,0.5396012,0.36932138,1.0832783,-1.266823,-3.4311752,-2.5540712,0.64514047,0.77697796,0.21442142,-3.5566707,-0.33362436,0.6679015,0.37152126,0.4966823,0.662942,1.5144771,1.1499578,1.1150177,1.1171756,-0.43437326,1.4333348,-0.58115864,0.12456005,1.2556434,-3.6094916,2.4764605,-6.8731437,-6.2610483,-7.387691,-7.451926,-1.3166269,-0.9059576,0.3599385,-0.110713884,-0.057539754,0.016010933,-0.49167413,1.5032083,-6.932543,-1.3611809,1.6673206,0.82986826,0.67055064,0.24768856,0.6387682,-0.1870613,0.8354885,0.343686,0.768584,0.66800976,0.6301829,0.84368205,1.1955342,-1.5342517,-1.2494694,6.843245,-1.7586151,6.8636594,-4.8553805,-4.79509,-4.864596,-4.761902,7.442146,-2.0005326,-4.951317,0.4723495,-2.5136027,-1.659974,0.53494054,-1.0328453,-1.3504926,-0.45623645,0.12594008,-0.67838115,1.0891908,-5.0651307,-10.476049,-0.47947195,0.41755128,0.3750494,0.89160913,1.3687434,-1.2217115,0.3870566,-0.002892771,0.722208,0.44837144,0.46071714,0.47753584,0.21271364,1.2866739,0.33302072,1.5392103,0.42531446,-0.9833483,0.60203475,0.31865194,-0.87051904,1.5709107,-0.9521497,-1.0618875,1.6152021,-1.7797748,-1.0624055,-1.1203464,-0.37088028,-0.872803,0.98480695,0.50706357,2.635792,0.9694934,-1.0184693,5.247805,-2.398981,-2.8539155,0.45386592,-3.027903,-7.553137,-6.3788013,-7.502809,0.059010893,-1.7103038,-0.2172362,-1.1959583,0.6149418,-0.1562723,0.8008857,-3.7893116,-3.161498,-1.0502342,0.80175203,11.038163,9.887942,0.97569627,1.3724144,0.9516086,1.3579059,0.82365173,0.88997364,-1.0521593,0.6572171,-0.4123261,0.009421484,0.5974327,0.4421828,0.34714815,-0.7106661,5.5863557,-4.862338,-10.551432,5.7541776,1.6848809,4.629568,4.342559,-3.6627896,-4.869046,1.573604,-0.32762542,-4.807085,-4.3707304,1.3698337,1.4533981,-0.41270494,-0.6894463,0.19349684,0.63785094,0.48071554,1.5288153,-1.3009166,-0.7821766,-0.53823817,0.4636305,-0.98474246,0.429987,0.6746928,0.6885009,-0.31459793,0.3045686,0.05572678,1.2591345,1.3922174,2.745949,-1.2502191,0.40265173,1.572774,0.9250707,-0.938063,-0.94609016,-0.91098726,0.21560645,1.0499151,0.6835116,1.1284922,1.1445998,1.119462,-1.7755784,1.3899868,1.1460376,-3.2607067,0.7451035,-1.9305676,-1.3042192,0.48725256,1.3339579,1.4536469,-0.915029,1.1215386,1.6390538,-1.1677202,0.86479205,0.2657809,-3.4277315,-4.421934,-1.3722358,-1.4448348,-1.3083454,4.7673497,-0.39022923,-0.28026113,-0.19426396,-0.06937314,1.4045901,1.038155,1.4984764,-4.032105,0.5823062,0.08643831,-0.8297959,1.0251615,0.87259984,0.88493794,0.17834844,-1.7912879,-2.0753038,-4.747062,1.1978352,-0.14442672,-1.0090221,-1.0954313,-1.0919062,-0.3021417,0.86043805,0.9020321,1.233111,-1.2834542,1.5751791,0.9136978,2.8143466,-4.793542,-10.622863,-0.7407129,0.40932983,0.90519315,-0.03585439,0.85865617,0.3124405,-0.13656315,-0.0701607,-0.5740078,0.98289675,0.46990508,4.4616003,1.1651671,-0.73601997,-0.8598868,-1.2541201,-1.3021272,-1.4505217,-1.7499384,-4.7309623,1.5848783,1.0088732,1.5108978,0.915665,1.4153163,-0.9169246,1.0346198,1.5374106,-8.4830885,-4.969982,-1.2276824,0.8055608,-0.9042648,-0.28882474,-1.5115572,-1.1340271,0.3075993,1.0567716,0.98251086,-0.91284317,-1.7244462,-1.7154593,-0.41034573,-5.4032407,-10.633766,1.5481743,-0.8748921,0.26607955,-0.81706125,-1.2748196,1.7758471,-1.31589,-0.68275917,0.9498126,-0.4826549,-0.25759104,1.6228744,-0.4501311,-0.9435496,1.5537088,-0.6664626,-0.93173844,-1.3895271,0.47329825,0.68532616,0.5711628,0.5101837,0.9168453,1.1216187,1.5050879,-1.4011968,-0.03781556,0.82321304,-0.005451117,-0.2512584,-0.28211156,-0.21046567,-0.10005766,0.35329387,-1.3178267,-0.60085875,1.2388605,0.8700117,0.14556846,3.038527,1.0734584,-1.2753835,0.90580136,-1.7701771,-4.577823,-4.385432,1.0334878,-1.8504313,-1.8159182,-1.0318702,0.5324463,-1.1185654,-2.3220541,-0.700094,0.3398626,0.018327627,-1.4759744,-7.980328,-1.3175654,-0.41658512,-0.06290826,-0.11314211,1.0901374,6.0164146,1.4336909,1.1067481,1.5777969],\"xaxis\":\"x\",\"y\":[2.2164445,4.293299,1.9684662,-3.8172646,0.3492556,-3.8128383,0.03468346,-1.6112007,-2.194665,-1.4670398,-2.0220273,-0.83696735,-4.007488,-2.7053843,-0.32879066,1.6118006,-4.1059527,-4.062071,1.5515164,1.4947917,1.1694053,-0.91541255,1.4058033,4.0370817,2.8461149,2.4742026,1.0026224,2.0303733,2.916233,2.6871345,3.599665,-2.7142549,0.6819837,3.0090616,2.3002589,1.8091718,2.4912755,2.5640981,2.2649922,0.82630444,1.8176535,-4.0164404,-6.1905537,1.8478866,2.761362,0.28605258,-6.928559,-6.8063483,-7.7213426,-7.4443183,-3.8345606,1.1069427,3.2761042,0.5640315,0.45646694,-7.007077,-3.8377583,1.3916004,-6.9880548,4.374181,-3.1227973,-2.6398473,-2.9705827,-6.246315,-6.504993,0.10940768,2.6096704,3.111111,2.373834,3.0866537,3.2469044,-1.958645,-2.7709115,4.423155,3.6731124,-4.400393,3.9992049,-4.3787065,0.008287498,-0.2691621,-0.12053594,-0.11265034,-4.380428,3.9082994,-0.33182877,2.9713354,-3.7487504,-1.2218807,-4.0597935,-4.1844964,-3.1872962,-4.096675,0.4255725,-1.7989904,1.4222363,-0.5471206,-3.9174037,6.6848845,6.0908904,6.315975,2.5865147,3.166105,6.013696,4.820818,5.143645,3.104479,6.1437564,6.5108333,6.6345787,6.2845693,3.66937,7.3041177,1.7813371,-4.257099,-3.9950337,0.88692516,-1.4737893,-3.296721,1.8068347,-3.9994144,-3.4735606,1.7929881,-4.0325294,-4.258664,-3.0103073,-4.1541543,-3.969685,1.583665,0.94576627,-0.558799,1.3406082,-3.1475127,0.26268792,2.5935256,1.6585546,0.30287415,2.3953044,-6.24545,-6.893148,-7.7529187,-4.1448417,-4.023602,-6.1612105,0.097987175,2.8961663,0.09302294,1.4614106,-5.4966917,2.350466,-3.4631255,1.5205781,-5.553795,-4.9413753,8.164385,2.8072007,2.74332,3.3516855,8.029367,8.586737,2.9801276,2.990492,2.9428062,2.8999772,2.7904546,1.9406302,1.2586089,1.8616278,0.7667413,-0.212196,-3.7365324,0.19511892,0.41709906,-2.636386,-2.4386692,0.3773175,-0.30794716,1.8079027,-3.0481129,1.0575773,1.4392151,2.6935322,3.336318,2.7086914,1.9537722,2.7338622,2.825748,2.29134,1.6002866,-4.7777996,-2.9942267,-2.8481402,0.88649774,-3.5302157,0.879813,1.4222149,1.0098578,4.365065,3.002191,3.345327,3.6676738,3.172987,0.17822161,-2.53861,-2.7511418,1.7520027,-4.1963153,-4.0564966,-3.3551567,-3.5612803,2.8592842,2.7311308,2.5600703,2.3554018,2.1849897,2.2511303,3.683129,2.0227299,2.0603077,-0.5813825,2.7557375,-2.201644,-4.001994,0.42098284,1.5967923,1.7514433,-4.1634617,1.8711138,2.390624,4.5242915,2.8345091,2.657373,-2.4214902,-1.2861276,-2.6505616,3.9219322,-3.1552646,-3.4597864,-2.677148,-3.924227,-4.2039914,-4.701829,1.6699278,1.9089038,2.4155507,-0.2818905,1.2748938,-6.5988,-3.9681783,2.2999566,0.77701443,1.5416613,6.6246643,7.3536925,7.869311,-0.13295503,1.6078112,-4.2093797,-4.073832,-4.08203,-3.1451395,-3.8534167,2.0959084,0.82261807,1.8473666,-2.780308,1.696255,1.5830863,-0.6507348,-0.11305992,-3.8404465,4.4414697,-0.7019901,-2.4853177,-0.32656184,2.6039455,3.1331973,0.20903152,-5.510528,0.32438487,2.2704568,-1.9870062,0.58834416,2.3509672,-0.34135532,4.9006143,4.616384,4.820101,5.763231,3.9944046,1.1872802,1.7975948,1.8107076,2.4643836,1.424372,1.7854128,-4.028456,1.9334183,2.3282158,-1.7801977,-0.16062452,4.4353004,2.5002565,-4.630945,-6.21198,-4.4671025,-3.6221912,3.1543245,1.0568868,1.9680778,-2.7175715,3.9063601,4.029587,-0.102349274,-1.411146,-3.7895153,1.7538716,-4.0278397,1.3757352,-3.2837548,6.7946386,0.8718306,-1.878188,-3.9849863,0.973973,-3.9155083,-4.8286195,1.713117,-4.1799006,-4.036597,1.4455808,-3.078302,-3.515802,4.2971573,3.090401,2.8325818,1.1885321,3.1032815,2.622348,1.6522382,1.7814453,3.0287967,3.364087,2.883664,-4.2307854,-2.0288277,-1.6996702,-1.8247774,-2.3643808,3.1834023,-0.22715273,3.7508194,2.0955997,2.7117815,0.7912341,-0.05477072,2.40235,-2.7442324,1.8200219,3.737164,0.23366441,1.1966293,2.0806587,3.61472,3.8676872,3.8394752,3.5718102,5.42523,3.4818628,3.6866498,2.8914225,2.2537122,3.458065,0.34663472,4.5131993,2.4951022,2.8145347,2.6856372,2.970675,-2.8678644,1.8067163,1.8582364,2.3194454],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ci\\u003eHuggingface Hub के लिए आधिकारिक पायथन क्लाइंट।\\u003c\\u002fi\\u003e\\n\\u003c\\u002fp\\u003e\\n\\n\\u003cp align=\\\"center\\\"...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\u002fblob\\u002fmai...\"],[\"---\\n\\n## huggingface_hub लाइब्रेरी में आपका स्वागत है\\n\\n`huggingface_hub` लाइब्रेरी आपको [हगिंग फेस हब...\"],[\"## प्रमुख विशेषताऐं\\n\\n- [फ़ाइलें डाउनलोड करें](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002fguides\\u002f...\"],[\"```\\n\\nयदि आप चाहें, तो आप इसे [conda](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002finstallation#ins...\"],[\"```\\n\\nया एक संपूर्ण भंडार\\n\\n```py\\nfrom huggingface_hub import snapshot_download\\n\\nsnapshot_download(\\\"st...\"],[\"```\\n\\nया एक संपूर्ण फ़ोल्डर\\n\\n```py\\nfrom huggingface_hub import upload_folder\\n\\nupload_folder(\\n    fold...\"],[\"```\\n\\n[अपलोड गाइड](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002fguides\\u002fupload) में विवरण के लिए।\\n\\n#...\"],[\"फायदे ये हैं:\\n\\n- पुस्तकालयों और उनके उपयोगकर्ताओं के लिए निःशुल्क मॉडल या डेटासेट होस्टिंग।\\n- गिट-आध...\"],[\"## योगदान (सुविधा अनुरोध, बग, आदि) का अति स्वागत है 💙💚💛💜🧡❤️\\n\\nयोगदान के लिए हर किसी का स्वागत है और ह...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"# Download from a dataset\\n\\u003e\\u003e\\u003e hf_hub_download(repo_id=\\\"google\\u002ffleurs\\\", filename=\\\"fleurs.py\\\", repo_ty...\"],[\"```\\n\\n### From specific version\\n\\nBy default, the latest version from the `main` branch is downloaded....\"],[\"```\\n\\n**Note:** When using the commit hash, it must be the full-length hash instead of a 7-character ...\"],[\"```\\n\\n### Filter files to download\\n\\n[`snapshot_download`] provides an easy way to download a reposito...\"],[\"```\\n\\n## Download file(s) to local folder\\n\\nThe recommended (and default) way to download files from t...\"],[\"However, in some cases you want to download files and move them to a specific folder. This is useful...\"],[\"Here is a table that summarizes the different options to help you choose the parameters that best su...\"],[\"**Note:** if you are on a Windows machine, you need to enable developer mode or run `huggingface_hub...\"],[\"```\\n\\nYou can download multiple files at once which displays a progress bar and returns the snapshot ...\"],[\"--\\n# For reference on dataset card metadata, see the spec: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fb...\"],[\"## Uses\\n\\n\\u003c!-- Address questions around how the dataset is intended to be used. --\\u003e\\n\\n### Direct Use\\n\\n...\"],[\"\\u003c!-- This section describes the people or systems who originally created the data. It should also in...\"],[\"{{ bias_risks_limitations | default(\\\"[More Information Needed]\\\", true)}}\\n\\n### Recommendations\\n\\n\\u003c!-- ...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## Upload a folder\\n\\nUse the [`upload_folder`] function to upload a local folder to an existing ...\"],[\"```\\n\\nBy default, the `.gitignore` file will be taken into account to know which files should be comm...\"],[\"```\\n\\nYou can also use the `delete_patterns` argument to specify files you want to delete from the re...\"],[\"```\\n\\n`local_path` and `path_in_repo` are optional and can be implicitly inferred. If `local_path` is...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nBackground jobs are queued when using `run_as_future=True`. This means that you are guar...\"],[\"```\\n\\n### Upload a folder by chunks\\n\\n[`upload_folder`] makes it easy to upload an entire folder to th...\"],[\"```\\n\\nIf you want a better control on the upload strategy (i.e. the commits that are created), you ca...\"],[\"```py\\n\\u003e\\u003e\\u003e import json\\n\\u003e\\u003e\\u003e import uuid\\n\\u003e\\u003e\\u003e from pathlib import Path\\n\\u003e\\u003e\\u003e import gradio as gr\\n\\u003e\\u003e\\u003e from ...\"],[\"```\\n\\nAnd that's it! User input\\u002foutputs and feedback will be available as a dataset on the Hub. By us...\"],[\"#### Space persistence demo\\n\\nPersisting data from a Space to a Dataset on the Hub is the main use ca...\"],[\"# 2. Zip png files in a single archive\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n       ...\"],[\"```\\n\\nWhen you overwrite `push_to_hub`, you have access to the attributes of [`CommitScheduler`] and ...\"],[\"- [`CommitOperationCopy`] copies a file within a repository. This operation accepts three arguments:...\"],[\"```\\n\\n2. Pass your operations to [`create_commit`]:\\n\\n```py\\n\\u003e\\u003e\\u003e api.create_commit(\\n...     repo_id=\\\"ly...\"],[\"```\\n\\nIn addition to [`upload_file`] and [`upload_folder`], the following functions also use [`create...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nHere is a simple example illustrating how to pre-upload files:\\n\\n```py\\n\\u003e\\u003e\\u003e from huggingface_h...\"],[\"```\\n\\nFirst, we create the [`CommitOperationAdd`] objects one by one. In a real-world example, those ...\"],[\"- **Start small**: We recommend starting with a small amount of data to test your upload script. It'...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nAlthough [`Repository`] is not formally deprecated, we recommend using the HTT...\"],[\"```\\n\\nYou should install this for each repository that has a very large file. Once installed, you'll ...\"],[\"```\\n\\nYou can check the status of your push with the `command_queue` method:\\n\\n```python\\n\\u003e\\u003e\\u003e last_comm...\"],[\"```\\n\\nHowever, if you aren't ready to push a file yet, you can use [`~Repository.git_add`] and [`~Rep...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"There are many ways you can contribute to this client library:\\n* Fixing outstanding issues with the ...\"],[\"If your issue is well written we're already 80% of the way there by the time you post it!\\n\\n## Submit...\"],[\"```\\n\\n3. Create a new branch to hold your development changes, and do this for every new PR you work ...\"],[\"```\\n\\n   This command will update your code to comply with the standards of the `huggingface_hub` rep...\"],[\"```\\n\\n   Please write [good commit messages](https:\\u002f\\u002fchris.beams.io\\u002fposts\\u002fgit-commit\\u002f).\\n\\n   It is a g...\"],[\"```\\n\\n10. Once you are satisfied (**and the [checklist below](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingf...\"],[\"### Tests\\n\\nAn extensive test suite is included to test the library behavior and several examples. Li...\"],[\"```\\n\\nYou can specify a smaller set of tests in order to test only the feature you're working on.\\n\\nFo...\"],[\"# Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pled...\"],[\"## Enforcement\\n\\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\\nreported ...\"],[\"**Consequence**: A permanent ban from any sort of public interaction within\\nthe community.\\n\\n## Attri...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nIf the CLI is correctly installed, you should see a list of all the options available in the CL...\"],[\"```\\n_|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      ...\"],[\"```\\n\\nFor more details about authentication, check out [this section](..\\u002fquick-start#authentication)....\"],[\"```\\n\\n### Download a single file\\n\\nTo download a single file from a repo, simply provide the repo_id a...\"],[\"```\\n\\n### Download multiple files\\n\\nYou can also download a subset of the files from a repository with...\"],[\"```\\n\\nThe other approach is to provide patterns to filter which files you want to download using `--i...\"],[\"```\\n\\n### Download to a local folder\\n\\nThe recommended (and default) way to download files from the Hu...\"],[\"```\\n\\n### Specify a token\\n\\nTo access private or gated repositories, you must use a token. By default,...\"],[\"```\\n\\nTo upload the current directory at the root of the repo, use:\\n\\n```bash\\n\\u003e\\u003e\\u003e huggingface-cli uplo...\"],[\"```\\n\\n### Upload multiple files\\n\\nTo upload multiple files from a folder at once without uploading the...\"],[\"```\\n\\n**Note:** if `revision` does not exist and `--create-pr` is not set, a branch will be created a...\"],[\"```\\n\\n### Specify a token\\n\\nTo upload files, you must use a token. By default, the token saved locally...\"],[\"```\\n\\n## huggingface-cli scan-cache\\n\\nScanning your cache directory is useful if you want to know whic...\"],[\"```bash\\n\\u003e\\u003e\\u003e huggingface-cli scan-cache\\nREPO ID                     REPO TYPE SIZE ON DISK NB FILES L...\"],[\"Done in 0.0s. Scanned 6 repo(s) for a total of 3.4G.\\nGot 1 warning(s) while scanning. Use -vvv to pr...\"],[\"```\\n\\nFor more details about how to scan your cache directory, please refer to the [Manage your cache...\"],[\"```bash\\n\\u003e\\u003e\\u003e huggingface-cli env\\n\\nCopy-and-paste the text below in your GitHub issue.\\n\\n- huggingface_...\"],[\"Running Tests\\n\\nTo run the test suite, please perform the following from the root directory of this r...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003e\\u003e\\u003e # Read a remote file \\n\\u003e\\u003e\\u003e with fs.open(\\\"datasets\\u002fmy-username\\u002fmy-dataset-repo\\u002fdata\\u002ftrain.csv\\\", \\\"r...\"],[\"```\\n\\nThe optional `revision` argument can be passed to run an operation from a specific commit such ...\"],[\"```\\n\\nThe same workflow can also be used for [Dask](https:\\u002f\\u002fdocs.dask.org\\u002fen\\u002fstable\\u002fhow-to\\u002fconnect-to...\"],[\"```\\n\\n* Using the Hub as an array store with [Zarr](https:\\u002f\\u002fzarr.readthedocs.io\\u002fen\\u002fstable\\u002ftutorial.ht...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"[[autodoc]] huggingface_hub.DeleteCacheStrategy\\n    - expected_freed_size_str\\n\\n## Exceptions\\n\\n### Co...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Let's fetch the collection with, `\\\"TheBloke\\u002frecent-models-64f9a55bb3115b4f513ec026\\\"`:\\n\\n```py\\n\\u003e\\u003e\\u003e fro...\"],[\"```\\n\\nThe [`Collection`] object returned by [`get_collection`] contains:\\n- high-level metadata: `slug...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nWhen listing collections, the item list per collection is truncated to 4 ...\"],[\"```\\n\\nParameter `sort` must be one of  `\\\"last_modified\\\"`,  `\\\"trending\\\"` or `\\\"upvotes\\\"`. Parameter `it...\"],[\"```\\n\\n## Manage items in a collection\\n\\nNow that we have a [`Collection`], we want to add items to it ...\"],[\"```\\n\\nIf an item already exists in a collection (same `item_id`\\u002f`item_type` pair), an HTTP 409 error ...\"],[\"```\\n\\n### Reorder items\\n\\nItems in a collection are ordered. The order is determined by the `position`...\"],[\"```\\n\\n## Delete collection\\n\\nA collection can be deleted using [`delete_collection`].\\n\\n\\u003cTip warning={t...\"],[\"--\\nlanguage:\\n- en\\nlicense: mit\\nlibrary_name: pytorch-lightning\\ntags:\\n- pytorch\\n- image-classificatio...\"],[\"--\\n[]\\n---\\n\\n# invalid-card-data\\n\\nThis card should fail when trying to load it in because the card dat...\"],[\"his document covers all steps that need to be done in order to do a release of the `huggingface_hub`...\"],[\"```\\ngit checkout main\\n   ```\\n\\n9. Update the version to contain the `.dev0` suffix:\\n```\\n__version__ =...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nAt this step, your app should already be running on the Hub for free !\\nHowever, you might want ...\"],[\"```\\n\\n\\u003cTip\\u003e\\nFrom within your Space, secrets are available as environment variables (or\\nStreamlit Secr...\"],[\"```\\n\\n**4. Configure the hardware**\\n\\nBy default, your Space will run on a CPU environment for free. Y...\"],[\"```\\n```py\\n\\u003e\\u003e\\u003e api.duplicate_space(\\n...     from_id=repo_id,\\n...     hardware=\\\"cpu-upgrade\\\",\\n...     ...\"],[\"```\\n\\nNote: if you are using a 'cpu-basic' hardware, you cannot configure a custom sleep time. Your S...\"],[\"```\\n\\nYou can also delete your storage, losing all the data permanently.\\n```py\\n\\u003e\\u003e\\u003e api.delete_space_s...\"],[\"```\\n\\n## More advanced: temporarily upgrade your Space !\\n\\nSpaces allow for a lot of different use cas...\"],[\"# Space own repo_id\\nTRAINING_SPACE_ID = \\\"Wauplin\\u002fdreambooth-training\\\"\\n\\nfrom huggingface_hub import H...\"],[\"```\\n\\n### Task scheduler\\n\\nScheduling tasks can be done in many ways. Here is an example how it could ...\"],[\"def mark_as_done(task):\\n    model_id, dataset_id = task\\n    with open(_get_csv_file()) as csv_file:\\n...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\"\\n       hr...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\"\\n       hr...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\"\\n       hr...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\"\\n       hr...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"[[autodoc]] SpaceCard\\n\\n### SpaceCardData\\n\\n[[autodoc]] SpaceCardData\\n\\n## Utilities\\n\\n### EvalResult\\n\\n[...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Inference Endpoints\\n\\nInference Endpoints provides a secure production solution to easily deploy mode...\"],[\"[[autodoc]] InferenceEndpoint\\n  - from_raw\\n  - client\\n  - async_client\\n  - all\\n\\n## InferenceEndpoint...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## HfApi\\n\\n[[autodoc]] HfApi\\n\\n[[autodoc]] plan_multi_commits\\n\\n## API Dataclasses\\n\\n### AccessRequ...\"],[\"### SafetensorsFileMetadata\\n\\n[[autodoc]] huggingface_hub.utils.SafetensorsFileMetadata\\n\\n### SpaceInf...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ci\\u003eThe official Python client for the Huggingface Hub.\\u003c\\u002fi\\u003e\\n\\u003c\\u002fp\\u003e\\n\\n\\u003cp align=\\\"ce...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003cb\\u003eEnglish\\u003c\\u002fb\\u003e |\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"## Key features\\n\\n- [Download files](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002fguides\\u002fdownload) ...\"],[\"```\\n\\nIf you prefer, you can also install it with [conda](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub...\"],[\"```\\n\\n### Create a repository\\n\\n```py\\nfrom huggingface_hub import create_repo\\n\\ncreate_repo(repo_id=\\\"su...\"],[\"```\\n\\nFor details in the [upload guide](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002fguides\\u002fupload)...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"### @webhook_endpoint\\n\\n[[autodoc]] huggingface_hub.webhook_endpoint\\n\\n## Payload\\n\\n[`WebhookPayload`] ...\"],[\"[[autodoc]] huggingface_hub.WebhookPayloadRepo\\n\\n### WebhookPayloadUrl\\n\\n[[autodoc]] huggingface_hub.W...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" href=\\\".\\u002fp...\"],[\"## Contribute\\n\\nAll contributions to the `huggingface_hub` are welcomed and equally valued! 🤗 Besides...\"],[\"--\\n# For reference on model card metadata, see the spec: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fblo...\"],[\"### Model Sources [optional]\\n\\n\\u003c!-- Provide the basic links for the model. --\\u003e\\n\\n- **Repository:** {{ ...\"],[\"### Recommendations\\n\\n\\u003c!-- This section is meant to convey recommendations with respect to the bias, ...\"],[\"## Evaluation\\n\\n\\u003c!-- This section describes the evaluation protocols and provides the results. --\\u003e\\n\\n#...\"],[\"- **Hardware Type:** {{ hardware_type | default(\\\"[More Information Needed]\\\", true)}}\\n- **Hours used:...\"],[\"## More Information [optional]\\n\\n{{ more_information | default(\\\"[More Information Needed]\\\", true)}}\\n\\n...\"],[\"MyCoolModel\\n\\nIn this example, we don't have any metadata at the top of the file. In cases like these...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\n## Create an endpoint\\n\\nImplementing a webhook endpoint is as simple as decorating a functio...\"],[\"```\\n\\nSave this snippet in a file called `'app.py'` and run it with `'python app.py'`. You should see...\"],[\"```\\n\\nGood job! You just launched a webhook server! Let's break down what happened exactly:\\n\\n1. By de...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\n## Configure a Webhook\\n\\nNow that you have a webhook server running, you want to configure a...\"],[\"Your webhook server is now running on a public Space. If most cases, you will want to secure it with...\"],[\"```\\n\\nWhich will create two endpoints:\\n\\n```text\\n(...)\\nWebhooks are correctly setup and ready to use:\\n...\"],[\"```\\n\\n1. We define a custom UI using Gradio blocks. This UI will be displayed on the landing page of ...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nThe docs will be viewable at [http:\\u002f\\u002flocalhost:3000](http:\\u002f\\u002flocalhost:3000). You can also previ...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\nFo...\"],[\"If you want to create a link to some internal class or function, you need to\\nprovide its path. For i...\"],[\"```\\n    Args:\\n        n_layers (`int`): The number of layers of the model.\\n```\\n\\nIf the description i...\"],[\"```\\n```\\n# first line of code\\n# second line\\n# etc\\n```\\n````\\n\\n#### Writing a return block\\n\\nThe return b...\"],[\"```\\n\\n#### Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that ...\"],[\"```\\n    Example:\\n\\n    ```python\\n    \\u003e\\u003e\\u003e from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\n ...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"### Create a repository\\n\\nCreate an empty repository with [`create_repo`] and give it a name with the...\"],[\"```\\n\\nBy default, [`create_repo`] creates a model repository. But you can use the `repo_type` paramet...\"],[\"```\\n\\n## Upload and download files\\n\\nNow that you have created your repository, you are interested in ...\"],[\"```\\n\\nYou can use the [`delete_branch`] and [`delete_tag`] functions in the same way to delete a bran...\"],[\"Some settings are specific to Spaces (hardware, environment variables,...). To configure those, plea...\"],[\"```\\n\\n### Rename your repository\\n\\nYou can rename your repository on the Hub using [`move_repo`]. Usin...\"],[\"```\\n\\n### Clone\\n\\nThe `clone_from` parameter clones a repository from a Hugging Face repository ID to ...\"],[\"```\\n\\n### Branch\\n\\nBranches are important for collaboration and experimentation without impacting your...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ci\\u003eHugging Face Hub Python 客户端\\u003c\\u002fi\\u003e\\n\\u003c\\u002fp\\u003e\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"http...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\u002fblob\\u002fmai...\"],[\"---\\n\\n## 欢迎使用 Hugging Face Hub 库\\n\\n通过`huggingface_hub` 库，您可以与面向机器学习开发者和协作者的平台 [Hugging Face Hub](https...\"],[\"```\\n\\n如果您更喜欢，也可以使用 conda 进行安装\\n\\n为了默认保持包的最小化，huggingface_hub 带有一些可选的依赖项，适用于某些用例。例如，如果您想要完整的推断体验，请运行：\\n\\n`...\"],[\"```\\n\\n### 创建一个存储库\\n\\n要创建一个新存储库，请运行以下代码：\\n\\n```py\\nfrom huggingface_hub import create_repo\\n\\ncreate_repo(rep...\"],[\"```\\n\\n有关详细信息，请查看 [上传指南](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002fguides\\u002fupload).\\n\\n## 集成到 Hub 中\\n...\"],[\"## 欢迎各种贡献（功能请求、错误等） 💙💚💛💜🧡❤️\\n\\n欢迎每个人来进行贡献，我们重视每个人的贡献。编写代码并非唯一的帮助社区的方式。回答问题、帮助他人、积极互动并改善文档对社区来说都是极其有价值的...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ci\\u003eDer offizielle Python-Client für den Huggingface Hub.\\u003c\\u002fi\\u003e\\n\\u003c\\u002fp\\u003e\\n\\n\\u003cp align=\\\"...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\u002fblob\\u002fmai...\"],[\"---\\n\\n## Willkommen bei der huggingface_hub Bibliothek\\n\\nDie `huggingface_hub` Bibliothek ermöglicht I...\"],[\"## Hauptmerkmale\\n\\n- Dateien vom Hub [herunterladen](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fde\\u002fg...\"],[\"```\\n\\nWenn Sie möchten, können Sie es auch mit [conda](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fde...\"],[\"```\\n\\nOder eine gesamte Repository\\n\\n```py\\nfrom huggingface_hub import snapshot_download\\n\\nsnapshot_dow...\"],[\"```\\n\\nWeitere Informationen finden Sie im [Upload-Leitfaden](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_...\"],[\"## Beiträge (Feature-Anfragen, Fehler usw.) sind super willkommen 💙💚💛💜🧡❤️\\n\\nJeder ist willkommen beiz...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Contrib test suite\\n\\nThe contrib folder contains simple end-to-end scripts to test integration of `hu...\"],[\"```\\n\\nThen tests can be run\\n\\n```sh\\n# Optional: -j4 to run in parallel. Output will be messy in that c...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"--\\nlanguage: en\\nlicense: mit\\nlibrary_name: timm\\ntags:\\n- pytorch\\n- image-classification\\ndatasets:\\n- b...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ci\\u003e공식 Huggingface Hub 파이썬 클라이언트\\u003c\\u002fi\\u003e\\n\\u003c\\u002fp\\u003e\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"htt...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\u002fblob\\u002fmai...\"],[\"---\\n\\n## huggingface_hub 라이브러리 개요\\n\\n`huggingface_hub` 라이브러리는 [Hugging Face Hub](https:\\u002f\\u002fhuggingface.co...\"],[\"## 주요 기능\\n\\n- Hub에서 [파일을 다운로드](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fmain\\u002fko\\u002fguides\\u002fdownload)\\n- ...\"],[\"```\\n\\n원한다면 [conda](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fko\\u002finstallation#install-with-conda)를 이...\"],[\"```\\n\\n파일은 로컬 캐시 폴더에 다운로드됩니다. 자세한 내용은 [이 가이드](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fko\\u002fguides\\u002fma...\"],[\"```\\n\\n레포지토리 전체의 경우:\\n\\n```py\\nfrom huggingface_hub import upload_folder\\n\\nupload_folder(\\n    folder_path=...\"],[\"```\\n\\n자세한 내용은 [업로드 가이드](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fko\\u002fguides\\u002fupload)를 참조하세요.\\n\\n## Hug...\"],[\"이렇게 하면 다음과 같은 장점이 있습니다:\\n\\n- 라이브러리 사용자들의 모델이나 데이터셋을 무료로 호스팅해줍니다.\\n- git을 기반으로 한 방식으로, 아주 큰 파일들도 버전을 관리할...\"],[\"여러분의 라이브러리를 통합하고 싶다면, 이슈를 열어서 의견을 나눠주세요. 통합 과정을 안내하기 위해 ❤️을 담아 [단계별 가이드](https:\\u002f\\u002fhuggingface.co\\u002fdocs...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"## Translating the `huggingface_hub` documentation into your language\\n\\nAs part of our mission to dem...\"],[\"```\\n\\nHere, `LANG-ID` should be one of the ISO 639-1 or ISO 639-2 language codes -- see [here](https:...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nThe levels should be understood as follows:\\n\\n- `error`: only show critical logs about usage whi...\"],[\"By default, progress bars are enabled. You can disable them globally by setting `HF_HUB_DISABLE_PROG...\"],[\"```\\n\\n### are_progress_bars_disabled\\n\\n[[autodoc]] huggingface_hub.utils.are_progress_bars_disabled\\n\\n#...\"],[\"`huggingface_hub` defines its own HTTP errors to refine the `HTTPError` raised by\\n`requests` with ad...\"],[\"```\\n\\n[[autodoc]] huggingface_hub.utils.hf_raise_for_status\\n\\n### HTTP errors\\n\\nHere is a list of HTTP ...\"],[\"#### OfflineModeIsEnabled\\n\\n[[autodoc]] huggingface_hub.utils.OfflineModeIsEnabled\\n\\n## Telemetry\\n\\n`hu...\"],[\"\\u003e\\u003e\\u003e @validate_hf_hub_args\\n... def my_cool_method(repo_id: str):\\n...     print(repo_id)\\n\\n\\u003e\\u003e\\u003e my_cool_...\"],[\"```\\n\\n#### validate_hf_hub_args\\n\\n[[autodoc]] utils.validate_hf_hub_args\\n\\n#### HFValidationError\\n\\n[[au...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n要下载文件的特定版本，请使用`revision`参数指定分支名称、标签或提交哈希。如果您选择使用提交哈希，它必须是完整长度的哈希，而不是较短的7个字符的提交哈希：\\n\\n```py\\n\\u003e\\u003e\\u003e fr...\"],[\"```\\n\\n或者，你可以在笔记本电脑或脚本中使用 [`login`] 来进行程序化登录,请运行以下代码:\\n\\n```py\\n\\u003e\\u003e\\u003e from huggingface_hub import login  \\n\\u003e...\"],[\"```\\n如果您想将存储库设置为私有，请按照以下步骤操作：\\n\\n```py\\n\\u003e\\u003e\\u003e from huggingface_hub import HfApi  \\n\\u003e\\u003e\\u003e api = HfApi()  \\n\\u003e\\u003e\\u003e ...\"],[\"```\\n\\n要一次上传多个文件，请查看[上传指南](.\\u002fguides\\u002fupload) ,该指南将向您介绍几种上传文件的方法（有或没有 git）。\\n\\n## 下一步\\n\\n`huggingface_hub`库为...\"],[\"Hugging Face Hub Client library\\n\\n## Download files from the Hub\\n\\nThe `hf_hub_download()` function is...\"],[\"```\\n\\n### `snapshot_download`\\n\\nUsing `hf_hub_download()` works well when you know which files you wan...\"],[\"If you check out this URL's headers with a `HEAD` http request (which you can do\\nfrom the command li...\"],[\"```\\n\\nWith the `HfApi` class there are methods to query models, datasets, and metrics by specific tag...\"],[\"```\\n\\nIf the repository you're cloning is one of yours or one of your organisation's,\\nthen having the...\"],[\"```\\n\\nFinally, you can choose to specify the Git username and email attributed to that\\nclone directly...\"],[\"```\\n\\nThe repository can be managed through this object, through wrappers of\\ntraditional Git methods:...\"],[\"These two methods also have support for the `blocking` parameter.\\n\\nExamples using the `commit` conte...\"],[\"```\\n\\n```python\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e model = torch.nn.Transformer()\\n\\u003e\\u003e\\u003e with Repository(\\\"torch-model\\\"...\"],[\"```\\n\\nThis should be executed once for each model repo that contains a model file\\n\\u003e5GB. If you just t...\"],[\"```\\n\\nThis is an example of a task (`question-answering`) which requires a dictionary\\nas input thas h...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nOnce done, [check installation](#check-installation) is working correctly.\\n\\n### Install optiona...\"],[\"```\\n\\nWhen installing from source, you can also specify a specific branch. This is useful if you\\nwant...\"],[\"```\\n\\nThis command will fetch information from the Hub about the [gpt2](https:\\u002f\\u002fhuggingface.co\\u002fgpt2) ...\"],[\"```\\n\\n## Windows limitations\\n\\nWith our goal of democratizing good ML everywhere, we built `huggingfac...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n`HfApi.get_repo_discussions` returns a [generator](https:\\u002f\\u002fdocs.python.org\\u002f3.7\\u002fhowto\\u002ffunctional...\"],[\"```\\n\\nThe [`Discussion`] object returned by [`HfApi.get_repo_discussions`] contains high-level overvi...\"],[\"```\\n\\n[`HfApi.get_discussion_details`] returns a [`DiscussionWithDetails`] object, which is a subclas...\"],[\"```\\n\\nYou can also use [`HfApi.create_discussion`] (respectively [`HfApi.create_pull_request`]) to cr...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"If you are interested in Inference and Widgets, you can follow [this guide](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"```\\n\\n### push_to_hub\\n\\nThe `push_to_hub` method often requires a bit more complexity to handle repo c...\"],[\"```\\n\\nThis is of course only an example. If you are interested in more complex manipulations (delete ...\"],[\"In a lot of cases, a library already implements its model using a Python class. The class contains t...\"],[\"The advantage of using [`ModelHubMixin`] is that once you take care of the serialization\\u002floading of ...\"],[\"```\\n\\n#### Implementation\\n\\nThe implementation is actually very straightforward, and the full implemen...\"],[\"```\\n\\n3. Implement the `_from_pretrained` method:\\n\\n```python\\nclass PyTorchModelHubMixin(ModelHubMixin...\"],[\"```\\n\\nAnd that's it! Your library now enables users to upload and download files to and from the Hub....\"],[\"Inference Endpoints\\n\\nInference Endpoints provides a secure production solution to easily deploy any ...\"],[\"```\\n\\nIn this example, we created a `protected` Inference Endpoint named `\\\"my-endpoint-name\\\"`, to ser...\"],[\"```\\n\\nIt's a dataclass that holds information about the endpoint. You can access important attributes...\"],[\"```python\\n# Start an Inference Endpoint running Zephyr-7b-beta on TGI\\n\\u003e\\u003e\\u003e from huggingface_hub impor...\"],[\"```\\n\\nThe value to pass as `custom_image` is a dictionary containing a url to the docker container an...\"],[\"```\\n\\n## Check deployment status\\n\\nIn the rest of this guide, we will assume that we have a [`Inferenc...\"],[\"```\\n\\nInstead of fetching the Inference Endpoint status while waiting for it to run, you can directly...\"],[\"```\\n\\nIf `timeout` is set and the Inference Endpoint takes too much time to load, a [`InferenceEndpoi...\"],[\"```\\n\\nFor more details about how to use the [`InferenceClient`], check out the [Inference guide](..\\u002fg...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n```py\\n# Pause and resume endpoint\\n\\u003e\\u003e\\u003e endpoint.pause()\\nInferenceEndpoint(name='my-endpoint-n...\"],[\"```\\n\\n### Update model or hardware requirements\\n\\nIn some cases, you might also want to update your In...\"],[\"```\\n\\n### Delete the endpoint\\n\\nFinally if you won't use the Inference Endpoint anymore, you can simpl...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nWhile filtering, you can also sort the models and take only the top results. For example,\\nthe f...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003cTip warning={true}\\u003e\\n\\n[`Repository`] is now deprecated in favor of the http-based alternatives. Give...\"],[\"This preference of the http-based [`HfApi`] over the git-based [`Repository`] does not mean that git...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nTo download a specific version of the file, use the `revision` parameter to specify the\\nbranch ...\"],[\"```\\n\\nThe command will tell you if you are already logged in and prompt you for your token. The token...\"],[\"```\\nfrom transformers import whoami\\n\\nuser = whoami(token=...)\\n```\\n\\nThis is usually discouraged excep...\"],[\"```\\n\\nPrivate repositories will not be visible to anyone except yourself.\\n\\n\\u003cTip\\u003e\\n\\nTo create a reposit...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n如果你想在Hub上创建和管理一个仓库，你的计算机必须处于登录状态。如果尚未登录，请参考[此部分](..\\u002fquick-start#login)。在本指南的其余部分，我们将假设你的计算机已...\"],[\"```\\n\\n默认情况下，[`create_repo`] 会创建一个模型仓库。但是你可以使用 `repo_type`参数来指定其他仓库类型。例如，如果你想创建一个数据集仓库\\n\\n请运行以下代码：\\n\\n```p...\"],[\"```\\n\\n### 克隆一个仓库（仅适用于 Spaces）\\n\\n在某些情况下，你可能想要复制别人的仓库并根据自己的用例进行调整。对于 Spaces，你可以使用 [`duplicate_space`] 方法...\"],[\"```\\n\\n## 上传和下载文件\\n\\n既然您已经创建了您的存储库，您现在也可以推送更改至其中并从中下载文件\\n\\n这两个主题有它们自己的指南。请[上传指南](.\\u002fupload) 和[下载指南](.\\u002fdownl...\"],[\"```\\n\\n同时,你可以以相同的方式使用 [`delete_branch`] 和 [`delete_tag`] 函数来删除分支或标签\\n\\n### 列出所有的分支和标签\\n\\n你还可以使用 [`list_rep...\"],[\"```\\n\\n## 修改存储库设置\\n\\n存储库具有一些可配置的设置。大多数情况下，您通常会在浏览器中的存储库设置页面上手动配置这些设置。要配置存储库，您必须具有对其的写访问权限（拥有它或属于组织）。在本节中...\"],[\"```\\n\\n## 管理存储库的本地副本\\n\\n上述所有操作都可以通过HTTP请求完成。然而，在某些情况下，您可能希望在本地拥有存储库的副本，并使用您熟悉的Git命令与之交互。\\n\\n[`Repository`]...\"],[\"```\\n\\n你可以将`clone_from`参数与[`create_repo`]结合使用，以创建并克隆一个存储库：\\n\\n请运行以下代码：\\n\\n```py\\n\\u003e\\u003e\\u003e repo_url = create_repo...\"],[\"```\\n\\n### 拉取\\n\\n[`~Repository.git_pull`] 允许你使用远程存储库的更改更新当前本地分支：\\n\\n请运行以下代码：\\n\\n```py\\n\\u003e\\u003e\\u003e from huggingface_h...\"],[\"--\\nlanguage: en\\nlicense: mit\\nlibrary_name: timm\\ntags:\\n- pytorch\\n- image-classification\\ndatasets:\\n- b...\"],[\"--\\nlicense: mit\\nlanguage: eo\\nthumbnail: https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fassets\\u002f01_how-to-train\\u002fEsperBERT...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nAnother way you might want to do this is with f-strings. In the following example, we:\\n\\n- Use [...\"],[\"```\\n---\\nlanguage: en\\nlicense: mit\\nlibrary: timm\\n---\\n\\n# My Model Card\\n\\nThis model created by [@natera...\"],[\"```\\n\\nNow, as you can see, the metadata header has been updated:\\n\\n```\\n---\\nlanguage: fr\\nlicense: apach...\"],[\"```\\n\\n## Share Model Cards\\n\\nIf you're authenticated with the Hugging Face Hub (either by using `huggi...\"],[\"```\\n\\nA resulting PR created from this command can be seen [here](https:\\u002f\\u002fhuggingface.co\\u002fnateraw\\u002fhf-h...\"],[\"```\\n\\nIt often happen that you want to suggest some changes to a repository\\non which you don't have w...\"],[\"```\\n\\nIf you have more than one evaluation result you'd like to share, just pass a list of `EvalResul...\"],[\"--\\nlanguage:\\n- en\\nlicense:\\n- bsd-3-clause\\nannotations_creators:\\n- crowdsourced\\n- expert-generated\\nla...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Defaults to `\\\"$HF_HOME\\u002fassets\\\"` (e.g. `\\\"~\\u002f.cache\\u002fhuggingface\\u002fassets\\\"` by default).\\n\\n### HF_TOKEN\\n\\nTo...\"],[\"### HF_HUB_DOWNLOAD_TIMEOUT\\n\\nInteger value to define the number of seconds to wait for server respon...\"],[\"### HF_HUB_DISABLE_IMPLICIT_TOKEN\\n\\nAuthentication is not mandatory for every requests to the Hub. Fo...\"],[\"For more details, see [cache limitations](..\\u002fguides\\u002fmanage-cache#limitations).\\n\\n### HF_HUB_DISABLE_E...\"],[\"### HF_HUB_ENABLE_HF_TRANSFER\\n\\nSet to `True` for faster uploads and downloads from the Hub using `hf...\"],[\"## From external tools\\n\\nSome environment variables are not specific to `huggingface_hub` but are sti...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"使用 [`Repository`] 的最大优点是它允许你在本地机器上维护整个存储库的本地副本。这也可能是一个缺点，因为它需要你不断更新和维护这个本地副本。这类似于传统软件开发中，每个开发人员都维护自己...\"],[\"如果您在本地机器上训练模型，使用传统的 git 工作流程并定期推送更新可能更有效。`Repository` 被优化为此类情况，因为它能够在后台运行。\\n如果您需要手动编辑大型文件，`git `是最佳选择...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\u003cCACHE_DIR\\u003e\\n├─ datasets--glue\\n│  ├─ refs\\n│  ├─ blobs\\n│  ├─ snapshots\\n...\\n```\\n\\nEach folder is des...\"],[\"```\\n\\u003cCACHE_DIR\\u003e\\u002f\\u003cREPO_NAME\\u003e\\u002fsnapshots\\u002faaaaaa\\u002fREADME.md\\n```\\n\\nThat `README.md` file is actually a syml...\"],[\"```\\n\\u003cCACHE_DIR\\u003e\\u002f\\u003cREPO_NAME\\u003e\\u002f.no_exist\\u002faaaaaa\\u002fconfig_that_does_not_exist.json\\n```\\n\\nUnlike the `snapsh...\"],[\"```\\n\\n### In practice\\n\\nIn practice, your cache should look like the following tree:\\n\\n```text\\n    [  9...\"],[\"```\\n\\n### Limitations\\n\\nIn order to have an efficient cache-system, `huggingface-hub` uses symlinks. H...\"],[\"```py\\nfrom huggingface_hub import cached_assets_path\\n\\nassets_path = cached_assets_path(library_name=...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n[`cached_assets_path`] is the recommended way to store assets but is not mandatory. If\\ny...\"],[\"```\\n\\n## Scan your cache\\n\\nAt the moment, cached files are never deleted from your local directory: wh...\"],[\"```text\\n➜ huggingface-cli scan-cache\\nREPO ID                     REPO TYPE SIZE ON DISK NB FILES LAS...\"],[\"```\\n\\nTo get a more detailed report, use the `--verbose` option. For each repo, you get a\\nlist of all...\"],[\"```text\\n➜ huggingface-cli scan-cache -v\\nREPO ID                     REPO TYPE REVISION              ...\"],[\"google\\u002ffleurs               dataset   129b6e96cf1967cd5d2b9b6aec75ce6cce7c89e8        25.4K        3...\"],[\"bert-base-cased             model     a8d257ba9925ef39f3036bfc338acf5283c512d9         1.4G        9...\"],[\"t5-small                    model     d78aea13fa7ecd06c29e3e46195d6341255065d5       970.7M        9...\"],[\"```\\n\\n#### Grep example\\n\\nSince the output is in tabular format, you can combine it with any `grep`-li...\"],[\"```\\n\\n### Scan cache from Python\\n\\nFor a more advanced usage, use [`scan_cache_dir`] which is the pyth...\"],[\"Here is a simple usage example. See reference for details.\\n\\n```py\\n\\u003e\\u003e\\u003e from huggingface_hub import sc...\"],[\"```\\n\\n## Clean your cache\\n\\nScanning your cache is interesting but what you really want to do next is ...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Clean cache from the terminal\\n\\nThe easiest way to delete some revisions from your HF cac...\"],[\"```\\npip install huggingface_hub[\\\"cli\\\"]\\n```\\n\\nThen run the command:\\n\\n```\\nhuggingface-cli delete-cache\\n...\"],[\"```\\n\\n#### Without TUI\\n\\nAs mentioned above, the TUI mode is currently in beta and is optional. It may...\"],[\"```\\n\\nExample of command file:\\n\\n```txt\\n# INSTRUCTIONS\\n# ------------\\n# This is a temporary file creat...\"],[\"# Dataset z-uo\\u002fmale-LJSpeech-italian (5.5G, used 5 days ago)\\n#    9cfa5647b32c0a30d0adfca06bf198d821...\"],[\"```\\n\\n### Clean cache from Python\\n\\nFor more flexibility, you can also use the [`~HFCacheInfo.delete_r...\"],[\"--\\n{{card_data}}\\n---\\n\\n# {{ model_name | default(\\\"MyModelName\\\", true)}}\\n\\n{{ some_data }}...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n[[autodoc]] AsyncInferenceClient\\n\\n## InferenceTimeoutError\\n\\n[[autodoc]] InferenceTimeoutError\\n\\n...\"],[\"You can find below the dataclasses used to validate data and in particular [`~huggingface_hub.infere...\"],[\"[[autodoc]] InferenceApi\\n    - __init__\\n    - __call__\\n    - all...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"For web development, a [JS client](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface.js\\u002finference\\u002fREADME) has ...\"],[\"```\\n\\nWe initialized an [`InferenceClient`] with the default parameters. The only thing you need to k...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nThere are more than 200k models on the Hugging Face Hub! Each task in the [`InferenceCli...\"],[\"```\\n\\n### Authentication\\n\\nCalls made with the [`InferenceClient`] can be authenticated using a [User ...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nAuthentication is NOT mandatory when using the Inference API. However, authenticated use...\"],[\"| Domain | Task                           | Supported    | Documentation                            ...\"],[\"| | [Object Detection](https:\\u002f\\u002fhuggingface.co\\u002ftasks\\u002fobject-detection)            | ✅ | [`~InferenceC...\"],[\"| | [Summarization](https:\\u002f\\u002fhuggingface.co\\u002ftasks\\u002fsummarization)                  | ✅ | [`~InferenceC...\"],[\"\\u003cTip\\u003e\\n\\nCheck out the [Tasks](https:\\u002f\\u002fhuggingface.co\\u002ftasks) page to learn more about each task, how t...\"],[\"```\\n\\n## Async client\\n\\nAn async version of the client is also provided, based on `asyncio` and `aioht...\"],[\"```\\n\\nFor more information about the `asyncio` module, please refer to the [official documentation](h...\"],[\"```\\n\\n### Binary inputs\\n\\nSome tasks require binary inputs, for example, when dealing with images or a...\"],[\"```\\n\\nto\\n\\n```python\\n\\u003e\\u003e\\u003e from huggingface_hub import InferenceClient\\n\\u003e\\u003e\\u003e inference = InferenceClient(m...\"],[\"```\\n\\n### Run with parameters\\n\\nChange from\\n\\n```python\\n\\u003e\\u003e\\u003e from huggingface_hub import InferenceApi\\n\\u003e\\u003e...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n完成后,[检查安装](#check-installation)是否正常工作\\n\\n### 安装可选依赖项\\n\\n`huggingface_hub`的某些依赖项是 [可选](https:\\u002f\\u002fsetup...\"],[\"```\\n\\n这里列出了 `huggingface_hub` 的可选依赖项：\\n\\n- `cli`：为 `huggingface_hub` 提供更方便的命令行界面\\n\\n- `fastai`,` torch`, ...\"],[\"```\\n\\n完成安装后，请[检查安装](#check-installation)是否正常工作\\n\\n### 可编辑安装\\n\\n从源代码安装允许您设置[可编辑安装](https:\\u002f\\u002fpip.pypa.io\\u002fen\\u002f...\"],[\"```\\n完成安装后，请[检查安装](#check-installation)是否正常工作\\n\\n## 验证安装\\n\\n安装完成后，通过运行以下命令检查`huggingface_hub`是否正常工作:\\n\\n```...\"],[\"```\\n\\n## Windows局限性\\n\\n为了实现让每个人都能使用机器学习的目标，我们构建了 `huggingface_hub`库，使其成为一个跨平台的库，尤其可以在 Unix 和 Windows 系统...\"],[\"--\\n{card_data}\\n---\\n\\n# {{ pretty_name | default(\\\"Dataset Name\\\", true)}}\\n\\n{{ some_data }}...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"`\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" href=\\\".\\u002f...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\n通过 `huggingface_hub`库，您可以与面向机器学习开发者和协作者的平台 [Hugging Face Hub](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"当然，贡献者也应该尊重我们的[行为准则](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\u002fblob\\u002fmain\\u002fCODE_OF_CONDUCT.md)，以便...\"]],\"hovertemplate\":\"source=huggingface_hub\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"huggingface_hub, circle\",\"marker\":{\"color\":\"#19d3f3\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"huggingface_hub, circle\",\"showlegend\":true,\"x\":[7.3001575,7.319214,7.4953556,7.4839597,7.301641,7.107574,6.803798,6.478473,7.5059023,7.713972,7.482135,4.8524795,3.7378445,4.0471206,4.547135,4.3686953,4.7285633,4.4417048,4.636455,4.95999,4.639189,3.64027,3.535809,3.5424914,3.88444,4.6525126,4.979076,4.4270205,4.3776436,4.3792095,4.0908957,4.5683556,4.520756,4.1579075,4.7421145,4.708998,3.3635285,4.251005,4.300531,3.3466394,4.5545306,4.490004,4.5715594,4.8881426,4.9138207,4.683314,4.079329,4.513665,5.9469895,5.5911913,5.1449165,4.8261786,4.6123114,5.00116,5.201012,3.7520518,2.7511184,6.753146,6.1851254,6.69568,5.403625,4.7945642,4.886955,4.922103,4.6327634,4.639795,4.394843,4.721988,4.8482213,4.4282684,4.7089925,4.5197573,4.7376823,5.064959,4.7922244,5.4552946,5.101056,4.597067,3.198155,4.3270717,3.1539276,3.8065517,3.5343864,4.019501,5.424599,5.346781,4.6889963,4.452353,4.379063,3.893787,4.3845673,4.211155,4.4401727,4.5708156,4.816452,0.8733483,3.555326,4.38503,4.7521,5.452417,6.9062967,6.6633105,6.810396,7.514747,7.4781632,7.387923,6.8855543,7.528182,7.8444753,3.8693807,3.3681324,6.0987864,4.6159177,7.2692485,7.4113107,7.1449914,3.839978,6.4387097,5.269507,4.9589643,4.3344235,5.4483347,5.162869,4.7630324,6.7677445,6.7470326,4.4997334,4.678503,4.9351296,5.1460805,4.3891134,5.903377,5.624231,6.6960373,4.9062576,6.6674542,5.870981,3.597343,4.472509,-1.944619,0.74650073,-0.63785625,4.5332494,3.8537226,5.88008,5.633411,5.6867576,5.880296,6.005205,5.8509564,5.8875356,11.550205,5.6907353,5.8256917,5.795822,1.3766761,-0.91888034,-1.573645,5.2343383,-7.197217,5.0475216,4.7704244,4.864228,4.981718,5.1203012,6.3432164,4.9323425,4.880439,5.1046615,6.999109,6.9596777,7.417573,6.972142,6.532946,6.4480586,7.003087,7.5094886,7.0029526,6.773785,7.4259305,4.665354,6.2122307,6.366406,5.964397,5.4276977,7.0992804,5.8708205,7.013379,3.755613,2.8387408,5.471634,1.2320671,7.3690248,7.2962027,7.52292,7.265456,6.9125156,6.8134136,6.2499795,6.010065,7.4138904,7.238531,7.236221,5.272746,-4.5270233,6.1569166,5.420845,1.7321211,4.8866305,5.2368026,5.164656,5.0268083,5.0947137,4.791624,4.7200875,6.67313,6.290025,6.226034,6.2973046,6.7619267,4.940455,4.578544,4.9285517,4.3568506,4.718038,5.00205,4.644487,3.8463902,4.317911,4.076737,-0.696328,5.6569147,3.265056,4.8423905,2.833818,5.554593,4.9324937,4.8493185,5.1312265,5.4349704,5.305186,4.0836554,3.5847144,2.8908002,4.931397,2.0087433,0.027503282,1.0937483,-0.49827284,4.460149,4.7710953,4.9929323,4.9172993,4.52051,4.633161,4.6432967,4.441766,4.3904495,4.3191347,4.325527,3.9646082,3.9850745,5.2125173,3.7747295,3.6444967,2.89738,6.418152,5.30045,5.745504,5.1560826,4.349327,4.949898,5.1036243,3.9242115,4.564481,6.6847615,6.6652,6.2858253,8.296674,6.6593833,6.901679,6.7235074,6.587906,6.412576,6.095405,0.7720438,7.0826025,3.426573,2.1477137,2.5365708,3.542404,3.4731355,3.538565,3.8927188,0.9789925,-6.6103115,2.106127,5.296132,5.3096867,5.253762,5.2897487,5.2675595,5.0685997,5.220332,6.540805,6.8419886,6.8758097,5.489828,4.7195115,4.791012,4.4866486,4.8764844,5.0101466,4.7812605,4.521494,5.0154424,4.996752,4.973371,5.008507,-6.2753615,-5.7702446,6.081023,4.732969,4.944188,5.045923,5.2864423,5.251866,5.1330404,5.32479,5.0193033,5.3966293,5.2097373,4.3242116,4.8880434,4.3538923,4.374466,4.0552735,4.836633,5.3292103,4.4282694,4.7901525,4.8530474,5.060703,-7.426928,-7.43796,-4.4682,4.666618,4.2783775,4.2928095,4.557625,4.5233665,3.7295377,6.6031528,6.7916837,6.659316,6.588438,6.941399,6.9823394,4.562711,6.763172,6.5816693,6.8923144,9.340968],\"xaxis\":\"x\",\"y\":[-1.0335152,-1.0221461,-1.097726,-1.7866505,-1.0827897,-1.0104848,-0.98364127,-1.026294,-1.2670802,-2.201264,-1.4211037,-1.1348219,-2.670297,-1.2879121,-1.5085422,-1.4220091,-1.4640459,-1.5534742,-2.0405796,-1.1478475,-1.4718497,-3.1386628,-4.2285295,-4.0833316,-4.1160564,-1.381143,-0.8783804,-0.9315021,-0.9488712,-0.8907974,-1.2719259,-0.9447132,-1.028968,-1.1497207,-1.1991335,-1.0071313,-2.7559812,-0.8661666,-1.0120916,-0.59585434,-0.9442758,-0.9836536,-1.0102227,-1.1661986,-1.0525471,-0.9239624,-0.73816097,-0.6749856,-0.39847767,-1.2746278,-0.732037,-0.20709491,-0.35899577,-0.29521692,-0.9665692,-0.05563339,-0.25742945,-3.6004739,-2.6952987,-3.908469,-0.7280413,-0.4006686,-0.49293992,-0.703508,-1.3228444,-1.5060021,-1.6939526,-1.4211042,-0.8029349,-0.72642434,-0.9867552,-0.74859285,-0.6916632,-1.8498073,-1.8909454,-3.7897866,-2.2251863,-0.38734528,-0.074797384,-2.1237795,-3.3169591,-1.5394332,-3.6980953,-0.77695817,-1.9358106,-2.7318351,-2.068025,-2.6115005,-2.7756448,-3.1090076,-2.8421893,-2.723846,-2.6567569,-3.0937696,-2.5451813,-2.6115487,-3.2199829,-0.36126423,-0.19166791,-2.185156,-2.138985,-2.15951,-2.2771468,-2.4932332,-2.5201206,-2.4636028,-2.2528405,-2.5184865,-2.6616786,-2.5322561,-1.8193839,-0.8504817,-1.5380621,-4.3155084,-3.778547,-2.8111136,-2.0168464,-2.3271196,-1.3823128,1.1246061,0.944709,-0.78312063,-1.3391575,-1.5094352,-0.73897195,-0.7054396,-0.8560772,-1.241533,-0.46068084,-0.85024476,-0.6753037,-2.3140848,-2.2788687,-1.9257115,-0.68834525,-0.81450236,-1.3124447,-1.7346392,-3.1786656,1.9571359,-5.889606,2.1458416,-3.4047008,-2.8476493,-2.3524303,-2.2798154,-2.3567278,-2.222873,-2.3635912,-2.280106,-2.372954,-3.9624748,0.01202921,-1.5944179,-1.2758733,-1.7253836,-3.4134758,-4.03426,-0.83933765,-6.9216905,-0.7495419,-0.64211977,-0.9135135,-0.6951089,-0.8853003,-2.0608475,-0.8233722,-0.7296771,-0.52459455,-0.7933769,-0.8351201,-1.014548,-0.90476066,-0.742564,-0.82884973,-1.0425355,-1.221061,-0.8073935,-0.7927496,-0.99777323,-0.7642874,-0.87568766,-0.7029036,-0.7483176,-1.0872656,-1.5496668,-0.8554106,-2.5619245,-0.33064458,-0.17869647,-1.1010953,-2.8330636,-0.94101137,-0.91445225,-0.9240564,-0.9276474,-0.9089939,-0.74327976,-0.7594994,-0.947714,-1.0071652,-3.9077973,-1.0156964,-2.3746948,-7.571715,-2.030793,-1.038023,-0.24408485,-1.5518633,-1.225234,-1.236074,-1.3445573,-1.3471702,-1.4778076,-1.536078,-0.83976805,-0.77304006,-0.9246274,-0.87957853,-0.8931239,-1.1682438,-1.5234157,-1.0578028,-1.5475769,-0.73684585,-0.7765075,-0.92195976,-1.4314041,-0.92171013,0.7700647,-3.5151575,-0.4803465,0.14319941,-0.29152316,-2.1491387,-0.94018066,-1.7016914,-2.3199844,-2.2403376,-1.721441,-1.7462649,-0.70191026,-0.41323194,-0.20240022,-0.9435067,-0.4571338,-0.17815314,-0.40102652,-0.40130147,-0.9734722,1.1288573,0.62644494,0.5812346,0.79963326,0.55917686,0.8238162,1.0693465,1.113212,1.2185072,0.95121473,0.98028284,0.969965,-0.7245485,-1.709217,-2.947851,-0.43659112,-0.8043919,-0.88808894,-0.99283195,-0.4836965,-0.83253944,-0.94649595,-0.91716975,-0.80949676,-0.7167188,-1.0209968,-0.9741002,-0.9046187,-2.5506446,-0.8560343,-1.137225,-0.99950016,-0.8754068,-0.85564286,-0.8553686,-3.1830013,-2.4763873,-1.7072858,-1.9901786,-1.7909385,-1.6807643,-0.008092925,-1.8647761,-1.796986,-2.9946668,-5.9943523,-0.43606284,-0.9476036,-1.322142,-1.3924543,-1.2904941,-1.2537813,-1.2016313,-1.3862641,-0.8352872,-0.93917125,-0.96282566,-1.0776694,-1.9110175,-1.739739,-1.582881,-3.47553,-1.370739,-1.1577792,-2.331859,-1.9176482,-2.2168016,-1.955814,-2.2114718,-0.7643006,-1.3005897,-3.172784,-3.4219723,-2.3763022,-1.860696,-2.3420274,-2.3389137,-2.34477,-2.447996,-2.2159066,-5.0473337,-2.4785428,-3.5573466,1.1094021,1.0118922,0.9578076,1.1100903,1.0696211,-0.5772994,0.5394908,0.88033986,-0.38107452,0.67261165,-5.300346,-0.3285544,-1.8143133,0.62849677,0.2533231,0.8665021,0.5140698,0.3250728,-2.256891,-0.91098225,-0.65738,-0.7606812,-0.6892979,-1.3024006,-0.97812796,-4.0747857,-0.7907305,-0.8751722,-0.79984045,-3.0489576],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"The tokenization pipeline\\n\\nWhen calling `Tokenizer.encode` or\\n`Tokenizer.encode_batch`, the input\\nte...\"],[\"## Normalization\\n\\nNormalization is, in a nutshell, a set of operations you apply to a raw\\nstring to ...\"],[\"You can manually test that normalizer by applying it to any string:\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython...\"],[\"When building a `Tokenizer`, you can\\ncustomize its normalizer by just changing the corresponding att...\"],[\"An easy way to pre-tokenize inputs is to split on spaces and\\npunctuations, which is done by the\\n`pre...\"],[\"You can combine together any `PreTokenizer` together. For instance, here is a pre-tokenizer that wil...\"],[\"As we saw in the `quicktour`, you can\\ncustomize the pre-tokenizer of a `Tokenizer` by just changing ...\"],[\"The role of the model is to split your \\\"words\\\" into tokens, using the\\nrules it has learned. It's als...\"],[\"As we saw in the quick tour, we can customize the post processor of a\\n`Tokenizer` by setting the\\ncor...\"],[\"Let's put all those pieces together to build a BERT tokenizer. First,\\nBERT relies on WordPiece, so w...\"],[\"Then we know that BERT preprocesses texts by removing accents and\\nlowercasing. We also use a unicode...\"],[\"The pre-tokenizer is just splitting on whitespace and punctuation:\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e...\"],[\"And the post-processing uses the template we saw in the previous\\nsection:\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003c...\"],[\"We can use this tokenizer and train on it on wikitext like in the\\n`quicktour`:\\n\\n\\u003ctokenizerslangconte...\"],[\"The `decoder` will first convert the IDs back to tokens\\n(using the tokenizer's vocabulary) and remov...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"But by changing it to a proper decoder, we get:\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{...\"],[\"Quicktour\\n\\nLet's have a quick look at the 🤗 Tokenizers library features. The\\nlibrary provides an imp...\"],[\"```\\n\\n### Training the tokenizer\\n\\nIn this tour, we will build and train a Byte-Pair Encoding (BPE)\\nto...\"],[\"To train our tokenizer on the wikitext files, we will need to\\ninstantiate a [trainer]{.title-ref}, i...\"],[\"The order in which you write the special tokens list matters: here `\\\"[UNK]\\\"` will get the ID 0,\\n`\\\"[C...\"],[\"Now, we can just call the `Tokenizer.train` method with any list of files we want to use:\\n\\n\\u003ctokenize...\"],[\"This should only take a few seconds to train our tokenizer on the full\\nwikitext dataset! To save the...\"],[\"and you can reload your tokenizer from that file with the\\n`Tokenizer.from_file`\\n`classmethod`:\\n\\n\\u003ctok...\"],[\"### Using the tokenizer\\n\\nNow that we have trained a tokenizer, we can use it on any text we want\\nwit...\"],[\"This `Encoding` object then has all the\\nattributes you need for your deep learning model (or other)....\"],[\"Similarly, the `ids` attribute will\\ncontain the index of each of those tokens in the tokenizer's\\nvoc...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"and those are the indices that correspond to the emoji in the original\\nsentence:\\n\\n\\u003ctokenizerslangcon...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"Here is how we can set the post-processing to give us the traditional\\nBERT inputs:\\n\\n\\u003ctokenizerslangc...\"],[\"Lastly, we specify the special tokens we used and their IDs in our\\ntokenizer's vocabulary.\\n\\nTo check...\"],[\"To check the results on a pair of sentences, we just pass the two\\nsentences to `Tokenizer.encode`:\\n\\n...\"],[\"You can then check the type IDs attributed to each token is correct with\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cp...\"],[\"To get the full speed of the 🤗 Tokenizers library, it's best to\\nprocess your texts by batches by usi...\"],[\"To process a batch of sentences pairs, pass two lists to the\\n`Tokenizer.encode_batch` method: the\\nli...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"In this case, the `attention mask` generated by the\\ntokenizer takes the padding into account:\\n\\n\\u003ctoke...\"],[\"```\\n\\n### Importing a pretrained tokenizer from legacy vocabulary files\\n\\nYou can also import a pretra...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002flanding\\u002fassets\\u002ftokenizers\\u002ftokenizers...\"],[\"## Bindings\\n\\nWe provide bindings to the following languages (more to come!):\\n  - [Rust](https:\\u002f\\u002fgith...\"],[\"```\\n\\nYou can customize how pre-tokenization (e.g., splitting into words) is done:\\n\\n```python\\nfrom to...\"],[\"`tokenizers-linux-arm64-musl`\\n\\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`...\"],[\"div align=\\\"center\\\"\\u003e\\n\\n  \\u003ch1\\u003e\\u003ccode\\u003ewasm-pack-template\\u003c\\u002fcode\\u003e\\u003c\\u002fh1\\u003e\\n\\n  \\u003cstrong\\u003eA template for kick start...\"],[\"Be sure to check out [other `wasm-pack` tutorials online][tutorials] for other\\ntemplates and usages ...\"],[\"```\\ncargo generate --git https:\\u002f\\u002fgithub.com\\u002frustwasm\\u002fwasm-pack-template.git --name my-project\\ncd my-...\"],[\"Post-processors\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BertProcessing\\n\\n[[autodoc]] tokenizers.processo...\"],[\"Training from memory\\n\\nIn the [Quicktour](quicktour), we saw how to build and train a\\ntokenizer using...\"],[\"Easy, right? You can use anything working as an iterator here, be it a\\n`List`{.interpreted-text role...\"],[\"With our iterator ready, we just need to launch the training. In order\\nto improve the look of our pr...\"],[\"Models\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BPE\\n\\n[[autodoc]] tokenizers.models.BPE\\n\\n## Model\\n\\n[[auto...\"],[\"Added Tokens\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## AddedToken\\n\\n[[autodoc]] tokenizers.AddedToken\\n    ...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002flanding\\u002fassets\\u002ftokenizers\\u002ftokenizers...\"],[\"```\\n\\n#### From sources:\\n\\nTo use this method, you need to have the Rust installed:\\n\\n```bash\\n# Install...\"],[\"```\\n\\nAnd you can train them just as simply:\\n\\n```python\\nfrom tokenizers import CharBPETokenizer\\n\\n# In...\"],[\"```\\n\\n#### Provided Tokenizers\\n\\n - `CharBPETokenizer`: The original BPE\\n - `ByteLevelBPETokenizer`: T...\"],[\"```\\n\\nNow, when you want to use this tokenizer, this is as simple as:\\n\\n```python\\nfrom tokenizers impo...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002flanding\\u002fassets\\u002ftokenizers\\u002ftokenizers...\"],[\"## What is a Tokenizer\\n\\nA Tokenizer works as a pipeline, it processes some raw text as input and out...\"],[\"```\\n\\n### Deserialization and tokenization example\\n\\n```rust\\nuse tokenizers::tokenizer::{Result, Token...\"],[\"```\\n\\n### Training and serialization example\\n\\n```rust\\nuse tokenizers::decoders::DecoderWrapper;\\nuse t...\"],[\"let mut tokenizer = TokenizerBuilder::new()\\n        .with_model(BPE::default())\\n        .with_normal...\"],[\"```\\n\\n## Additional information\\n\\n- tokenizers is designed to leverage CPU parallelism when possible. ...\"],[\"`tokenizers-win32-x64-msvc`\\n\\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`...\"],[\"`tokenizers-freebsd-x64`\\n\\nThis is the **x86_64-unknown-freebsd** binary for `tokenizers`...\"],[\"`tokenizers-win32-ia32-msvc`\\n\\nThis is the **i686-pc-windows-msvc** binary for `tokenizers`...\"],[\"Visualizer\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## Annotation\\n\\n[[autodoc]] tokenizers.tools.Annotation\\n...\"],[\"Components\\n\\nWhen building a Tokenizer, you can attach various types of components to\\nthis Tokenizer ...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| NFD | NFD...\"],[\"| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| NFD | NFD unicode normalization |  |\\n| NFK...\"],[\"| NFD | NFD unicode normalization |  |\\n| NFKD | NFKD unicode normalization |  |\\n| NFC | NFC unicode ...\"],[\"## Pre-tokenizers\\n\\nThe `PreTokenizer` takes care of splitting the input according to a set\\nof rules....\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| ByteLevel...\"],[\"| CharDelimiterSplit | Splits on a given character | Example with `x`: \\u003cbr\\u003e Input: `\\\"Helloxthere\\\"` \\u003c...\"],[\"\\u003c\\u002fpython\\u003e\\n\\u003crust\\u003e\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| ByteLevel | Splits on whi...\"],[\"| CharDelimiterSplit | Splits on a given character | Example with `x`: \\u003cbr\\u003e Input: `\\\"Helloxthere\\\"` \\u003c...\"],[\"\\u003c\\u002frust\\u003e\\n\\u003cnode\\u003e\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| ByteLevel | Splits on white...\"],[\"| CharDelimiterSplit | Splits on a given character | Example with `x`: \\u003cbr\\u003e Input: `\\\"Helloxthere\\\"` \\u003c...\"],[\"## Models\\n\\nModels are the core algorithms used to actually tokenize, and therefore,\\nthey are the onl...\"],[\"After the whole pipeline, we sometimes want to insert some special\\ntokens before feed a tokenized st...\"],[\"| Name | Description |\\n| :--- | :--- |\\n| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTok...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# Tokenizers\\n\\nFast State-of-the-art tokenizers, optimized for ...\"],[\"Decoders\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BPEDecoder\\n\\n[[autodoc]] tokenizers.decoders.BPEDecoder...\"],[\"`tokenizers-darwin-arm64`\\n\\nThis is the **aarch64-apple-darwin** binary for `tokenizers`...\"],[\"Input Sequences\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\nThese types represent all the different kinds of s...\"],[\"Encoding\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## Encoding\\n\\n[[autodoc]] tokenizers.Encoding\\n    - all\\n  ...\"],[\"Pre-tokenizers\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BertPreTokenizer\\n\\n[[autodoc]] tokenizers.pre_tok...\"],[\"Normalizers\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BertNormalizer\\n\\n[[autodoc]] tokenizers.normalizers....\"],[\"Trainers\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BpeTrainer\\n\\n[[autodoc]] tokenizers.trainers.BpeTrainer...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002flanding\\u002fassets\\u002ftokenizers\\u002ftokenizers-log...\"],[\"```\\n\\n## Basic example\\n\\n```ts\\nimport { Tokenizer } from \\\"tokenizers\\\";\\n\\nconst tokenizer = await Tokeni...\"],[\"# Requirements\\n\\nIn order to generate the documentation, it is necessary to have a Python environment...\"],[\"`tokenizers-linux-x64-gnu`\\n\\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`...\"],[\"Changelog\\nAll notable changes to this project will be documented in this file.\\n\\nThe format is based ...\"],[\"- [#952] Fixed the vocabulary size of UnigramTrainer output (to respect added tokens)\\n- [#954] Fixed...\"],[\"### Changed\\n- [#234]: Completely changed the alignement mappings available on `Encoding`. Previous m...\"],[\"### Added\\n- [#236]: RobertaProcessing is now also taking care of trimming offsets, and works just as...\"],[\"## [0.9.0]\\n\\n### Changed\\n- Only one progress bar while reading files during training. This is better ...\"],[\"### How to migrate\\n- Add the `ByteLevel` `PostProcessor` to your byte-level BPE tokenizers if releva...\"],[\"[#1072]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f1072\\n[#956]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002f...\"],[\"[#916]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f916\\n[#884]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#276]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f276\\n[#272]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#174]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fissues\\u002f174\\n[#165]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002f...\"],[\"`tokenizers-win32-arm64-msvc`\\n\\nThis is the **aarch64-pc-windows-msvc** binary for `tokenizers`...\"],[\"`tokenizers-android-arm-eabi`\\n\\nThis is the **armv7-linux-androideabi** binary for `tokenizers`...\"],[\"`tokenizers-linux-arm-gnueabihf`\\n\\nThis is the **armv7-unknown-linux-gnueabihf** binary for `tokenize...\"],[\"`tokenizers-linux-arm64-gnu`\\n\\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`...\"],[\"`tokenizers-linux-x64-musl`\\n\\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`...\"],[\"`tokenizers-android-arm64`\\n\\nThis is the **aarch64-linux-android** binary for `tokenizers`...\"],[\"Changelog\\nAll notable changes to this project will be documented in this file.\\n\\nThe format is based ...\"],[\"- [#952] Fixed the vocabulary size of UnigramTrainer output (to respect added tokens)\\n- [#954] Fixed...\"],[\"### Added\\n- [#657]: Add SplitDelimiterBehavior customization to Punctuation constructor\\n- [#845]: Do...\"],[\"## [0.10.0]\\n\\n### Added\\n- [#508]: Add a Visualizer for notebooks to help understand how the tokenizer...\"],[\"### Changed\\n- [#506]: Improve Encoding mappings for pairs of sequence\\n\\n## [0.9.3]\\n\\n### Fixed\\n- [#470...\"],[\"## [0.8.1]\\n\\n### Fixed\\n- [#333]: Fix deserialization of `AddedToken`, where the content was not resto...\"],[\"### Fixed\\n- [#286]: Fix various crash when training a BPE model\\n- [#309]: Fixed a few bugs related t...\"],[\"## [0.7.0]\\n\\n### Changed\\n- Only one progress bar while reading files during training. This is better ...\"],[\"### Added\\n- [#188]: `ByteLevel` is also a `PostProcessor` now and handles trimming the offsets if ac...\"],[\"### How to migrate\\n- Add the `ByteLevel` `PostProcessor` to your byte-level BPE tokenizers if releva...\"],[\"## [0.5.2]\\n- [#163]: Do not open all files directly while training\\n\\n### Fixed\\n- We introduced a bug ...\"],[\"### Fixed\\n- [#137]: Fix a bug in the class `WordPieceTrainer` that prevented `BertWordPieceTokenizer...\"],[\"```\\noutput = tokenizer.encode(...)\\nprint(output.original_str.offsets(output.offsets[3]))...\"],[\"```\\n- [#99]: Exposed the vocabulary size on all tokenizers\\n\\n### Added\\n- Added `CharDelimiterSplit`: ...\"],[\"[#1096]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f1096\\n[#1072]: https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"[#960]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f960\\n[#919]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#770]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f770\\n[#762]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#616]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f616\\n[#590]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#492]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f492\\n[#481]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#378]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f378\\n[#363]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#272]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f272\\n[#249]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#160]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fissues\\u002f160\\n[#156]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002f...\"],[\"div align=\\\"center\\\"\\u003e\\n\\n  \\u003ch1\\u003e\\u003ccode\\u003ecreate-wasm-app\\u003c\\u002fcode\\u003e\\u003c\\u002fh1\\u003e\\n\\n  \\u003cstrong\\u003eAn \\u003ccode\\u003enpm init\\u003c\\u002fcode\\u003e tem...\"],[\"```\\nnpm init wasm-app\\n```\\n\\n## 🔋 Batteries Included\\n\\n- `.gitignore`: ignores `node_modules`\\n- `LICENS...\"],[\"Encode Inputs\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\nThese types represent all the different kinds of inp...\"],[\"alias of `Union[List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[st...\"],[\"Tokenizer\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## Tokenizer\\n\\n[[autodoc]] tokenizers.Tokenizer\\n    - all...\"],[\"Installation\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n🤗 Tokenizers is tested on Python 3.5+.\\n\\nYou should in...\"],[\"```\\n\\u003c\\u002frust\\u003e\\n\\u003cnode\\u003e\\n## Installation with npm\\n\\nYou can simply install 🤗 Tokenizers with npm using:\\n\\n``...\"],[\"`tokenizers-darwin-x64`\\n\\nThis is the **x86_64-apple-darwin** binary for `tokenizers`...\"],[\"# How to release\\n\\n# Before the release\\n\\nSimple checklist on how to make releases for `tokenizers`.\\n\\n...\"],[\"# Rust\\n\\n- `tokenizers` (rust, python & node) versions don't have to be in sync but it's\\n  very commo...\"],[\"# Python\\n\\n- Edit `bindings\\u002fpython\\u002fsetup.py` to reflect new version.\\n- Edit `bindings\\u002fpython\\u002fpy_src\\u002ft...\"],[\"# Node\\n\\n- Edit `bindings\\u002fnode\\u002fpackage.json` to reflect new version.\\n- Edit `CHANGELOG.md`:\\n    - Add...\"]],\"hovertemplate\":\"source=tokenizers\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"tokenizers, circle\",\"marker\":{\"color\":\"#FF6692\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"tokenizers, circle\",\"showlegend\":true,\"x\":[-3.4333642,-3.751231,-3.480599,-3.5754004,-3.6287682,-3.5079677,-3.4770544,-3.637348,-3.6554747,-3.6446226,-3.7679536,-3.59441,-3.5238523,-3.5266373,-3.3204732,-3.5066292,-3.497285,-3.340819,-3.540929,-3.4127905,-3.4900277,-3.2057445,-3.13014,-3.384641,-3.4598968,-3.443508,-3.342775,-3.443448,-3.4403005,-3.535739,-3.4458477,-3.4051294,-3.2828429,-3.4127786,-3.0695999,-2.3884017,-3.5174816,-3.5294187,-3.6110456,-3.4136605,-3.6586192,-3.5990891,-3.4631085,-3.6777635,5.0944743,4.930632,4.8495483,-3.558796,-3.3489084,0.40731415,1.0872934,-3.63619,-3.599891,-3.7436335,-2.8299923,-3.1639576,-3.651291,-3.1012547,-3.6185272,-3.6561856,-3.570023,-3.5672555,-3.4559603,-1.4556242,-3.6671965,-3.6872482,-3.6742854,-3.5995214,-3.6722817,-3.7589796,-3.7781625,-3.8743849,-3.637244,-3.551424,-3.6293273,-3.6711302,-3.567107,-3.682054,-3.6569793,-3.6976995,-3.264005,-3.8717792,-3.6402082,-3.589204,-3.6702495,-3.5939395,-3.5435295,-3.6496496,-3.640583,-3.630854,-3.7317696,6.0153947,5.2154856,-3.6375892,-4.090614,-4.1798987,-4.2432833,-4.251382,-4.243628,-4.1875978,8.279801,8.418094,8.217858,8.340814,-3.681777,-3.6950088,-3.6943824,-3.7088735,-3.6617866,-3.6779132,-4.0996127,-4.1755567,-4.220223,-4.2723093,-4.247791,-4.228446,-4.220068,-4.281436,-4.200729,-4.2285337,-4.221343,-4.2237315,-2.4290688,-4.082,8.288169,8.266426,8.258786,8.403676,8.391025,8.18959,8.251439,8.256521,5.235523,5.100437,-3.273596,-3.619071,-3.651767,-3.2893934,4.5611672,-3.664916,2.805526,4.039556,3.5613441,4.3196874],\"xaxis\":\"x\",\"y\":[-5.519006,-5.5548224,-5.5263042,-5.526639,-5.6907973,-5.5880218,-5.324195,-4.7181907,-5.024577,-4.97275,-5.4993377,-5.647392,-5.5594773,-5.260812,-5.539824,-5.6938667,-5.661982,-5.351774,-5.4544525,-5.4693694,-5.567738,-5.2728624,-5.1747355,-5.4086003,-5.433134,-5.51238,-5.6521144,-5.748305,-5.66985,-5.662508,-5.5988703,-5.6299305,-5.581453,-5.5708814,-5.3699746,-4.5250583,-5.579759,-5.5875163,-5.4685154,-4.290661,-5.720839,-5.7512856,-5.3753705,-7.82045,-2.379442,-1.8631877,-1.7751997,-6.0733232,-5.2269573,-4.1195283,-3.544637,-5.982571,-6.122679,-5.9146504,-4.97692,-5.153982,-4.941411,-5.1414075,-5.7618895,-5.5915694,-5.8963594,-5.8363094,-5.332587,2.6169915,-7.7780704,-7.738372,-7.741933,-6.071557,-5.675294,-5.645538,-5.580761,-5.57647,-5.564323,-5.563461,-5.735356,-5.7083645,-5.69195,-5.733312,-5.674751,-5.334568,-5.508954,-5.911816,-5.548334,-6.07183,-7.736009,-6.049575,-5.9239516,-6.0369077,-6.0816517,-6.011793,-5.7454915,-3.631894,-1.2421168,-7.817695,-7.076783,-7.1659293,-7.2355494,-7.2066236,-7.234396,-7.189422,-0.9962103,-0.916816,-1.0070863,-0.9618266,-7.7683525,-7.76862,-7.8126516,-7.806561,-7.7810535,-7.793796,-6.989038,-7.1286182,-7.257481,-7.12874,-7.2429624,-7.1990676,-7.2602196,-7.22771,-7.1678247,-7.167305,-7.112887,-7.1930842,-4.927286,-6.9293494,-0.97636735,-0.99872947,-0.957498,-0.96148324,-0.9782444,-0.9910076,-0.95631903,-0.9877831,-2.270144,-2.520497,-5.4914675,-6.072772,-6.0815277,-6.431824,-1.3427385,-7.762608,0.14148732,-0.59537643,-0.3643362,-0.29556984],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nYou can then provide the `generate_map` method as an argument to the `sm.ParallelRLEnv` class, ...\"],[\"How to contribute to simulate?\\n[![Contributor Covenant](https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002fContributor%20C...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n\\t```bash\\n\\tgit checkout -b a-descripti...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Simulate with Godot\\n\\n### Install in Godot 4\\nThis integration has been developed for Godot 4.x. You...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\npip install --upgrade simulate\\n```\\nBefore you merge a PR, fix the style (we use `isort` + `black...\"],[\"```\\nimport simulate as sm\\n\\nscene = sm.Scene()\\nscene += sm.Plane() + sm.Sphere(position=[0, 1, 0], ra...\"],[\"```\\npython examples\\u002fbasic\\u002fobjects.py\\n```\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fuser-imag...\"],[\"```\\n# Add two copy of the sphere to the scene as children of the root node (using list will add all ...\"],[\"```\\n\\n### Visualization engine\\n\\nA default vizualization engine is provided with the vtk backend of `p...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\nscene = sm.Scene.create_from(\\\"simulate-tests\\u002fBox\\u002fglTF-Embedded\\u002fBox.gltf\\\")\\n```\\n\\nor, they can be t...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" href=\\\".\\u002fc...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"docs\\u002fsource\\u002fassets\\u002fsimulate_library.png\\\" width=\\\"400\\\"\\u002f\\u003e\\n    ...\"],[\"```\\npip install --upgrade simulate\\n```\\nBefore you merge a PR, fix the style (we use `isort` + `black...\"],[\"```\\nfrom simulate import Scene\\n\\nscene = Scene.create_from('tests\\u002ftest_assets\\u002ffixtures\\u002fBox.gltf')  # ...\"],[\"```\\n\\nAn object (as well as the Scene) is just a node in a tree provided with optional mesh (under th...\"],[\"```\\npython examples\\u002fbasic\\u002fobjects.py\\n```\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fuser-imag...\"],[\"```\\n# Add two copy of the sphere to the scene as children of the root node (using list will add all ...\"],[\"```\\n\\nEditing objects:\\n- mesh of the object can be edited with all the manipulation operator provided...\"],[\"# Unity Integration\\n\\n### Install with the Unity editor\\nCurrently we use Unity version `2021.3.2f1` a...\"],[\"```\\n{\\n    \\\"type\\\": \\\"MyCommand\\\",\\n    \\\"contents\\\": json.dumps({\\n        \\\"message\\\": \\\"hello from python AP...\"],[\"```\\n\\nThis currently only supports Box, Sphere, and Capsule colliders (the Unity\\u002fPhysX colliders).\\n\\nD...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Security Policy\\n\\n## Supported Versions\\n\\u003c!--\\nUse this section to tell people about which versions of ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"his package provides core backend functionality for the Hugging Face Simulate project: (https:\\u002f\\u002fgith...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The \\\"leaf\\\" reward functions can be combined in a tree structure with the following predicate functio...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\nfor i in range(60):\\n    event = scene.step()\\n```\\nYou should see the cube falling onto the plane....\"],[\"```\\nscene.config.return_nodes = False\\nscene.config.return_frames = False\\nscene.show()\\n```\\nFor advanc...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The created environments are stored in a language\\u002fframework agnostic format and can be loaded and ru...\"],[\"# Blender Integration\\n\\n### Install addon in Blender\\nThis integration has been developed for Blender ...\"],[\"Tests examples taken from the original great gltflib\\n\\nFind the great gltflib by Lukas Shawford here:...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Examples for Simulate\\n\\nThe examples are organized by level of complexity or application. \\nCurrentl...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nNow we need to identify which build your GPU is using and add it to your xorg config file:\\n\\n```...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Reward function:\\n- A dense reward based on improvement in best euclidean distance to the object\\n- A ...\"],[\"Example: [`sb3_procgen.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fsimulate\\u002fexamples\\u002frl\\u002fsb3_procgen.py)\\n\\nObj...\"],[\"Parallel: 16 independent instances of the same environment configuration.\\n\\n\\n## Reward functions base...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"]],\"hovertemplate\":\"source=simulate\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"simulate, circle\",\"marker\":{\"color\":\"#B6E880\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"simulate, circle\",\"showlegend\":true,\"x\":[-5.2717657,1.3499266,5.0406337,4.657643,6.691695,6.3234754,-5.8047366,-4.3552656,-5.4676704,-5.194463,1.937646,-5.1341915,-6.0270743,6.677778,-4.9588876,-5.3330374,6.446724,7.096133,7.259197,-5.0474634,-5.514407,-5.3393364,-5.2096562,2.1671128,-5.1740465,-5.2769566,-5.88496,3.096627,-4.886616,6.76028,5.6041727,5.7471256,6.7368546,6.3664136,-5.506091,-7.1779366,5.7926536,-5.2858534,-5.3846717,-6.128853,6.8182597,6.7800927,-5.457885,-5.702678,-5.4504414,5.3366404,6.5647,-5.8116827,6.7470164,-2.3479028,0.88467234,6.6707525,-5.836884,-7.03331,-7.077304,-6.4454603,-6.898385,6.708356,-4.5996284],\"xaxis\":\"x\",\"y\":[6.87408,-4.21551,-1.1062146,-0.16148481,0.76166284,0.6996109,6.841178,7.1452804,6.985228,7.0614243,-3.084436,7.085855,6.6916237,0.6974267,7.0690775,7.012791,0.7187758,-4.642788,-4.664166,6.99356,6.944912,7.055874,7.0861053,-3.0260036,7.080659,7.050475,6.798744,-2.6428075,7.014972,0.72705704,0.03799682,-1.7707467,0.57254845,0.59066087,6.845179,6.6170106,-4.459823,6.9883146,7.040501,6.8140516,-0.76790756,0.72462696,6.801898,6.832177,6.993382,-4.2241077,0.6328018,6.8426185,0.75873756,2.1732943,1.5749011,0.74283427,6.6510262,6.5902543,6.5654683,6.6819263,6.5394945,0.71839744,6.8178787],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"# How to release\\n\\n# Before the release\\n\\nSimple checklist on how to make releases for `safetensors`.\\n...\"],[\"# Rust\\n\\n- `safetensors` (rust, python & node) versions don't have to be in sync but it's\\n  very comm...\"],[\"# Python\\n\\n- Edit `bindings\\u002fpython\\u002fsetup.py` to reflect new version.\\n- Edit `bindings\\u002fpython\\u002fpy_src\\u002fs...\"],[\"# Node\\n\\n- Edit `bindings\\u002fnode\\u002fpackage.json` to reflect new version.\\n- Edit `CHANGELOG.md`:\\n    - Add...\"],[\"Flax API\\n\\n[[autodoc]] safetensors.flax.load_file\\n[[autodoc]] safetensors.flax.load\\n[[autodoc]] safet...\"],[\"Convert weights to safetensors\\n\\nPyTorch model weights are commonly saved and stored as `.bin` files ...\"],[\"Numpy API\\n\\n[[autodoc]] safetensors.numpy.load_file\\n[[autodoc]] safetensors.numpy.load\\n[[autodoc]] sa...\"],[\"Speed Comparison\\n\\n\\u003ca href=\\\"https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002f...\"],[\"```\\n\\n### CPU benchmark\\n\\n```py\\n\\u003e\\u003e\\u003e start_st = datetime.datetime.now()\\n\\u003e\\u003e\\u003e weights = load_file(sf_file...\"],[\"```\\n\\nThis speedup is due to the fact that this library avoids unnecessary copies by mapping the file...\"],[\"```\\n\\nThe speedup works because this library is able to skip unnecessary CPU allocations. It is unfor...\"],[\"# Installation\\n\\n```\\npip install safetensors\\n```\\n\\n\\n## Usage\\n\\n### Numpy\\n\\n```python\\nfrom safetensors.nu...\"],[\"PaddlePaddle API\\n\\n[[autodoc]] safetensors.paddle.load_file\\n[[autodoc]] safetensors.paddle.load\\n[[aut...\"],[\"he purpose of this directory is to showcase various attacks (and creating your own).\\n\\n\\n# Torch Arbit...\"],[\"```\\npython numpy_dos_create.py\\npython numpy_dos_get_pwned.py\\n```\\n\\n# Safetensors abuse attempts\\n\\nIn o...\"],[\"```\\npython safetensors_abuse_attempt_1.py\\npython safetensors_abuse_attempt_2.py\\npython safetensors_a...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cpicture\\u003e\\n    \\u003csource media=\\\"(prefers-color-scheme: dark)\\\" srcset=\\\"https:\\u002f\\u002fhuggi...\"],[\"Rust\\n[![Crates.io](https:\\u002f\\u002fimg.shields.io\\u002fcrates\\u002fv\\u002fsafetensors.svg)](https:\\u002f\\u002fcrates.io\\u002fcrates\\u002fsafete...\"],[\"```\\n\\n#### From source\\n\\nFor the sources, you need Rust\\n\\n```bash\\n# Install Rust\\ncurl --proto '=https' ...\"],[\"```\\n\\n[Python documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fsafetensors\\u002findex)\\n\\n\\n### Format\\n\\n- 8 bytes: ...\"],[\"Notes:\\n - Duplicate keys are disallowed. Not all parsers may respect this.\\n - In general the subset ...\"],[\"Let's take a look at alternatives and why this format is deemed interesting.\\nThis is my very persona...\"],[\"- Safe: Can I use a file randomly downloaded and expect not to run arbitrary code ?\\n- Zero-copy: Doe...\"],[\"### Main oppositions\\n\\n- Pickle: Unsafe, runs arbitrary code\\n- H5: Apparently now discouraged for TF\\u002f...\"],[\"### Notes\\n\\n- Zero-copy: No format is really zero-copy in ML, it needs to go from disk to RAM\\u002fGPU RAM...\"],[\"- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to\\nload only...\"],[\"Metadata Parsing\\n\\nGiven the simplicity of the format, it's very simple and efficient to fetch and pa...\"],[\"## Usage\\n\\n### JavaScript\\u002fTypeScript[[js]]\\n\\nUsing [`huggingface.js`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhugg...\"],[\"```\\n\\nDepending on whether the safetensors weights are sharded into multiple files or not, the output...\"],[\"```\\n\\n### Python\\n\\nIn this example python script, we are parsing metadata of [gpt2](https:\\u002f\\u002fhuggingfac...\"],[\"```\\n\\n## Example output\\n\\nFor instance, here are the number of params per dtype for a few models on th...\"],[\"model | safetensors | params\\n--- | --- | ---\\n[gpt2](https:\\u002f\\u002fhuggingface.co\\u002fgpt2?show_tensors=true) |...\"],[\"[bigscience\\u002fbloom](https:\\u002f\\u002fhuggingface.co\\u002fbigscience\\u002fbloom?show_tensors=true) | sharded | { 'BF16' =...\"],[\"Torch API\\n\\n[[autodoc]] safetensors.torch.load_file\\n[[autodoc]] safetensors.torch.load\\n[[autodoc]] sa...\"],[\"Tensorflow API\\n\\n[[autodoc]] safetensors.tensorflow.load_file\\n[[autodoc]] safetensors.tensorflow.load...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"block dark:h...\"],[\"```\\n\\n### Save tensors\\n\\n```python\\nimport torch\\nfrom safetensors.torch import save_file\\n\\ntensors = {\\n ...\"],[\"```\\n\\n## Format\\n\\nLet's say you have safetensors file named `model.safetensors`, then `model.safetenso...\"],[\"* [huggingface\\u002ftransformers](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers)\\n* [AUTOMATIC1111\\u002fstable-di...\"],[\"* [PaddlePaddle\\u002fPaddleNLP](https:\\u002f\\u002fgithub.com\\u002fPaddlePaddle\\u002fPaddleNLP)\\n* [AIGC-Audio\\u002fAudioGPT](https:...\"],[\"Torch shared tensors\\n\\n\\n## TL;DR\\n\\nUsing specific functions, which should work in most cases for you.\\n...\"],[\"```\\n\\n## Why are shared tensors not saved in `safetensors` ?\\n\\nMultiple reasons for that:\\n\\n- *Not all ...\"],[\"```\\n\\nNow with all those reasons being mentioned, nothing is set in stone in there.\\nShared tensors do...\"]],\"hovertemplate\":\"source=safetensors\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"safetensors, circle\",\"marker\":{\"color\":\"#FF97FF\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"safetensors, circle\",\"showlegend\":true,\"x\":[3.4263628,3.886827,3.6149855,4.2934346,1.9914811,0.026747432,2.3794277,0.019404855,-0.9675772,-1.0208704,-0.7933103,1.2369108,2.7056625,1.5757048,1.8778493,1.9168224,6.265657,3.7021153,1.3079119,1.496586,-0.1798868,-5.446523,0.0034037754,0.019514097,-0.49941424,-1.4451163,4.481037,4.808927,2.1451433,2.83162,3.8422952,-3.1991377,5.2752852,0.9088623,1.0884011,1.1869665,0.60286486,4.9379725,-0.83421844,-8.381611,-0.5763189,-0.079823114,0.08119413],\"xaxis\":\"x\",\"y\":[-0.20417634,-0.46752486,-0.17914881,-0.52124816,-1.9695855,-0.10710679,-1.8376666,1.9241318,1.9634575,2.9706192,2.4561834,-0.28342533,-2.0960383,-0.3555279,-1.6094842,-1.6792114,-1.8258092,-1.2160692,0.041813344,-1.7907273,-0.59945273,1.3748176,0.21445362,0.21990548,1.0652695,1.9986341,-2.951185,-3.8876057,-1.5805833,-2.561762,-3.4591017,-0.26652527,-4.352551,-0.9141138,-0.78496563,-0.5257457,-0.51820207,-2.0403268,7.56493,-5.547133,0.09123667,0.29936698,-0.39649114],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"How to create a pipeline object?\"]],\"hovertemplate\":\"source=User query\\u003cbr\\u003esymbol=star\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"User query, star\",\"marker\":{\"color\":\"black\",\"size\":[100],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"diamond\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"User query, star\",\"showlegend\":true,\"x\":[1.8477427],\"xaxis\":\"x\",\"y\":[6.627674],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"title\":{\"text\":\"\\u003cb\\u003eChunk source\\u003c\\u002fb\\u003e\"},\"tracegroupgap\":0,\"itemsizing\":\"constant\"},\"margin\":{\"t\":60},\"height\":700,\"width\":1000,\"title\":{\"text\":\"\\u003cb\\u003e2D Projection of Chunk Embeddings via PaCMAP\\u003c\\u002fb\\u003e\"}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('64c5821f-3b02-4014-a2ae-3a4eb1608749');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"}],"source":["df = pd.DataFrame.from_dict(\n","    [\n","        {\n","            \"x\": documents_projected[i, 0],\n","            \"y\": documents_projected[i, 1],\n","            \"source\": docs_processed[i].metadata[\"source\"].split(\"/\")[1],\n","            \"extract\": docs_processed[i].page_content[:100] + \"...\",\n","            \"symbol\": \"circle\",\n","            \"size_col\": 4,\n","        }\n","        for i in range(len(docs_processed))\n","    ]\n","    + [\n","        {\n","            \"x\": documents_projected[-1, 0],\n","            \"y\": documents_projected[-1, 1],\n","            \"source\": \"User query\",\n","            \"extract\": user_query,\n","            \"size_col\": 100,\n","            \"symbol\": \"star\",\n","        }\n","    ]\n",")\n","\n","# Visualize the embedding\n","fig = px.scatter(\n","    df,\n","    x=\"x\",\n","    y=\"y\",\n","    color=\"source\",\n","    hover_data=\"extract\",\n","    size=\"size_col\",\n","    symbol=\"symbol\",\n","    color_discrete_map={\"User query\": \"black\"},\n","    width=1000,\n","    height=700,\n",")\n","fig.update_traces(\n","    marker=dict(opacity=1, line=dict(width=0, color=\"DarkSlateGrey\")),\n","    selector=dict(mode=\"markers\"),\n",")\n","fig.update_layout(\n","    legend_title_text=\"<b>Chunk source</b>\",\n","    title=\"<b>2D Projection of Chunk Embeddings via PaCMAP</b>\",\n",")\n","fig.show()"]},{"cell_type":"code","execution_count":16,"id":"c48f8c1b","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:04:53.930985Z","iopub.status.busy":"2024-06-14T03:04:53.930586Z","iopub.status.idle":"2024-06-14T03:04:53.934874Z","shell.execute_reply":"2024-06-14T03:04:53.933988Z"},"papermill":{"duration":0.102998,"end_time":"2024-06-14T03:04:53.936724","exception":false,"start_time":"2024-06-14T03:04:53.833726","status":"completed"},"tags":[]},"outputs":[],"source":["# print(f\"\\nStarting retrieval for {user_query=}...\")\n","# retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n","# print(\"\\n==================================Top document==================================\")\n","# print(retrieved_docs[0].page_content)\n","# print(\"==================================Metadata==================================\")\n","# print(retrieved_docs[0].metadata)"]},{"cell_type":"markdown","id":"9fc0159c","metadata":{"papermill":{"duration":0.09952,"end_time":"2024-06-14T03:04:54.138335","exception":false,"start_time":"2024-06-14T03:04:54.038815","status":"completed"},"tags":[]},"source":["## Create Reader Section"]},{"cell_type":"markdown","id":"3552c4c8","metadata":{"papermill":{"duration":0.100113,"end_time":"2024-06-14T03:04:54.338779","exception":false,"start_time":"2024-06-14T03:04:54.238666","status":"completed"},"tags":[]},"source":["#### LLM prompt template"]},{"cell_type":"code","execution_count":17,"id":"65b19234","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:04:54.544428Z","iopub.status.busy":"2024-06-14T03:04:54.544069Z","iopub.status.idle":"2024-06-14T03:04:54.59987Z","shell.execute_reply":"2024-06-14T03:04:54.598772Z"},"papermill":{"duration":0.162714,"end_time":"2024-06-14T03:04:54.601933","exception":false,"start_time":"2024-06-14T03:04:54.439219","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n"]},{"name":"stdout","output_type":"stream","text":["<|im_start|>system\n","Using the information contained in the context,\n","give a comprehensive answer to the question.\n","Respond only to the question asked, response should be concise and relevant to the question.\n","Provide the number of the source document when relevant.\n","If the answer cannot be deduced from the context, do not give an answer.<|im_end|>\n","<|im_start|>user\n","Context:\n","{context}\n","---\n","Now here is the question you need to answer.\n","\n","Question: {question}<|im_end|>\n","<|im_start|>assistant\n","\n"]}],"source":["prompt_in_chat_format = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": \"\"\"Using the information contained in the context,\n","give a comprehensive answer to the question.\n","Respond only to the question asked, response should be concise and relevant to the question.\n","Provide the number of the source document when relevant.\n","If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n","    },\n","    {\n","        \"role\": \"user\",\n","        \"content\": \"\"\"Context:\n","{context}\n","---\n","Now here is the question you need to answer.\n","\n","Question: {question}\"\"\",\n","    },\n","]\n","RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n","    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",")\n","print(RAG_PROMPT_TEMPLATE)"]},{"cell_type":"code","execution_count":18,"id":"dab9154e","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:04:54.806485Z","iopub.status.busy":"2024-06-14T03:04:54.805878Z","iopub.status.idle":"2024-06-14T03:07:29.755025Z","shell.execute_reply":"2024-06-14T03:07:29.753986Z"},"papermill":{"duration":155.05396,"end_time":"2024-06-14T03:07:29.757561","exception":false,"start_time":"2024-06-14T03:04:54.703601","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-06-14 03:04:56.278096: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-06-14 03:04:56.278187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-06-14 03:04:56.391708: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cef196e9fd84477f893316eac0755f3f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1e7e7dd7c7c43a48cb5ba45ec8525f0","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70ed47c1e7ea4fdea6f735a6f03f9944","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff8501c4cddb42c0813be09473730bb0","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eaedf05453f349c883684ed246b9dce8","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e70591f7c5a743669317e5f2e9712d65","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3baf9cf7c606432ab3f25c6e38280b2e","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54cfe43c109f49f2afccdde77a6dff9a","version_major":2,"version_minor":0},"text/plain":["model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e87deb700ea249caaf42659f8db228c4","version_major":2,"version_minor":0},"text/plain":["model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bc03ffa2c024bb789e6ba9e27d6fd71","version_major":2,"version_minor":0},"text/plain":["model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c49dde521d9f4662a68c88afdae5b733","version_major":2,"version_minor":0},"text/plain":["model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d68241589944c2d9799e8e0117691c9","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3702cd715e442a4867d312067d96404","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e63b55bba8245acab2b85ff935766b4","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55659a47f5f14402b049f88c00026f5d","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28b75f1581154a4c9f000a62b6429120","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8156d8f6021c48cf88f55352926ef89a","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d29cdbda3aa408787f976d335f8ab2b","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import pipeline\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n",")\n","model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\n","tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n","\n","READER_LLM = pipeline(\n","    model=model,\n","    tokenizer=tokenizer,\n","    task=\"text-generation\",\n","    do_sample=True,\n","    temperature=0.2,\n","    repetition_penalty=1.1,\n","    return_full_text=False,\n","    max_new_tokens=500,\n",")"]},{"cell_type":"markdown","id":"0146e14b","metadata":{"papermill":{"duration":0.11157,"end_time":"2024-06-14T03:07:29.991845","exception":false,"start_time":"2024-06-14T03:07:29.880275","status":"completed"},"tags":[]},"source":["## Rerank Top K documents (optional)"]},{"cell_type":"code","execution_count":19,"id":"b62aeaec","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:07:30.217003Z","iopub.status.busy":"2024-06-14T03:07:30.215561Z","iopub.status.idle":"2024-06-14T03:07:40.129944Z","shell.execute_reply":"2024-06-14T03:07:40.129161Z"},"papermill":{"duration":10.032372,"end_time":"2024-06-14T03:07:40.132186","exception":false,"start_time":"2024-06-14T03:07:30.099814","status":"completed"},"tags":[]},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c678c5ad778e4aa4937dfea62ca38f33","version_major":2,"version_minor":0},"text/plain":["artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning:\n","\n","`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a99305452f8449ad8ed66534895e8c92","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d476ee1c9007471d81923b879b009acf","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c7d6fcf2861456e860a3489f06e9bd8","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"051902140e4d40fd8ab36cbbfc29f366","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09b07c1afa5d4d90adf37d50ba959d06","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96ddbad0ad7b4c4b8abd47105ffdaf07","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from ragatouille import RAGPretrainedModel\n","\n","RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"]},{"cell_type":"markdown","id":"f9e7ae47","metadata":{"papermill":{"duration":0.105246,"end_time":"2024-06-14T03:07:40.345427","exception":false,"start_time":"2024-06-14T03:07:40.240181","status":"completed"},"tags":[]},"source":["## Function for Retriever and Reader part of the rag"]},{"cell_type":"code","execution_count":20,"id":"bbae42eb","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:07:40.562779Z","iopub.status.busy":"2024-06-14T03:07:40.562036Z","iopub.status.idle":"2024-06-14T03:07:40.57102Z","shell.execute_reply":"2024-06-14T03:07:40.570155Z"},"papermill":{"duration":0.122182,"end_time":"2024-06-14T03:07:40.572963","exception":false,"start_time":"2024-06-14T03:07:40.450781","status":"completed"},"tags":[]},"outputs":[],"source":["from transformers import Pipeline\n","\n","\n","def answer_with_rag(\n","    question: str,\n","    llm: Pipeline,\n","    knowledge_index: FAISS,\n","    reranker: Optional[RAGPretrainedModel] = None,\n","    num_retrieved_docs: int = 30,\n","    num_docs_final: int = 5,\n",") -> Tuple[str, List[LangchainDocument]]:\n","    # Gather documents with retriever\n","    print(\"=> Retrieving documents...\")\n","    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n","    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n","\n","    # Optionally rerank results\n","    if reranker:\n","        print(\"=> Reranking documents...\")\n","        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n","        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n","\n","    relevant_docs = relevant_docs[:num_docs_final]\n","\n","    # Build the final prompt\n","    context = \"\\nExtracted documents:\\n\"\n","    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n","\n","    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n","\n","    # Redact an answer\n","    print(\"=> Generating answer...\")\n","    answer = llm(final_prompt)[0][\"generated_text\"]\n","\n","    return answer, relevant_docs"]},{"cell_type":"markdown","id":"7a8858f4","metadata":{"papermill":{"duration":0.105338,"end_time":"2024-06-14T03:07:40.787199","exception":false,"start_time":"2024-06-14T03:07:40.681861","status":"completed"},"tags":[]},"source":["## Testing the RAG\n","#### Asking question 'how to create a pipeline object'"]},{"cell_type":"code","execution_count":21,"id":"fa996a03","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:07:41.000546Z","iopub.status.busy":"2024-06-14T03:07:40.999744Z","iopub.status.idle":"2024-06-14T03:08:47.164731Z","shell.execute_reply":"2024-06-14T03:08:47.163872Z"},"papermill":{"duration":66.275145,"end_time":"2024-06-14T03:08:47.167283","exception":false,"start_time":"2024-06-14T03:07:40.892138","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["=> Retrieving documents...\n","=> Reranking documents...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["=> Generating answer...\n"]}],"source":["question = \"how to create a pipeline object?\"\n","\n","answer, relevant_docs = answer_with_rag(question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER)"]},{"cell_type":"markdown","id":"c174b643","metadata":{"papermill":{"duration":0.107108,"end_time":"2024-06-14T03:08:47.385024","exception":false,"start_time":"2024-06-14T03:08:47.277916","status":"completed"},"tags":[]},"source":["#### The answer"]},{"cell_type":"code","execution_count":22,"id":"7c8ab314","metadata":{"execution":{"iopub.execute_input":"2024-06-14T03:08:47.601795Z","iopub.status.busy":"2024-06-14T03:08:47.60106Z","iopub.status.idle":"2024-06-14T03:08:47.606982Z","shell.execute_reply":"2024-06-14T03:08:47.606122Z"},"papermill":{"duration":0.118008,"end_time":"2024-06-14T03:08:47.609824","exception":false,"start_time":"2024-06-14T03:08:47.491816","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["==================================Answer==================================\n","To create a pipeline object, follow these steps:\n","\n","1. Import the necessary module, `pipeline`, from the Hugging Face Transformers library:\n","\n","   ```python\n","   from transformers import pipeline\n","   ```\n","\n","2. Specify the task you want to perform with the pipeline. Here, we will use object detection as an example:\n","\n","   ```python\n","   >>> object_detector = pipeline('object-detection')\n","   ```\n","\n","   Alternatively, you can also specify the exact version of the model you want to use by passing its name to the `from_pretrained()` function:\n","\n","   ```python\n","   >>> from transformers import AutoModelForObjectDetection\n","   >>> from transformers import AutoConfig\n","   >>> from transformers import TrainingArguments\n","   >>> from transformers import Trainer\n","   >>> config = AutoConfig.from_pretrained('google/vision_transformer_large_patch16_224')\n","   >>> model = AutoModelForObjectDetection.from_config(config)\n","   >>> trainer = Trainer(model=model, args=TrainingArguments(...))\n","   >>> trainer.train()\n","   >>> object_detector = pipeline('object-detection', model='my_custom_model')\n","   ```\n","\n","3. If you want to use multiple pipelines, you can reuse the same components from a checkpoint instead of reloading them to save memory. To do this, use the `from_pipe()` method provided by the `AutoPipelineForImage2Image` class:\n","\n","   ```python\n","   from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\n","   import torch\n","\n","   pipeline_text2img = AutoPipelineForText2Image.from_pretrained(\n","       \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n","   )\n","   print(type(pipeline_text2img))\n","   # Output: \"<class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'>\"\n","\n","   pipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)\n","   print(\n","==================================Source docs==================================\n","Document 0------------------------------------------------------------\n","# Allocate a pipeline for object detection\n",">>> object_detector = pipeline('object-detection')\n",">>> object_detector(image)\n","[{'score': 0.9982201457023621,\n","  'label': 'remote',\n","  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\n"," {'score': 0.9960021376609802,\n","  'label': 'remote',\n","  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},\n"," {'score': 0.9954745173454285,\n","  'label': 'couch',\n","  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},\n"," {'score': 0.9988006353378296,\n","  'label': 'cat',\n","  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},\n"," {'score': 0.9986783862113953,\n","  'label': 'cat',\n","  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]\n","Document 1------------------------------------------------------------\n","# Allocate a pipeline for object detection\n",">>> object_detector = pipeline('object_detection')\n",">>> object_detector(image)\n","[{'score': 0.9982201457023621,\n","  'label': 'remote',\n","  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\n"," {'score': 0.9960021376609802,\n","  'label': 'remote',\n","  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},\n"," {'score': 0.9954745173454285,\n","  'label': 'couch',\n","  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},\n"," {'score': 0.9988006353378296,\n","  'label': 'cat',\n","  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},\n"," {'score': 0.9986783862113953,\n","  'label': 'cat',\n","  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]\n","Document 2------------------------------------------------------------\n","Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this guide, you'll use the [`pipeline`] for sentiment analysis as an example:\n","\n","```py\n",">>> from transformers import pipeline\n","\n",">>> classifier = pipeline(\"sentiment-analysis\")\n","Document 3------------------------------------------------------------\n","```\n","\n","2. Pass a prompt to the pipeline to generate an image:\n","\n","```py\n","image = pipeline(\n","\t\"stained glass of darth vader, backlight, centered composition, masterpiece, photorealistic, 8k\"\n",").images[0]\n","image\n","Document 4------------------------------------------------------------\n","```\n","\n","## Use multiple pipelines\n","\n","For some workflows or if you're loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory. For example, if you're using a checkpoint for text-to-image and you want to use it again for image-to-image, use the [`~AutoPipelineForImage2Image.from_pipe`] method. This method creates a new pipeline from the components of a previously loaded pipeline at no additional memory cost.\n","\n","The [`~AutoPipelineForImage2Image.from_pipe`] method detects the original pipeline class and maps it to the new pipeline class corresponding to the task you want to do. For example, if you load a `\"stable-diffusion\"` class pipeline for text-to-image:\n","\n","```py\n","from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\n","import torch\n","\n","pipeline_text2img = AutoPipelineForText2Image.from_pretrained(\n","    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n",")\n","print(type(pipeline_text2img))\n","\"<class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'>\"\n","```\n","\n","Then [`~AutoPipelineForImage2Image.from_pipe`] maps the original `\"stable-diffusion\"` pipeline class to [`StableDiffusionImg2ImgPipeline`]:\n","\n","```py\n","pipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)\n","print(type(pipeline_img2img))\n","\"<class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'>\"\n"]}],"source":["print(\"==================================Answer==================================\")\n","print(f\"{answer}\")\n","print(\"==================================Source docs==================================\")\n","for i, doc in enumerate(relevant_docs):\n","    print(f\"Document {i}------------------------------------------------------------\")\n","    print(doc)"]},{"cell_type":"code","execution_count":null,"id":"36a883ff","metadata":{"papermill":{"duration":0.116695,"end_time":"2024-06-14T03:08:47.840461","exception":false,"start_time":"2024-06-14T03:08:47.723766","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":610.232805,"end_time":"2024-06-14T03:08:50.780015","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-14T02:58:40.54721","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0058e3538d5f4f41bd0d1869d0ccd304":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"022a622513534f61ad636fe3f846f703":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"033a6d18544e4f77ba56910eb59b7ce2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e91f1a20958b4bc99cbf0f4ea445ced2","max":21.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_b97f941dda234c6096ba2ed9babde775","value":21.0}},"03628380c5cf4722824a5e9211007556":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03d1547f580440e88e7f4524b94b07d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0496a042f25341669e625bf714c8d81a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"051721480b9742249c7d92fc1d8a0f96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"051902140e4d40fd8ab36cbbfc29f366":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be789cf5db61448d9e91c8fb80a86922","IPY_MODEL_76532d63110c46b99300214596a02944","IPY_MODEL_3e8d039e025c4ba5bfd44940e79496ba"],"layout":"IPY_MODEL_2c063816e59043239cee465bc6585380"}},"053a793f70804a9db7e1214199a8fdf3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05e5e606a1534cd39fcf0c52cf9de6ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07fed3b9edd74cd09a855e3fb137b1a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0818e70d89c345b29ac483e4583b3efc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0882c96bacd04c29903d9733082c29ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddca534a2c854489bca3e0fb97a4a51a","placeholder":"​","style":"IPY_MODEL_85b3536e2e1b43bab3a431f787d21325","value":" 405/405 [00:00&lt;00:00, 36.4kB/s]"}},"08b238fa6b7742bb951e0588350097ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"08c16952e7124c4f8d3fb4b9974837b7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09b07c1afa5d4d90adf37d50ba959d06":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef585135bccf4d0c90322dbd04fb7191","IPY_MODEL_0e16f629756e49178f4d5e16b0581574","IPY_MODEL_c0810651ddd64527a1bd20619ab7b785"],"layout":"IPY_MODEL_03628380c5cf4722824a5e9211007556"}},"0a66f6f8eb5847eba71f17ffd0d9c3eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b0ebff82e0540d7b73beb6b427e50d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b8c46fb5d694da688ef869d32143498":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a0f8c50d9bdf4131b05bc90851c0fa3d","IPY_MODEL_4ff23737098f4b908bc24144e51f9df7","IPY_MODEL_fc8e7c29d76242e38596d952ef4c1d88"],"layout":"IPY_MODEL_127c992b77414626af471e5201ee2eb7"}},"0b9e12bb3c4f4dc4967a7452b0696bfe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd3f165a119f4e44b435e5053777e617","max":711661.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_ccde035c629d42d49461e2606a063828","value":711661.0}},"0bae1c7af3e54d779808d00cc097265d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bd1fc9d4f6b4a40996cae139ae8bb23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_26f9f1b0c76140c0801c0de8afa1ec26","max":815834680.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_6601a05235c34765b14b31ae26e792df","value":815834680.0}},"0c972ee5b8174023a56262da80e72d88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0caf31725a744766aeae13171135f7f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0d007fccd0e94d52934c76615f015c91":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e16f629756e49178f4d5e16b0581574":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ecd4abbc5a14fef9059a2c40686e912","max":466081.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_d9593ae0a47641fe87e32ade47d9ab8f","value":466081.0}},"0e63b55bba8245acab2b85ff935766b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72c174939ca04485b7cb9656f414a9a1","IPY_MODEL_ae9cd5af5fdb4b079102f38217dbd082","IPY_MODEL_e2ffa09801db458390deafdab584e921"],"layout":"IPY_MODEL_33e2fd8ed3d14577952fb991f4bef9f3"}},"0e9062d37b2c4f9b8c91c0bbef019701":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ef1cdad9ce8446aa0cd61cfcbbcb15f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f58b9392b8243389419eb1bdacb0587":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fe20625b3b54256b8bb71caa3f1b85f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8248774dccc44276b3aa960b5a293b27","placeholder":"​","style":"IPY_MODEL_053a793f70804a9db7e1214199a8fdf3","value":"model.safetensors: 100%"}},"104189e5632e4c0d93fc00947209ae3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d569c53d1a9f4c7f952efda6262c1f5a","placeholder":"​","style":"IPY_MODEL_262f60a4c0a94dc8aa4ad27743bde8e8","value":"Generating train split: 100%"}},"10ca9298b50f4d5489ea6b889e8476c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"127c992b77414626af471e5201ee2eb7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"134159dc913d44ea955dc75be9d6bb35":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13f82f14c7184b119d612d73cd2baf00":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14d226019db9470d89bee458caae5997":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_709e805c95224f12b5966a4e1d482c97","placeholder":"​","style":"IPY_MODEL_59f3eaa96abc48379a5749ac5900d3ec","value":" 638/638 [00:00&lt;00:00, 55.4kB/s]"}},"156386eecf4544ed8fbc8a3902ed2673":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"159aeed34fb045fc9f3ff256b6de7b53":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15af7a4a83cf41e19dccba3fdab52e4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"163a9ea619c440c19caaf527a80e4311":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21c07c4f503d4baaa750d7a83f510bc2","placeholder":"​","style":"IPY_MODEL_10ca9298b50f4d5489ea6b889e8476c8","value":"100%"}},"1673ee3d96184fe2bf8617d3e745cc34":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16768b26682f4b878b6005c7f9532316":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16a84543a9664aa9ab14981fd4b26dec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1745a18f2b1c41b19f795f1c8824fcb9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17648a59a4ce42cda833b97e0ae00f5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eed26568ebec4514af9b60c60fe0fd13","placeholder":"​","style":"IPY_MODEL_c687c1be13c84a86a55a391c0ed4ddc4","value":"100%"}},"19b40566aa7f45589257a913bee2f6c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e4350b4a3ec41aead5a94e4787cd541","max":1979781448.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_25909d2e00c641f1befc506b34d129e0","value":1979781448.0}},"1a91b938d8a9451d81dd5f158ed7dacf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7c0b65bcc1b4e9f9d48f848e0bc6f45","placeholder":"​","style":"IPY_MODEL_2f5127e799e540d19518285d4a9dbe94","value":"tokenizer_config.json: 100%"}},"1b276eb6997e47238d22e79c112106bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b29928272894de191e34cd3c32148e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df98349ff0cd413a892e50ce8eb3ca65","placeholder":"​","style":"IPY_MODEL_d926fd432d4942eea5d4857b861f0359","value":"config.json: 100%"}},"1cd779ac8b1f45d9a0a277e31e77105e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1df91716bceb4b3c984d3e3507fabbaf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e9aa7df2ec64a598cde4c465d0bb91e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ecd4abbc5a14fef9059a2c40686e912":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f3196db74cc42a28de30298f858e814":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb9c590ec6734faba2fa353f19238355","placeholder":"​","style":"IPY_MODEL_0b0ebff82e0540d7b73beb6b427e50d3","value":" 1.89G/1.89G [00:12&lt;00:00, 187MB/s]"}},"1fb2e2fc900949a8b3bf249271302515":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b74846acc1d4544910a90b8a8cde119","max":112.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_29a01c94ef9f49bcbfc869d2c7ac47b0","value":112.0}},"20206e40324945a0a4abddf68693e1fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21c07c4f503d4baaa750d7a83f510bc2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22286f4edee245d89f87a2882e092395":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd9cda9c835a4188870afab300dd2671","placeholder":"​","style":"IPY_MODEL_f2df2d42c78d464abf0deee03d315221","value":" 1.98G/1.98G [00:14&lt;00:00, 184MB/s]"}},"23238e110d9f46bf9e2de33ac8f9d603":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"236f8a2aa0544a75973eafcae1999fca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"23870fa0c3484e8fa318135f50f6edf5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23c06c6680ea476c97b275f581adce6c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23f9408b622240fda255b18798572ded":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b85fc9c13bff4e62a4fe3999a6883b50","placeholder":"​","style":"IPY_MODEL_b1c3ae0598bf4a27a9cad84d02ae6cd8","value":"model-00001-of-00008.safetensors: 100%"}},"2443eb90348f47e8ab92555a544d714e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2470ae86e799478998c051fcd29affaf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24ac939d856e466eb306ab8bdb2900ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0bae1c7af3e54d779808d00cc097265d","placeholder":"​","style":"IPY_MODEL_6cda8360243a4a12893ae83c956450be","value":"model-00004-of-00008.safetensors: 100%"}},"24fb8548cb40434db4fa582c89b47c20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4f59e44a52b4a8e8f851d020bb0d297","max":1.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_a6b2ff3526e9462db4b04429681d5a59","value":1.0}},"2535107dd0df47b4a7b6aa8e1f0c901b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25436cd266484802b38a80e60204315a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25909d2e00c641f1befc506b34d129e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2617131e81fc4514a7ce3eebc34736e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1a91b938d8a9451d81dd5f158ed7dacf","IPY_MODEL_30a884a866224045a4792ca539aeddcf","IPY_MODEL_9da8cc5f0323482fbc33987440ab209a"],"layout":"IPY_MODEL_7c1be75d191a4711b567505ddd703c8b"}},"262f60a4c0a94dc8aa4ad27743bde8e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26643a86d4294d94b27b5ed653f4fad2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2698714cb9bf495da2664029a3feef8e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26e8297ebf35418f8ef2d4f82991eb82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42fc5a3077334b3da27111154a0c094f","placeholder":"​","style":"IPY_MODEL_1cd779ac8b1f45d9a0a277e31e77105e","value":" 438M/438M [00:02&lt;00:00, 203MB/s]"}},"26f9f1b0c76140c0801c0de8afa1ec26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27384bb8262f4de2a2dc8c14bda1f656":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ecac4542a1543fb931ed50c9d3b70bb","placeholder":"​","style":"IPY_MODEL_e633037fb5aa460195359be4dc504b2b","value":"model-00007-of-00008.safetensors: 100%"}},"27d481b7a8fa4c66ad5bf122f6a25fe2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33ce797be8334582b5163047d16c1fe6","placeholder":"​","style":"IPY_MODEL_60b825d707f640c589cbfb3c67480815","value":" 125/125 [00:00&lt;00:00, 10.9kB/s]"}},"27e1aeee80694d86adbca74895c3840e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_17648a59a4ce42cda833b97e0ae00f5c","IPY_MODEL_5231ec6f2e1b4395b5f28f19765401d0","IPY_MODEL_e5db3d1e14fc4dc69b053282d7828b13"],"layout":"IPY_MODEL_559273c533c647778a4532be2d7b5911"}},"27f11d84f7c64267994ae383a6b1751f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"287a136889f9428c80a56247076ec53c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbe94b9abe0c47cc82efd5f5fb0df152","placeholder":"​","style":"IPY_MODEL_13f82f14c7184b119d612d73cd2baf00","value":" 1.98G/1.98G [00:11&lt;00:00, 190MB/s]"}},"28b75f1581154a4c9f000a62b6429120":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef5f5b76077e46bb91efe8637db04c38","IPY_MODEL_7db350c7ef3943cd90e8fb70c0c43714","IPY_MODEL_798ec0add7534dafb95fced0b61b9f8f"],"layout":"IPY_MODEL_9349c3f768634a8d82b83239c2f2b43d"}},"29a01c94ef9f49bcbfc869d2c7ac47b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a9201193f33410fb04b50e1a54cf2ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b2b134ac85e4fbcbbefbd2d6ec2d63f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ba514c48ca24f46aaeadeb3b9735586":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c063816e59043239cee465bc6585380":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c7d6fcf2861456e860a3489f06e9bd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93c44bb0b3af41818a85ea83f64ba333","IPY_MODEL_47a48bf8468f4dcdbb17964f654e3c2e","IPY_MODEL_0882c96bacd04c29903d9733082c29ee"],"layout":"IPY_MODEL_53988e01983f4721b3f56d963e358ce0"}},"2cb657bc21dc4baa9ba2cb61cc8471fe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cff17dc1dec42a59d516418a7294727":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_359958f561194a6d9069fb803449b3a2","max":23950.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_2443eb90348f47e8ab92555a544d714e","value":23950.0}},"2e06f1c423ed49398aef0936c621d741":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a18ae14f50a45bb97e2afbe30b7580a","placeholder":"​","style":"IPY_MODEL_0496a042f25341669e625bf714c8d81a","value":"artifact.metadata: 100%"}},"2e58feb8348845cca292bb1dfca607a8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f5127e799e540d19518285d4a9dbe94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f6565cf0fb44626b6992d84af75105d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30927822481040daa461c599caeae942":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efc50b7bf43c46cc91ab112e386097ee","placeholder":"​","style":"IPY_MODEL_08b238fa6b7742bb951e0588350097ed","value":"model.safetensors: 100%"}},"30a884a866224045a4792ca539aeddcf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cab9cd0611fb4c27914d27a14e487756","max":394.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_a9ee99b61d2d4287a73250d2d3768e9f","value":394.0}},"31774a83a3114a6d8b694204400b23ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"322c5edec02b4dae9b8d2121967d22d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"324b4be7e5b447fdaff01bb4b8af98c0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"336aa9a9f08a4f40a001ad09f6af9fdc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f565266147c240c89ae70fe7c52af313","placeholder":"​","style":"IPY_MODEL_707242cedb374ae8b37f89c31dab768f","value":"vocab.txt: 100%"}},"3389cf4106474ff2904a04385586d38c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33ce797be8334582b5163047d16c1fe6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33e2fd8ed3d14577952fb991f4bef9f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3431c3baa3f549328f274cbfaeeb8232":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"344149e43c9b4114bc7253e82754332d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75cbc28407864efb8bca1741a92cb03a","placeholder":"​","style":"IPY_MODEL_2a9201193f33410fb04b50e1a54cf2ba","value":"Downloading shards: 100%"}},"359958f561194a6d9069fb803449b3a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"363cbbc6e7684424b6e4dc477d46003f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36831d58104642fcb768cbf0f41b6891":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31774a83a3114a6d8b694204400b23ce","placeholder":"​","style":"IPY_MODEL_3a8a015ed4ca4c9b8faa068558d29b72","value":" 57.0/57.0 [00:00&lt;00:00, 4.52kB/s]"}},"3815ff465f6d4ec5b47f4a091e0f3731":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38b56073067e4710af6151ada1331c10":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a8a015ed4ca4c9b8faa068558d29b72":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b9f366727d54008a8642c400faebf45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d633afeca44497dae1bcaddc2d0e199","placeholder":"​","style":"IPY_MODEL_0818e70d89c345b29ac483e4583b3efc","value":" 1/1 [00:00&lt;00:00, 115.34it/s]"}},"3baf9cf7c606432ab3f25c6e38280b2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_24ac939d856e466eb306ab8bdb2900ee","IPY_MODEL_c01dabc6ce5f489da6e5d70ad78311b5","IPY_MODEL_42ef364e24994127a228cda924881f0e"],"layout":"IPY_MODEL_856d054c6f6c4a7faea79d78311a0362"}},"3bc03ffa2c024bb789e6ba9e27d6fd71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27384bb8262f4de2a2dc8c14bda1f656","IPY_MODEL_eaf8043b8d0445be8147c48cd9613575","IPY_MODEL_22286f4edee245d89f87a2882e092395"],"layout":"IPY_MODEL_530a7f91d624434a91dcdaa2247ca3bb"}},"3d68241589944c2d9799e8e0117691c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cdc3e71a498744a0923569588b242210","IPY_MODEL_810929c1b49f4fbdb08d91c971622572","IPY_MODEL_aab9d96fe8144028b547ed4199e86652"],"layout":"IPY_MODEL_5131530b16884370ba0e627b8fe0b636"}},"3e8a245d326542e2aa7afd015d7ef674":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eedf2fa735434adb96329b8932ed5d39","placeholder":"​","style":"IPY_MODEL_d068129630d54a819602376852bf7467","value":"special_tokens_map.json: 100%"}},"3e8d039e025c4ba5bfd44940e79496ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f748613f377e4a23a1f4f381a84b8929","placeholder":"​","style":"IPY_MODEL_4c99517e49054a00b101b3ae66b27130","value":" 232k/232k [00:00&lt;00:00, 4.32MB/s]"}},"3ed0e78785b3445b8a0266f74573e6de":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fe6fa28e0b74c01b7fe12515f94e8a9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40f50c37aac24efab71ef2126c039e71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_82b24df3b1394b54b2ed75d62c29a305","max":1946243984.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_888f088bc244495791a3d416c411aacf","value":1946243984.0}},"416f14a71bd849f9936a116722092456":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4277c955a2394d0aba90f4f2994829a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42ef364e24994127a228cda924881f0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c221092c424e4521bdf7022c57864942","placeholder":"​","style":"IPY_MODEL_8b8bccd7ecdf4271b7253d4344537ddf","value":" 1.95G/1.95G [00:09&lt;00:00, 202MB/s]"}},"42fc5a3077334b3da27111154a0c094f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43215d82025d449bafaf7f61ba6eab3e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4671362f977d4097b949f17cba721514","placeholder":"​","style":"IPY_MODEL_c859aa46d21f411092b43267157d6e26","value":"Computing checksums: 100%"}},"4539836770514d239a72474cb4d2b3b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45566f4eed5e49b5a7fccb155c168c16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4671362f977d4097b949f17cba721514":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"467f98dcf01e4e26bc367b8c09a68d8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a649a49121a24d5782e030a99f36deaf","max":21954601.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_2ba514c48ca24f46aaeadeb3b9735586","value":21954601.0}},"46a497d3317a411cad4d06fc75e830b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"474157391ed64e4b8971eed553d42c5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47a48bf8468f4dcdbb17964f654e3c2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_16a84543a9664aa9ab14981fd4b26dec","max":405.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_0caf31725a744766aeae13171135f7f1","value":405.0}},"488f122808864bbe920363da9d7d98ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48d75f06ce9e4d0687ce21affd271fc9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"49917084d3b24f5c838a5c826769ac04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a092c824a15422db781f9c28fe3521e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4aefd97c8e4e4920ad8b95d219d00ca7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb0383d178334a34a5e377859fa1186e","placeholder":"​","style":"IPY_MODEL_eff8831fc09a4d3b9d6f1624d7c124f0","value":"README.md: 100%"}},"4afc52a144ad45bfb985088525808ed7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bb0584900eb4261bcf001a655f088dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd80a541d2a54654a918dcb037ad65dd","placeholder":"​","style":"IPY_MODEL_8f95c30137884fe88187ebd18a900775","value":" 1.95G/1.95G [00:10&lt;00:00, 198MB/s]"}},"4bec3b89214242c2b37a315254829759":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46a497d3317a411cad4d06fc75e830b5","placeholder":"​","style":"IPY_MODEL_77fbbcadc45d41beaae5ed35155353ba","value":" 8/8 [01:29&lt;00:00,  9.74s/it]"}},"4c610f54a47e46cbaa8d8d98d66da2fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4c99517e49054a00b101b3ae66b27130":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d0df0a680e145588d6cb23cc9b4abd0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d633afeca44497dae1bcaddc2d0e199":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f699cdf5c8a4278951fdc958f1610e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bd7aef78e1347108b3afc9edd7ec9fd","placeholder":"​","style":"IPY_MODEL_e3b65eb9e2204caaa8c5308c08d1ae5a","value":"model-00003-of-00008.safetensors: 100%"}},"4f8e3d3b725045bea4aef9b64acf4a95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ff23737098f4b908bc24144e51f9df7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_504086ffd13b434aa110fe0c1938ce95","max":583.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_e1530c52892946508b2a32913a6cc341","value":583.0}},"504086ffd13b434aa110fe0c1938ce95":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5106869476af45fdb9d08290d3580cfa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e66bb753ec884a6eaa0c63e2948893e1","placeholder":"​","style":"IPY_MODEL_4539836770514d239a72474cb4d2b3b4","value":" 190/190 [00:00&lt;00:00, 16.5kB/s]"}},"5131530b16884370ba0e627b8fe0b636":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5231ec6f2e1b4395b5f28f19765401d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_55aff3c73a4f4b468e9bcf7870a80697","max":2647.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_f12904d8305a43649e549e188e382b66","value":2647.0}},"52a651faa0374aaeb95069a20e39566f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeba585c2a85425d842666b3b3dfd05e","placeholder":"​","style":"IPY_MODEL_156386eecf4544ed8fbc8a3902ed2673","value":"model-00006-of-00008.safetensors: 100%"}},"530a7f91d624434a91dcdaa2247ca3bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53988e01983f4721b3f56d963e358ce0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54cfe43c109f49f2afccdde77a6dff9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9e64644748a4919985b0decd3d121b2","IPY_MODEL_19b40566aa7f45589257a913bee2f6c6","IPY_MODEL_b7819ff15e944097a1825ac517edc7ee"],"layout":"IPY_MODEL_bc1016a03b5d4ec0a68b14cebac4178a"}},"551431cfc1664815813db44b08ff4994":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55659a47f5f14402b049f88c00026f5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7a97081b5cb4893ad5dd074ad69facf","IPY_MODEL_5e71fa31918045babdade7d046a629b5","IPY_MODEL_c139f964bedb43ea9e892ffe16a67431"],"layout":"IPY_MODEL_363cbbc6e7684424b6e4dc477d46003f"}},"5591fba4ea1541efbf29ed38a77b391b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"559273c533c647778a4532be2d7b5911":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55aff3c73a4f4b468e9bcf7870a80697":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5747d27c4a034c7ab8a2fb5133ce1659":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_163a9ea619c440c19caaf527a80e4311","IPY_MODEL_b8956f7e94d3446c9e378bdf0ae13b1b","IPY_MODEL_d66e3c46d2174b90b5a14d1e9e6bab69"],"layout":"IPY_MODEL_6f5fdae41e1c4c5d9865cb31a50be76e"}},"5801e9fbf9fb4caab44e5742d87351d4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58aa0547a6e94d9e935abb625fc32dd2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5930caacba034eada237fa716006498f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"595957daa7384963957086c0ff8693e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59c84c6ffa5f4548ac3abeda6d837a88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59f3eaa96abc48379a5749ac5900d3ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a1e2a5b89c944848c48e56cb9c4a045":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a3fb7d512df4b7abb2f5449ef103b47":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b1b102a4afc414cb99cfb8d6334a040":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e71fa31918045babdade7d046a629b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6799526de87241b8a608639cf0d622ef","max":493443.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_8d23359a4ff84cedb05db067faa35ddc","value":493443.0}},"5ed3d0e0dbbb4f328872f4491dcef493":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5fbedb00edae4cc8ad32ba8847537552":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"602b39eb4c96424ea97e290903830bfb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"60b825d707f640c589cbfb3c67480815":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61387eb572b343348c3e3f78e0b247c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61640b4c47334d7a9bb73a71b9263f5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9489199f07bb4a53ad8936d7eb66ec8b","placeholder":"​","style":"IPY_MODEL_2e58feb8348845cca292bb1dfca607a8","value":"tokenizer.json: 100%"}},"618af5ff2f5d441aa39bfb6a2952729b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ade422e9b07540299bdace5acf183f54","max":8.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_b4da168574ec413ba3783f5e7404b31e","value":8.0}},"61c324e50d04435c876e3ccba90bdcac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"622bf53abae646a3b4a4330b68c52a26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6349880dbba04532af393fb0fbb5b104":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"640b70bb25f8401abae61617fc0c8443":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64ed7f748c6f42c9a82acd24789cf56b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64fed371f518450dbe84312759c98186":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65b7b1b1d44c4a708f11c03ad0a954fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6601a05235c34765b14b31ae26e792df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6799526de87241b8a608639cf0d622ef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6804b577279c4637938545b020350cf9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68898d0155984776958e2b3b63ff1df1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ed0e78785b3445b8a0266f74573e6de","placeholder":"​","style":"IPY_MODEL_7786c40f75f3427d84b18eeb10002117","value":" 112/112 [00:00&lt;00:00, 10.4kB/s]"}},"6901d91032cf4ca2a382b2acc6ba49cc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ae1fcf75d27403e9932007ba77c7a7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38b56073067e4710af6151ada1331c10","placeholder":"​","style":"IPY_MODEL_9322db3d85344328849ab1c3e43f9b33","value":"Downloading readme: 100%"}},"6b1c860a0ec84d2ca81cd85245f56ee4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b5814ab327142fab562dc8468746ed9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b74846acc1d4544910a90b8a8cde119":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c58ffba735a4887bdc6d7016f39cd77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c957a5262c64c9b903fd31d4cd92251":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cda8360243a4a12893ae83c956450be":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6cf912dad85642b78fa59f9f97ea64ef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e4350b4a3ec41aead5a94e4787cd541":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e5e539653f84a599d7ea41a00280db2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d0df0a680e145588d6cb23cc9b4abd0","max":168.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_f52cdb74338045d0b5e986961d654d48","value":168.0}},"6f1a42b9bfb3435b92dfab4105409a28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_134159dc913d44ea955dc75be9d6bb35","placeholder":"​","style":"IPY_MODEL_4277c955a2394d0aba90f4f2994829a6","value":"model.safetensors.index.json: 100%"}},"6f5fdae41e1c4c5d9865cb31a50be76e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ffa3f88ab2141a6a6dc7db5fca73c78":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"700af83a913c4f99a71f66803f3f5a8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"707242cedb374ae8b37f89c31dab768f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"709e805c95224f12b5966a4e1d482c97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70ed47c1e7ea4fdea6f735a6f03f9944":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_344149e43c9b4114bc7253e82754332d","IPY_MODEL_618af5ff2f5d441aa39bfb6a2952729b","IPY_MODEL_4bec3b89214242c2b37a315254829759"],"layout":"IPY_MODEL_ea34d8ec8d504bd8b64277d34e081b58"}},"715548ef7e5e46c9859401809c721aab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72c174939ca04485b7cb9656f414a9a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2e0b6af57844bfca1846cd6121d3d91","placeholder":"​","style":"IPY_MODEL_c691ae66f68849a6bf55c1bc7caf2291","value":"tokenizer_config.json: 100%"}},"75cbc28407864efb8bca1741a92cb03a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7604b5911b9d4612a48d4abf586b3d38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af55e3a2e2fb40e48b3945de3bad853e","IPY_MODEL_da84bc594d4e4d4c941087f770f297a5","IPY_MODEL_27d481b7a8fa4c66ad5bf122f6a25fe2"],"layout":"IPY_MODEL_b951dd9fc13241148658ce2515af66f8"}},"76532d63110c46b99300214596a02944":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bbd68dd9ef3426992c3040eea00c768","max":231508.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_0a66f6f8eb5847eba71f17ffd0d9c3eb","value":231508.0}},"7786c40f75f3427d84b18eeb10002117":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77885a5eb3bb4ca6b3a1ea13a09a5aaa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77ac768d2f284da383e486027ae5a72a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77fbbcadc45d41beaae5ed35155353ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77fbe2cc35b246ca9ec7cbced46600c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_551431cfc1664815813db44b08ff4994","max":66746168.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_5ed3d0e0dbbb4f328872f4491dcef493","value":66746168.0}},"78bb88ad75554889aaa836852c14a223":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"798ec0add7534dafb95fced0b61b9f8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbcab67f0e6947bb87e6ef9012c4a07b","placeholder":"​","style":"IPY_MODEL_45566f4eed5e49b5a7fccb155c168c16","value":" 1.80M/1.80M [00:00&lt;00:00, 16.6MB/s]"}},"79eba380073d404d9f17e3e25c42d5cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a18ae14f50a45bb97e2afbe30b7580a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b8be120977844a383bece25dd699bcb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e53a9629325d40fba6bc420f5ac50874","max":385.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_5930caacba034eada237fa716006498f","value":385.0}},"7c1be75d191a4711b567505ddd703c8b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cbfcfa73d61484ebac7ca895fbe756f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_58aa0547a6e94d9e935abb625fc32dd2","max":231508.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_64fed371f518450dbe84312759c98186","value":231508.0}},"7d23ffa35ac9443588bbd0d1d90662ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d29cdbda3aa408787f976d335f8ab2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e8a245d326542e2aa7afd015d7ef674","IPY_MODEL_6e5e539653f84a599d7ea41a00280db2","IPY_MODEL_df7295dc376541859a4c28369d6b6e9a"],"layout":"IPY_MODEL_881f4bb58dc9464783470be65b302aaa"}},"7db350c7ef3943cd90e8fb70c0c43714":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d228464a29141ce8f36c14ad765c4ee","max":1795303.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_03d1547f580440e88e7f4524b94b07d5","value":1795303.0}},"7e90bff7cd2d48f3b86747e7fdc39d4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ed6f1277de948cd84bebb7612bac862":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9349156db47043cd8d6d773d3464e402","placeholder":"​","style":"IPY_MODEL_15af7a4a83cf41e19dccba3fdab52e4c","value":"model-00008-of-00008.safetensors: 100%"}},"7f08593f9e5e470b83089ca8fe70d3fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f64e90c32984fd78e1ecb344d061ad9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80f367c68cb54fe2a8d01cba232e91c8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"810929c1b49f4fbdb08d91c971622572":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_23c06c6680ea476c97b275f581adce6c","max":8.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_236f8a2aa0544a75973eafcae1999fca","value":8.0}},"8156d8f6021c48cf88f55352926ef89a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db7ad46552234d67828ec508e4025a2a","IPY_MODEL_ca6d878b57b44bb5afdcfa734abfbbb4","IPY_MODEL_880afabbb2d74ad3abb65734e13317c1"],"layout":"IPY_MODEL_26643a86d4294d94b27b5ed653f4fad2"}},"81d28b84706c473d97820dc8071c702a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8248774dccc44276b3aa960b5a293b27":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82b24df3b1394b54b2ed75d62c29a305":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"831b2aa3ccc24ca88dea2acb7e06fb5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f082c6f7af3b434fa0ba8839103faf5c","IPY_MODEL_8443822d37ac425ca44bdcbdc7bbe469","IPY_MODEL_5106869476af45fdb9d08290d3580cfa"],"layout":"IPY_MODEL_5a3fb7d512df4b7abb2f5449ef103b47"}},"8336e3891719489d8f9f8f5a32b78455":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9b7b0d3526a43c98b29c9250bb1ff55","placeholder":"​","style":"IPY_MODEL_474157391ed64e4b8971eed553d42c5f","value":"config.json: 100%"}},"8443822d37ac425ca44bdcbdc7bbe469":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc0559f363b84b5c94baa3ebb731e89c","max":190.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_8482687fa48f4d808be9e453102cfd44","value":190.0}},"8482687fa48f4d808be9e453102cfd44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84afd069b8bc440c8364dcb352785acd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84cee26cae8c44e2b4381a23fe79303c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"856d054c6f6c4a7faea79d78311a0362":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8584ae62401046af901e36911f8d3826":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85b3536e2e1b43bab3a431f787d21325":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"866206654d194dcc8de516f04c05409e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8fce6b0f3be4ee0a966c457fc1bd8ca","placeholder":"​","style":"IPY_MODEL_23870fa0c3484e8fa318135f50f6edf5","value":" 1.63k/1.63k [00:00&lt;00:00, 140kB/s]"}},"86b9caf316cf49dcbc9f99a55333af6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30927822481040daa461c599caeae942","IPY_MODEL_77fbe2cc35b246ca9ec7cbced46600c6","IPY_MODEL_880f6a9dae4040f5b09c6f1ff01b720c"],"layout":"IPY_MODEL_77885a5eb3bb4ca6b3a1ea13a09a5aaa"}},"86cf5d9f747c4098935898d52b945a8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"874dee6a291e4f94a390ba839a93bac2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"875bf5a52bc2474bb7463d735b6de600":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87cac24a03a54cd1b6645464b7bfcf2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"880afabbb2d74ad3abb65734e13317c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b63773521e6c4d0ba78ee7f38f85ecfd","placeholder":"​","style":"IPY_MODEL_b7769a0706054099bd21e4241b7e1fed","value":" 42.0/42.0 [00:00&lt;00:00, 3.02kB/s]"}},"880f6a9dae4040f5b09c6f1ff01b720c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6901d91032cf4ca2a382b2acc6ba49cc","placeholder":"​","style":"IPY_MODEL_27f11d84f7c64267994ae383a6b1751f","value":" 66.7M/66.7M [00:00&lt;00:00, 103MB/s]"}},"8811c74571464defa0756d71b3fc789e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"881f4bb58dc9464783470be65b302aaa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"888f088bc244495791a3d416c411aacf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8b7aa8875d2b40f289b44b96738d4e44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b7c7a1f1f60489aa9b1bf7f326dc066":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_875bf5a52bc2474bb7463d735b6de600","max":68084.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_eaf50c82b7a1445aaf4ff24c07ea30c1","value":68084.0}},"8b8bccd7ecdf4271b7253d4344537ddf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8bac41f89c034db5bf44c7b59fd88fea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8bae539a181c4efea54f087a77b524cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e9aa7df2ec64a598cde4c465d0bb91e","placeholder":"​","style":"IPY_MODEL_23238e110d9f46bf9e2de33ac8f9d603","value":"generation_config.json: 100%"}},"8bbd68dd9ef3426992c3040eea00c768":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c0a356638cf46529841506117330a37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ae1fcf75d27403e9932007ba77c7a7a","IPY_MODEL_033a6d18544e4f77ba56910eb59b7ce2","IPY_MODEL_935816edbc484834bdc12f9d252a7a8a"],"layout":"IPY_MODEL_0f58b9392b8243389419eb1bdacb0587"}},"8cabfed48d9842fb9cddf9613a1d91f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ced9e67c3cc416bb0417ccf81f804c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d23359a4ff84cedb05db067faa35ddc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8dc4698f15e047f6a0171e9739e451f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ecac4542a1543fb931ed50c9d3b70bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f95c30137884fe88187ebd18a900775":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90cdc4026ee245e19d8f0336af2ed54e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91417795c8374ff589700b4b5cd1bffc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_104189e5632e4c0d93fc00947209ae3c","IPY_MODEL_9c037dcd267240cfa84d16ef35fbdcf7","IPY_MODEL_9667bf5980524188913339d6fdd13486"],"layout":"IPY_MODEL_16768b26682f4b878b6005c7f9532316"}},"9322db3d85344328849ab1c3e43f9b33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9349156db47043cd8d6d773d3464e402":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9349c3f768634a8d82b83239c2f2b43d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"935816edbc484834bdc12f9d252a7a8a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2698714cb9bf495da2664029a3feef8e","placeholder":"​","style":"IPY_MODEL_d08f25a24559415bb4745311c5ad92d3","value":" 21.0/21.0 [00:00&lt;00:00, 1.75kB/s]"}},"9391890ffb24494ea795d046520a7a03":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5477b93e9f447a7afcc4e18a8504a7d","placeholder":"​","style":"IPY_MODEL_78bb88ad75554889aaa836852c14a223","value":"model-00002-of-00008.safetensors: 100%"}},"93c44bb0b3af41818a85ea83f64ba333":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5591fba4ea1541efbf29ed38a77b391b","placeholder":"​","style":"IPY_MODEL_874dee6a291e4f94a390ba839a93bac2","value":"tokenizer_config.json: 100%"}},"9489199f07bb4a53ad8936d7eb66ec8b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9571cddb7e23482ea49e439e428c69ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9667bf5980524188913339d6fdd13486":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4337c285e0c4d21bd25f853dd42cb6a","placeholder":"​","style":"IPY_MODEL_79eba380073d404d9f17e3e25c42d5cf","value":" 2647/2647 [00:00&lt;00:00, 5834.82 examples/s]"}},"96ddbad0ad7b4c4b8abd47105ffdaf07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ddc6faa760274979ab1e3acf18ca6b9a","IPY_MODEL_1fb2e2fc900949a8b3bf249271302515","IPY_MODEL_68898d0155984776958e2b3b63ff1df1"],"layout":"IPY_MODEL_0d007fccd0e94d52934c76615f015c91"}},"96fc871455c647c0a206678abe56f77c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99d108fd975c4a38b01749e1b7ee3ce4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e9062d37b2c4f9b8c91c0bbef019701","placeholder":"​","style":"IPY_MODEL_2535107dd0df47b4a7b6aa8e1f0c901b","value":" 23.9k/23.9k [00:00&lt;00:00, 2.15MB/s]"}},"9a17a48f5a8244498be7135cee097c14":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bd7aef78e1347108b3afc9edd7ec9fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c037dcd267240cfa84d16ef35fbdcf7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1d9fe1e98b24584abe94c40327fcf94","max":2647.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_61c324e50d04435c876e3ccba90bdcac","value":2647.0}},"9ce88be3fe1d4462abc19987afeaefd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a982c35cd4904db8916d42ef71b258d5","placeholder":"​","style":"IPY_MODEL_b64deb31be6f409684acb483382f5044","value":" 111/111 [00:00&lt;00:00, 9.60kB/s]"}},"9d228464a29141ce8f36c14ad765c4ee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d78e807cdcc48edaa38c3e13621f7cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9da8cc5f0323482fbc33987440ab209a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5415a14216544eda4a3f599db6c86d0","placeholder":"​","style":"IPY_MODEL_fa3fdaa0a2c2460ca9091e3e9e617fc4","value":" 394/394 [00:00&lt;00:00, 34.7kB/s]"}},"9f97c698d43b48909dd4add03a92dde3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0f8c50d9bdf4131b05bc90851c0fa3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_622bf53abae646a3b4a4330b68c52a26","placeholder":"​","style":"IPY_MODEL_700af83a913c4f99a71f66803f3f5a8d","value":"config.json: 100%"}},"a119358f611a4c96887a38707d653661":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1d9fe1e98b24584abe94c40327fcf94":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a20a1984c82542b7ac9ffe735de035b8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2e78784562a41edbfe00288881f76bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8811c74571464defa0756d71b3fc789e","placeholder":"​","style":"IPY_MODEL_6b5814ab327142fab562dc8468746ed9","value":"modules.json: 100%"}},"a31003fb138441cab058bddf0240586d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a4337c285e0c4d21bd25f853dd42cb6a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4ce9d4a80214df8a21a3a20886a48ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_64ed7f748c6f42c9a82acd24789cf56b","max":1979781432.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_8bac41f89c034db5bf44c7b59fd88fea","value":1979781432.0}},"a5415a14216544eda4a3f599db6c86d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5f97ee5bffd4b1f86d6604f349aa211":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a649a49121a24d5782e030a99f36deaf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a67c77a4c5ef4e62b3f8538939b37c2d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6b2ff3526e9462db4b04429681d5a59":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a97983dc4dd74c5a93ba55757741f87e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a982c35cd4904db8916d42ef71b258d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a99305452f8449ad8ed66534895e8c92":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8336e3891719489d8f9f8f5a32b78455","IPY_MODEL_d1382854131c4c2790b5d436408b21e2","IPY_MODEL_f8f48cdd6a7b42e29fb38d4bd3362427"],"layout":"IPY_MODEL_9a17a48f5a8244498be7135cee097c14"}},"a9ee99b61d2d4287a73250d2d3768e9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aab86397feb04ff2b9b3a45fd109c820":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aab9d96fe8144028b547ed4199e86652":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_051721480b9742249c7d92fc1d8a0f96","placeholder":"​","style":"IPY_MODEL_322c5edec02b4dae9b8d2121967d22d1","value":" 8/8 [00:52&lt;00:00,  4.75s/it]"}},"aacaf184cd9c4b23a28ea7b3c9d138e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7921a22a50842c1a6832da1eb8387f7","placeholder":"​","style":"IPY_MODEL_84cee26cae8c44e2b4381a23fe79303c","value":" 385/385 [00:00&lt;00:00, 32.3kB/s]"}},"ab75bcf9e53c48abb7ff9406ab4cf59e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac1cb8cbbf6e42bdb950ed7bce79cb47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_43215d82025d449bafaf7f61ba6eab3e","IPY_MODEL_24fb8548cb40434db4fa582c89b47c20","IPY_MODEL_3b9f366727d54008a8642c400faebf45"],"layout":"IPY_MODEL_a97983dc4dd74c5a93ba55757741f87e"}},"aca836b9feed492abe53e06a05ed2206":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad8b0da7be2b4f2890ecf18e7bbdf2f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ade422e9b07540299bdace5acf183f54":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae9cd5af5fdb4b079102f38217dbd082":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a1e2a5b89c944848c48e56cb9c4a045","max":1431.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_6b1c860a0ec84d2ca81cd85245f56ee4","value":1431.0}},"af55e3a2e2fb40e48b3945de3bad853e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7a1afd71cf04164aad5d5299056a223","placeholder":"​","style":"IPY_MODEL_e2e41bf193da486b96de6342677e1cc4","value":"special_tokens_map.json: 100%"}},"b013a5a9b24b4463913bdc3af86b60d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b1f32d9eb85e45a7b2ebe90b752aecfa","IPY_MODEL_eea20cfa9ed0422e91de162d6911460b","IPY_MODEL_36831d58104642fcb768cbf0f41b6891"],"layout":"IPY_MODEL_81d28b84706c473d97820dc8071c702a"}},"b11e715a1aa74c11a46799daf9c79236":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1c3ae0598bf4a27a9cad84d02ae6cd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1ef10ffe1d64bb2a00aa64440168c7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a119358f611a4c96887a38707d653661","placeholder":"​","style":"IPY_MODEL_07fed3b9edd74cd09a855e3fb137b1a4","value":" 232k/232k [00:00&lt;00:00, 3.86MB/s]"}},"b1f32d9eb85e45a7b2ebe90b752aecfa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f64e90c32984fd78e1ecb344d061ad9","placeholder":"​","style":"IPY_MODEL_c26ec08b10c54c4c95917616061df532","value":"sentence_bert_config.json: 100%"}},"b3702cd715e442a4867d312067d96404":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8bae539a181c4efea54f087a77b524cb","IPY_MODEL_b825fb29061145049ad32e9cbdf0de9e","IPY_MODEL_9ce88be3fe1d4462abc19987afeaefd8"],"layout":"IPY_MODEL_159aeed34fb045fc9f3ff256b6de7b53"}},"b4da168574ec413ba3783f5e7404b31e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b63773521e6c4d0ba78ee7f38f85ecfd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b64deb31be6f409684acb483382f5044":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6eadb09b5c4418eaf7123b55bae08ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b84b7ad415684867a0b345c349d2c9cf","IPY_MODEL_467f98dcf01e4e26bc367b8c09a68d8f","IPY_MODEL_f00bd512c1b043f2a9c23f4573c0715c"],"layout":"IPY_MODEL_aca836b9feed492abe53e06a05ed2206"}},"b7769a0706054099bd21e4241b7e1fed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7819ff15e944097a1825ac517edc7ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fe6fa28e0b74c01b7fe12515f94e8a9","placeholder":"​","style":"IPY_MODEL_cf67bca06c5346b59a170dfeb70665b9","value":" 1.98G/1.98G [00:10&lt;00:00, 190MB/s]"}},"b825fb29061145049ad32e9cbdf0de9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_80f367c68cb54fe2a8d01cba232e91c8","max":111.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_2f6565cf0fb44626b6992d84af75105d","value":111.0}},"b84b7ad415684867a0b345c349d2c9cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1df91716bceb4b3c984d3e3507fabbaf","placeholder":"​","style":"IPY_MODEL_96fc871455c647c0a206678abe56f77c","value":"Downloading data: 100%"}},"b85fc9c13bff4e62a4fe3999a6883b50":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8956f7e94d3446c9e378bdf0ae13b1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a20a1984c82542b7ac9ffe735de035b8","max":17995.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_8ced9e67c3cc416bb0417ccf81f804c5","value":17995.0}},"b951dd9fc13241148658ce2515af66f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b97f941dda234c6096ba2ed9babde775":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b997e37be24646a39c450111677924f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9a02332d8ca474e806ee2ce419cf1c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cf912dad85642b78fa59f9f97ea64ef","max":1889587040.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_e8bf21371b654fb8b82748544b302b6f","value":1889587040.0}},"ba2d08280ab3461bb215dd6a2c987155":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90cdc4026ee245e19d8f0336af2ed54e","placeholder":"​","style":"IPY_MODEL_2b2b134ac85e4fbcbbefbd2d6ec2d63f","value":" 712k/712k [00:00&lt;00:00, 10.7MB/s]"}},"bb0383d178334a34a5e377859fa1186e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc0559f363b84b5c94baa3ebb731e89c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc1016a03b5d4ec0a68b14cebac4178a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd04d5cfbc3b49dc8390f4f1975eb7b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a2e78784562a41edbfe00288881f76bf","IPY_MODEL_7b8be120977844a383bece25dd699bcb","IPY_MODEL_aacaf184cd9c4b23a28ea7b3c9d138e2"],"layout":"IPY_MODEL_a67c77a4c5ef4e62b3f8538939b37c2d"}},"bd167f9c7ceb404d8585adef002ded62":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4aefd97c8e4e4920ad8b95d219d00ca7","IPY_MODEL_8b7c7a1f1f60489aa9b1bf7f326dc066","IPY_MODEL_d0ae7527f46041dc957d9541a294408c"],"layout":"IPY_MODEL_7d23ffa35ac9443588bbd0d1d90662ad"}},"bd80a541d2a54654a918dcb037ad65dd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd9cda9c835a4188870afab300dd2671":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be789cf5db61448d9e91c8fb80a86922":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20206e40324945a0a4abddf68693e1fa","placeholder":"​","style":"IPY_MODEL_61387eb572b343348c3e3f78e0b247c9","value":"vocab.txt: 100%"}},"c01dabc6ce5f489da6e5d70ad78311b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3389cf4106474ff2904a04385586d38c","max":1946243984.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_49917084d3b24f5c838a5c826769ac04","value":1946243984.0}},"c0810651ddd64527a1bd20619ab7b785":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dc4698f15e047f6a0171e9739e451f8","placeholder":"​","style":"IPY_MODEL_ec6ab8a309fb4ab5a0f15977740a68eb","value":" 466k/466k [00:00&lt;00:00, 5.96MB/s]"}},"c139f964bedb43ea9e892ffe16a67431":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc9d510b9f4d48189fc3323aa6fbf1fd","placeholder":"​","style":"IPY_MODEL_cafec398f70b469fb2ee8f7e732ea002","value":" 493k/493k [00:00&lt;00:00, 34.9MB/s]"}},"c221092c424e4521bdf7022c57864942":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c26ec08b10c54c4c95917616061df532":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c49dde521d9f4662a68c88afdae5b733":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ed6f1277de948cd84bebb7612bac862","IPY_MODEL_0bd1fc9d4f6b4a40996cae139ae8bb23","IPY_MODEL_e683729f3a584d0db8132e5837b47ec9"],"layout":"IPY_MODEL_b997e37be24646a39c450111677924f6"}},"c518aceddbb54d828ce04080eb77c02b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65b7b1b1d44c4a708f11c03ad0a954fc","max":438349816.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_ec0bcdb12d9c4455b1f3036b36a7b08a","value":438349816.0}},"c678c5ad778e4aa4937dfea62ca38f33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e06f1c423ed49398aef0936c621d741","IPY_MODEL_fa123bc232be4a8e918fb0086ff89524","IPY_MODEL_866206654d194dcc8de516f04c05409e"],"layout":"IPY_MODEL_2470ae86e799478998c051fcd29affaf"}},"c687c1be13c84a86a55a391c0ed4ddc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c691ae66f68849a6bf55c1bc7caf2291":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7c0b65bcc1b4e9f9d48f848e0bc6f45":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c859aa46d21f411092b43267157d6e26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca6ac7d866794843bc4e4b4d0b812f69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_61640b4c47334d7a9bb73a71b9263f5a","IPY_MODEL_0b9e12bb3c4f4dc4967a7452b0696bfe","IPY_MODEL_ba2d08280ab3461bb215dd6a2c987155"],"layout":"IPY_MODEL_9571cddb7e23482ea49e439e428c69ea"}},"ca6d878b57b44bb5afdcfa734abfbbb4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab75bcf9e53c48abb7ff9406ab4cf59e","max":42.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_5fbedb00edae4cc8ad32ba8847537552","value":42.0}},"cab9cd0611fb4c27914d27a14e487756":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cafec398f70b469fb2ee8f7e732ea002":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbcab67f0e6947bb87e6ef9012c4a07b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc9d510b9f4d48189fc3323aa6fbf1fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccde035c629d42d49461e2606a063828":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd3f165a119f4e44b435e5053777e617":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdc3e71a498744a0923569588b242210":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05e5e606a1534cd39fcf0c52cf9de6ab","placeholder":"​","style":"IPY_MODEL_e29cf21e8cd94232b12d19098518947f","value":"Loading checkpoint shards: 100%"}},"cef196e9fd84477f893316eac0755f3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1b29928272894de191e34cd3c32148e2","IPY_MODEL_f7ab02128b2f48bd90d23644c25b96ec","IPY_MODEL_14d226019db9470d89bee458caae5997"],"layout":"IPY_MODEL_4afc52a144ad45bfb985088525808ed7"}},"cf67bca06c5346b59a170dfeb70665b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfa7a0d0ed2e4be1afc0ac2f79346ab3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d56ea14315044998bf30b37bf4bf7a69","placeholder":"​","style":"IPY_MODEL_416f14a71bd849f9936a116722092456","value":" 1.95G/1.95G [00:11&lt;00:00, 192MB/s]"}},"d068129630d54a819602376852bf7467":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d08f25a24559415bb4745311c5ad92d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0ae7527f46041dc957d9541a294408c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1673ee3d96184fe2bf8617d3e745cc34","placeholder":"​","style":"IPY_MODEL_f1c431a19a154953bcfb3aba87a2c96c","value":" 68.1k/68.1k [00:00&lt;00:00, 6.21MB/s]"}},"d1382854131c4c2790b5d436408b21e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b276eb6997e47238d22e79c112106bc","max":743.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_4a092c824a15422db781f9c28fe3521e","value":743.0}},"d25d23672f684fdcbbdd5fc5e77eb455":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_336aa9a9f08a4f40a001ad09f6af9fdc","IPY_MODEL_7cbfcfa73d61484ebac7ca895fbe756f","IPY_MODEL_b1ef10ffe1d64bb2a00aa64440168c7e"],"layout":"IPY_MODEL_488f122808864bbe920363da9d7d98ad"}},"d476ee1c9007471d81923b879b009acf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0fe20625b3b54256b8bb71caa3f1b85f","IPY_MODEL_c518aceddbb54d828ce04080eb77c02b","IPY_MODEL_26e8297ebf35418f8ef2d4f82991eb82"],"layout":"IPY_MODEL_1745a18f2b1c41b19f795f1c8824fcb9"}},"d569c53d1a9f4c7f952efda6262c1f5a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d56ea14315044998bf30b37bf4bf7a69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d66e3c46d2174b90b5a14d1e9e6bab69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ffa3f88ab2141a6a6dc7db5fca73c78","placeholder":"​","style":"IPY_MODEL_25436cd266484802b38a80e60204315a","value":" 17995/17995 [00:19&lt;00:00, 962.17it/s]"}},"d7921a22a50842c1a6832da1eb8387f7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7a1afd71cf04164aad5d5299056a223":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d812fc08bfb6474b945676c2c3695ad1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8c3012ca0e44ea4b9f2f635d0166a4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d926fd432d4942eea5d4857b861f0359":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9593ae0a47641fe87e32ade47d9ab8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d9e64644748a4919985b0decd3d121b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aab86397feb04ff2b9b3a45fd109c820","placeholder":"​","style":"IPY_MODEL_7e90bff7cd2d48f3b86747e7fdc39d4d","value":"model-00005-of-00008.safetensors: 100%"}},"da84bc594d4e4d4c941087f770f297a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b11e715a1aa74c11a46799daf9c79236","max":125.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_4c610f54a47e46cbaa8d8d98d66da2fe","value":125.0}},"db7ad46552234d67828ec508e4025a2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3431c3baa3f549328f274cbfaeeb8232","placeholder":"​","style":"IPY_MODEL_d8c3012ca0e44ea4b9f2f635d0166a4a","value":"added_tokens.json: 100%"}},"dbe94b9abe0c47cc82efd5f5fb0df152":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddc6faa760274979ab1e3acf18ca6b9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_595957daa7384963957086c0ff8693e0","placeholder":"​","style":"IPY_MODEL_4f8e3d3b725045bea4aef9b64acf4a95","value":"special_tokens_map.json: 100%"}},"ddca534a2c854489bca3e0fb97a4a51a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df7295dc376541859a4c28369d6b6e9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d812fc08bfb6474b945676c2c3695ad1","placeholder":"​","style":"IPY_MODEL_87cac24a03a54cd1b6645464b7bfcf2b","value":" 168/168 [00:00&lt;00:00, 13.9kB/s]"}},"df98349ff0cd413a892e50ce8eb3ca65":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1530c52892946508b2a32913a6cc341":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e29cf21e8cd94232b12d19098518947f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2e41bf193da486b96de6342677e1cc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2ffa09801db458390deafdab584e921":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5801e9fbf9fb4caab44e5742d87351d4","placeholder":"​","style":"IPY_MODEL_8b7aa8875d2b40f289b44b96738d4e44","value":" 1.43k/1.43k [00:00&lt;00:00, 123kB/s]"}},"e3b65eb9e2204caaa8c5308c08d1ae5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3bd64c44caf4c41b52c9a1e9ba611b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_324b4be7e5b447fdaff01bb4b8af98c0","max":1946243936.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_48d75f06ce9e4d0687ce21affd271fc9","value":1946243936.0}},"e53a9629325d40fba6bc420f5ac50874":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5db3d1e14fc4dc69b053282d7828b13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cabfed48d9842fb9cddf9613a1d91f9","placeholder":"​","style":"IPY_MODEL_e7585183f7d544c7bc84c2e7b4e9a29f","value":" 2647/2647 [00:00&lt;00:00, 13237.20it/s]"}},"e633037fb5aa460195359be4dc504b2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e66bb753ec884a6eaa0c63e2948893e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e683729f3a584d0db8132e5837b47ec9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5f97ee5bffd4b1f86d6604f349aa211","placeholder":"​","style":"IPY_MODEL_6c58ffba735a4887bdc6d7016f39cd77","value":" 816M/816M [00:04&lt;00:00, 199MB/s]"}},"e70591f7c5a743669317e5f2e9712d65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f699cdf5c8a4278951fdc958f1610e1","IPY_MODEL_a4ce9d4a80214df8a21a3a20886a48ec","IPY_MODEL_287a136889f9428c80a56247076ec53c"],"layout":"IPY_MODEL_0c972ee5b8174023a56262da80e72d88"}},"e7585183f7d544c7bc84c2e7b4e9a29f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e87deb700ea249caaf42659f8db228c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52a651faa0374aaeb95069a20e39566f","IPY_MODEL_40f50c37aac24efab71ef2126c039e71","IPY_MODEL_4bb0584900eb4261bcf001a655f088dd"],"layout":"IPY_MODEL_f1a81cf8652e49248593401ed24d2d7f"}},"e8bf21371b654fb8b82748544b302b6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e91f1a20958b4bc99cbf0f4ea445ced2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9b7b0d3526a43c98b29c9250bb1ff55":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea34d8ec8d504bd8b64277d34e081b58":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eaedf05453f349c883684ed246b9dce8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9391890ffb24494ea795d046520a7a03","IPY_MODEL_e3bd64c44caf4c41b52c9a1e9ba611b4","IPY_MODEL_cfa7a0d0ed2e4be1afc0ac2f79346ab3"],"layout":"IPY_MODEL_9d78e807cdcc48edaa38c3e13621f7cb"}},"eaf50c82b7a1445aaf4ff24c07ea30c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eaf8043b8d0445be8147c48cd9613575":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f97c698d43b48909dd4add03a92dde3","max":1979781448.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_022a622513534f61ad636fe3f846f703","value":1979781448.0}},"ec0bcdb12d9c4455b1f3036b36a7b08a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec6ab8a309fb4ab5a0f15977740a68eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eea20cfa9ed0422e91de162d6911460b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_84afd069b8bc440c8364dcb352785acd","max":57.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_f148741a81064195b819f51b8887abf5","value":57.0}},"eeba585c2a85425d842666b3b3dfd05e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eed26568ebec4514af9b60c60fe0fd13":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eedf2fa735434adb96329b8932ed5d39":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef585135bccf4d0c90322dbd04fb7191":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77ac768d2f284da383e486027ae5a72a","placeholder":"​","style":"IPY_MODEL_3815ff465f6d4ec5b47f4a091e0f3731","value":"tokenizer.json: 100%"}},"ef5f5b76077e46bb91efe8637db04c38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59c84c6ffa5f4548ac3abeda6d837a88","placeholder":"​","style":"IPY_MODEL_f97e0f6bc8fc4816b9c733c5f8891a29","value":"tokenizer.json: 100%"}},"efc50b7bf43c46cc91ab112e386097ee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eff8831fc09a4d3b9d6f1624d7c124f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f00bd512c1b043f2a9c23f4573c0715c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6349880dbba04532af393fb0fbb5b104","placeholder":"​","style":"IPY_MODEL_ad8b0da7be2b4f2890ecf18e7bbdf2f2","value":" 22.0M/22.0M [00:00&lt;00:00, 45.8MB/s]"}},"f082c6f7af3b434fa0ba8839103faf5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ef1cdad9ce8446aa0cd61cfcbbcb15f","placeholder":"​","style":"IPY_MODEL_640b70bb25f8401abae61617fc0c8443","value":"1_Pooling/config.json: 100%"}},"f12904d8305a43649e549e188e382b66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f148741a81064195b819f51b8887abf5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f1a81cf8652e49248593401ed24d2d7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1c431a19a154953bcfb3aba87a2c96c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1e7e7dd7c7c43a48cb5ba45ec8525f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6f1a42b9bfb3435b92dfab4105409a28","IPY_MODEL_2cff17dc1dec42a59d516418a7294727","IPY_MODEL_99d108fd975c4a38b01749e1b7ee3ce4"],"layout":"IPY_MODEL_0058e3538d5f4f41bd0d1869d0ccd304"}},"f2df2d42c78d464abf0deee03d315221":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2e0b6af57844bfca1846cd6121d3d91":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4f59e44a52b4a8e8f851d020bb0d297":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f52cdb74338045d0b5e986961d654d48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f5477b93e9f447a7afcc4e18a8504a7d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f565266147c240c89ae70fe7c52af313":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f748613f377e4a23a1f4f381a84b8929":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7a97081b5cb4893ad5dd074ad69facf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6804b577279c4637938545b020350cf9","placeholder":"​","style":"IPY_MODEL_715548ef7e5e46c9859401809c721aab","value":"tokenizer.model: 100%"}},"f7ab02128b2f48bd90d23644c25b96ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_08c16952e7124c4f8d3fb4b9974837b7","max":638.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_602b39eb4c96424ea97e290903830bfb","value":638.0}},"f8f48cdd6a7b42e29fb38d4bd3362427":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cb657bc21dc4baa9ba2cb61cc8471fe","placeholder":"​","style":"IPY_MODEL_86cf5d9f747c4098935898d52b945a8d","value":" 743/743 [00:00&lt;00:00, 69.8kB/s]"}},"f8fce6b0f3be4ee0a966c457fc1bd8ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f97e0f6bc8fc4816b9c733c5f8891a29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa123bc232be4a8e918fb0086ff89524":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c957a5262c64c9b903fd31d4cd92251","max":1633.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_a31003fb138441cab058bddf0240586d","value":1633.0}},"fa3fdaa0a2c2460ca9091e3e9e617fc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb9c590ec6734faba2fa353f19238355":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc8e7c29d76242e38596d952ef4c1d88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b1b102a4afc414cb99cfb8d6334a040","placeholder":"​","style":"IPY_MODEL_8584ae62401046af901e36911f8d3826","value":" 583/583 [00:00&lt;00:00, 52.4kB/s]"}},"ff8501c4cddb42c0813be09473730bb0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23f9408b622240fda255b18798572ded","IPY_MODEL_b9a02332d8ca474e806ee2ce419cf1c1","IPY_MODEL_1f3196db74cc42a28de30298f858e814"],"layout":"IPY_MODEL_7f08593f9e5e470b83089ca8fe70d3fd"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":5}